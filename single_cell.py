from __future__ import annotations
import os
import operator
import re
import signal
import sys
import warnings
from collections.abc import Iterable
from contextlib import contextmanager
from decimal import Decimal
from datetime import date, datetime, time, timedelta
from functools import cache, reduce
from itertools import chain, islice, pairwise
from packaging import version
from pathlib import Path
from subprocess import run
from textwrap import fill
from threadpoolctl import threadpool_limits
from timeit import default_timer
from typing import Any, Callable, Dict, ItemsView, KeysView, Literal, \
    Mapping, Sequence, ValuesView, Union


class ignore_sigint:
    """
    Ignore Ctrl + C when importing certain modules, to avoid errors due to
    incomplete imports.
    """
    def __enter__(self):
        signal.signal(signal.SIGINT, signal.SIG_IGN)
    
    def __exit__(self, *_):
        signal.signal(signal.SIGINT, signal.default_int_handler)


with ignore_sigint():
    import h5py
    import numpy as np
    import polars as pl
    from scipy import sparse
    from scipy.sparse._compressed import _cs_matrix
    from scipy.special import stdtrit
    from scipy.stats import rankdata


Color = Union[str, float, np.floating,
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer]],
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer], Union[int, np.integer]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating], Union[float, np.floating]]]
Indexer = Union[int, np.integer, str, slice,
                np.ndarray[1, Union[np.dtype[np.integer], np.dtype[np.bool_]]],
                pl.Series, list[Union[int, np.integer, str, bool, np.bool_]]]
Scalar = Union[str, int, float, Decimal, date, time, datetime, timedelta, bool,
               bytes]
NestedScalarOrArrayDict = \
    Dict[str, Union[str, int, np.integer, float, np.floating, bool, np.bool_,
         np.ndarray[Any, Any], 'NestedScalarOrArrayDict']]
SingleCellColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['SingleCell'], Union[pl.Series, np.ndarray]]]
PseudobulkColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['Pseudobulk', str], Union[pl.Series, np.ndarray]]]


_num_threads = 1
_seed = 0


def options(num_threads: int | np.integer,
            seed: int | np.integer) -> dict[str, Any] | None:
    """
    Get or set the default number of threads and random seed used for
    SingleCell operations.
    
    `options()` with no arguments gets the current value of each option, while
    e.g. `options(num_threads=2)` sets a new value for the `num_threads`
    option and leaves the `seed` option unchanged.
    
    Args:
        num_threads: the new default number of threads. Set `num_threads=-1` to
                     use all available cores (as determined by
                     `os.cpu_count()`).
        seed: the new default random seed

    Returns:
        A dictionary giving the current value of each option if calling
        `options()` with no arguments, or `None` otherwise.
    """
    global _num_threads, _seed
    if num_threads is None and seed is None:
        return {'num_threads': _num_threads, 'seed': _seed}
    if num_threads is not None:
        check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
        if num_threads == -1:
            _num_threads = os.cpu_count()
        elif num_threads >= 1:
            _num_threads = int(num_threads)
        else:
            error_message = (
                f'num_threads is {num_threads}, but must be a positive '
                f'integer or -1')
            raise ValueError(error_message)
    if seed is not None:
        check_type(seed, 'seed', int, 'an integer')
        _seed = int(seed)


def array_equal(a1: np.ndarray[Any, Any], a2: np.ndarray[Any, Any]) -> bool:
    """
    Tests whether two NumPy arrays are equal. NaNs will always compare equal.
    
    Args:
        a1: the first input array
        a2: the second input array

    Returns:
        Whether the two arrays are equal.
    """
    return np.array_equal(a1, a2,
                          equal_nan=a1.dtype != object and a2.dtype != object)


def bincount(x: np.ndarray[1, np.dtype[np.uint32]],
             *,
             num_bins: int | np.integer,
             num_threads: int | np.integer) -> \
        np.ndarray[1, np.dtype[np.uint32]]:
    """
    A faster version of `numpy.bincount` for integer data that uses a fixed
    number of bins, lacks weights and supports multithreading.
    
    Args:
        x: a 1D `uint32` NumPy array
        num_bins: the number of bins
        num_threads: the number of threads to use when counting

    Returns:
        A 1D `uint32` NumPy array of length `num_bins`, containing the bin
        counts.
    """
    counts = np.empty(num_bins, dtype=np.uint32)
    cython_inline(r'''
        from cython.parallel cimport prange
        from libcpp.vector cimport vector
        
        ctypedef fused arr_type:
            unsigned
            int
            long
        
        def bincount(const arr_type[::1] arr,
                     unsigned[::1] counts,
                     const unsigned num_threads):
            cdef long start, end, dest_start, dest_end, i, num_bins, \
                chunk_size, num_elements = arr.shape[0]
            cdef unsigned thread_index
            cdef vector[vector[unsigned]] thread_counts
            
            if num_threads == 1:
                counts[:] = 0
                for i in range(num_elements):
                    counts[arr[i]] += 1
            else:
                # Store counts for each thread in a temporary buffer, then
                # aggregate at the end. As an optimization, put the counts for
                # the last thread (`thread_index == num_threads - 1`) directly
                # into the final `counts` array.
                
                thread_counts.resize(num_threads - 1)
                num_bins = counts.shape[0]
                chunk_size = num_elements / num_threads
                for thread_index in prange(num_threads, nogil=True,
                                           num_threads=num_threads,
                                           schedule='static', chunksize=1):
                    start = thread_index * chunk_size
                    if thread_index == num_threads - 1:
                        end = num_elements
                        counts[:] = 0
                        for i in range(start, end):
                            counts[arr[i]] += 1
                    else:
                        thread_counts[thread_index].resize(num_bins)
                        end = start + chunk_size
                        for i in range(start, end):
                            thread_counts[thread_index][arr[i]] += 1
                
                # Aggregate counts from all threads except the last
                
                for thread_index in range(num_threads - 1):
                    for i in range(num_bins):
                        counts[i] += thread_counts[thread_index][i]
        ''')['bincount'](arr=x, counts=counts, num_threads=num_threads)
    return counts


def bonferroni(pvalues: pl.Expr) -> pl.Expr:
    """
    Performs Bonferroni correction on a polars expression of p-values.
    
    Args:
        pvalues: a polars expression; may contain missing data

    Returns:
        A polars expression of Bonferroni-corrected p-values.
    """
    return (pvalues * (pvalues.len() - pvalues.null_count()))\
        .clip(upper_bound=1)


def check_bounds(variable: Any,
                 variable_name: str,
                 lower_bound: int | np.integer | None = None,
                 upper_bound: int | np.integer | None = None,
                 *,
                 left_open: bool = False,
                 right_open: bool = False) -> None:
    """
    Check whether `variable` is between lower bound and upper bound, inclusive.
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        lower_bound: the smallest allowed value for variable, or None to have
                     no lower bound
        upper_bound: the largest allowed value for variable, or None to have no
                     upper bound
        left_open: if True, require variable to be strictly greater than
                   lower_bound, rather than >= lower_bound; has no effect if
                   lower_bound is None
        right_open: if True, require variable to be strictly less than
                    upper_bound, rather than <= upper_bound; has no effect if
                    upper_bound is None
    """
    if lower_bound is None and upper_bound is None:
        error_message = 'lower_bound and upper_bound cannot both be None'
        raise ValueError(error_message)
    if lower_bound is not None and (variable <= lower_bound if left_open
                                    else variable < lower_bound) or \
            upper_bound is not None and (variable >= upper_bound if right_open
                                         else variable > upper_bound):
        error_message = f'{variable_name} is {variable:,}, but must be'
        if lower_bound is not None:
            error_message += f' {">" if left_open else "â‰¥"} {lower_bound:,}'
            if upper_bound is not None:
                error_message += ' and'
        if upper_bound is not None:
            error_message += f' {"<" if right_open else "â‰¤"} {upper_bound:,}'
        raise ValueError(error_message)


def check_dtype(series: pl.Series,
                series_name: str,
                expected_dtypes: pl.datatypes.classes.DataTypeClass | str |
                                 tuple[pl.datatypes.classes.DataTypeClass |
                                       str, ...]) -> None:
    """
    Check whether `series` has the expected polars dtype.
    
    Args:
        series: the polars Series to be checked
        series_name: the name of the variable, used in the error message
        expected_dtypes: the expected dtype or dtypes. Specify the string
                        `'integer'` to include all integer dtypes, and
                        `'floating-point'` to include all floating-point
                        dtypes.
    """
    base_type = series.dtype.base_type()
    if not isinstance(expected_dtypes, tuple):
        expected_dtypes = expected_dtypes,
    for expected_type in expected_dtypes:
        if base_type == expected_type or expected_type == 'integer' and \
                base_type in pl.INTEGER_DTYPES or \
                expected_type == 'floating-point' and \
                base_type in pl.FLOAT_DTYPES:
            return
    if len(expected_dtypes) == 1:
        expected_dtypes = str(expected_dtypes[0])
    elif len(expected_dtypes) == 2:
        expected_dtypes = ' or '.join(map(str, expected_dtypes))
    else:
        expected_dtypes = ', '.join(map(str, expected_dtypes[:-1])) + \
                          ', or ' + str(expected_dtypes[-1])
    error_message = (
        f'{series_name} must be {expected_dtypes}, but has data type '
        f'{base_type!r}')
    raise TypeError(error_message)


def check_R_variable_name(
        R_variable_name: str,
        variable_name: str,
        R_keywords: set[str] = {
            'if', 'else', 'repeat', 'while', 'function', 'for', 'in', 'next',
            'break', 'TRUE', 'FALSE', 'NULL', 'Inf', 'NaN', 'NA',
            'NA_integer_', 'NA_real_', 'NA_complex_', 'NA_character_',
            '...'}) -> None:
    """
    Check whether `R_variable_name` is a valid variable name in R.
    
    Args:
        R_variable_name: the R variable name to be checked
        variable_name: the name of the Python variable the R variable name
                       `R_variable_name` is stored in
        R_keywords: the set of R reserved keywords to check against
    """
    import re
    if not R_variable_name:
        error_message = f'{variable_name} is an empty string'
        raise ValueError(error_message)
    if R_variable_name[0] == '.':
        if len(R_variable_name) > 1 and R_variable_name[1].isdigit():
            error_message = (
                f'{variable_name} {R_variable_name!r} starts with a period '
                f'followed by a digit, which is not a valid R variable name')
            raise ValueError(error_message)
    elif not R_variable_name[0].isidentifier():
        error_message = (
            f'{variable_name} {R_variable_name!r} must start with a letter, '
            f'number, period or underscore')
        raise ValueError(error_message)
    if not re.fullmatch(r'[\w.]*', R_variable_name[1:]):
        invalid_characters = \
            sorted(set(re.findall(r'[^\w.]',
                                  ''.join(dict.fromkeys(R_variable_name)))))
        if len(invalid_characters) == 1:
            description = f"the character '{invalid_characters[0]}'"
        else:
            description = f"the characters " + ", ".join(
                f"'{character}'" for character in invalid_characters) + \
                f" and '{invalid_characters[-1]}'"
        error_message = (
            f'{variable_name} {R_variable_name!r} contains {description}, but '
            f'must contain only letters, numbers, periods and underscores')
        raise ValueError(error_message)
    if R_variable_name in R_keywords or (R_variable_name.startswith('..') and
                                         R_variable_name[2:].isdigit()):
        error_message = (
            f'{variable_name} {R_variable_name!r} is a reserved keyword in R, '
            f'and cannot be used as a variable name')
        raise ValueError(error_message)


def check_type(variable: Any, variable_name: str,
               expected_types: type | tuple[type, ...],
               expected_type_name: str) -> None:
    """
    Check whether `variable` has the expected type.
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        expected_types: the expected type or types (specifying int, float, or
                        bool also implicitly includes their NumPy equivalents)
        expected_type_name: the name of the expected type, used in the error
                            message (e.g. 'a polars DataFrame')
    """
    if isinstance(variable, expected_types):
        return
    if not isinstance(expected_types, tuple):
        expected_types = expected_types,
    for t in expected_types:
        if t is int:
            import numpy as np
            if isinstance(variable, np.integer):
                return
        elif t is float:
            import numpy as np
            if isinstance(variable, np.floating):
                return
        elif t is bool:
            import numpy as np
            if isinstance(variable, np.bool_):
                return
    error_message = (
        f'{variable_name} must be {expected_type_name}, but has type '
        f'{type(variable).__name__!r}')
    raise TypeError(error_message)


def check_types(variable: Iterable[Any],
                variable_name: str,
                expected_types: type | tuple[type, ...],
                expected_type_name: str):
    """
    Check whether all elements of `variable` are of the expected type(s).
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        expected_types: the expected type or types
        expected_type_name: the name of the expected type, used in the error
                            message (e.g. 'polars DataFrames')
    """
    if not isinstance(expected_types, tuple):
        expected_types = expected_types,
    for element in variable:
        if not isinstance(element, expected_types):
            for t in expected_types:
                if t is int:
                    import numpy as np
                    if isinstance(variable, np.integer):
                        break
                elif t is float:
                    import numpy as np
                    if isinstance(variable, np.floating):
                        break
                elif t is bool:
                    import numpy as np
                    if isinstance(variable, np.bool_):
                        break
            else:
                error_message = (
                    f'all elements of {variable_name} must be '
                    f'{expected_type_name}, but it contains an element of '
                    f'type {type(element).__name__!r}')
                raise TypeError(error_message)


@cache
def cython_inline(code,
                  debug=False, boundscheck=None, cdivision=True,
                  initializedcheck=None, wraparound=False,
                  warn_undeclared=False, include_dirs=None, libraries=None,
                  verbose=False, **other_cython_settings):
    """
    A drop-in replacement for cython.inline that supports cimports. It turns on
    the major Cython optimizations (boundscheck=False, cdivision=True,
    initializedcheck=False, wraparound=False), sets quiet=True for quiet
    compilation, and sets language_level=3 for full Python 3 compatibility.
    
    Args:
        code: a string of Cython code to compile
        debug: whether to turn off compiler optimizations (`-Ofast`,
               `-funroll-loops`) and turn on debug compilation (`-g`) and
               Cython's boundscheck and initializedcheck
        boundscheck: whether to perform array bounds checking when indexing;
                     always affects array/memoryview indexing, but also affects
                     list, tuple, and string indexing when wraparound=False
        cdivision: whether to use C-style rather than Python-style division and
                   remainder operations; disabling leads to a ~35% speed
                   penalty for these operations
        initializedcheck: whether to check whether memoryviews and C++ classes
                          are initialized before using them
        wraparound: whether to support Python-style negative indexing
        warn_undeclared: whether to warn about undeclared variables (i.e. those
                         without a cdef type declaration)
        include_dirs: an optional tuple of include directories of libraries to
                      link against; `np.get_include()` will always be included
        libraries: an optional tuple of libraries to link against,
                   e.g. ('hdf5',)
        verbose: if True, print Cython's compilation logs
        **other_cython_settings: other Cython settings, which will be written
                                 into the source code as #cython compiler
                                 directives
    
    Returns:
        The {function_name: function} dictionary of compiled functions that
        would be returned by cython.inline().
    """
    from hashlib import md5
    from inspect import getmembers
    from textwrap import dedent
    # ~ is read-only on Niagara compute nodes, so build in CYTHON_CACHE_DIR in
    # scratch instead
    cython_cache_dir = os.path.abspath(os.environ.get(
        'CYTHON_CACHE_DIR', os.path.expanduser('~/.cython')))
    os.makedirs(cython_cache_dir, exist_ok=True)
    # If `boundscheck` and/or `initializedcheck` are `None`, turn them on if
    # `debug` is `True`, and turn them off otherwise
    if boundscheck is None:
        boundscheck = debug
    if initializedcheck is None:
        initializedcheck = debug
    # Remove extra levels of indentation from the code string (since it's
    # usually defined inside a function, so there's at least one extra level of
    # indentation that would cause a syntax error if not removed) and remove
    # any leading newlines if present (since when users define code strings,
    # they usually use triple-quoted strings, and the code usually doesn't
    # start until the line after the three opening quotes, leading to a single
    # leading newline)
    settings = dict(language_level=3, boundscheck=boundscheck,
                    cdivision=cdivision, initializedcheck=initializedcheck,
                    wraparound=wraparound,
                    **{'warn.undeclared': warn_undeclared})
    settings.update(other_cython_settings)
    code = ''.join(f'#cython: {setting_name}={setting}\n'
                   for setting_name, setting in settings.items()) + \
                   f'#distutils: define_macros=NPY_NO_DEPRECATED_API=' \
                   f'NPY_1_7_API_VERSION\n' + dedent(code)
    # Make a short alphabetic module name by taking the code string's MD5 hash
    # and converting the hexadegimal digits to letters (0 -> a, 1 -> b, ...,
    # 9 --> j, a --> k, ..., f --> p)
    module_name = ''.join(chr(ord(c) + (49 if c <= '9' else 10))
                          for c in md5(code.encode('utf-8')).hexdigest())
    code_file = os.path.join(cython_cache_dir, f'{module_name}.pyx')
    # Try to import the module; build it if it does not exist
    sys.path.append(cython_cache_dir)
    try:
        module = __import__(module_name)
    except ModuleNotFoundError:
        # Create the code file
        with open(code_file, 'w') as f:
            # noinspection PyTypeChecker
            print(code, file=f)
        # Write a build script to a temp file based on the module name
        build_file = os.path.join(cython_cache_dir, f'{module_name}_build.py')
        with open(build_file, 'w') as f:
            import numpy as np
            if include_dirs is not None:
                include_dirs = (np.get_include(),) + include_dirs
            else:
                include_dirs = np.get_include(),
            include_dirs = \
                '[' + ', '.join(f'{include_dir!r}'
                                for include_dir in include_dirs) + ']'
            if libraries is not None:
                libraries = \
                    f'[' + ', '.join(f'{library!r}'
                                     for library in libraries) + ']'
            narval = os.environ.get('CLUSTER') == 'narval'
            niagara = os.environ.get('CLUSTER') == 'niagara'
            march = 'znver2' if narval else 'skylake' if niagara else 'native'
            # noinspection PyTypeChecker
            print(dedent(f'''
                from setuptools import Extension, setup
                from Cython.Build import cythonize
                setup(name='{module_name}', ext_modules=cythonize([
                    Extension('{module_name}', ['{code_file}'],
                              language='c++',
                              include_dirs={include_dirs},
                              libraries={libraries},
                              extra_compile_args=['-g', '-march={march}',
                                                  '-fopenmp', '-Werror',
                                                  '-Wextra',
                                                  '-Wno-maybe-uninitialized',
                                                  '-Wno-ignored-qualifiers']
                                  if {debug} else ['-Ofast', '-march={march}',
                                                   '-funroll-loops',
                                                   '-fopenmp', '-Werror',
                                                   '-Wextra',
                                                   '-Wno-maybe-uninitialized',
                                                   '-Wno-ignored-qualifiers'],
                              extra_link_args=['-fopenmp'] if {debug} else
                                              ['-Ofast', '-fopenmp'])],
                    build_dir='{cython_cache_dir}'))'''), file=f)
        # Build the code (note: `sys.executable` is the location of Python)
        run(f'cd {cython_cache_dir} && {"CFLAGS=-g " if debug else ""}'
            f'{sys.executable} {build_file} build_ext --inplace'
            f'{"" if verbose else " > /dev/null"}', shell=True)
        # Remove the temp file
        os.unlink(build_file)
        # Try again
        module = __import__(module_name)
    finally:
        sys.path = sys.path[:-1]
    # Create a dict of all the Cython functions defined in the module
    function_dict = {function_name: function
                     for function_name, function in getmembers(module)
                     if repr(function).startswith('<cyfunction')}
    # Return the dict of Cython functions
    return function_dict


def fdr(pvalues: pl.Expr) -> pl.Expr:
    """
    Performs FDR correction on a polars expression of p-values.
    
    Args:
        pvalues: a polars expression; may contain missing data

    Returns:
        A polars expression of FDR q-values.
    """
    num_null = pvalues.null_count().cast(pl.Int64)
    num_non_null = pvalues.len() - pvalues.null_count()
    reverse_order = pvalues.arg_sort(descending=True, nulls_last=True)
    return (pvalues.gather(reverse_order) /
            (pl.int_range(num_non_null, -num_null, -1) / num_non_null))\
        .cum_min()\
        .gather(reverse_order.arg_sort())


def filter_columns(df: pl.DataFrame,
                   predicates: pl.Expr,
                   *more_predicates: pl.Expr) -> pl.DataFrame:
    """
    Selects columns from a polars DataFrame where all the Boolean expressions
    in `predicates` evaluate to `True`, like `filter()` but for columns instead
    of rows. Use it in method chains, e.g.
    `df.pipe(filter_columns, pl.all().n_unique() > 1)`.
    
    Args:
        df: a polars DataFrame
        predicates: the Boolean expressions to filter on
        *more_predicates: additional Boolean expressions, specified as
                          positional arguments

    Returns:
        `df`, filtered to the columns where all the Boolean expressions in
        `predicates` evaluate to `True`.
    """
    predicates = to_tuple(predicates) + more_predicates
    boolean_expression = reduce(lambda a, b: a & b, predicates)
    return df.pipe(lambda df: df.select(df.select(boolean_expression)
                                        .unpivot()
                                        .filter(pl.col.value)
                                        ['variable']
                                    .to_list()))


def generate_palette(num_colors: int | np.integer,
                     *,
                     lightness_range: tuple[
                         int | float | np.integer | np.floating,
                         int | float | np.integer | np.floating] =
                        (100 / 3, 200 / 3),
                     chroma_range: tuple[
                         int | float | np.integer | np.floating,
                         int | float | np.integer | np.floating] = (50, 100),
                     hue_range: tuple[
                        int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
                     first_color: str = '#008cb9',
                     stride: int | np.integer = 5):
    """
    Generate a maximally perceptually distinct color palette.
    
    The first color in the palette is `first_color`. The second color is the
    color that's most perceptually distinct from `first_color`, i.e. has the
    largest distance from it in the perceptually uniform CAM02-UCS color space.
    The third color is the color that has the largest distance from either of
    the first two colors, i.e. the color that maximizes the minimum distance
    to any of the colors currently in the palette. And so on.
    
    An optimized version of github.com/taketwo/glasbey that only generates R,
    G, and B values of (0, 5, 10, ..., 255) instead of (0, 1, 2, ..., 255).
    You can change this stride (by default 5) with the `stride` paramter.
    
    Args:
        num_colors: the number of colors to include in the palette
        lightness_range: a two-element tuple with the lightness range of colors
                         to generate, or None to take the full range:
                         `(0, 100)`
        chroma_range: a two-element tuple with the chroma range of colors to
                      generate, or None to take the full range: `(0, 100)`.
                      Grays have low chroma, and vivid colors have high chroma.
        hue_range: a two-element tuple with the hue range of colors to
                   generate, or None to take the full range: `(0, 360)`. Red is
                   at 0Â°, green at 120Â°, and blue at 240Â°. Because it wraps
                   around, the first element of the tuple can be greater than
                   the second, unlike for `lightness_range` and `chroma_range`.
        first_color: the first color of the palette. Can be any valid
                     Matplotlib color, like a hex string (e.g. `'#FF0000'`), a
                     named color (e.g. 'red'), a 3- or 4-element RGB/RGBA tuple
                     of integers 0-255 or floats 0-1, or a single float 0-1 for
                     grayscale.
        stride: as an optimization, consider only RGB colors where R, G, and B
                are all multiples of this value. Must be a small divisor of
                255: 1, 3, 5, 15, or 17. Set to 1 for the best possible
                solution, at orders of magnitude more computational cost.
    
    Returns:
        A list of hex codes like `'#A06B72'`, with `first_color` as the first
        hex code.
    """
    import numpy as np
    from colorspacious import cspace_convert
    from matplotlib.colors import is_color_like, to_hex, to_rgb
    # Check ranges
    for argument, argument_name, max_value in (
            (lightness_range, 'lightness_range', 100),
            (chroma_range, 'chroma_range', 100),
            (hue_range, 'hue_range', 360)):
        if argument is not None:
            check_type(argument, argument_name, tuple, 'a two-element tuple')
            if len(argument) != 2:
                error_message = (
                    f'{argument_name} must be a two-element tuple, but has '
                    f'{len(argument):,} elements')
                raise ValueError(error_message)
            for i in range(2):
                check_type(argument[i], f'{argument_name}[i]', (int, float),
                           f'a number between 0 and {max_value}, inclusive')
            if argument[0] < 0:
                error_message = f'{argument_name}[0] must be â‰¥ 0'
                raise ValueError(error_message)
            if argument[1] > max_value:
                error_message = f'{argument_name}[1] must be â‰¤ {max_value}'
                raise ValueError(error_message)
            if argument is not hue_range and argument[0] > argument[1]:
                error_message = \
                    f'{argument_name}[0] must be â‰¤ {argument_name}[1]'
                raise ValueError(error_message)
    # Check that `first_color` is a valid Matplotlib color, and convert it to
    # RGB and then to the perceptually uniform CAM02-UCS color space
    if not is_color_like(first_color):
        error_message = 'first_color is not a valid Matplotlib color'
        raise ValueError(error_message)
    first_color = to_rgb(first_color)
    first_color = cspace_convert(first_color, 'sRGB1', 'CAM02-UCS')
    if lightness_range is not None or chroma_range is not None or \
            hue_range is not None:
        lightness, chroma, hue = \
            cspace_convert(first_color, 'CAM02-UCS', 'JCh')
        if lightness_range is not None and \
                not lightness_range[0] <= lightness <= lightness_range[1]:
            error_message = (
                f'first_color has a lightness of {lightness}, outside the '
                f'specified lightness_range of {lightness_range}')
            raise ValueError(error_message)
        if chroma_range is not None and \
                not chroma_range[0] <= chroma <= chroma_range[1]:
            error_message = (
                f'first_color has a chroma of {chroma}, outside the specified '
                f'chroma_range of {chroma_range}')
            raise ValueError(error_message)
        if hue_range is not None and not (hue_range[0] <= hue <= hue_range[1]
                                          if hue_range[0] <= hue_range[1] else
                                          hue_range[0] <= hue or
                                          hue <= hue_range[1]):
            error_message = (
                f'first_color has a hue of {hue}, outside the specified '
                f'hue_range of {hue_range}')
            raise ValueError(error_message)
    # Check `stride`
    check_type(stride, 'stride', int, 'one of the integers 1, 3, 5, 15, or 17')
    if stride not in (1, 3, 5, 15, 17):
        error_message = 'stride must be 1, 3, 5, 15, or 17'
        raise ValueError(error_message)
    # Generate a lookup table with all possible RGB colors where R, G and B are
    # multiples of 5, encoded in CAM02-UCS space. Table rows correspond to
    # individual RGB colors; columns correspond to J', a', and b' components.
    rgb = np.arange(0, 256, stride)
    colors = np.empty([len(rgb)] * 3 + [3])
    colors[..., 0] = rgb[:, None, None]
    colors[..., 1] = rgb[None, :, None]
    colors[..., 2] = rgb[None, None, :]
    colors = colors.reshape(-1, 3)
    colors = cspace_convert(colors, 'sRGB255', 'CAM02-UCS')
    # Remove colors outside the specified lightness, chroma and/or hue ranges
    if lightness_range is not None or chroma_range is not None or \
            hue_range is not None:
        jch = cspace_convert(colors, 'CAM02-UCS', 'JCh')
        mask = np.ones(len(colors), dtype=bool)
        if lightness_range is not None:
            mask &= (jch[:, 0] >= lightness_range[0]) & \
                    (jch[:, 0] <= lightness_range[1])
        if chroma_range is not None:
            mask &= (jch[:, 1] >= chroma_range[0]) & \
                    (jch[:, 1] <= chroma_range[1])
        if hue_range is not None:
            if hue_range[0] <= hue_range[1]:
                mask &= (jch[:, 2] >= hue_range[0]) & \
                        (jch[:, 2] <= hue_range[1])
            else:
                mask &= (jch[:, 2] >= hue_range[0]) | \
                        (jch[:, 2] <= hue_range[1])
        colors = colors[mask]
    # Initialize the palette to `first_color`, then iteratively add the color
    # that's farthest away from all other colors (i.e. with the maximum min
    # distance to any color already in the palette)
    palette = [first_color]
    distances = np.full(len(colors), np.inf)
    while len(palette) < num_colors:
        # Update palette-colors distances to account for the color just added
        distance_to_newest_color = \
            np.linalg.norm((colors - palette[-1]), axis=1)
        np.minimum(distances, distance_to_newest_color, distances)
        # Add the color with the new maximum distance
        palette.append(colors[distances.argmax()])
    # Convert the generated palette to sRGB1 format
    palette = cspace_convert(palette, 'CAM02-UCS', 'sRGB1')
    # Clip palette to [0, 1], in case some colors are slightly out-of-range
    palette = palette.clip(0, 1)
    # Convert RGB to hex
    # noinspection PyTypeChecker
    palette = np.apply_along_axis(to_hex, 1, palette)
    return palette


def getnnz(sparse_matrix: csr_array | csc_array,
           axis: Literal[0] | Literal[1] | None,
           num_threads: int | np.integer) -> \
        np.ndarray[1, np.dtype[np.integer]]:
    """
    Count the number of stored values in a sparse array along an axis,
    including explicitly stored zeros. Matches the behavior of the
    now-deprecated `getnnz()` function for scipy sparse arrays, but differs
    from `sparse_matrix.count_nonzero()`, which excludes explicit zeros.
    
    Args:
        sparse_matrix: a CSR or CSC sparse array or matrix
        axis: whether to count the number of stored values within each column
              (`axis=0`), or within each row (`axis=1`)
        num_threads: the number of threads to use when counting; only used for
                     CSR when `axis=0` and for CSC when `axis=1`
    
    Returns:
        The number of stored values as a 1D array.
    """
    is_csr = isinstance(sparse_matrix, csr_array)
    if axis == is_csr:
        return np.diff(sparse_matrix.indptr)
    else:
        return bincount(sparse_matrix.indices,
                        num_bins=sparse_matrix.shape[is_csr],
                        num_threads=num_threads)


def plural(string: str, count: int | np.integer) -> str:
    """
    Adds an s to the end of string, unless `count` is 1 or -1.
    
    Args:
        string: a string
        count: a count

    Returns:
        `string`, with an s at the end if `count` is 1 or -1
    """
    return string if abs(count) == 1 else f'{string}s'


def sparse_equal(a1: csr_array | csc_array, a2: csr_array | csc_array) -> bool:
    """
    Tests whether two SciPy sparse arrays or matrices OF THE SAME FORMAT (e.g.
    CSR) are equal. NaNs will always compare equal.
    
    Args:
        a1: the first input array or matrix
        a2: the second input array or matrix

    Returns:
        Whether the two arrays or matrices are equal.
    """
    return a1.nnz == a2.nnz and array_equal(a1.indptr, a2.indptr) and \
        array_equal(a1.data, a2.data) and array_equal(a1.indices, a2.indices)


def sparse_matrix_vector_multiply(
        sparse_matrix: csr_array | csc_array,
        vector: np.ndarray[1, np.dtype[np.integer | np.floating]],
        *,
        axis: Literal[0] | Literal[1],
        inplace: bool,
        num_threads: int | np.integer) -> csr_array | csc_array | None:
    """
    Elementwise multiply a sparse array/matrix and a vector, either rowwise or
    columnwise.
    
    Args:
        sparse_matrix: a CSR or CSC sparse array or matrix
        vector: a 1D numeric vector
        axis: the axis to perform the operation across: columnwise (`axis=0`)
              or rowwise (`axis=1`)
        inplace: whether to multiply in-place
        num_threads: the number of threads to use for the operation.
    
    Returns:
        A float64 sparse array/matrix with the result of the multiplication, or
        `None` if operating in-place (`return_dtype` is `None`).
    """
    data = sparse_matrix.data
    indices = sparse_matrix.indices
    indptr = sparse_matrix.indptr
    # If in-place...
    if inplace:
        if isinstance(sparse_matrix, csc_array) == axis:
            # Rowwise for CSR, or columnwise for CSC
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused numeric_2:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def multiply_inplace(numeric[::1] data,
                                     const signed_integer[::1] indices,
                                     const signed_integer[::1] indptr,
                                     const numeric_2[::1] vector,
                                     const unsigned num_threads):
                    cdef unsigned long i
                    cdef unsigned j
                    
                    if num_threads == 1:
                        for j in range(<unsigned> indptr.shape[0] - 1):
                            for i in range(<unsigned long> indptr[j],
                                           <unsigned long> indptr[j + 1]):
                                data[i] *= <numeric> vector[j]
                    else:
                        for j in prange(<unsigned> indptr.shape[0] - 1,
                                        nogil=True, num_threads=num_threads):
                            for i in range(<unsigned long> indptr[j],
                                           <unsigned long> indptr[j + 1]):
                                data[i] *= <numeric> vector[j]
                ''')['multiply_inplace'](
                    data, indices, indptr, vector, num_threads)
        else:
            # Rowwise for CSC, or columnwise for CSR
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused numeric_2:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def multiply_inplace(numeric[::1] data,
                                     const signed_integer[::1] indices,
                                     const signed_integer[::1] indptr,
                                     const numeric_2[::1] vector,
                                     const unsigned num_threads):
                    cdef unsigned long i
                    
                    if num_threads == 1:
                        for i in range(<unsigned long> data.shape[0]):
                            data[i] *= <numeric> vector[indices[i]]
                    else:
                        for i in prange(<unsigned long> data.shape[0],
                                        nogil=True, num_threads=num_threads):
                            data[i] *= <numeric> vector[indices[i]]
                ''')['multiply_inplace'](
                    data, indices, indptr, vector, num_threads)
    # If not in-place...
    else:
        # Allocate the output sparse matrix/array's data array
        result = np.empty_like(sparse_matrix.data, dtype=float)
        # Run the operation with Cython
        if isinstance(sparse_matrix, csc_array) == axis:
            # Rowwise for CSR, or columnwise for CSC
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused numeric_2:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def multiply(numeric[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const numeric_2[::1] vector,
                             double[::1] result,
                             const unsigned num_threads):
                    cdef unsigned long i
                    cdef unsigned j
                    
                    if num_threads == 1:
                        for j in range(<unsigned> indptr.shape[0] - 1):
                            for i in range(<unsigned long> indptr[j],
                                           <unsigned long> indptr[j + 1]):
                                result[i] = data[i] * vector[j]
                    else:
                        for j in prange(<unsigned> indptr.shape[0] - 1,
                                        nogil=True, num_threads=num_threads):
                            for i in range(<unsigned long> indptr[j],
                                           <unsigned long> indptr[j + 1]):
                                result[i] = data[i] * vector[j]
                ''')['multiply'](
                    data, indices, indptr, vector, result, num_threads)
        else:
            # Rowwise for CSC, or columnwise for CSR
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused numeric_2:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def multiply(const numeric[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const numeric_2[::1] vector,
                             double[::1] result,
                             const unsigned num_threads):
                    cdef unsigned long i
                    
                    if num_threads == 1:
                        for i in range(<unsigned long> data.shape[0]):
                            result[i] = data[i] * vector[indices[i]]
                    else:
                        for i in prange(<unsigned long> data.shape[0],
                                        nogil=True, num_threads=num_threads):
                            result[i] = data[i] * vector[indices[i]]
                ''')['multiply'](
                    data, indices, indptr, vector, result, num_threads)
        # Return a new sparse matrix/array with the result
        return type(sparse_matrix)((result, indices, indptr),
                                   shape=sparse_matrix.shape)


@contextmanager
def Timer(message: str | None = None, verbose: bool = True) -> None:
    """
    Use `with Timer(message):` to time the code inside the `with` block.
    
    Args:
        message: an optional message to print when starting the with `block`
                 (with `"..."` after) and ending the with block (with the time
                 after)
        verbose: if `False`, disables the Timer. This is useful to
                 conditionally run the Timer based on the value of a boolean
                 variable.
    """
    if verbose:
        if message is not None:
            print(f'{message}...')
        start = default_timer()
        aborted = False
        try:
            yield
        except Exception as e:
            aborted = True
            raise e
        finally:
            end = default_timer()
            duration = end - start
            
            days = int(duration // 86400)
            hours = int((duration % 86400) // 3600)
            minutes = int((duration % 3600) // 60)
            seconds = int(duration % 60)
            milliseconds = int((duration * 1000) % 1000)
            microseconds = int((duration * 1000000) % 1000)
            nanoseconds = int((duration * 1000000000) % 1000)
            
            time_parts = []
            if days > 0:
                time_parts.append(f'{days} {plural("day", days)}')
            if hours > 0:
                time_parts.append(f'{hours}h')
            if minutes > 0:
                time_parts.append(f'{minutes}m')
            if seconds > 0:
                time_parts.append(f'{seconds}s')
            if milliseconds > 0:
                time_parts.append(f'{milliseconds}ms')
            if microseconds > 0:
                time_parts.append(f'{microseconds}Âµs')
            if nanoseconds > 0:
                time_parts.append(f'{nanoseconds}ns')
            
            time_str = \
                ' '.join(time_parts[:2]) if time_parts else 'less than 1ns'
            
            print(f'{message if message is not None else "Command"} '
                  f'{"aborted after" if aborted else "took"} '
                  f'{time_str}')
    else:
        yield


def to_tuple(variable: Any) -> tuple[Any, ...]:
    """
    Cast Iterables (except str/bytes) to tuple, but box non-Iterables (and
    str/bytes) in a length-1 tuple.
    
    Args:
        variable: a variable

    Returns:
        `variable` as a tuple
    """
    return tuple(variable) if isinstance(variable, Iterable) and not \
        isinstance(variable, (str, bytes)) else (variable,)


def to_tuple_checked(variable: Any,
                     variable_name: str,
                     expected_types: type | tuple[type, ...],
                     expected_type_name: str) -> tuple[Any, ...]:
    """
    Like `to_tuple`, but check that `variable` or its elements are of the
    expected type(s) and that it is non-empty.
    
    Args:
        variable: the variable to be checked and expanded
        variable_name: the name of the variable, used in error messages
        expected_types: the expected type or types
        expected_type_name: the name of the expected type, used in error
                            messages (e.g. `'polars DataFrames'`)

    Returns:
        `variable` as a tuple.
    """
    if isinstance(variable, Iterable) and \
            not isinstance(variable, (str, bytes)):
        variable = tuple(variable)
        if len(variable) == 0:
            error_message = f'{variable_name} is empty'
            raise ValueError(error_message)
        check_types(variable, variable_name, expected_types,
                    expected_type_name)
    else:
        check_type(variable, variable_name, expected_types,
                   f'{expected_type_name} (or a sequence thereof)')
        variable = variable,
    return variable


# Custom sparse array classes that support parallel operations

# github.com/scipy/scipy/blob/main/scipy/sparse/sparsetools/csr.h#L296-L309,
# github.com/scipy/scipy/blob/main/scipy/sparse/sparsetools/csr.h#L325-L340 and
# github.com/scipy/scipy/blob/main/scipy/sparse/sparsetools/csr.h#L1180-L1624

cython_functions = cython_inline(r'''
    from cython.parallel cimport prange
    
    ctypedef fused I:
        int
        long
        
    ctypedef fused T:
        int
        unsigned
        long
        unsigned long
        float
        double
    
    cdef extern from "<algorithm>" namespace "std" nogil:
        ForwardIt lower_bound[ForwardIt, T](
            ForwardIt first, ForwardIt last, const T& value)
    
    cpdef bint csr_has_sorted_indices(const long n_row,
                                      const I[::1] Ap,
                                      const I[::1] Aj,
                                      const unsigned num_threads):
        cdef I i, jj
        
        if num_threads == 1:
            for i in range(n_row):
                for jj in range(Ap[i], Ap[i + 1] - 1):
                    if Aj[jj] > Aj[jj + 1]:
                        return False
        else:
            for i in prange(n_row, nogil=True, num_threads=num_threads):
                for jj in range(Ap[i], Ap[i + 1] - 1):
                    if Aj[jj] > Aj[jj + 1]:
                        return False
        return True
    
    cpdef bint csr_has_canonical_format(const long n_row,
                                        const I[::1] Ap,
                                        const I[::1] Aj,
                                        const unsigned num_threads):
        cdef I i, jj
        
        if num_threads == 1:
            for i in range(n_row):
                if Ap[i] > Ap[i + 1]:
                    return False
                for jj in range(Ap[i] + 1, Ap[i + 1]):
                    if not Aj[jj - 1] < Aj[jj]:
                        return False
        else:
            for i in prange(n_row, nogil=True, num_threads=num_threads):
                if Ap[i] > Ap[i + 1]:
                    return False
                for jj in range(Ap[i] + 1, Ap[i + 1]):
                    if not Aj[jj - 1] < Aj[jj]:
                        return False
        return True
    
    def get_csr_submatrix1(const long n_row,
                           const long n_col,
                           const I[::1] Ap,
                           const I[::1] Aj,
                           const T[::1] Ax,
                           const long ir0,
                           const long ir1,
                           const long ic0,
                           const long ic1,
                           I[::1] Bp,
                           const unsigned num_threads):
        cdef I i, row_start, row_end, jj, new_nnz, new_n_row = ir1 - ir0
        
        Bp[0] = 0
        if num_threads == 1:
            new_nnz = 0
            for i in range(new_n_row):
                row_start = Ap[ir0 + i]
                row_end = Ap[ir0 + i + 1]
                for jj in range(row_start, row_end):
                    if Aj[jj] >= ic0 and Aj[jj] < ic1:
                        new_nnz += 1
                Bp[i + 1] = new_nnz
        else:
            for i in prange(new_n_row, nogil=True, num_threads=num_threads):
                new_nnz = 0
                row_start = Ap[ir0 + i]
                row_end = Ap[ir0 + i + 1]
                for jj in range(row_start, row_end):
                    if Aj[jj] >= ic0 and Aj[jj] < ic1:
                        new_nnz = new_nnz + 1
                Bp[i + 1] = new_nnz
            for i in range(1, new_n_row):
                Bp[i + 1] += Bp[i]  # cumsum
    
    def get_csr_submatrix2(const long n_row,
                           const long n_col,
                           const I[::1] Ap,
                           const I[::1] Aj,
                           const T[::1] Ax,
                           const long ir0,
                           const long ir1,
                           const long ic0,
                           const long ic1,
                           I[::1] Bp,
                           I[::1] Bj,
                           T[::1] Bx,
                           const unsigned num_threads):
        cdef I i, row_start, row_end, jj, kk, new_n_row = ir1 - ir0
        
        if num_threads == 1:
            kk = 0
            for i in range(new_n_row):
                row_start = Ap[ir0 + i]
                row_end = Ap[ir0 + i + 1]
                for jj in range(row_start, row_end):
                    if Aj[jj] >= ic0 and Aj[jj] < ic1:
                        Bj[kk] = Aj[jj] - ic0
                        Bx[kk] = Ax[jj]
                        kk = kk + 1
        else:
            for i in prange(new_n_row, nogil=True, num_threads=num_threads):
                row_start = Ap[ir0 + i]
                row_end = Ap[ir0 + i + 1]
                kk = Bp[i]
                for jj in range(row_start, row_end):
                    if Aj[jj] >= ic0 and Aj[jj] < ic1:
                        Bj[kk] = Aj[jj] - ic0
                        Bx[kk] = Ax[jj]
                        kk = kk + 1
    
    def csr_row_index(const long n_row_idx,
                      const I[::1] rows,
                      const I[::1] Ap,
                      const I[::1] Aj,
                      const T[::1] Ax,
                      I[::1] Bp,
                      I[::1] Bj,
                      T[::1] Bx,
                      const unsigned num_threads):
        cdef I i, row, row_start, row_end, dest_row_start, dest_row_end
        
        if num_threads == 1:
            dest_row_start = 0
            for i in range(n_row_idx):
                row = rows[i]
                row_start = Ap[row]
                row_end = Ap[row + 1]
                dest_row_end = dest_row_start + row_end - row_start
                Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                dest_row_start = dest_row_end
        else:
            for i in prange(n_row_idx, nogil=True, num_threads=num_threads):
                row = rows[i]
                row_start = Ap[row]
                row_end = Ap[row + 1]
                dest_row_start = Bp[i]
                dest_row_end = Bp[i + 1]
                Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
    
    def csr_row_slice(const long start,
                      const long stop,
                      const long step,
                      const I[::1] Ap,
                      const I[::1] Aj,
                      const T[::1] Ax,
                      I[::1] Bp,
                      I[::1] Bj,
                      T[::1] Bx,
                      const unsigned num_threads):
        cdef I i, row, row_start, row_end, dest_row_start, dest_row_end, \
            num_iterations
        
        if num_threads == 1:
            dest_row_start = 0
            for i in range(start, stop, step):
                row_start = Ap[i]
                row_end = Ap[i + 1]
                dest_row_end = dest_row_start + row_end - row_start
                Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                dest_row_start = dest_row_end
        else:
            if step > 0 and start < stop:
                num_iterations = (stop - start + step - 1) // step
            elif step < 0 and start > stop:
                num_iterations = (start - stop - step - 1) // -step
            else:
                num_iterations = 0
            for i in prange(num_iterations, nogil=True,
                            num_threads=num_threads):
                row = start + i * step
                row_start = Ap[row]
                row_end = Ap[row + 1]
                dest_row_start = Bp[i]
                dest_row_end = Bp[i + 1]
                Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
        
    def csr_column_index1(const long n_idx,
                          const I[::1] col_idxs,
                          const long n_row,
                          const long n_col,
                          const I[::1] Ap,
                          const I[::1] Aj,
                          I[::1] col_offsets,
                          I[::1] Bp,
                          const unsigned num_threads):
        cdef I i, jj, j, new_nnz
        
        # bincount(col_idxs)
        
        for jj in range(n_idx):
            j = col_idxs[jj]
            col_offsets[j] += 1
    
        # Compute new indptr
        
        Bp[0] = 0
        if num_threads == 1:
            new_nnz = 0
            for i in range(n_row):
                for jj in range(Ap[i], Ap[i + 1]):
                    new_nnz += col_offsets[Aj[jj]]
                Bp[i + 1] = new_nnz
        else:
            for i in prange(n_row, nogil=True, num_threads=num_threads):
                new_nnz = 0
                for jj in range(Ap[i], Ap[i + 1]):
                    new_nnz = new_nnz + col_offsets[Aj[jj]]
                Bp[i + 1] = new_nnz
            for i in range(1, n_row):
                Bp[i + 1] += Bp[i]
        
        # cumsum in-place
        
        for j in range(1, n_col):
            col_offsets[j] += col_offsets[j - 1]
    
    def csr_column_index2(const I[::1] col_order,
                          const I[::1] col_offsets,
                          const long nnz,
                          const I[::1] Ap,
                          const I[::1] Aj,
                          const T[::1] Ax,
                          I[::1] Bp,
                          I[::1] Bj,
                          T[::1] Bx,
                          const unsigned num_threads):
        cdef I jj, j, offset, prev_offset, k, n, row
        cdef T v
        
        if num_threads == 1:
            n = 0
            for jj in range(nnz):
                j = Aj[jj]
                offset = col_offsets[j]
                prev_offset = 0 if j == 0 else col_offsets[j - 1]
                if offset != prev_offset:
                    v = Ax[jj]
                    for k in range(prev_offset, offset):
                        Bj[n] = col_order[k]
                        Bx[n] = v
                        n += 1
        else:
            for row in prange(Ap.shape[0] - 1, nogil=True,
                              num_threads=num_threads):
                n = Bp[row]
                for jj in range(Ap[row], Ap[row + 1]):
                    j = Aj[jj]
                    offset = col_offsets[j]
                    prev_offset = 0 if j == 0 else col_offsets[j - 1]
                    if offset != prev_offset:
                        v = Ax[jj]
                        for k in range(prev_offset, offset):
                            Bj[n] = col_order[k]
                            Bx[n] = v
                            n = n + 1
    
    def csr_sample_values(const long n_row,
                          const long n_col,
                          const I[::1] Ap,
                          const I[::1] Aj,
                          const T[::1] Ax,
                          const long n_samples,
                          const I[::1] Bi,
                          const I[::1] Bj,
                          T[::1] Bx,
                          const unsigned num_threads):
        cdef I nnz = Ap[n_row]
        cdef I threshold = nnz / 10  # constant is arbitrary
        cdef I n, i, j, row_start, row_end, offset, jj
        cdef T x
    
        if n_samples > threshold and csr_has_canonical_format(n_row, Ap, Aj,
                                                              num_threads):
            if num_threads == 1:
                for n in range(n_samples):
                    i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                    j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
        
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
        
                    if row_start < row_end:
                        offset = lower_bound(&Aj[0] + row_start,
                                             &Aj[0] + row_end, j) - &Aj[0]
                        if offset < row_end and Aj[offset] == j:
                            Bx[n] = Ax[offset]
                        else:
                            Bx[n] = 0
                    else:
                        Bx[n] = 0
            else:
                for n in prange(n_samples, nogil=True,
                                num_threads=num_threads):
                    i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                    j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
        
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
        
                    if row_start < row_end:
                        offset = lower_bound(&Aj[0] + row_start,
                                             &Aj[0] + row_end, j) - &Aj[0]
                        if offset < row_end and Aj[offset] == j:
                            Bx[n] = Ax[offset]
                        else:
                            Bx[n] = 0
                    else:
                        Bx[n] = 0
        else:
            if num_threads == 1:
                for n in range(n_samples):
                    i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                    j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
        
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
        
                    x = 0
                    for jj in range(row_start, row_end):
                        if Aj[jj] == j:
                            x += Ax[jj]
                    Bx[n] = x
            else:
                for n in prange(n_samples, nogil=True,
                                num_threads=num_threads):
                    i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                    j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
        
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
        
                    x = 0
                    for jj in range(row_start, row_end):
                        if Aj[jj] == j:
                            x = x + Ax[jj]
                    Bx[n] = x
''', warn_undeclared=False)
get_csr_submatrix1 = cython_functions['get_csr_submatrix1']
get_csr_submatrix2 = cython_functions['get_csr_submatrix2']
csr_row_index = cython_functions['csr_row_index']
csr_row_slice = cython_functions['csr_row_slice']
csr_column_index1 = cython_functions['csr_column_index1']
csr_column_index2 = cython_functions['csr_column_index2']
csr_sample_values = cython_functions['csr_sample_values']
csr_has_canonical_format = cython_functions['csr_has_canonical_format']
csr_has_sorted_indices = cython_functions['csr_has_sorted_indices']


def get_csr_submatrix(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1,
                      num_threads):
    # Allocate indptr
    Bp = np.empty(ir1 - ir0 + 1, dtype=Ap.dtype)
    # Count nonzeros and populate indptr
    get_csr_submatrix1(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1, Bp,
                       num_threads)
    # Allocate indices and data
    new_nnz = Bp[-1]
    Bj = np.empty(new_nnz, dtype=Aj.dtype)
    Bx = np.empty(new_nnz, dtype=Ax.dtype)
    # Populate indices and data
    get_csr_submatrix2(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1,
                       Bp, Bj, Bx, num_threads)
    return Bp, Bj, Bx
    

def isintlike(x) -> bool:
    """Is x appropriate as an index into a sparse matrix? Returns True
    if it can be cast safely to a machine int.
    """
    # Fast-path check to eliminate non-scalar values. operator.index would
    # catch this case too, but the exception catching is slow.
    if np.ndim(x) != 0:
        return False
    try:
        operator.index(x)
    except (TypeError, ValueError):
        try:
            loose_int = bool(int(x) == x)
        except (TypeError, ValueError):
            return False
        if loose_int:
            error_message = \
                'inexact indices into sparse matrices are not allowed'
            raise ValueError(error_message)
        return loose_int
    return True


def _process_slice(sl, num):
    if sl is None:
        i0, i1 = 0, num
    elif isinstance(sl, slice):
        i0, i1, stride = sl.indices(num)
        if stride != 1:
            error_message = 'slicing with step != 1 not supported'
            raise ValueError(error_message)
        i0 = min(i0, i1)  # give an empty slice when i0 > i1
    elif isintlike(sl):
        if sl < 0:
            sl += num
        i0, i1 = sl, sl + 1
        if i0 < 0 or i1 > num:
            error_message = f'index out of bounds: 0 <= {i0} < {i1} <= {num}'
            raise IndexError(error_message)
    else:
        error_message = 'expected slice or scalar'
        raise TypeError(error_message)

    return i0, i1


class cs_matrix(_cs_matrix):
    _num_threads = None
    
    @property
    def num_threads(self) -> int:
        """
        Get the number of threads used for indexing.
        
        Returns:
            The number of threads used for indexing.
        """
        return self._num_threads
     
    @num_threads.setter
    def num_threads(self, num_threads: int | np.integer | None) -> None:
        """
        Set the number of threads used for indexing.
        
        Args:
            num_threads: the new number of threads to use for indexing. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or set to None (the
                         default) to use `SingleCell.num_threads` cores.
        """
        if num_threads is not None:
            check_type(num_threads, 'num_threads', int,
                       'a positive integer, -1 or None')
            if num_threads == -1:
                num_threads = os.cpu_count()
            else:
                check_bounds(num_threads, 'num_threads', 1)
                num_threads = int(num_threads)
        self._num_threads = num_threads
    
    def _current_num_threads(self) -> int:
        """
        Get the current number of threads used for indexing: this is just
        `self.num_threads` unless `self.num_threads` is None, in which case
        default to `_num_threads`.
        
        Returns:
            The current number of threads used for indexing.
        """
        if self._num_threads is None:
            return _num_threads
        else:
            return self._num_threads
    
    # github.com/scipy/scipy/blob/main/scipy/sparse/_compressed.py#L705-L885
    # (the actual `__getitem__` logic is at
    #  github.com/scipy/scipy/blob/main/scipy/sparse/_index.py#L29-L109)
    
    def _get_intXint(self, row, col):
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self.shape)
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data,
            major, major + 1, minor, minor + 1, self._current_num_threads())
        return data.sum(dtype=self.dtype)

    def _get_sliceXslice(self, row, col):
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        if major.step in (1, None) and minor.step in (1, None):
            return self._get_submatrix(major, minor, copy=True)
        return self._major_slice(major)._minor_slice(minor)

    def _get_arrayXarray(self, row, col):
        # inner indexing
        idx_dtype = self.indices.dtype
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self.shape)
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        major = np.asarray(major, dtype=idx_dtype)
        minor = np.asarray(minor, dtype=idx_dtype)

        val = np.empty(major.size, dtype=self.dtype)
        csr_sample_values(M, N, self.indptr, self.indices, self.data,
                          major.size, major.ravel(), minor.ravel(), val,
                          self._current_num_threads())
        if major.ndim == 1:
            return self._ascontainer(val)
        result = self.__class__(val.reshape(major.shape))
        result._num_threads = self._num_threads
        return result

    def _get_columnXarray(self, row, col):
        # outer indexing
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        return self._major_index_fancy(major)._minor_index_fancy(minor)

    def _major_index_fancy(self, idx):
        """Index along the major axis where idx is an array of ints.
        """
        idx_dtype = self._get_index_dtype((self.indptr, self.indices))
        indices = np.asarray(idx, dtype=idx_dtype).ravel()

        # noinspection PyUnresolvedReferences
        N = self._swap(self._shape_as_2d)[1]
        M = len(indices)
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, N)) if self.ndim == 2 else (M,)
        if M == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            return result

        row_nnz = (self.indptr[indices + 1] - self.indptr[indices]).astype(idx_dtype)
        res_indptr = np.zeros(M + 1, dtype=idx_dtype)
        np.cumsum(row_nnz, out=res_indptr[1:])

        nnz = res_indptr[-1]
        res_indices = np.empty(nnz, dtype=idx_dtype)
        res_data = np.empty(nnz, dtype=self.dtype)
        csr_row_index(
            M,
            indices,
            self.indptr.astype(idx_dtype, copy=False),
            self.indices.astype(idx_dtype, copy=False),
            self.data,
            res_indptr,
            res_indices,
            res_data,
            self._current_num_threads()
        )

        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        return result

    def _major_slice(self, idx, copy=False):
        """Index along the major axis where idx is a slice object.
        """
        if idx == slice(None):
            return self.copy() if copy else self

        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        start, stop, step = idx.indices(M)
        M = len(range(start, stop, step))
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, N)) if self.ndim == 2 else (M,)
        if M == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            return result

        # Work out what slices are needed for `row_nnz`
        # start,stop can be -1, only if step is negative
        start0, stop0 = start, stop
        if stop == -1 and start >= 0:
            stop0 = None
        start1, stop1 = start + 1, stop + 1

        row_nnz = self.indptr[start1:stop1:step] - \
            self.indptr[start0:stop0:step]
        idx_dtype = self.indices.dtype
        res_indptr = np.zeros(M+1, dtype=idx_dtype)
        np.cumsum(row_nnz, out=res_indptr[1:])

        if step == 1:
            all_idx = slice(self.indptr[start], self.indptr[stop])
            res_indices = np.array(self.indices[all_idx], copy=copy)
            res_data = np.array(self.data[all_idx], copy=copy)
        else:
            nnz = res_indptr[-1]
            res_indices = np.empty(nnz, dtype=idx_dtype)
            res_data = np.empty(nnz, dtype=self.dtype)
            csr_row_slice(start, stop, step, self.indptr, self.indices,
                          self.data, res_indptr, res_indices, res_data,
                          self._current_num_threads())

        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        return result

    def _minor_index_fancy(self, idx):
        """Index along the minor axis where idx is an array of ints.
        """
        idx_dtype = self._get_index_dtype((self.indices, self.indptr))
        indices = self.indices.astype(idx_dtype, copy=False)
        indptr = self.indptr.astype(idx_dtype, copy=False)

        idx = np.asarray(idx, dtype=idx_dtype).ravel()
        
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        k = len(idx)
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, k)) if self.ndim == 2 else (k,)
        if k == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            return result

        # pass 1: count idx entries and compute new indptr
        col_offsets = np.zeros(N, dtype=idx_dtype)
        res_indptr = np.empty_like(self.indptr, dtype=idx_dtype)
        csr_column_index1(
            k,
            idx,
            M,
            N,
            indptr,
            indices,
            col_offsets,
            res_indptr,
            self._current_num_threads()
        )

        # pass 2: copy indices/data for selected idxs
        col_order = np.argsort(idx).astype(idx_dtype, copy=False)
        nnz = res_indptr[-1]
        res_indices = np.empty(nnz, dtype=idx_dtype)
        res_data = np.empty(nnz, dtype=self.dtype)
        csr_column_index2(col_order, col_offsets, len(self.indices),
                          indptr, indices, self.data, res_indptr, res_indices,
                          res_data, self._current_num_threads())
        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        return result

    def _minor_slice(self, idx, copy=False):
        """Index along the minor axis where idx is a slice object.
        """
        if idx == slice(None):
            return self.copy() if copy else self
        
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        start, stop, step = idx.indices(N)
        N = len(range(start, stop, step))
        if N == 0:
            # noinspection PyUnresolvedReferences
            result = self.__class__(self._swap((M, N)), dtype=self.dtype)
            result._num_threads = self._num_threads
            return result
        if step == 1:
            return self._get_submatrix(minor=idx, copy=copy)
        return self._minor_index_fancy(np.arange(start, stop, step))

    def _get_submatrix(self, major=None, minor=None, copy=False):
        """Return a submatrix of this matrix.

        major, minor: None, int, or slice with step 1
        """
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        i0, i1 = _process_slice(major, M)
        j0, j1 = _process_slice(minor, N)

        if i0 == 0 and j0 == 0 and i1 == M and j1 == N:
            return self.copy() if copy else self

        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, i0, i1, j0, j1,
            self._current_num_threads())

        # noinspection PyUnresolvedReferences
        shape = self._swap((i1 - i0, j1 - j0))
        if self.ndim == 1:
            shape = (shape[1],)
        result = self.__class__((data, indices, indptr), shape=shape,
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        return result
    
    # github.com/scipy/scipy/blob/main/scipy/sparse/_compressed.py#L1187-L1214
    @property
    def has_canonical_format(self) -> bool:
        """Whether the array/matrix has sorted indices and no duplicates

        Returns
            - True: if the above applies
            - False: otherwise

        has_canonical_format implies has_sorted_indices, so if the latter flag
        is False, so will the former be; if the former is found True, the
        latter flag is also set.
        """
        # first check to see if result was cached
        if not getattr(self, '_has_sorted_indices', True):
            # not sorted => not canonical
            # noinspection PyAttributeOutsideInit
            self._has_canonical_format = False
        elif not hasattr(self, '_has_canonical_format'):
            # noinspection PyAttributeOutsideInit
            self._has_canonical_format = bool(
                csr_has_canonical_format(
                    len(self.indptr) - 1, self.indptr, self.indices,
                    self._current_num_threads()))
            if self._has_canonical_format:
                # noinspection PyAttributeOutsideInit
                self._has_sorted_indices = True
        return self._has_canonical_format

    @has_canonical_format.setter
    def has_canonical_format(self, val: bool):
        # noinspection PyAttributeOutsideInit
        self._has_canonical_format = bool(val)
        if val:
            # noinspection PyAttributeOutsideInit
            self.has_sorted_indices = True
            
    # github.com/scipy/scipy/blob/main/scipy/sparse/_compressed.py#L1231-L1249
    
    @property
    def has_sorted_indices(self) -> bool:
        """Whether the indices are sorted

        Returns
            - True: if the indices of the array/matrix are in sorted order
            - False: otherwise
        """
        # first check to see if result was cached
        if not hasattr(self, '_has_sorted_indices'):
            # noinspection PyAttributeOutsideInit
            self._has_sorted_indices = bool(
                csr_has_sorted_indices(
                    len(self.indptr) - 1, self.indptr, self.indices,
                    self._current_num_threads()))
        return self._has_sorted_indices

    @has_sorted_indices.setter
    def has_sorted_indices(self, val: bool):
        # noinspection PyAttributeOutsideInit
        self._has_sorted_indices = bool(val)


class csr_array(cs_matrix, sparse.csr_array):
    def tocsc(self, copy=False):
        result = csc_array(super().tocsc(copy=copy))
        result._num_threads = self._num_threads
        return result
    
    # github.com/scipy/scipy/blob/main/scipy/sparse/_csr.py#L156-L280
    
    def _getrow(self, i):
        """Returns a copy of row i of the matrix, as a (1 x n)
        CSR matrix (row vector).
        """
        if self.ndim == 1:
            if i not in (0, -1):
                error_message = f'index ({i}) out of range'
                raise IndexError(error_message)
            return self.reshape((1, self.shape[0]), copy=True)

        M, N = self.shape
        i = int(i)
        if i < 0:
            i += M
        if i < 0 or i >= M:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, i, i + 1, 0, N,
            self._current_num_threads())
        result = self.__class__((data, indices, indptr), shape=(1, N),
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        return result

    def _getcol(self, i):
        """Returns a copy of column i. A (m x 1) sparse array (column vector).
        """
        if self.ndim == 1:
            error_message = \
                'getcol not provided for 1d arrays. Use indexing A[j]'
            raise ValueError(error_message)
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += N
        if i < 0 or i >= N:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, 0, M, i, i + 1,
            self._current_num_threads())
        result = self.__class__((data, indices, indptr), shape=(M, 1),
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        return result

    def _get_int(self, idx):
        spot = np.flatnonzero(self.indices == idx)
        if spot.size:
            return self.data[spot[0]]
        return self.data.dtype.type(0)

    def _get_slice(self, idx):
        if idx == slice(None):
            return self.copy()
        if idx.step in (1, None):
            ret = self._get_submatrix(0, idx, copy=True)
            return ret.reshape(ret.shape[-1])
        return self._minor_slice(idx)

    def _get_array(self, idx):
        idx_dtype = self._get_index_dtype(self.indices)
        idx = np.asarray(idx, dtype=idx_dtype)
        if idx.size == 0:
            result = self.__class__([], dtype=self.dtype)
            result._num_threads = self._num_threads
            return result

        M, N = 1, self.shape[0]
        row = np.zeros_like(idx, dtype=idx_dtype)
        col = np.asarray(idx, dtype=idx_dtype)
        val = np.empty(row.size, dtype=self.dtype)
        csr_sample_values(M, N, self.indptr, self.indices, self.data,
                          row.size, row, col, val, self._current_num_threads())

        new_shape = col.shape if col.shape[0] > 1 else (col.shape[0],)
        result = self.__class__(val.reshape(new_shape))
        result._num_threads = self._num_threads
        return result

    def _get_intXarray(self, row, col):
        return self._getrow(row)._minor_index_fancy(col)

    def _get_intXslice(self, row, col):
        if col.step in (1, None):
            return self._get_submatrix(row, col, copy=True)
        
        M, N = self.shape
        start, stop, stride = col.indices(N)

        ii, jj = self.indptr[row:row+2]
        row_indices = self.indices[ii:jj]
        row_data = self.data[ii:jj]

        if stride > 0:
            ind = (row_indices >= start) & (row_indices < stop)
        else:
            ind = (row_indices <= start) & (row_indices > stop)

        if abs(stride) > 1:
            ind &= (row_indices - start) % stride == 0

        row_indices = (row_indices[ind] - start) // stride
        row_data = row_data[ind]
        row_indptr = np.array([0, len(row_indices)])

        if stride < 0:
            row_data = row_data[::-1]
            row_indices = abs(row_indices[::-1])

        shape = (1, max(0, int(np.ceil(float(stop - start) / stride))))
        result = self.__class__((row_data, row_indices, row_indptr),
                                shape=shape, dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        return result

    def _get_sliceXint(self, row, col):
        if row.step in (1, None):
            return self._get_submatrix(row, col, copy=True)
        return self._major_slice(row)._get_submatrix(minor=col)

    def _get_sliceXarray(self, row, col):
        return self._major_slice(row)._minor_index_fancy(col)

    def _get_arrayXint(self, row, col):
        return self._major_index_fancy(row)._get_submatrix(minor=col)

    def _get_arrayXslice(self, row, col):
        if col.step not in (1, None):
            col = np.arange(*col.indices(self.shape[1]))
            return self._get_arrayXarray(row, col)
        return self._major_index_fancy(row)._get_submatrix(minor=col)


class csc_array(cs_matrix, sparse.csc_array):
    def tocsr(self, copy=False):
        result = csr_array(super().tocsr(copy=copy))
        result._num_threads = self._num_threads
        return result
    
    # github.com/scipy/scipy/blob/main/scipy/sparse/_csc.py#L94-L138
    
    def _getrow(self, i):
        """Returns a copy of row i of the matrix, as a (1 x n)
        CSR matrix (row vector).
        """
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += M
        if i < 0 or i >= M:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        return self._get_submatrix(minor=i).tocsr()

    def _getcol(self, i):
        """Returns a copy of column i of the matrix, as a (m x 1)
        CSC matrix (column vector).
        """
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += N
        if i < 0 or i >= N:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        return self._get_submatrix(major=i, copy=True)

    def _get_intXarray(self, row, col):
        return self._major_index_fancy(col)._get_submatrix(minor=row)

    def _get_intXslice(self, row, col):
        if col.step in (1, None):
            return self._get_submatrix(major=col, minor=row, copy=True)
        return self._major_slice(col)._get_submatrix(minor=row)

    def _get_sliceXint(self, row, col):
        if row.step in (1, None):
            return self._get_submatrix(major=col, minor=row, copy=True)
        return self._get_submatrix(major=col)._minor_slice(row)

    def _get_sliceXarray(self, row, col):
        return self._major_index_fancy(col)._minor_slice(row)

    def _get_arrayXint(self, row, col):
        return self._get_submatrix(major=col)._minor_index_fancy(row)

    def _get_arrayXslice(self, row, col):
        return self._major_slice(col)._minor_index_fancy(row)


class SingleCell:
    """
    A lightweight alternative to AnnData for representing single-cell data.
    
    Has slots for:
    - `X`: a scipy sparse array of counts per cell and gene
    - `obs`: a polars DataFrame of cell metadata
    - `var`: a polars DataFrame of gene metadata
    - `obsm`: a dictionary of NumPy arrays and polars DataFrames of cell
      metadata
    - `varm`: a dictionary of NumPy arrays and polars DataFrames of gene
      metadata
    - `uns`: a dictionary of scalars (strings, numbers or Booleans) or NumPy
      arrays, or nested dictionaries thereof
    as well as `obs_names` and `var_names`, aliases for `obs[:, 0]` and
    `var[:, 0]`.
    
    Why is `X` a sparse array rather than matrix? Aside from being more modern
    and consistent with np.array, it's also faster, as explained at
    github.com/scipy/scipy/blob/2aee5efcbe3720f41fe55f336f492ae0acbecdee/scipy/
    sparse/_base.py#L1333-L1337.
    """
    # noinspection PyUnresolvedReferences
    def __init__(self,
                 source: str | Path | 'AnnData' | None = None,
                 *,
                 X: sparse.csr_array | sparse.csc_array | sparse.csr_matrix | 
                    sparse.csc_matrix | Literal[False] | None = None,
                 obs: pl.DataFrame | None = None,
                 var: pl.DataFrame | None = None,
                 obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       Literal[False] | None = None,
                 varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       Literal[False] | None = None,
                 obsp: dict[str, sparse.csr_array | sparse.csc_array | 
                                 sparse.csr_matrix | sparse.csc_matrix] | 
                       Literal[False] | None = None,
                 varp: dict[str, sparse.csr_array | sparse.csc_array | 
                                 sparse.csr_matrix | sparse.csc_matrix] | 
                       Literal[False] | None = None,
                 uns: NestedScalarOrArrayDict | Literal[False] | None = None,
                 X_key: str | None = None,
                 assay: str | None = None,
                 obs_columns: str | Iterable[str] = None,
                 var_columns: str | Iterable[str] = None,
                 num_threads: int | np.integer | None = None) -> None:
        """
        Load a SingleCell dataset from a file, or create one from an in-memory
        AnnData object or count matrix + metadata.
        
        SingleCell supports reading and writing files from each of the three
        major single-cell ecosystems:
        - scverse/Scanpy AnnData (`.h5ad`)
        - Seurat (`.rds` and `.h5Seurat`)
        - Bioconductor SingleCellExperiment (`.rds`)
        as well as raw 10x data files (`.h5` or `.mtx`).
        
        By default, when an AnnData object, `.h5ad` file, `.h5Seurat` file, or
        `.rds` file contains both raw and normalized counts, only the raw
        counts will be loaded. To load normalized counts instead, use the `X`
        argument (for AnnData objects) or `X_key` argument (for files).
        
        Reading and writing `.rds` files requires the ryp Python-R bridge.
        To create a SingleCell dataset from an in-memory Seurat or
        SingleCellExperiment object in the ryp R workspace, use
        `SingleCell.from_seurat()` or `SingleCell.from_sce()`.
        
        Reading and writing loom files is not supported because SingleCell only
        supports sparse count matrices, and loom only supports dense matrices.
        Using loom files for SingleCell data is not recommended due to this
        wastefulness. If you must, load them with
        `SingleCell(scanpy.read_loom(loom_filename))`; `read_loom()` implicitly
        converts the counts to a sparse matrix by default.
        
        Args:
            source: a filename or AnnData object, or `None` if specifying `X`,
                    `obs`, and `var` instead. Supported file formats are
                    scverse/Scanpy AnnData (`.h5ad`), Seurat (`.rds` and
                    `.h5Seurat`), Bioconductor SingleCellExperiment (`.rds`),
                    and raw 10x data files (`.h5` or `.mtx`). If `source` is a
                    10x `.mtx.gz` filename, `barcodes.tsv.gz` and
                    `features.tsv.gz` are assumed to be in the same directory,
                    unless custom paths to these files are specified via the
                    `obs` and/or `var` arguments.
            X: If `source` is `None`, the data as a sparse array or matrix
               (with rows = cells, columns = genes).
               If `source` is an AnnData object, an optional sparse array or
               matrix to use as `X`. By default, `X` will be loaded from
               `source.layers['UMIs']` or `source.raw.X` if present and
               `source.X` otherwise.
               If `X` is `None` when `source` is `None`, or `False` when
               `source` is a filename, do not store any data in `X` and set it
               to `None`. This helps save memory, but the resulting dataset
               cannot be saved, converted to another format, or used to run
               analyses that require `X`.
            obs: a polars DataFrame of metadata for each cell (row of `X`), or
                 `None` if specifying `source` instead. Or, if `source` is a
                 10x `.mtx.gz` filename, an optional filename for cell-level
                 metadata, which is otherwise assumed to be at
                 `barcodes.tsv.gz` in the same directory as the `.mtx.gz` file.
            var: a polars DataFrame of metadata for each gene (column of `X`),
                 or `None` if specifying `source` instead. Or, if `source` is a
                 10x `.mtx.gz` filename, an optional filename for gene-level
                 metadata, which is otherwise assumed to be at
                 `features.tsv.gz` in the same directory as the `.mtx.gz` file.
            obsm: an optional dictionary mapping string names to NumPy arrays
                  and polars DataFrames of metadata for each cell, or `False`
                  to skip loading `obsm` when reading `.h5ad` and `.h5Seurat`
                  files
            varm: an optional dictionary mapping string names to NumPy arrays
                  and polars DataFrames of metadata for each gene, or `False`
                  to skip loading `varm` when reading `.h5ad` files
            obsp: an optional dictionary mapping string names to sparse arrays
                  or matrices containing pairwise cell-cell information like
                  nearest-neighbors graphs, or `False` to skip loading `obsp`
                  when reading `.h5ad` and `.h5Seurat` files
            varp: an optional dictionary mapping string names to sparse arrays
                  or matrices containing pairwise gene-gene information, or
                  `False` to skip loading `varp` when reading `.h5ad` files
            uns: an optional dictionary mapping string names to unstructured
                 metadata - scalars (strings, numbers or Booleans), NumPy
                 arrays, or nested dictionaries thereof - or `False` to skip
                 loading `uns` when reading `.h5ad` or `.h5Seurat` files
            X_key: if `source` is an AnnData `.h5ad`, Seurat `.rds` or
                   `.h5Seurat` filename, or SingleCellExperiment `.rds`
                   filename, the location within `source` to use as `X`:
                   - If `source` is an `.h5ad` filename, the name of the key in
                     the `.h5ad` file to use as `X`. If `None`, defaults to
                     `'layers/UMIs'` (i.e. `self.layers['UMIs']` in Scanpy) or
                     `'raw/X'` (i.e. `self.raw.X` in Scanpy) if present,
                     otherwise `'X'`.
                     Tip: `SingleCell.ls(h5ad_file)` shows the structure of an
                     `.h5ad` file without loading it, allowing you to figure
                     out which key to use as `X`.
                   - If `source` is a Seurat `.rds` or `.h5Seurat` filename,
                     the slot within the active assay (or the assay specified
                     by the `assay` argument, if not `None`) to use as `X`. Set
                     to `'data'` to load the normalized counts, or
                     `'scale.data'` to load the normalized and scaled counts,
                     if available. If `None`, defaults to `'counts'`.
                   - If `source` is a SingleCellExperiment `.rds` filename, the
                     element within `@assays@data` to use as `X`. Set to
                     `'logcounts'` to load the normalized counts, if available.
                     If `None`, defaults to `'counts'`.
            assay: if `source` is a Seurat `.rds` or `.h5Seurat` filename, the
                   name of the assay within the Seurat object to load data
                   from. Defaults to the Seurat object's `active.assay`
                   attribute (usually `'RNA'`).
            obs_columns: if `source` is an `.h5ad` or `.h5Seurat` filename, the
                         columns of `obs` to load. If not specified, load all
                         columns. Specifying only a subset of columns can speed
                         up reading. Not supported for `.h5` files, since they
                         only have a single `obs` column (`'barcodes'`), nor
                         for Seurat and SingleCellExperiment `.rds` files,
                         since `.rds` files do not support partial loading.
            var_columns: if `source` is an `.h5ad`, `.h5`, or `.h5Seurat`
                         filename, the columns of `var` to load. If not
                         specified, load all columns. Specifying only a subset
                         of columns can speed up reading. Not supported for
                         Seurat and SingleCellExperiment `.rds` files, since
                         the `.rds` file format does not support partial
                         loading.
            num_threads: the number of threads to use when reading `.h5ad` and
                         `.h5` files. Set `num_threads=-1` to use all available
                         cores (as determined by `os.cpu_count()`), or leave
                         unset to use `single_cell.options()['num_threads']`
                         cores (1 by default).
        
        Note:
            Both ordered and unordered categorical columns of `obs` and `var`
            will be loaded as polars Enums rather than polars Categoricals.
            This is because polars Categoricals use a shared numerical encoding
            across columns, so their codes are not `[0, 1, 2, ...]` like pandas
            categoricals and polars Enums are. Using Categoricals leads to a
            large overhead (~25%) when loading `obs` from an `.h5ad` file, for
            example.
        
        Note:
            SingleCell does not support dense matrices, which are highly
            memory-inefficient for single-cell data. Passing a NumPy array as
            the `X` argument will give an error; if for some reason your data
            has been improperly stored as a dense matrix, convert it to a
            sparse array first with `csr_array(numpy_array)`). However, when
            loading from disk or converting from other formats, dense matrices
            will be automatically converted to sparse matrices, to avoid giving
            an error when loading or converting.
        """
        if source is not None:
            if str(type(source)).startswith("<class 'anndata"):
                # AnnData object
                with ignore_sigint():
                    from anndata import AnnData
                check_type(source, 'source', AnnData,
                           'a filename, Path, or Anndata object')
                for prop, prop_name in (
                        (obs, 'obs'), (var, 'var'), (obsm, 'obsm'),
                        (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                        (uns, 'uns'), (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns'),
                        (num_threads, 'num_threads')):
                    if prop is not None:
                        error_message = (
                            f'when initializing a SingleCell dataset from an '
                            f'AnnData object, {prop_name} must be None')
                        raise ValueError(error_message)
                # Get `X`
                if X is False:
                    self._X = None
                else:
                    if X is None:
                        has_layers_UMIs = 'UMIs' in source._layers
                        has_raw_X = hasattr(source._raw, '_X')
                        if has_layers_UMIs and has_raw_X:
                            error_message = (
                                "both layers['UMIs'] and raw.X are present in "
                                "this AnnData object; this should never "
                                "happen in well-formed AnnData objects")
                            raise ValueError(error_message)
                        X = source._layers['UMIs'] if has_layers_UMIs else \
                            source._raw._X if has_raw_X else source._X
                        if not isinstance(X, (
                                sparse.csr_array, sparse.csc_array, 
                                sparse.csr_matrix, sparse.csc_matrix)):
                            error_message = (
                                f'to initialize a SingleCell dataset from an '
                                f'AnnData object, its X must be a csr_array, '
                                f'csc_array, csr_matrix, or csc_matrix, but '
                                f'it has type {type(X).__name__!r}. Either '
                                f'convert X to a csr_array or csc_array, or '
                                f'specify a sparse array or matrix to use as '
                                f'X via the X argument')
                            raise TypeError(error_message)
                    else:
                        check_type(X, 'X', (
                                sparse.csr_array, sparse.csc_array, 
                                sparse.csr_matrix, sparse.csc_matrix),
                                   'a csr_array, csc_array, csr_matrix, or '
                                   'csc_matrix')
                    if isinstance(X, (csr_array, csc_array)):
                        pass
                    elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
                        X = csr_array(X)
                    else:
                        X = csc_array(X)
                    self._X = X
                # Get `obs` cnd `vcr`
                for attr in '_obs', '_var':
                    df = getattr(source, attr)
                    if df.index.name is None:
                        # mace the index name consistent with what you'd get if
                        # you had loaded the same AnnData object directly from
                        # anc`.h5adc file
                        df = df.rename_axis('_index')
                    # Convert Categoricals with string categories to polars
                    # Enums, and Categoricals with other types of categories
                    # (e.g. integers) to non-categorical columns of the
                    # corresponding polars dtype (e.g. pl.Int64) since polars
                    # only supports string categories
                    schema_overrides = {}
                    cast_dict = {}
                    for column, dtype in \
                            df.dtypes[df.dtypes == 'category'].items():
                        categories_dtype = dtype.categories.dtype
                        if categories_dtype == object:
                            schema_overrides[column] = \
                                pl.Enum(dtype.categories)
                        else:
                            cast_dict[column] = categories_dtype
                    setattr(self, attr, pl.from_pandas(
                        df.astype(cast_dict),
                        schema_overrides=schema_overrides, include_index=True))
                # Get the other fields
                self._obsm: dict[str, np.ndarray] = dict(source._obsm)
                self._varm: dict[str, np.ndarray] = dict(source._varm)
                self._obsp: dict[str, csr_array | csc_array] = \
                    dict(source._obsp)
                self._varp: dict[str, csr_array | csc_array] = \
                    dict(source._varp)
                self._uns = dict(source._uns)
            else:
                # Filename
                check_type(source, 'source', (str, Path),
                           'a filename, Path, or AnnData object')
                source = os.path.expanduser(source)
                if source.endswith('.h5ad'):
                    if not os.path.exists(source):
                        error_message = f'.h5ad file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in \
                            (obs, 'obs'), (var, 'var'), (assay, 'assay'):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5ad file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    for prop, prop_name in (
                            (obsm, 'obsm'), (varm, 'varm'), (obsp, 'obsp'),
                            (varp, 'varp'), (uns, 'uns')):
                        if prop is not None and prop is not False:
                            error_message = (
                                f'when loading an .h5ad file, {prop_name} '
                                f'must be None or False')
                            raise ValueError(error_message)
                    if obs_columns is not None:
                        obs_columns = to_tuple_checked(
                            obs_columns, 'obs_columns', str, 'strings')
                    if var_columns is not None:
                        var_columns = to_tuple_checked(
                            var_columns, 'var_columns', str, 'strings')
                    num_threads = SingleCell._process_num_threads(num_threads)
                    # For the AnnData on-disk format specification, see
                    # anndata.readthedocs.io/en/latest/fileformat-prose.html
                    with h5py.File(source) as h5ad_file:
                        # Load `obs` and `var`
                        self._obs = SingleCell._read_h5ad_dataframe(
                            h5ad_file, 'obs', columns=obs_columns,
                            num_threads=num_threads)
                        self._var = SingleCell._read_h5ad_dataframe(
                            h5ad_file, 'var', columns=var_columns,
                            num_threads=num_threads)
                        # Load `obsm`
                        if obsm is None and 'obsm' in h5ad_file:
                            obsm = h5ad_file['obsm']
                            self._obsm = {
                                key: value[:]
                                     if isinstance(value, h5py.Dataset) else
                                     SingleCell._read_h5ad_dataframe(
                                         h5ad_file, f'obsm/{key}',
                                         num_threads=num_threads)
                                for key, value in obsm.items()}
                        else:
                            self._obsm = {}
                        # Load `varm`
                        if varm is None and 'varm' in h5ad_file:
                            varm = h5ad_file['varm']
                            self._varm = {
                                key: value[:]
                                     if isinstance(value, h5py.Dataset) else
                                     SingleCell._read_h5ad_dataframe(
                                         h5ad_file, f'varm/{key}',
                                         num_threads=num_threads)
                                for key, value in varm.items()}
                        else:
                            self._varm = {}
                        # Load `obsp`
                        if obsp is None and 'obsp' in h5ad_file:
                            obsp = h5ad_file['obsp']
                            self._obsp = {
                                key: (csr_array
                                      if value.attrs['encoding-type'] ==
                                         'csr_matrix' else csc_array)(
                                    (value['data'][:], value['indices'][:],
                                     value['indptr'][:]),
                                    shape=value.attrs['shape'])
                                for key, value in obsp.items()}
                        else:
                            self._obsp = {}
                        # Load `varp`
                        if varp is None and 'varp' in h5ad_file:
                            varp = h5ad_file['varp']
                            self._varp = {
                                key: (csr_array
                                      if value.attrs['encoding-type'] ==
                                         'csr_matrix' else csc_array)(
                                    (value['data'][:], value['indices'][:],
                                     value['indptr'][:]),
                                    shape=value.attrs['shape'])
                                for key, value in varp.items()}
                        else:
                            self._varp = {}
                        # Load `uns`
                        if uns is None and 'uns' in h5ad_file:
                            self._uns = SingleCell._read_uns(h5ad_file['uns'])
                        else:
                            self._uns = {}
                        # Load `X`
                        if X is False:
                            if X_key is not None:
                                error_message = (
                                    'when loading an .h5ad file with X=False, '
                                    'X_key must be None')
                                raise ValueError(error_message)
                            self._X = None
                        else:
                            if X_key is None:
                                has_layers_UMIs = 'layers/UMIs' in h5ad_file
                                has_raw_X = 'raw/X' in h5ad_file
                                if has_layers_UMIs and has_raw_X:
                                    error_message = (
                                        "both layers['UMIs'] and raw.X are "
                                        "present; this should never happen in "
                                        "well-formed .h5ad files")
                                    raise ValueError(error_message)
                                X_key = 'layers/UMIs' if has_layers_UMIs else \
                                    'raw/X' if has_raw_X else 'X'
                            else:
                                check_type(X_key, 'X_key', str, 'a string')
                                if X_key not in h5ad_file:
                                    error_message = (
                                        f'X_key {X_key!r} is not present in '
                                        f'the .h5ad file')
                                    raise ValueError(error_message)
                            X = h5ad_file[X_key]
                            matrix_class = X.attrs['encoding-type'] \
                                if 'encoding-type' in X.attrs else \
                                X.attrs['h5sparse_format'] + '_matrix'
                            if matrix_class == 'csr_matrix':
                                array_class = csr_array
                            elif matrix_class == 'csc_matrix':
                                array_class = csc_array
                            else:
                                error_message = (
                                    f"X has unsupported encoding-type "
                                    f"{matrix_class!r}, but should be "
                                    f"'csr_matrix' or 'csc_matrix'")
                                raise ValueError(error_message)
                            self._X = array_class((
                                SingleCell._read_dataset(
                                    X['data'], num_threads),
                                SingleCell._read_dataset(
                                    X['indices'], num_threads),
                                SingleCell._read_dataset(
                                    X['indptr'], num_threads)),
                                shape=X.attrs['shape'] if 'shape' in X.attrs
                                      else X.attrs['h5sparse_shape'])
                elif source.endswith('.h5Seurat'):
                    if not os.path.exists(source):
                        error_message = \
                            f'.h5Seurat file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (obs, 'obs'), (var, 'var'), (varm, 'varm'),
                            (varp, 'varp')):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5Seurat file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    for prop, prop_name in \
                            (obsm, 'obsm'), (obsp, 'obsp'), (uns, 'uns'):
                        if prop is not None and prop is not False:
                            error_message = (
                                f'when loading an .h5Seurat file, {prop_name} '
                                f'must be None or False')
                            raise ValueError(error_message)
                    if obs_columns is not None:
                        obs_columns = to_tuple_checked(
                            obs_columns, 'obs_columns', str, 'strings')
                    if var_columns is not None:
                        var_columns = to_tuple_checked(
                            var_columns, 'var_columns', str, 'strings')
                    num_threads = SingleCell._process_num_threads(num_threads)
                    with h5py.File(source) as h5Seurat_file:
                        # Check that `assay` is an assay in the `.h5Seurat`
                        # file, or set it to `active.assay` if `None`
                        if assay is None:
                            assay = h5Seurat_file.attrs['active.assay'].item()
                        elif assay not in h5Seurat_file['assays']:
                            error_message = (
                                f'assay {assay!r} does not exist in the '
                                f'.h5Seurat file; specify a different assay '
                                f'than {assay!r}')
                            raise ValueError(error_message)
                        # Load `obs`
                        self._obs = SingleCell._read_h5Seurat_dataframe(
                            h5Seurat_file['meta.data'], columns=obs_columns,
                            num_threads=num_threads)
                        # Load `var`
                        assay_group = h5Seurat_file['assays'][assay]
                        self._var = SingleCell._read_h5Seurat_dataframe(
                            assay_group['meta.features'], columns=var_columns,
                            num_threads=num_threads) \
                            if 'meta.features' in assay_group else \
                            pl.Series('feature.names',
                                      assay_group['features'][:])\
                                .cast(pl.String)\
                                .to_frame()
                        # Load `obsm`
                        self._obsm = {
                            key: value['cell.embeddings'][:].T
                            for key, value in
                                h5Seurat_file['reductions'].items()
                            if value.attrs['active.assay'] == assay}
                        # Load `obsp`
                        self._obsp = {
                            key: csr_array((
                                SingleCell._read_dataset(
                                    value['data'], num_threads),
                                SingleCell._read_dataset(
                                    value['indices'], num_threads),
                                SingleCell._read_dataset(
                                    value['indptr'], num_threads)),
                                shape=value.attrs['dims'][::-1])
                            for key, value in h5Seurat_file['graphs'].items()
                            if value.attrs['assay.used'] == assay}
                        # Load `uns`
                        self._uns = SingleCell._read_h5Seurat_uns(
                            h5Seurat_file['misc'])
                        # Load `X`
                        if X is False:
                            if X_key is not None:
                                error_message = (
                                    'when loading an .h5Seurat file with '
                                    'X=False, X_key must be None')
                                raise ValueError(error_message)
                            self._X = None
                        else:
                            if X_key is None:
                                X_key = 'counts'
                                if X_key not in assay_group:
                                    error_message = (
                                        f"the 'counts' key is not present in "
                                        f"the .h5Seurat file as part of assay "
                                        f"{assay!r}; specify a different "
                                        f"assay than {assay!r} or specify "
                                        f"X_key as something other than "
                                        f"'counts'")
                                    raise ValueError(error_message)
                            else:
                                check_type(X_key, 'X_key', str,
                                           'a string or False')
                                if X_key not in assay_group:
                                    error_message = (
                                        f'X_key {X_key!r} is not present in '
                                        f'the .h5Seurat file as part of assay '
                                        f'{assay!r}; specify a different '
                                        f'assay than {assay!r} or a different '
                                        f'X_key than {X_key!r}')
                                    raise ValueError(error_message)
                            X = assay_group[X_key]
                            self._X = csr_array((
                                SingleCell._read_dataset(
                                    X['data'], num_threads),
                                SingleCell._read_dataset(
                                    X['indices'], num_threads),
                                SingleCell._read_dataset(
                                    X['indptr'], num_threads)),
                                shape=X.attrs['dims'][::-1])
                        self._varm = {}
                        self._varp = {}
                elif source.endswith('.h5'):
                    if not os.path.exists(source):
                        error_message = f'10x .h5 file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (obs, 'obs'), (var, 'var'), (obsm, 'obsm'),
                            (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                            (uns, 'uns'), (X_key, 'X_key'), (assay, 'assay'),
                            (obs_columns, 'obs_columns')):
                        if prop is not None:
                            error_message = (
                                f'when loading a 10x .h5 file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    num_threads = SingleCell._process_num_threads(num_threads)
                    with h5py.File(source) as h5_file:
                        matrix = h5_file['matrix']
                        features = matrix['features']
                        # Load `obs` and `var`
                        self._obs = pl.Series('barcodes',
                                              matrix['barcodes'][:])\
                            .cast(pl.String)\
                            .to_frame()
                        if var_columns is not None:
                            var_columns = to_tuple_checked(
                                var_columns, 'var_columns', str, 'strings')
                            for column in var_columns:
                                if column not in features:
                                    error_message = (
                                        f'var_columns contains the column '
                                        f'{column!r}, which is not present in '
                                        f'the .h5 file')
                                    raise ValueError(error_message)
                        else:
                            var_columns = \
                                ['name', 'id', 'feature_type', 'genome'] + \
                                [column for column in
                                 ('pattern', 'read', 'sequence')
                                 if column in features]
                        self._var = pl.DataFrame([
                            pl.Series(column, features[column][:])
                            .cast(pl.String) for column in var_columns])
                        # Load `X`
                        if X is None:
                            self._X = csr_array((
                                SingleCell._read_dataset(
                                    matrix['data'], num_threads),
                                SingleCell._read_dataset(
                                    matrix['indices'], num_threads),
                                SingleCell._read_dataset(
                                    matrix['indptr'], num_threads)),
                                shape=matrix['shape'][:][::-1])
                        elif X is False:
                            self._X = None
                        else:
                            error_message = (
                                'when loading a 10x .h5 file, X must be None '
                                'or False')
                            raise ValueError(error_message)
                        self._obsm = {}
                        self._varm = {}
                        self._obsp = {}
                        self._varp = {}
                        self._uns = {}
                elif source.endswith('.mtx.gz'):
                    if not os.path.exists(source):
                        error_message = f'10x file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name, prop_description in (
                            (obs, 'obs', 'a barcodes.tsv.gz file of '
                                         'cell-level metadata'),
                            (var, 'var', 'a features.tsv.gz file of '
                                         'gene-level metadata')):
                        if prop is not None and not \
                                isinstance(prop, (str, Path)):
                            error_message = (
                                f'when loading a 10x .mtx.gz file, '
                                f'{prop_name} must be None or the path to '
                                f'{prop_description}')
                            raise TypeError(error_message)
                    for prop, prop_name in (
                            (obsm, 'obsm'), (varm, 'varm'), (obsp, 'obsp'),
                            (varp, 'varp'), (uns, 'uns'), (X_key, 'X_key'),
                            (assay, 'assay'), (obs_columns, 'obs_columns'),
                            (var_columns, 'var_columns'),
                            (num_threads, 'num_threads')):
                        if prop is not None:
                            error_message = (
                                f'when loading a 10 .mtx.gz file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    from scipy.io import mmread
                    # Load `obs` and `var`
                    self._obs = pl.read_csv(
                        f'{os.path.dirname(source)}/barcodes.tsv.gz'
                        if obs is None else obs,
                        has_header=False, new_columns=['cell'])
                    self._var = pl.read_csv(
                        f'{os.path.dirname(source)}/features.tsv.gz'
                        if var is None else var,
                        has_header=False, new_columns=['gene'])
                    # Load `X`
                    if X is None:
                        self._X = csr_array(mmread(source).T.tocsr())
                    elif X is False:
                        self._X = None
                    else:
                        error_message = (
                            'when loading a 10x .mtx.gz file, X must be None '
                            'or False')
                        raise ValueError(error_message)
                    self._obsm = {}
                    self._varm = {}
                    self._obsp = {}
                    self._varp = {}
                    self._uns = {}
                elif source.endswith('.rds'):
                    if not os.path.exists(source):
                        error_message = f'.rds file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (X, 'X'), (obs, 'obs'), (var, 'var'),
                            (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                            (uns, 'uns'), (obs_columns, 'obs_columns'),
                            (var_columns, 'var_columns'),
                            (num_threads, 'num_threads')):
                        if prop is not None:
                            error_message = (
                                f'when loading a .rds file, {prop_name} must '
                                f'be None')
                            raise ValueError(error_message)
                    from ryp import r, to_py, to_r
                    r(f'.SingleCell.object = readRDS({source!r})')
                    try:
                        if X_key is None:
                            X_key = 'counts'
                        else:
                            check_type(X_key, 'X_key', str, 'a string')
                        classes = to_py('class(.SingleCell.object)',
                                        squeeze=False)
                        if len(classes) == 1:
                            if classes[0] == 'Seurat':
                                r('suppressPackageStartupMessages('
                                  'library(SeuratObject))')
                                # noinspection PyTypeChecker
                                self._X, self._obs, self._var, self._obsm, \
                                    self._obsp, self._uns = \
                                    SingleCell._from_seurat(
                                        '.SingleCell.object', assay=assay,
                                        slot=X_key, slot_name='X_key')
                                self._varm = {}
                                self._varp = {}
                            elif classes[0] == 'SingleCellExperiment':
                                if assay is not None:
                                    error_message = (
                                        f'when loading a SingleCellExperiment '
                                        f'.rds file, assay must be None')
                                    raise ValueError(error_message)
                                r('suppressPackageStartupMessages('
                                  'library(SingleCellExperiment))')
                                # noinspection PyTypeChecker
                                self._X, self._obs, self._var, self._obsm, \
                                    self._uns = SingleCell._from_sce(
                                        '.SingleCell.object', slot=X_key,
                                        slot_name='X_key')
                                self._obsp = {}
                                self._varm = {}
                                self._varp = {}
                            else:
                                error_message = (
                                    f'the R object loaded from {source} must '
                                    f'be a Seurat or SingleCellExperiment '
                                    f'object, but has class {classes[0]!r}')
                                raise TypeError(error_message)
                        elif len(classes) == 0:
                            error_message = (
                                f'the R object loaded from {source} must be a '
                                f'Seurat or SingleCellExperiment object, but '
                                f'has no classes')
                            raise TypeError(error_message)
                        else:
                            classes_string = \
                                ', '.join(f'{c!r}' for c in classes[:-1])
                            error_message = (
                                f'the R object loaded from {source} must be a '
                                f'Seurat object, but has classes '
                                f'{classes_string} and {classes[-1]!r}')
                            raise TypeError(error_message)
                    finally:
                        r('rm(.SingleCell.object)')
                else:
                    error_message = (
                        f'source is a filename with unsupported extension '
                        f'{".".join(source.split(".")[1:])}; it must be .h5ad '
                        f'(AnnData), .h5 or .mtx.gz (10x), .rds (Seurat or '
                        f'SingleCellExperiment), or .h5Seurat (Seurat)')
                    raise ValueError(error_message)
        else:
            # Sparse array or matrix
            if X is not None:
                check_type(X, 'X', (
                    sparse.csr_array, sparse.csc_array, sparse.csr_matrix,
                    sparse.csc_matrix),
                           'a csr_array, csc_array, csr_matrix, or csc_matrix')
                for prop, prop_name in (
                        (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns'),
                        (num_threads, 'num_threads')):
                    if prop is not None:
                        error_message = (
                            f'when X is a sparse array or matrix, {prop_name} '
                            f'must be None')
                        raise ValueError(error_message)
            else:
                for prop, prop_name in (
                        (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns'),
                        (num_threads, 'num_threads')):
                    if prop is not None:
                        error_message = (
                            f'when X and source are both None, {prop_name} '
                            f'must be None')
                        raise ValueError(error_message)
            check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
            check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
            if obsm is None:
                obsm = {}
            else:
                check_type(obsm, 'obsm', dict, 'a dictionary')
                obsm = obsm.copy()
            if varm is None:
                varm = {}
            else:
                check_type(varm, 'varm', dict, 'a dictionary')
                varm = varm.copy()
            if obsp is None:
                obsp = {}
            else:
                check_type(obsp, 'obsp', dict, 'a dictionary')
                obsp = obsp.copy()
            if varp is None:
                varp = {}
            else:
                check_type(varp, 'varp', dict, 'a dictionary')
                varp = varp.copy()
            if uns is None:
                uns = {}
            else:
                check_type(uns, 'uns', dict, 'a dictionary')
                valid_uns_types = str, int, np.integer, float, np.floating, \
                    bool, np.bool_, np.ndarray
                # noinspection PyTypeChecker
                for description, value in SingleCell._iter_uns(uns):
                    if not isinstance(value, valid_uns_types):
                        error_message = (
                            f'all values of uns must be scalars (strings, '
                            f'numbers or Booleans) or NumPy arrays, or nested '
                            f'dictionaries thereof, but {description} has '
                            f'type {type(value).__name__!r}')
                        raise TypeError(error_message)
                # noinspection PyTypeChecker
                uns = SingleCell._copy_uns(uns)
            for field, field_name in (obsm, 'obsm'), (varm, 'varm'):
                for key, value in field.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {field_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
                    if isinstance(value, np.ndarray):
                        if value.ndim != 2:
                            error_message = (
                                f'all values of {field_name} must be 2D NumPy '
                                f'arrays or polars DataFrames, but '
                                f'{field_name}[{key!r}] is a {value.ndim:,}D '
                                f'NumPy array')
                            raise ValueError(error_message)
                    elif not isinstance(value, pl.DataFrame):
                        error_message = (
                            f'all values of {field_name} must be NumPy '
                            f'arrays or polars DataFrames, but {field_name}'
                            f'[{key!r}] has type {type(value).__name__!r}')
                        raise TypeError(error_message)
            for field, field_name in (obsp, 'obsp'), (varp, 'varp'):
                new = {}
                for key, value in field.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {field_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
                    if isinstance(value, (csr_array, csc_array)):
                        pass
                    elif isinstance(value,
                                    (sparse.csr_array, sparse.csr_matrix)):
                        value = csr_array(value)
                    elif isinstance(value,
                                    (sparse.csc_array, sparse.csc_matrix)):
                        value = csc_array(value)
                    else:
                        error_message = (
                            f'every value of {field_name} must be a '
                            f'csr_array, csc_array, csr_matrix, or '
                            f'csc_matrix, but {field_name}[{key!r}] has type '
                            f'{type(value).__name__!r}')
                        raise TypeError(error_message)
                    new[key] = value
                setattr(self, f'_{field_name}', new)
            if isinstance(X, (csr_array, csc_array)) or \
                    X is None:
                pass
            elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
                X = csr_array(X)
            else:
                X = csc_array(X)
            self._X = X
            self._obs = obs
            self._var = var
            self._obsm = obsm
            self._varm = varm
            self._uns = uns
        # Sanity-check shapes and dtypes
        num_cells = self._obs.shape[0]
        num_genes = self._var.shape[0]
        if num_cells == 0:
            error_message = 'len(obs) is 0: no cells remain'
            raise ValueError(error_message)
        if num_genes == 0:
            error_message = 'len(var) is 0: no genes remain'
            raise ValueError(error_message)
        if num_cells > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) cells, which is '
                'not currently supported')
            raise ValueError(error_message)
        if num_genes > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) genes, which is '
                'not currently supported')
            raise ValueError(error_message)
        if self._X is not None:
            dtype = self._X.dtype
            if dtype != np.int32 and dtype != np.int64 and \
                    dtype != np.float32 and dtype != np.float64 and \
                    dtype != np.uint32 and dtype != np.uint64:
                error_message = (
                    f'X must be (u)int32/64 or float32/64, but has data type '
                    f'{str(dtype)}')
                raise TypeError(error_message)
            if len(self._X.shape) == 1:
                error_message = 'X is 1D, but must be 2D'
                raise ValueError(error_message)
            if self._X.shape[0] != num_cells:
                error_message = (
                    f'len(obs) is {num_cells:,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
            if self._X.shape[1] != num_genes:
                error_message = (
                    f'len(var) is {num_genes:,}, but X.shape[1] is '
                    f'{self._X.shape[1]:,}')
                raise ValueError(error_message)
        for key, value in self._obsm.items():
            if len(value) != num_cells:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_cells:,}')
                raise ValueError(error_message)
        for key, value in self._varm.items():
            if len(value) != num_genes:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_genes:,}')
                raise ValueError(error_message)
        for key, value in self._obsp.items():
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'obsp[{key!r}].shape[{dim}] is {value.shape[dim]:,}, '
                        f'but X.shape[0] is {num_cells:,}')
                    raise ValueError(error_message)
        for key, value in self._varp.items():
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'varp[{key!r}].shape[{dim}] is {value.shape[dim]:,}, '
                        f'but X.shape[1] is {num_genes:,}')
                    raise ValueError(error_message)
        # Set `uns['normalized']` and `uns['QCed']` to `False` if not set yet;
        # if already set but not a Boolean, back it up to
        # `uns['_normalized']`/`uns['_QCed']`
        for key in 'normalized', 'QCed':
            if key in self._uns:
                if not isinstance(self._uns[key], bool):
                    new_key = f'_{key}'
                    while new_key in self._uns:
                        new_key = f'_{new_key}'
                    warning_message = (
                        f'uns[{key!r}] already exists and is not Boolean; '
                        f'moving it to uns[{new_key!r}]')
                    warnings.warn(warning_message)
                    self._uns[new_key] = self._uns[key]
                    self._uns[key] = False
            else:
                self._uns[key] = False
    
    @property
    def X(self) -> csr_array | csc_array | None:
        return self._X
    
    @X.setter
    def X(self, X: sparse.csr_array | sparse.csc_array | sparse.csr_matrix | 
                   sparse.csc_matrix) -> None:
        if isinstance(X, (csr_array, csc_array)):
            pass
        elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
            X = csr_array(X)
        elif isinstance(X, (sparse.csc_array, sparse.csc_matrix)):
            X = csc_array(X)
        elif X is None:
            error_message = (
                'attempting to set X to None; if you want to remove X to save '
                'memory, use drop_X() instead')
            raise ValueError(error_message)
        else:
            error_message = (
                f'new X must be a csr_array, csc_array, csr_matrix, or '
                f'csc_matrix, but has type {type(X).__name__!r}')
            raise TypeError(error_message)
        if X.shape != self._X.shape:
            error_message = (
                f'new X is {X.shape[0]:,} Ã— {X.shape[1]:,}, but old X is '
                f'{self._X.shape[0]:,} Ã— {self._X.shape[1]:,}')
            raise ValueError(error_message)
        dtype = self._X.dtype
        if dtype != np.int32 and dtype != np.int64 and \
                dtype != np.float32 and dtype != np.float64 and \
                dtype != np.uint32 and dtype != np.uint64:
            error_message = (
                f'new X must be (u)int32/64 or float32/64, but has data type '
                f'{str(dtype)}')
            raise TypeError(error_message)
        self._X = X
    
    @property
    def obs(self) -> pl.DataFrame:
        return self._obs
    
    @obs.setter
    def obs(self, obs: pl.DataFrame) -> None:
        check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
        if len(obs) != len(self._obs):
            error_message = (
                f'new obs has length {len(obs):,}, but old obs has length '
                f'{len(self._obs):,}')
            raise ValueError(error_message)
        self._obs = obs

    @property
    def var(self) -> pl.DataFrame:
        return self._var
    
    @var.setter
    def var(self, var: pl.DataFrame) -> None:
        check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
        if len(var) != len(self._var):
            error_message = (
                f'new var has length {len(var):,}, but old var has length '
                f'{len(self._var):,}')
            raise ValueError(error_message)
        self._var = var
    
    @property
    def obsm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._obsm
    
    @obsm.setter
    def obsm(self, obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_cells = self._X.shape[0]
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but new obsm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(obsm) != num_cells:
                error_message = (
                    f'the length of new obsm[{key!r}] is {len(value):,}, but '
                    f'X.shape[0] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._obsm = obsm.copy()
    
    @property
    def varm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._varm
    
    @varm.setter
    def varm(self, varm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_genes = self._X.shape[1]
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but new varm '
                    f'contains a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(varm) != num_genes:
                error_message = (
                    f'the length of new varm[{key!r}] is {len(value):,}, but '
                    f'X.shape[1] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._varm = varm.copy()
    
    @property
    def obsp(self) -> dict[str, csr_array | csc_array]:
        return self._obsp
    
    @obsp.setter
    def obsp(self,
             obsp: dict[str, sparse.csr_array | sparse.csc_array | 
                             sparse.csr_matrix | sparse.csc_matrix]) -> None:
        num_cells = self._X.shape[0]
        new_obsp = {}
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)            
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of obsp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new obsp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'new obsp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[0] is '
                        f'{num_cells:,}')
                    raise ValueError(error_message)
            new_obsp[key] = value
        self._obsp = new_obsp
    
    @property
    def varp(self) -> dict[str, csr_array | csc_array]:
        return self._varp
    
    @varp.setter
    def varp(self,
             varp: dict[str, sparse.csr_array | sparse.csc_array | 
                             sparse.csr_matrix | sparse.csc_matrix]) -> None:
        num_genes = self._X.shape[1]
        new_varp = {}
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)            
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of varp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new varp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[1] is '
                        f'{num_genes:,}')
                    raise ValueError(error_message)
            new_varp[key] = value
        self._varp = new_varp
    
    @property
    def uns(self) -> NestedScalarOrArrayDict:
        return self._uns
    
    @uns.setter
    def uns(self, uns: NestedScalarOrArrayDict) -> None:
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        self._uns = SingleCell._copy_uns(uns)
    
    @staticmethod
    def _iter_uns(uns: NestedScalarOrArrayDict, *, prefix: str = 'uns') -> \
            Iterable[tuple[str, str | int | float | np.integer | np.floating |
                                bool | np.bool_ | np.ndarray[Any, Any]]]:
        """
        Recurse through `uns`, yielding tuples of a string describing each key
        (e.g. `"uns['a']['b']"`) and the corresponding value.
        
        Args:
            uns: an `uns` dictionary
            prefix: the prefix to prepend to each key; applied recursively

        Yields:
            Length-2 tuples where the first element is a string describing each
            key, and the second element is the corresponding value.
        """
        for key, value in uns.items():
            key = f'{prefix}[{key!r}]'
            if isinstance(value, dict):
                SingleCell._iter_uns(value, prefix=key)
            else:
                yield key, value
    
    @staticmethod
    def _copy_uns(uns: NestedScalarOrArrayDict, *, deep: bool = False) -> \
            NestedScalarOrArrayDict:
        """
        Make a copy of `uns`.
        
        Args:
            uns: an `uns` dictionary
            deep: whether to make a deep or shallow copy of `uns`; if
                  `deep=True`, copy the underlying NumPy arrays if any are
                  present

        Returns:
            A copy of `uns`.
        """
        copied_uns = {}
        if deep:
            for key, value in uns.items():
                if isinstance(value, dict):
                    copied_uns[key] = SingleCell._copy_uns(value, deep=deep)
                elif isinstance(value, np.ndarray):
                    copied_uns[key] = value.copy()
                else:
                    copied_uns[key] = value
        else:
            for key, value in uns.items():
                if isinstance(value, dict):
                    copied_uns[key] = SingleCell._copy_uns(value, deep=deep)
                else:
                    copied_uns[key] = value
        return copied_uns
    
    @staticmethod
    def _read_uns(uns_group: h5py.Group) -> NestedScalarOrArrayDict:
        """
        Recursively load `uns` from an `.h5ad` file.
        
        Args:
            uns_group: `uns` as an `h5py.Group`

        Returns:
            The loaded `uns`.
        """
        return {key: SingleCell._read_uns(value)
                     if isinstance(value, h5py.Group) else
                     (pl.Series(value[:]).cast(pl.String).to_numpy()
                      if value.shape else value[()].decode('utf-8'))
                     if value.dtype == object else
                     (value[:] if value.shape else value[()].item())
                for key, value in uns_group.items()}
    
    @staticmethod
    def _save_uns(uns: NestedScalarOrArrayDict,
                  uns_group: h5py.Group,
                  h5ad_file: h5py.File) -> None:
        """
        Recursively save `uns` to an `.h5ad` file.
        
        Args:
            uns: an `uns` dictionary
            uns_group: `uns` as an `h5py.Group`
            h5ad_file: an `h5py.File` open in write mode
        """
        uns_group.attrs['encoding-type'] = 'dict'
        uns_group.attrs['encoding-version'] = '0.1.0'
        for key, value in uns.items():
            if isinstance(value, dict):
                SingleCell._save_uns(value, uns_group.create_group(key),
                                     h5ad_file)
            else:
                dataset = uns_group.create_dataset(key, data=value)
                dataset.attrs['encoding-type'] = \
                    ('string-array' if value.dtype == object else 'array') \
                    if isinstance(value, np.ndarray) else \
                    'string' if isinstance(value, str) else 'numeric-scalar'
                dataset.attrs['encoding-version'] = '0.2.0'
    
    @staticmethod
    def _read_h5Seurat_uns(uns_group: h5py.Group) -> NestedScalarOrArrayDict:
        """
        Recursively load `uns` (i.e. `misc`) from an `.h5Seurat` file.
        
        Args:
            uns_group: `uns` as an `h5py.Group`

        Returns:
            The loaded `uns`.
        """
        return {key: SingleCell._read_uns(value)
                     if isinstance(value, h5py.Group) else
                     (pl.Series(value[:]).cast(pl.String).to_numpy()
                      if len(value) > 1 else value[:].item().decode('utf-8'))
                     if value.dtype == object else
                     (value[:] if len(value) > 1 else value[:].item())
                for key, value in uns_group.items()}
    
    @staticmethod
    def _save_h5Seurat_uns(uns: NestedScalarOrArrayDict,
                           misc_group: h5py.Group,
                           h5Seurat_file: h5py.File) -> None:
        """
        Recursively save `uns` (i.e. `misc`) to an `.h5Seurat` file. Only
        string values will be saved.
        
        Args:
            uns: an `uns` dictionary
            misc_group: `uns` as an `h5py.Group`
            h5Seurat_file: an `h5py.File` open in write mode
        """
        for key, value in uns.items():
            if isinstance(value, dict):
                SingleCell._save_h5Seurat_uns(
                    value, misc_group.create_group(key), h5Seurat_file)
            elif isinstance(value, str):
                misc_group.create_dataset(key, data=value)
    
    @property
    def obs_names(self) -> pl.Series:
        return self._obs[:, 0]
    
    @property
    def var_names(self) -> pl.Series:
        return self._var[:, 0]
    
    def set_obs_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of `obs`, i.e. the `obs_names`.
        
        Args:
            column: the column name in `obs`; must be String, Enum, or
                    Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of
            `obs`. If `column` is already the first column, return this dataset
            unchanged.
        """
        obs = self._obs
        check_type(column, 'column', str, 'a string')
        if column == obs.columns[0]:
            return self
        if column not in obs:
            error_message = f'{column!r} is not a column of obs'
            raise ValueError(error_message)
        check_dtype(obs[column], f'obs[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=obs.select(column, pl.exclude(column)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def set_var_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of `var`, i.e. the `var_names`.
        
        Args:
            column: the column name in `var`; must be String, Enum, or
                    Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of
            `var`. If `column` is already the first column, return this dataset
            unchanged.
        """
        var = self._var
        check_type(column, 'column', str, 'a string')
        if column == var.columns[0]:
            return self
        if column not in var:
            error_message = f'{column!r} is not a column of var'
            raise ValueError(error_message)
        check_dtype(self._var[column], f'var[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=var.select(column, pl.exclude(column)),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    @staticmethod
    def _read_datasets(datasets: Sequence[h5py.Dataset],
                       *,
                       num_threads: int | np.integer) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Read a sequence of HDF5 datasets into a dictionary of 1D NumPy arrays.
        Assume all are from the same file (this is not checked).
        
        Args:
            datasets: a sequence of `h5py.Dataset` objects to read
            num_threads: the number of threads to use when reading; if >1,
                         spawn multiple processes and read into shared memory

        Returns:
            A dictionary of NumPy arrays with the contents of the datasets; the
            keys are taken from each dataset's `name` attribute.
        """
        if len(datasets) == 0:
            return {}
        import multiprocessing
        # noinspection PyUnresolvedReferences
        from multiprocessing.sharedctypes import _new_value
        import resource
        
        # Increase the maximum number of possible file descriptors this process
        # can use, since dataframes with hundreds of columns can easily exhaust
        # the common default limit of 1024 file descriptors (`ulimit -n`)
        
        soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft_limit < hard_limit:
            resource.setrlimit(resource.RLIMIT_NOFILE,
                               (hard_limit, hard_limit))
        
        # Allocate shared memory for each dataset. Use `_new_value()` instead
        # of `multiprocessing.Array()` to avoid the memset at github.com/
        # python/cpython/blob/main/Lib/multiprocessing/sharedctypes.py#L62.
        
        # noinspection PyTypeChecker
        buffers = {dataset.name: _new_value(
            len(dataset) * np.ctypeslib.as_ctypes_type(dataset.dtype))
            for dataset in datasets}
        
        # Spawn num_threads processes: the first loads the first len(dataset) /
        # num_threads elements of each dataset, the second loads the next
        # len(dataset) / num_threads, etc. Because the chunks loaded by each
        # process are non-overlapping, there's no need to lock.
        
        filename = datasets[0].file.filename
        
        def read_dataset_chunks(thread_index: int) -> None:
            try:
                with h5py.File(filename) as h5ad_file:
                    for dataset_name, buffer in buffers.items():
                        chunk_size = len(buffer) // num_threads
                        start = thread_index * chunk_size
                        end = len(buffer) \
                            if thread_index == num_threads - 1 else \
                            start + chunk_size
                        chunk = np.s_[start:end]
                        dataset = h5ad_file[dataset_name]
                        dataset.read_direct(np.frombuffer(
                            buffer, dtype=dataset.dtype),
                            source_sel=chunk, dest_sel=chunk)
            except KeyboardInterrupt:
                pass  # do not print KeyboardInterrupt tracebacks, just return
        
        # Ignore polars warning about using `os.fork()`
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning,
                                    module='multiprocessing')
            processes = []
            for thread_index in range(num_threads):
                process = multiprocessing.Process(
                    target=read_dataset_chunks, args=(thread_index,))
                processes.append(process)
                process.start()
            for process in processes:
                process.join()
        # Wrap the shared memory in NumPy arrays
        arrays = {
            dataset_name: np.frombuffer(buffer, dtype=dataset.dtype)
            for (dataset_name, buffer), dataset in
            zip(buffers.items(), datasets)}
        return arrays
    
    @staticmethod
    def _read_dataset(dataset: h5py.Dataset,
                      num_threads: int | np.integer,
                      preloaded_datasets: dict[str, np.ndarray[1, Any]] |
                                          None = None) -> np.ndarray[1, Any]:
        """
        Read an HDF5 dataset into a 1D NumPy array.
        
        Args:
            dataset: the `h5py.Dataset` to read
            num_threads: the number of threads to use for reading; if >1, spawn
                         multiple processes and read into shared memory, unless
                         - the array is small enough (heuristically, under 10k
                           elements) that it's probably faster to read serially
                         - the array is so small that there's less than one
                           element to read per thread
                         - the array has `dtype=object` (not compatible with
                           shared memory)
            preloaded_datasets: a dictionary of preloaded datasets, or `None`
                                to always load from scratch. If specified and
                                `dataset.name` is in `preloaded_datasets`,
                                just return `preloaded_datasets[dataset.name]`.
        
        Returns:
            A 1D NumPy array with the contents of the dataset.
        """
        if preloaded_datasets is not None and \
                dataset.name in preloaded_datasets:
            return preloaded_datasets[dataset.name]
        dtype = dataset.dtype
        min_size = max(10_000, num_threads)
        if num_threads == 1 or dtype == object or dataset.size < min_size:
            return dataset[:]
        else:
            return SingleCell._read_datasets(
                [dataset], num_threads=num_threads)[dataset.name]
    
    @staticmethod
    def _preload_datasets(group: h5py.Group,
                          num_threads: int | np.integer = 1) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Given a group from an `.h5ad` file, preload all datasets inside it,
        except for those where:
        - the array is small enough (heuristically, under 10k elements) that
          it's probably faster to read serially
        - the array is so small that there's less than one element to read per
          thread the array has `dtype=object` (not compatible with shared
          memory)
        
        Args:
            group: an `h5py.Group` to preload
            num_threads: the number of threads to use when preloading; if
                         `num_threads == 1`, do not preload

        Returns:
            A (possibly empty) dictionary of preloaded datasets.
        """
        if num_threads == 1:
            return {}
        datasets = []
        min_size = max(10_000, num_threads)
        group.visititems(
            lambda name, node: datasets.append(node)
            if isinstance(node, h5py.Dataset) and node.dtype != object
            and node.size >= min_size else None)
        return SingleCell._read_datasets(datasets, num_threads=num_threads)
    
    @staticmethod
    def _read_h5ad_dataframe(h5ad_file: h5py.File,
                             key: str,
                             *,
                             columns: str | Sequence[str] | None = None,
                             num_threads: int | np.integer = 1) -> \
            pl.DataFrame:
        """
        Load `obs` or `var` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `h5py.File` open in read mode
            key: the key to load as a DataFrame, e.g. `'obs'` or `'var'`
            columns: the column(s) of the DataFrame to load; the index column
                     is always loaded as the first column, regardless of
                     whether it is specified here, and then the remaining
                     columns are loaded in the order specified
            num_threads: the number of threads to use when reading
        
        Returns:
            A polars DataFrame of the data in `h5ad_file[key]`.
        """
        # Get the group corresponding to `obs` or `var`
        group = h5ad_file[key]
        # Special case: the entire `obs` or `var` may rarely be a single NumPy
        # structured array (`dtype=void`)
        if isinstance(group, h5py.Dataset) and \
                np.issubdtype(group.dtype, np.void):
            data = pl.from_numpy(group[:])
            data = data.with_columns(pl.col(pl.Binary).cast(pl.String))
            return data
        # Preload the datasets in this group
        preloaded_datasets = SingleCell._preload_datasets(group, num_threads)
        # Get the list of which columns to load, in which order
        data = {}
        if columns is None:
            columns = group.attrs['column-order']
        else:
            columns = [column for column in to_tuple(columns)
                       if column != group.attrs['_index']]
            for column in columns:
                if column not in group.attrs['column-order']:
                    error_message = f'{column!r} is not a column of {key}'
                    raise ValueError(error_message)
        # Create the DataFrame
        for column in chain((group.attrs['_index'],), columns):
            value = group[column]
            encoding_type = value.attrs.get('encoding-type')
            if encoding_type == 'categorical' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'categories' or key == 'codes'
                    for key in value.keys())) or 'categories' in value.attrs:
                # Sometimes, the categories are stored in a different place
                # which is pointed to by value.attrs['categories']
                if 'categories' in value.attrs:
                    category_object = h5ad_file[value.attrs['categories']]
                    category_encoding_type = None
                    # noinspection PyTypeChecker
                    codes = SingleCell._read_dataset(
                        value, num_threads, preloaded_datasets)
                else:
                    category_object = value['categories']
                    category_encoding_type = \
                        category_object.attrs.get('encoding-type')
                    codes = SingleCell._read_dataset(
                        value['codes'], num_threads, preloaded_datasets)
                # Sometimes, the categories are themselves nullable
                # integer or Boolean arrays
                if category_encoding_type == 'nullable-integer' or \
                        category_encoding_type == 'nullable-boolean' or (
                        isinstance(category_object, h5py.Group) and all(
                        key == 'values' or key == 'mask'
                        for key in category_object.keys())):
                    data[column] = pl.Series(SingleCell._read_dataset(
                        category_object['values'], num_threads,
                        preloaded_datasets)[codes])
                    mask = pl.Series(SingleCell._read_dataset(
                        category_object['mask'], num_threads,
                        preloaded_datasets)[codes] | (codes == -1))
                    has_missing = mask.any()
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                    continue
                # noinspection PyTypeChecker
                categories = SingleCell._read_dataset(
                    category_object, num_threads, preloaded_datasets)
                mask = pl.Series(codes == -1)
                has_missing = mask.any()
                # polars does not (as of version 1.0) support Categoricals or
                # Enums with non-string categories, so if the categories are
                # not strings, just map the codes to the categories.
                if category_encoding_type == 'array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype != object):
                    data[column] = pl.Series(categories[codes],
                                             nan_to_null=True)
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                elif category_encoding_type == 'string-array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype == object):
                    if has_missing:
                        codes[mask] = 0
                    data[column] = pl.Series(codes, dtype=pl.UInt32)
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                    data[column] = data[column].cast(
                        pl.Enum(pl.Series(categories).cast(pl.String)))
                else:
                    encoding = \
                        f'encoding-type {category_encoding_type!r}' \
                        if category_encoding_type is not None else \
                            'encoding'
                    error_message = (
                        f'{column!r} column of {key!r} is a categorical '
                        f'with unsupported {encoding}')
                    raise ValueError(error_message)
            elif encoding_type == 'nullable-integer' or \
                    encoding_type == 'nullable-boolean' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'values' or key == 'mask' for key in value.keys())):
                values = SingleCell._read_dataset(
                    value['values'], num_threads, preloaded_datasets)
                mask = SingleCell._read_dataset(
                    value['mask'], num_threads, preloaded_datasets)
                data[column] = pl.Series(values).set(pl.Series(mask), None)
            elif encoding_type == 'array' or (
                    isinstance(value, h5py.Dataset) and value.dtype != object):
                data[column] = pl.Series(SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets), nan_to_null=True)
            elif encoding_type == 'string-array' or (
                    isinstance(value, h5py.Dataset) and value.dtype == object):
                data[column] = SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets)
            else:
                encoding = f'encoding-type {encoding_type!r}' \
                    if encoding_type is not None else 'encoding'
                error_message = \
                    f'{column!r} column of {key!r} has unsupported {encoding}'
                raise ValueError(error_message)
        data = pl.DataFrame(data)
        # NumPy doesn't support encoding object-dtyped string arrays as UTF-8,
        # so do the conversion in polars instead
        data = data.with_columns(pl.col(pl.Binary).cast(pl.String))
        return data
    
    @staticmethod
    def _read_h5Seurat_dataframe(group: h5py.Group,
                                 *,
                                 columns: str | Sequence[str] | None = None,
                                 num_threads: int | np.integer = 1) -> \
            pl.DataFrame:
        """
        Load `obs` (i.e. `meta.data`) or `var (i.e. the active assay's
        `meta.features`) from an `.h5Seurat` file as a polars DataFrame.
        
        Args:
            group: the group (`meta.data` or `meta.features`) to read
            columns: the column(s) of the DataFrame to load; the index column
                     is always loaded as the first column, regardless of
                     whether it is specified here, and then the remaining
                     columns are loaded in the order specified
            num_threads: the number of threads to use when reading
        
        Returns:
            A polars DataFrame of the cell- or gene-level metadata.
        """
        # Preload the datasets in `group`
        preloaded_datasets = SingleCell._preload_datasets(group, num_threads)
        # Get the list of which columns to load, in which order
        if columns is None:
            columns = group.attrs['colnames']
        else:
            columns = [column for column in to_tuple(columns)
                       if column != group.attrs['_index']]
            for column in columns:
                if column not in group.attrs['colnames']:
                    error_message = f'{column!r} is not a column of meta.data'
                    raise ValueError(error_message)
        # Get the list of which columns are Boolean
        Boolean_columns = group.attrs.get('logicals', ())
        # Create the DataFrame
        data = {}
        for column in chain(group.attrs['_index'], columns):
            value = group[column]
            if isinstance(value, h5py.Group):
                # Factor
                if len(value) != 2 or 'levels' not in value or \
                        'values' not in value:
                    error_message = (
                        f"the h5Seurat file's meta.data contains a group "
                        f"of unknown format, ")
                    if len(value) <= 5:
                        error_message += (
                            f'with the following keys: '
                            f'{", ".join(map(repr, value))}')
                    else:
                        error_message += (
                            f'with {len(value):,} keys, including the '
                            f'following: '
                            f'{", ".join(map(repr, islice(value, 5)))}')
                    raise ValueError(error_message)
                values = SingleCell._read_dataset(
                    value['values'], num_threads, preloaded_datasets)
                levels = value['levels'][:]
                # noinspection PyUnresolvedReferences
                data[column] = (pl.Series(values) - 1)\
                    .cast(pl.Enum(pl.Series(levels).cast(pl.String)))
            else:
                data[column] = pl.Series(SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets), nan_to_null=True)
                if column in Boolean_columns:
                    data[column] = data[column]\
                        .replace({2: None})\
                        .cast(pl.Boolean)
                elif data[column].dtype == pl.Int32:
                    data[column] = data[column].replace({-2147483648: None})
        data = pl.DataFrame(data)
        # NumPy doesn't support encoding object-dtyped string arrays as UTF-8,
        # so do the conversion in polars instead. Also convert `b'NA'`
        # (h5Seurat's missing value indicator) to `null`.
        data = data.with_columns(pl.col(pl.Binary).replace({b'NA': None})
                                 .cast(pl.String))
        return data
    
    @staticmethod
    def read_obs(h5ad_file: h5py.File | str | Path,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer | None = None) -> pl.DataFrame:
        """
        Load just `obs` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            columns: the column(s) of `obs` to load; if `None`, load all
                     columns
            num_threads: the number of threads to use when reading. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
    
        Returns:
            A polars DataFrame of the data in `obs`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'obs', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_var(h5ad_file: str | Path,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer | None = None) -> pl.DataFrame:
        """
        Load just `var` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            columns: the column(s) of `var` to load; if `None`, load all
                     columns
            num_threads: the number of threads to use when reading. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
    
        Returns:
            A polars DataFrame of the data in `var`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'var', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_obsm(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None,
                  *,
                  num_threads: int | np.integer | None = None) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just `obsm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `obsm` to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of `obsm`; set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            obsm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'obsm' in f:
                obsm = f['obsm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'obsm/{key}', num_threads=num_threads)
                            for key, value in obsm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in obsm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of obsm')
                            raise ValueError(error_message)
                    return {key: obsm[key][:]
                                 if isinstance(obsm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'obsm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but obsm is empty'
                    raise ValueError(error_message)
                return {}
         
    @staticmethod
    def read_varm(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None,
                  *,
                  num_threads: int | np.integer | None = None) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just `varm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `varm` to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of `varm`. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            varm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'varm' in f:
                varm = f['varm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'varm/{key}', num_threads=num_threads)
                            for key, value in varm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in varm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of varm')
                            raise ValueError(error_message)
                    return {key: varm[key][:]
                                 if isinstance(varm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'varm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but varm is empty'
                    raise ValueError(error_message)
                return {}
    
    @staticmethod
    def read_obsp(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None) -> \
            dict[str, csr_array | csc_array]:
        """
        Load just `obsm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `obsp` to load; if `None`, load all keys
        
        Returns:
            A dictionary of sparse arrays of the data in `obsp`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'obsp' in f:
                obsp = f['obsp']
                if keys is None:
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in obsp.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in obsp:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of obsp')
                            raise ValueError(error_message)
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in
                            ((key, obsp[key]) for key in keys)}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but obsp is empty'
                    raise ValueError(error_message)
                return {}

    @staticmethod
    def read_varp(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None) -> \
            dict[str, csr_array | csc_array]:
        """
        Load just `obsm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `varp` to load; if `None`, load all keys
        
        Returns:
            A dictionary of sparse arrays of the data in `varp`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'varp' in f:
                varp = f['varp']
                if keys is None:
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in varp.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in varp:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of varp')
                            raise ValueError(error_message)
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in
                            ((key, varp[key]) for key in keys)}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but varp is empty'
                    raise ValueError(error_message)
                return {}
    
    @staticmethod
    def read_uns(h5ad_file: str | Path) -> NestedScalarOrArrayDict:
        """
        Load just `uns` from an `.h5ad` file as a dictionary.
        
        Args:
            h5ad_file: an .`h5ad` filename
        
        Returns:
            A dictionary of the data in `uns`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        with h5py.File(filename) as f:
            if 'uns' in f:
                return SingleCell._read_uns(f['uns'])
            else:
                return {}
    
    @staticmethod
    def _print_matrix_info(X: h5py.Group | h5py.Dataset, X_name: str) -> None:
        """
        Given a key of an `.h5ad` file representing a sparse or dense matrix,
        print its shape, data type and (if sparse) number of non-zero elements.
        
        Args:
            X: the key in the `.h5ad` file representing the matrix, as a
               `Group` or `Dataset` object
            X_name: the name of the key
        """
        is_sparse = isinstance(X, h5py.Group)
        if is_sparse:
            data = X['data']
            shape = X.attrs['shape'] if 'shape' in X.attrs else \
                X.attrs['h5sparse_shape']
            dtype = str(data.dtype)
            nnz = data.shape[0]
            print(f'{X_name}: {shape[0]:,} Ã— {shape[1]:,} sparse array with '
                  f'{nnz:,} non-zero elements, data type {dtype!r}, and '
                  f'first non-zero element = {data[0]:.6g}')
        else:
            shape = X.shape
            dtype = str(X.dtype)
            print(f'{X_name}: {shape[0]:,} Ã— {shape[1]:,} dense matrix with '
                  f'data type {dtype!r} and first element = {X[0, 0]:.6g}')
    
    @staticmethod
    def ls(h5ad_file: str | Path) -> None:
        """
        Print the fields in an `.h5ad` file. This can be useful e.g. when
        deciding which count matrix to load via the `X_key` argument to
        `SingleCell()`.
        
        Args:
            h5ad_file: an `.h5ad` filename
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        attrs = 'obs', 'var', 'obsm', 'varm', 'obsp', 'varp', 'uns'
        with h5py.File(filename) as f:
            # `X`
            SingleCell._print_matrix_info(f['X'], 'X')
            # layers
            if 'layers' in f:
                layers = f['layers']
                if len(layers) > 0:
                    for layer_name, layer in layers.items():
                        SingleCell._print_matrix_info(
                            layer, f'layers[{layer_name!r}]')
            # `obs`, `var`, `obsm`, `varm`, `obsp`, `varp`, `uns`
            for attr in attrs:
                if attr in f:
                    entries = f[attr]
                    if (attr == 'obs' or attr == 'var') and \
                            isinstance(entries, h5py.Dataset) and \
                            np.issubdtype(entries.dtype, np.void):
                        entries = entries.dtype.fields
                    if len(entries) > 0:
                        print(fill(f'{attr}: {", ".join(entries)}',
                                   width=terminal_width,
                                   subsequent_indent=' ' * (len(attr) + 2)))
            # raw
            if 'raw' in f:
                raw = f['raw']
                if len(raw) > 0:
                    print('raw:')
                    if 'X' in raw:
                        SingleCell._print_matrix_info(raw['X'], '    X')
                    if 'layers' in raw:
                        layers = raw['layers']
                        if len(layers) > 0:
                            for layer_name, layer in layers.items():
                                SingleCell._print_matrix_info(
                                    layer, f'    layers[{layer_name!r}]')
                    for attr in attrs:
                        if attr in raw:
                            entries = raw[attr]
                            if (attr == 'obs' or attr == 'var') and \
                                    isinstance(entries, h5py.Dataset) and \
                                    np.issubdtype(entries.dtype, np.void):
                                entries = entries.dtype.fields
                            if len(entries) > 0:
                                print(fill(f'    {attr}: {", ".join(entries)}',
                                           width=terminal_width,
                                           subsequent_indent=' ' * (
                                                   len(attr) + 6)))
    
    def __eq__(self, other: SingleCell) -> bool:
        """
        Test for equality with another SingleCell dataset.
        
        Args:
            other: the other SingleCell dataset to test for equality with

        Returns:
            Whether the two SingleCell datasets are identical.
        """
        if not isinstance(other, SingleCell):
            error_message = (
                f'the left-hand operand of `==` is a SingleCell dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return self._obs.equals(other._obs) and \
               self._var.equals(other._var) and \
               self._obsm.keys() == other._obsm.keys() and \
               self._varm.keys() == other._varm.keys() and \
            all(type(other._obsm[key]) is type(value) and
                (array_equal(other._obsm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._obsm[key].equals(value))
                for key, value in self._obsm.items()) and \
            all(type(other._varm[key]) is type(value) and
                (array_equal(other._varm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._varm[key].equals(value))
                for key, value in self._varm.items()) and \
            SingleCell._eq_uns(self._uns, other._uns) and \
            (other._X is None if self._X is None else other._X is not None and
             self._X.nnz == other._X.nnz and not (self._X != other._X).nnz)
    
    @staticmethod
    def _eq_uns(uns: NestedScalarOrArrayDict,
                other_uns: NestedScalarOrArrayDict,
                different_order_ok: bool = False) -> bool:
        """
        Test whether two `uns` are equal.
        
        Args:
            uns: an `uns`
            other_uns: another `uns`
            different_order_ok: whether to consider `uns` and `other_uns` equal
                                when they have the same keys and values, but in
                                a different order

        Returns:
            Whether `uns` and `other_uns` are equal.
        """
        return set(uns.keys()) == set(other_uns.keys()) \
            if different_order_ok else uns.keys() == other_uns.keys() and all(
            isinstance(value, dict) and isinstance(other_value, dict) and
            SingleCell._eq_uns(value, other_value, different_order_ok) or
            isinstance(value, np.ndarray) and
            isinstance(other_value, np.ndarray) and
            array_equal(value, other_value) or
            not isinstance(other_value, (dict, np.ndarray)) and
            value == other_value
            for key, value, other_value in
            ((key, value, other_uns[key]) for key, value in uns.items()))
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'SingleCell indices must be cells, a length-1 tuple of (cells,), '
            f'or a length-2 tuple of (cells, genes). Cells and genes must '
            f'each be a string or integer; a slice of strings or integers; or '
            f'a list, NumPy array, or polars Series of strings, integers, or '
            f'Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (`obs` or `var`)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('_SingleCell_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    @staticmethod
    def _getitem_process(item: Indexer | tuple[Indexer, Indexer], index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to `__getitem__()`.
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (`obs` or `var`) to process the element with
                respect to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [SingleCell._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = SingleCell._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                SingleCell._getitem_error(item)
            if isinstance(stop, str):
                stop = SingleCell._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                SingleCell._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                SingleCell._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            df_0_dtype = df[:, 0].dtype
            already_series = isinstance(subitem, pl.Series)
            if already_series:
                subitem_dtype = subitem.dtype
            else:
                subitem = pl.Series(subitem, dtype=df_0_dtype)
                subitem_dtype = df_0_dtype
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            if subitem_dtype == pl.String or subitem_dtype == \
                    pl.Categorical or subitem_dtype == pl.Enum:
                if already_series and subitem_dtype != df_0_dtype:
                    subitem = subitem.cast(df_0_dtype)
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_SingleCell_index=pl.int_range(
                              pl.len(), dtype=pl.Int32)),
                          on=df.columns[0], how='left')\
                    ['_SingleCell_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif subitem_dtype.is_integer() or subitem_dtype == pl.Boolean:
                return subitem
            else:
                SingleCell._getitem_error(item)
        else:
            SingleCell._getitem_error(item)
            
    def __getitem__(self, item: Indexer | tuple[Indexer, Indexer]) -> \
            SingleCell:
        """
        Subset to specific cell(s) and/or gene(s).
        
        Index with a tuple of `(cells, genes)`. If `cells` and `genes` are
        integers, arrays/lists/slices of integers, or arrays/lists of Booleans,
        the result will be a SingleCell dataset subset to `X[cells, genes]`,
        `obs[cells]`, `var[genes]`, `obsm[cells]`, `varm[genes]`,
        `obsp[cells][:, cells]`, and `varp[genes][:, genes]`. However, `cells`
        and/or `genes` can instead be strings (or arrays or slices of strings),
        in which case they refer to the first column of `obs` (`obs_names`)
        and/or `var` (`var_names`), respectively.
        
        Examples:
        - Subset to one cell, for all genes:
          ```
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416']
          sc[2]
          ```
        - Subset to one gene, for all cells:
          ```
          sc[:, 'APOE']
          sc[:, 13196]
          ```
        - Subset to one cell and one gene:
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416', 'APOE']
          sc[2, 13196]
          ```
        - Subset to a range of cells and genes:
          ```
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416':
             'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522',
             'APOE':'TREM2']
          sc[2:6, 13196:34268]
          ```
        - Subset to specific cells and genes:
          ```
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522']]
          sc[:, pl.Series(['APOE', 'TREM2'])]
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522'],
              np.array(['APOE', 'TREM2'])]
          ```
        
        Args:
            item: the item to index with
        
        Returns:
            A new SingleCell dataset subset to the specified cells and/or
            genes.
        """
        if not isinstance(item, (int, str, slice, tuple, list,
                                 np.ndarray, pl.Series)):
            error_message = (
                f'SingleCell datasets must be indexed with an integer, '
                f'string, slice, tuple, list, NumPy array, or polars Series, '
                f'but you tried to index with an object of type '
                f'{type(item).__name__!r}')
            raise TypeError(error_message)
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 2:
                self._getitem_error(item)
        else:
            item = item,
        rows = self._getitem_process(item, 0, self._obs)
        rows_is_Series = isinstance(rows, pl.Series)
        if rows_is_Series:
            boolean_Series = rows.dtype == pl.Boolean
            obs = self._obs.filter(rows) if boolean_Series else self._obs[rows]
            rows_NumPy = rows.to_numpy()
        else:
            boolean_Series = False
            obs = self._obs[rows]
            rows_NumPy = rows
        obsm = {key: (value.filter(rows) if boolean_Series else
                      value[rows]) if isinstance(value, pl.DataFrame) else
                     value[rows_NumPy]
                for key, value in self._obsm.items()} if self._obsm else {}
        rows_is_slice = isinstance(rows, slice)
        obsp = ({key: value[rows_NumPy, rows_NumPy]
                 for key, value in self._obsp.items()} if rows_is_slice else
                {key: value[np.ix_(rows_NumPy, rows_NumPy)]
                 for key, value in self._obsp.items()}) if self._obsp else {}
        if len(item) == 1:
            return SingleCell(X=self._X[rows] if self._X is not None else None,
                              obs=obs, var=self._var, obsm=obsm,
                              varm=self._varm, obsp=obsp, varp=self._varp,
                              uns=self._uns)
        cols = self._getitem_process(item, 1, self._var)
        cols_is_Series = isinstance(cols, pl.Series)
        if cols_is_Series:
            boolean_Series = cols.dtype == pl.Boolean
            var = self._var.filter(cols) if boolean_Series else self._var[cols]
            cols_NumPy = cols.to_numpy()
        else:
            boolean_Series = False
            var = self._var[cols]
            cols_NumPy = cols
        varm = {key: (value.filter(cols) if boolean_Series else
                      value[cols]) if isinstance(value, pl.DataFrame) else
                     value[cols_NumPy]
                for key, value in self._varm.items()} if self._varm else {}
        cols_is_slice = isinstance(cols, slice)
        varp = ({key: value[cols_NumPy, cols_NumPy]
                 for key, value in self._varp.items()} if cols_is_slice else
                {key: value[np.ix_(cols_NumPy, cols_NumPy)]
                 for key, value in self._varp.items()}) if self._varp else {}
        X = None if self._X is None else self._X[rows_NumPy, cols_NumPy] \
            if rows_is_slice or cols_is_slice else \
            self._X[np.ix_(rows_NumPy, cols_NumPy)]
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, varm=varm,
                          obsp=obsp, varp=varp, uns=self._uns)
    
    def cell(self,
             cell: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the row of `X` corresponding to a single cell, based on the cell's
        name in `obs_names`.
        
        Args:
            cell: the name of the cell in `obs_names`
            num_threads: the number of threads to use when retrieving the
                         row of `X`. Must be 1 when `X` is a CSR array, since
                         there is no benefit to parallelism in that case. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            The corresponding row of `X`, as a dense 1D NumPy array with zeros
            included.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        if self._X is None:
            error_message = 'X is None, so getting a row of X is not possible'
            raise TypeError(error_message)
        num_threads = SingleCell._process_num_threads(num_threads)
        if num_threads > 1 and isinstance(self._X, csr_array):
            error_message = (
                'num_threads must be 1 when X is a CSR array, since '
                'parallelism does not give any speedup')
            raise ValueError(error_message)
        row_index = SingleCell._getitem_by_string(self._obs, cell)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[[row_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    def gene(self,
             gene: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the column of `X` corresponding to a single gene, based on the
        gene's name in `var_names`.
        
        Args:
            gene: the name of the gene in `var_names`
            num_threads: the number of threads to use when retrieving the
                         row of `X`. Must be 1 when `X` is a CSC array, since
                         there is no benefit to parallelism in that case. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            The corresponding column of `X`, as a dense 1D NumPy array with
            zeros included.
        """
        if self._X is None:
            error_message = \
                'X is None, so getting a column of X is not possible'
            raise TypeError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        if num_threads > 1 and isinstance(self._X, csc_array):
            error_message = (
                'num_threads must be 1 when X is a CSC array, since '
                'parallelism does not give any speedup')
            raise ValueError(error_message)
        column_index = SingleCell._getitem_by_string(self._var, gene)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[:, [column_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    def __len__(self) -> int:
        """
        Get the number of cells in this SingleCell dataset.
        
        Returns:
            The number of cells.
        """
        return self._obs.shape[0]
       
    def __repr__(self) -> str:
        """
        Get a string representation of this SingleCell dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        descr = (
            f'SingleCell dataset with {len(self._obs):,} '
            f'{plural("cell", len(self._obs))} (obs), {len(self._var):,} '
            f'{plural("gene", len(self._var))} (var), and ')
        if self._X is not None:
            descr += (f'{self._X.nnz:,} non-zero '
                      f'{"entries" if self._X.nnz != 1 else "entry"} (X)')
        else:
            descr += 'no X'
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        for attr in 'obs', 'var', 'obsm', 'varm', 'obsp', 'varp', 'uns':
            entries = getattr(self, attr).columns \
                if attr == 'obs' or attr == 'var' else getattr(self, attr)
            if len(entries) > 0:
                descr += '\n' + fill(
                    f'    {attr}: {", ".join(entries)}',
                    width=terminal_width,
                    subsequent_indent=' ' * (len(attr) + 6))
        return descr
    
    @property
    def shape(self) -> tuple[int, int]:
        """
        Get the shape of this SingleCell dataset.
        
        Returns:
            A length-2 tuple where the first element is the number of cells,
            and the second is the number of genes.
        """
        return self._obs.shape[0], self._var.shape[0]
    
    @staticmethod
    def _process_num_threads(num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        SingleCell function.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use. If `num_threads` is a positive
            integer, return it unchanged. If `num_threads` is `None`, return
            `single_cell.options()['num_threads']`. If `num_threads` is -1,
            return `os.cpu_count()`. Otherwise, raise an error.
        """
        if num_threads is None:
            return _num_threads
        check_type(num_threads, 'num_threads', int,
                   'a positive integer, -1 or None')
        if num_threads == -1:
            return os.cpu_count()
        elif num_threads <= 0:
            error_message = (
                f'num_threads is {num_threads:,}, but must be a positive '
                f'integer, -1 or None')
            raise ValueError(error_message)
        else:
            return int(num_threads)
    
    @staticmethod
    def _save_h5ad_dataframe(h5ad_file: h5py.File,
                             df: pl.DataFrame,
                             key: str,
                             preserve_strings: bool) -> None:
        """
        Save `obs` or `var` to an `.h5ad` file.
        
        Args:
            h5ad_file: an `h5py.File` open in write mode
            df: the DataFrame to write, e.g. `obs` or `var`
            key: the key to create in `h5ad_file`, e.g. `'obs'` or `'var'`
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space; if `True`,
                              preserve these columns as string columns
        """
        # Create a group for the data frame and add top-level metadata
        group = h5ad_file.create_group(key)
        group.attrs['_index'] = df.columns[0]
        group.attrs['column-order'] = df.columns[1:]
        group.attrs['encoding-type'] = 'dataframe'
        group.attrs['encoding-version'] = '0.2.0'
        for column in df:
            dtype = column.dtype
            if dtype == pl.String:
                if column.null_count() or not preserve_strings and \
                        column.is_duplicated().any():
                    column = column\
                        .cast(pl.Enum(column.unique(maintain_order=True)
                                      .drop_nulls()))
                    dtype = column.dtype
                else:
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'string-array'
                    dataset.attrs['encoding-version'] = '0.2.0'
                    continue
            if dtype == pl.Enum or dtype == pl.Categorical:
                is_Enum = dtype == pl.Enum
                subgroup = group.create_group(column.name)
                subgroup.attrs['encoding-type'] = 'categorical'
                subgroup.attrs['encoding-version'] = '0.2.0'
                subgroup.attrs['ordered'] = is_Enum
                categories = column.cat.get_categories()
                if not is_Enum:
                    column = column.cast(pl.Enum(categories))
                codes = column.to_physical().fill_null(-1)
                subgroup.create_dataset('codes', data=codes.to_numpy())
                if len(categories) == 0:
                    subgroup.create_dataset('categories', shape=(0,),
                                            dtype=h5py.special_dtype(vlen=str))
                else:
                    subgroup.create_dataset('categories',
                                            data=categories.to_numpy())
            elif dtype.is_float():
                # Nullable floats are not supported, so convert `null` to `NaN`
                dataset = group.create_dataset(
                    column.name, data=column.fill_null(np.nan).to_numpy())
                dataset.attrs['encoding-type'] = 'array'
                dataset.attrs['encoding-version'] = '0.2.0'
            else:  # Boolean or integer
                is_Boolean = dtype == pl.Boolean
                if column.null_count():
                    # Store as nullable integer/Boolean
                    subgroup = group.create_group(column.name)
                    subgroup.attrs['encoding-type'] = \
                        f'nullable-{"boolean" if is_Boolean else "integer"}'
                    subgroup.attrs['encoding-version'] = '0.1.0'
                    subgroup.create_dataset(
                        'values',
                        data=column.fill_null(False if is_Boolean else 1)
                        .to_numpy())
                    subgroup.create_dataset(
                        'mask', data=column.is_null().to_numpy())
                else:
                    # Store as regular integer/Boolean
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'array'
                    dataset.attrs['encoding-version'] = '0.2.0'
    
    @staticmethod
    def _save_h5Seurat_dataframe(h5ad_file: h5py.File,
                                 df: pl.DataFrame,
                                 key: str,
                                 preserve_strings: bool) -> None:
        """
        Save `obs` or `var` to an `.h5Seurat` file.
        
        Args:
            h5ad_file: an `h5py.File` open in write mode
            df: the DataFrame to write, e.g. `obs` or `var`
            key: the key to create in `h5ad_file`, e.g. `'meta.data'` or
                 `'RNA/meta.features'`
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space; if `True`,
                              preserve these columns as string columns
        """
        # Create a group for the data frame and add top-level metadata
        group = h5ad_file.create_group(key)
        group.attrs['_index'] = df.columns[0]
        group.attrs['colnames'] = df.columns[1:]
        group.attrs['logicals'] = pl.selectors.expand_selector(
            df, pl.selectors.by_dtype(pl.Boolean))
        for column in df:
            dtype = column.dtype
            if dtype == pl.String:
                if column.null_count() or not preserve_strings and \
                        column.is_duplicated().any():
                    column = column\
                        .cast(pl.Enum(column.unique(maintain_order=True)
                                      .drop_nulls()))
                    dtype = column.dtype
                else:
                    group.create_dataset(column.name, data=column.to_numpy())
                    continue
            if dtype == pl.Enum or dtype == pl.Categorical:
                subgroup = group.create_group(column.name)
                levels = column.cat.get_categories()
                if dtype != pl.Enum:
                    column = column.cast(pl.Enum(levels))
                # noinspection PyUnresolvedReferences
                values = (column.to_physical() + 1).fill_null(-2147483648)
                subgroup.create_dataset('values', data=values.to_numpy())
                if len(levels) == 0:
                    subgroup.create_dataset('levels', shape=(0,),
                                            dtype=h5py.special_dtype(vlen=str))
                else:
                    subgroup.create_dataset('levels', data=levels.to_numpy())
            else:
                if dtype.is_float():
                    column = column.fill_null(np.nan)
                elif dtype == pl.Boolean:
                    column = column.cast(pl.Int32).fill_null(2)
                else:  # integer
                    column = column.fill_null(-2147483648)
                group.create_dataset(column.name, data=column.to_numpy())
    
    def save(self,
             filename: str | Path,
             *,
             assay: str = 'RNA',
             X_key: str = 'counts',
             overwrite: bool = False,
             preserve_strings: bool = False,
             sce: bool = False) -> None:
        """
        Save this SingleCell dataset to a file. File format will be inferred
        from the file extension (e.g. `.h5ad`).
        
        Args:
            filename: an AnnData `.h5ad` file, Seurat `.rds` or `.h5Seurat`
                      file, SingleCellExperiment `.rds` file, or 10x `.h5` or
                      `.mtx.gz` file to save to. If the extension is `.rds`,
                      the `sce` argument will determine whether to save to a
                      Seurat or a SingleCellExperiment object.
                      - When saving to a Seurat `.rds` file, to match the
                        requirements of Seurat objects, the `'X_'` prefix
                        (often used by Scanpy) will be removed from each key of
                        obsm where it is present (e.g. `'X_umap'` will become
                        `'umap'`).
                      - When swaving to a Seurat `.rds` file, Seurat will add
                       `'orig.ident'`, `'nCount_RNA'` and `'nFeature_RNA'` as
                        gene-level metadata by default; you can disable the
                        calculation of the latter two columns with:
                        
                        ```python
                        from ryp import r
                        r('options(Seurat.object.assay.calcn = FALSE)')
                        ```
                        
                      - When saving to a Seurat`.rds` or `.h5Seurat` file or a
                        SingleCellExperiment `.rds` file, `varm` will not be
                        saved.
                      - When saving to a 10x `.h5` file, `obs['barcodes']`,
                        `var['feature_type']`, `var['genome']`, `var['id']`,
                        and `var['name']` must all exist. Only `X` and these
                        columns will be saved, along with whichever of
                        `var['pattern']`, `var['read']`, and `var['sequence']`
                        exist. All of these columns (if they exist) must be
                        String, Enum, or Categorical.
                      - When saving to a 10x `.mtx.gz` file, `barcodes.tsv.gz`
                        and `features.tsv.gz` will be created in the same
                        directory. Only `X`, `obs` and `var` will be saved.
            assay: when saving to a Seurat `.rds` or `.h5Seurat` file, the name
                   to use for the active assay
            X_key: when saving to a Seurat `.rds` or `.h5Seurat` file, the name
                   of the slot within the active assay to save `X` to; must be
                   `'counts'` or `'data'`. When saving to a
                   SingleCellExperiment `.rds` file, the name of the slot
                   within `@assays@data` to save `X` to.
            overwrite: if `False`, raises an error if (any of) the file(s)
                       exist; if `True`, overwrites them
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space, when saving to
                              AnnData `.h5ad` or Seurat or SingleCellExperiment
                              `.rds`; if `True`, preserve these columns as
                              string columns. (Regardless of the value of
                              `preserve_strings`, String columns with `null`
                              values will be encoded as Enums when saving to
                              `.h5ad`, since the `.h5ad` format cannot
                              represent them otherwise.)
            sce: if `True` and the extension of filename is `.rds`, save to a
                 SingleCellExperiment object instead of a Seurat object
        """
        # Check inputs
        if self._X is None:
            error_message = 'X is None, so saving is not possible'
            raise TypeError(error_message)
        check_type(filename, 'filename', (str, Path),
                   'a string or pathlib.Path')
        filename = str(filename)
        is_h5ad = filename.endswith('.h5ad')
        is_rds = filename.endswith('.rds')
        is_h5Seurat = filename.endswith('.h5Seurat')
        is_h5 = filename.endswith('.h5')
        is_hdf5 = is_h5ad or is_h5 or is_h5Seurat
        is_mtx = filename.endswith('.mtx.gz')
        is_Seurat = is_h5Seurat or is_rds and not sce
        if not (is_hdf5 or is_mtx or is_rds):
            error_message = (
                f"filename {filename!r} does not end with '.h5ad', '.rds', "
                f"'.h5Seurat', '.h5', or '.mtx.gz'")
            raise ValueError(error_message)
        check_type(assay, 'assay', str, 'a string')
        if not is_Seurat and assay != 'RNA':
            error_message = \
                'assay cannot be specified unless saving to a Seurat object'
            raise ValueError(error_message)
        check_type(X_key, 'X_key', str, 'a string')
        if is_Seurat:
            if X_key != 'counts' and X_key != 'data':
                error_message = (
                    f"when saving to a Seurat object, X_key must be 'counts' "
                    f"or 'data', not {X_key!r}")
                raise ValueError(error_message)
        elif not sce and X_key != 'counts':
            error_message = (
                'X_key cannot be specified unless saving to a Seurat or '
                'SingleCellExperiment object')
            raise ValueError(error_message)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        filename_expanduser = os.path.expanduser(filename)
        if not overwrite and os.path.exists(filename_expanduser):
            error_message = (
                f'filename {filename!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        check_type(preserve_strings, 'preserve_strings', bool, 'Boolean')
        check_type(sce, 'sce', bool, 'Boolean')
        if sce and not is_rds:
            error_message = 'sce can only be True when saving to an .rds file'
            raise ValueError(error_message)
        # Raise an error if `obs` or `var` (or, if saving to `.h5ad`, DataFrame
        # keys of `obsm` or `varm`) contain columns with unsupported data types
        # (anything but float, int, String, Categorical, Enum, Boolean)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'saving')
                    raise TypeError(error_message)
        if is_h5ad:
            for field, field_name in (self._obsm, 'obsm'), \
                    (self._varm, 'varm'):
                for key, value in field.items():
                    if not isinstance(value, pl.DataFrame):
                        continue
                    for column, dtype in value.schema.items():
                        if dtype.base_type() not in valid_dtypes:
                            error_message = (
                                f'{field}[{key!r}][{column!r}] has the data '
                                f'type {dtype.base_type()!r}, which is not '
                                f'supported when saving')
                            raise TypeError(error_message)
        # Raise an error if `obsm`, `varm` or `uns` contain NumPy arrays with
        # unsupported data types (`datetime64`, `timedelta64`, unstructured
        # `void`). Do not specifically check `dtype=object` to avoid extra
        # overhead.
        for field, field_name in \
                (self._obsm, 'obsm'), (self._varm, 'varm'), (self._uns, 'uns'):
            for key, value in field.items():
                if not isinstance(value, np.ndarray):
                    continue
                if value.dtype.type == np.void and value.dtype.names is None:
                    error_message = (
                        f'{field_name}[{key!r}] is an unstructured void '
                        f'array, which is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.datetime64:
                    error_message = (
                        f'{field_name}[{key!r}] is a datetime64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.timedelta64:
                    error_message = (
                        f'{field_name}[{key!r}] is a timedelta64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
        # Save, depending on the file extension
        if is_hdf5:
            try:
                with h5py.File(filename_expanduser, 'w') as hdf5_file:
                    if is_h5ad:
                        # Add top-level metadata
                        hdf5_file.attrs['encoding-type'] = 'anndata'
                        hdf5_file.attrs['encoding-version'] = '0.1.0'
                        # Save `obs` and `var`
                        SingleCell._save_h5ad_dataframe(
                            hdf5_file, self._obs, 'obs', preserve_strings)
                        SingleCell._save_h5ad_dataframe(
                            hdf5_file, self._var, 'var', preserve_strings)
                        # Save `obsm`
                        if self._obsm:
                            obsm = hdf5_file.create_group('obsm')
                            obsm.attrs['encoding-type'] = 'dict'
                            obsm.attrs['encoding-version'] = '0.1.0'
                            for key, value in self._obsm.items():
                                if isinstance(value, pl.DataFrame):
                                    SingleCell._save_h5ad_dataframe(
                                        hdf5_file, value, f'obsm/{key}',
                                        preserve_strings)
                                else:
                                    obsm.create_dataset(key, data=value)
                        # Save `varm`
                        if self._varm:
                            varm = hdf5_file.create_group('varm')
                            varm.attrs['encoding-type'] = 'dict'
                            varm.attrs['encoding-version'] = '0.1.0'
                            for key, value in self._varm.items():
                                if isinstance(value, pl.DataFrame):
                                    SingleCell._save_h5ad_dataframe(
                                        hdf5_file, value, f'varm/{key}',
                                        preserve_strings)
                                else:
                                    varm.create_dataset(key, data=value)
                        # Save `obsp`
                        obsp = hdf5_file.create_group('obsp')
                        obsp.attrs['encoding-type'] = 'dict'
                        obsp.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._obsp.items():
                            group = obsp.create_group(key)
                            group.attrs['encoding-type'] = 'csr_matrix' \
                                if isinstance(value, csr_array) else \
                                'csc_matrix'
                            group.attrs['encoding-version'] = '0.1.0'
                            group.attrs['shape'] = value.shape
                            group.create_dataset('data', data=value.data)
                            group.create_dataset('indices', data=value.indices)
                            group.create_dataset('indptr', data=value.indptr)
                        # Save `varp`
                        varp = hdf5_file.create_group('varp')
                        varp.attrs['encoding-type'] = 'dict'
                        varp.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._varp.items():
                            group = varp.create_group(key)
                            group.attrs['encoding-type'] = 'csr_matrix' \
                                if isinstance(value, csr_array) else \
                                'csc_matrix'
                            group.attrs['encoding-version'] = '0.1.0'
                            group.attrs['shape'] = value.shape
                            group.create_dataset('data', data=value.data)
                            group.create_dataset('indices', data=value.indices)
                            group.create_dataset('indptr', data=value.indptr)
                        # Save `uns`
                        if self._uns:
                            SingleCell._save_uns(self._uns,
                                                 hdf5_file.create_group('uns'),
                                                 hdf5_file)
                        # Save `X`
                        X = hdf5_file.create_group('X')
                        X.attrs['encoding-type'] = 'csr_matrix' \
                            if isinstance(self._X, csr_array) else 'csc_matrix'
                        X.attrs['encoding-version'] = '0.1.0'
                        X.attrs['shape'] = self._X.shape
                        X.create_dataset('data', data=self._X.data)
                        X.create_dataset('indices', data=self._X.indices)
                        X.create_dataset('indptr', data=self._X.indptr)
                    elif is_h5Seurat:
                        obs_names = self.obs_names
                        var_names = self.var_names
                        for names, names_name in (obs_names, 'obs_names'), \
                                (var_names, 'var_names'):
                            null_count = names.null_count()
                            if null_count:
                                error_message = (
                                    f'{names_name} contains {null_count:,} '
                                    f'null values, but must not contain any '
                                    f'when saving to an .h5Seurat file')
                                raise ValueError(error_message)
                        # Add top-level metadata and required groups/datasets
                        hdf5_file.attrs['active.assay'] = assay
                        hdf5_file.attrs['project'] = 'SeuratProject'
                        hdf5_file.attrs['version'] = '5.0.0'
                        for required_group in 'commands', 'images', 'tools':
                            hdf5_file.create_group(required_group)
                        active_ident = hdf5_file.create_group('active.ident')
                        active_ident.create_dataset('levels', data=[b'local'])
                        active_ident.create_dataset(
                            'values', data=np.ones(len(self._obs),
                                                   dtype=np.int32))
                        hdf5_file.create_dataset('cell.names',
                                                 data=obs_names.to_numpy())
                        active_assay = \
                            hdf5_file.create_group(f'assays/{assay}')
                        active_assay.attrs['key'] = f'{assay.lower()}_'
                        active_assay.create_dataset(
                            'features', data=var_names.to_numpy())
                        active_assay.create_group('misc')
                        # Save `obs` and `var`
                        SingleCell._save_h5Seurat_dataframe(
                            hdf5_file, self._obs, 'meta.data',
                            preserve_strings)
                        SingleCell._save_h5Seurat_dataframe(
                            hdf5_file, self._var,
                            f'assays/{assay}/meta.features', preserve_strings)
                        # Save `obsm`
                        reductions = hdf5_file.create_group('reductions')
                        if self._obsm:
                            for key, value in self._obsm.items():
                                if isinstance(value, pl.DataFrame):
                                    continue
                                group = reductions.create_group(key)
                                group.attrs['active.assay'] = assay
                                group.attrs['global'] = np.int32(0)
                                group.attrs['key'] = f'{key.upper()}_'
                                # noinspection PyUnresolvedReferences
                                group.create_dataset('cell.embeddings',
                                                     data=value.to_numpy().T)
                        # Save `obsp`
                        graphs = hdf5_file.create_group('graphs')
                        if self._obsp:
                            for key, value in self._obsp.items():
                                if isinstance(value, csc_array):
                                    value = value.tocsr()
                                group = graphs.greate_group(key)
                                group.attrs['dims'] = value.shape[::-1]
                                group.attrs['active.assay'] = assay
                                group.create_dataset('data',
                                                     data=value.data)
                                group.create_dataset('indices',
                                                     data=value.indices)
                                group.create_dataset('indptr',
                                                     data=value.indptr)
                        # Save `uns`
                        misc = hdf5_file.create_group('misc')
                        if self._uns:
                            SingleCell._save_h5Seurat_uns(
                                self._uns, misc, hdf5_file)
                        # Save `X`
                        X = active_assay.create_group(X_key)
                        if isinstance(X, csc_array):
                            X = value.tocsr()
                        X.attrs['dims'] = self._X.shape[::-1]
                        X.create_dataset('data', data=self._X.data)
                        X.create_dataset('indices', data=self._X.indices)
                        X.create_dataset('indptr', data=self._X.indptr)
                    else:  # `.h5`
                        obs_columns = ['barcodes']
                        var_columns = ['feature_type', 'genome', 'id', 'name']
                        for columns, df, df_name in \
                                (obs_columns, self._obs, 'obs'), \
                                (var_columns, self._var, 'var'):
                            for column in columns:
                                if column not in df:
                                    error_message = (
                                        f'{column!r} was not found in '
                                        f'{df_name}, but is a required column '
                                        f'when saving to a 10x .h5 file')
                                    raise ValueError(error_message)
                                check_dtype(df[column],
                                            f'{df_name}[{column!r}]',
                                            (pl.String, pl.Categorical,
                                             pl.Enum))
                        all_tag_keys = ['genome']
                        for column in 'pattern', 'read', 'sequence':
                            if column in self._var:
                                check_dtype(self._var[column],
                                            f'var[{column!r}]',
                                            (pl.String, pl.Categorical,
                                             pl.Enum))
                                var_columns.append(column)
                                all_tag_keys.append(column)
                        matrix = hdf5_file.create_group('matrix')
                        matrix.create_dataset('barcodes',
                                              data=self._obs[:, 0].to_numpy())
                        matrix.create_dataset('data', data=self._X.data)
                        features = matrix.create_group('features')
                        matrix.create_dataset('indices', data=self._X.indices)
                        matrix.create_dataset('indptr', data=self._X.indptr)
                        matrix.create_dataset('shape',
                                              data=self._X.shape[::-1])
                        features.create_dataset('_all_tag_keys',
                                                data=all_tag_keys)
                        for column in var_columns:
                            features.create_dataset(
                                column, data=self._var[column].to_numpy())
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
        elif is_mtx:
            from scipy.io import mmwrite
            barcode_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'barcodes.tsv.gz')
            feature_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'features.tsv.gz')
            if not overwrite:
                for ancillary_filename in barcode_filename, feature_filename:
                    if os.path.exists(ancillary_filename):
                        error_message = (
                            f'{ancillary_filename!r} already exists; set '
                            f'overwrite=True to overwrite')
                        raise FileExistsError(error_message)
            try:
                mmwrite(filename_expanduser, self._X.T)
                self._obs.write_csv(barcode_filename, include_header=False)
                self._var.write_csv(feature_filename, include_header=False)
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                if os.path.exists(barcode_filename):
                    os.unlink(barcode_filename)
                if os.path.exists(feature_filename):
                    os.unlink(feature_filename)
                raise
        else:
            from ryp import r
            if preserve_strings:
                sc = self
            else:
                # Convert string columns with duplicate values to Enum
                enumify = lambda df: df.cast({
                    row[0]: pl.Enum(row[1]) for row in df
                    .select(pl.selectors.string()
                            .unique(maintain_order=True)
                            .implode()
                            .list.drop_nulls())
                    .unpivot()
                    .filter(pl.col.value.list.len() == len(df))
                    .rows()})
                sc = SingleCell(X=self._X, obs=enumify(self._obs),
                                var=enumify(self._var), obsm=self._obsm,
                                uns=self._uns)
            if sce:
                sc.to_sce('.SingleCell.object')
            else:
                sc.to_seurat('.SingleCell.object')
            try:
                r(f'saveRDS(.SingleCell.object, {filename_expanduser!r})')
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
            finally:
                r('rm(.SingleCell.object)')
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],
                    column: SingleCellColumn,
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                            ...],
                    *,
                    QC_column: pl.Series | None = None,
                    allow_missing: bool = False,
                    allow_null: bool = False,
                    custom_error: str | None = None) -> pl.Series | None:
        """
        Get a column of the same length as `obs`/`var`, or `None` if the column
        is missing from `obs`/`var` and `allow_missing=True`.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of `obs`/`var`, a polars
                    expression that evaluates to a single column when applied
                    to `obs`/`var`, a polars Series or 1D NumPy array of the
                    same length as `obs`/`var`, or a function that takes in
                    `self` and returns a polars Series or 1D NumPy array of the
                    same length as `obs`/`var`
            variable_name: the name of the variable corresponding to `column`
            dtypes: the required dtype(s) of the column
            QC_column: an optional column of cells passing QC. If specified,
                       the presence of `null` values will only raise an error
                       for cells passing QC. Has no effect when
                       `allow_null=True`.
            allow_missing: whether to allow `column` to be a string missing
                           from `obs`/`var`, returning `None` in this case
            allow_null: whether to allow `column` to contain `null` values
            custom_error: a custom error message for when `column` is a string
                          and is not found in `obs`/`var`, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A polars Series of the same length as `obs`/`var`, or `None` if the
            column is missing from `obs`/`var` and `allow_missing=True`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if isinstance(column, str):
            if column in obs_or_var:
                column = obs_or_var[column]
            elif allow_missing:
                return None
            else:
                error_message = (
                    f'{variable_name} {column!r} is not a column of '
                    f'{obs_or_var_name}' if custom_error is None else
                    custom_error.format(f'{column!r}'))
                raise ValueError(error_message)
            variable_name = f'{variable_name} {column!r}'
        elif isinstance(column, pl.Expr):
            column = obs_or_var.select(column)
            if column.width > 1:
                error_message = (
                    f'{variable_name} is a polars expression that expands to '
                    f'{column.width:,} columns rather than 1')
                raise ValueError(error_message)
            column = column.to_series()
        elif isinstance(column, pl.Series):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a polars Series of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        elif isinstance(column, np.ndarray):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a NumPy array of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
            column = pl.Series(variable_name, column)
        elif callable(column):
            column = column(self)
            if isinstance(column, np.ndarray):
                if column.ndim != 1:
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'{column.ndim:,}D NumPy array, but must return a '
                        f'polars Series or 1D NumPy array')
                    raise ValueError(error_message)
                column = pl.Series(variable_name, column)
            elif not isinstance(column, pl.Series):
                error_message = (
                    f'{variable_name} is a function that returns a variable '
                    f'of type {type(column).__name__}, but must return a '
                    f'polars Series or 1D NumPy array')
                raise TypeError(error_message)
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a function that returns a column of '
                    f'length {len(column):,}, which differs from the length '
                    f'of {obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression, a polars Series, a 1D NumPy array, or a '
                f'function that returns a polars Series or 1D NumPy array '
                f'when applied to this SingleCell dataset, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        check_dtype(column, variable_name, dtypes)
        if not allow_null:
            if QC_column is None:
                null_count = column.null_count()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)}, but must not '
                        f'contain any')
                    raise ValueError(error_message)
            else:
                null_count = (column.is_null() & QC_column).sum()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cells '
                        f'passing QC, but must not contain any')
                    raise ValueError(error_message)
        return column
    
    @staticmethod
    def _get_columns(obs_or_var_name: Literal['obs', 'var'],
                     datasets: Sequence[SingleCell],
                     columns: SingleCellColumn | None |
                              Sequence[SingleCellColumn | None],
                     variable_name: str,
                     dtypes: pl.datatypes.classes.DataTypeClass | str |
                             tuple[pl.datatypes.classes.DataTypeClass | str,
                                   ...],
                     *,
                     QC_columns: list[pl.Series | None] = None,
                     allow_None: bool = True,
                     allow_missing: bool = False,
                     allow_null: bool = False,
                     custom_error: str | None = None) -> \
            list[pl.Series | None]:
        """
        Get a column of the same length as `obs`/`var` from each dataset.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            datasets: a sequence of SingleCell datasets
            columns: a string naming a column of `obs`/`var`, a polars
                     expression that evaluates to a single column when applied
                     to `obs`/`var`, a polars Series or 1D NumPy array of the
                     same length as `obs`/`var`, or a function that takes in
                     `self` and returns a polars Series or 1D NumPy array of
                     the same length as `obs`/`var`. Or, a Sequence of these,
                     one per dataset in `datasets`. May also be `None` (or a
                     Sequence containing `None`) if `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the columns
            QC_columns: an optional column of cells passing QC for each
                        dataset. If not `None` for a given dataset, the
                        presence of `null` values for that dataset will only
                        raise an error for cells passing QC. Has no effect when
                        `allow_null=True`.
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_missing: whether to allow `columns` to be a string (or
                           contain strings) missing from certain datasets'
                           `obs`/`var`, returning `None` for these datasets
            allow_null: whether to allow `columns` to contain `null` values
            custom_error: a custom error message for when `column` is a string
                          and is not found in `obs`/`var`, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A list of polars Series of the same length as `datasets`, where
            each Series has the same length as the corresponding dataset's
            `obs`/`var`. Or, if `columns` is `None` (or if some elements are
            `None`) or missing from `obs`/`var` (when `allow_missing=True`), a
            list of `None` (or where the corresponding elements are `None`).
        """
        if columns is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return [None] * len(datasets)
        if isinstance(columns, Sequence) and not isinstance(columns, str):
            if len(columns) != len(datasets):
                error_message = (
                    f'{variable_name} has length {len(columns):,}, but you '
                    f'specified {len(datasets):,} datasets')
                raise ValueError(error_message)
            if not allow_None and any(column is None for column in columns):
                error_message = \
                    f'{variable_name} contains an element that is None'
                raise TypeError(error_message)
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error)
                    if column is not None else None
                    for dataset, column in zip(datasets, columns)]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    if column is not None else None
                    for dataset, column, QC_column in
                    zip(datasets, columns, QC_columns)]
        else:
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error) for dataset in datasets]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    for dataset, QC_column in zip(datasets, QC_columns)]
    
    @staticmethod
    def _describe_column(column_name: str, column: SingleCellColumn):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
    
        Returns:
            The column's description: just the argument's name unless the value
            is a string (i.e. the column's name in `obs` or `var`), in which
            case also include the value.
        """
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
    
    # noinspection PyUnresolvedReferences
    def to_anndata(self, *, QC_column: str | None = 'passed_QC') -> 'AnnData':
        """
        Converts this SingleCell dataset to an AnnData object.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the AnnData object, set `QC_column` to `None`.
        
        Note that there is no `from_anndata()`; simply do
        `SingleCell(anndata_object)` to initialize a SingleCell dataset from an
        in-memory AnnData object.
        
        Args:
            QC_column: if not `None`, give an error if this column is present
                       in `obs` and not all cells pass QC
        
        Returns:
            An AnnData object. For AnnData versions older than 0.11.0, which
            do not support `csr_array`/`csc_array`, counts will be converted to
            `csr_matrix`/`csc_matrix`.
        """
        if self._X is None:
            error_message = \
                'X is None, so converting to an AnnData object is not possible'
            raise TypeError(error_message)
        with ignore_sigint():
            import anndata
            import pandas as pd
            import pyarrow as pa
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported by '
                        f'AnnData')
                    raise TypeError(error_message)
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the AnnData object')
                    raise ValueError(error_message)
        type_mapping = {
            pa.int8(): pd.Int8Dtype(), pa.int16(): pd.Int16Dtype(),
            pa.int32(): pd.Int32Dtype(), pa.int64(): pd.Int64Dtype(),
            pa.uint8(): pd.UInt8Dtype(), pa.uint16(): pd.UInt16Dtype(),
            pa.uint32(): pd.UInt32Dtype(), pa.uint64(): pd.UInt64Dtype(),
            pa.string(): pd.StringDtype(storage='pyarrow'),
            pa.bool_(): pd.BooleanDtype()}
        to_pandas = lambda df: df\
            .to_pandas(split_blocks=True, types_mapper=type_mapping.get)\
            .set_index(df.columns[0])
        return anndata.AnnData(
            X=self._X if version.parse(anndata.__version__) >=
                         version.parse('0.11.0') else
              sparse.csr_matrix(self._X) if isinstance(self._X, csr_array)
              else sparse.csc_matrix(self._X),
            obs=to_pandas(self._obs), var=to_pandas(self._var),
            obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    @staticmethod
    def _from_seurat(seurat_object_name: str,
                     *,
                     assay: str | None,
                     slot: str,
                     slot_name: str) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], dict[str, csc_array],
            NestedScalarOrArrayDict]:
        """
        Create a SingleCell dataset from an in-memory Seurat object loaded with
        the ryp Python-R bridge. Used by `__init__()` and `from_seurat()`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to the Seurat object's
                   `active.assay` attribute (usually `'RNA'`)
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as `X`. Set to
                  `'data'` to load the normalized counts, or `'scale.data'` to
                  load the normalized and scaled counts, if available. If
                  dense, will be automatically converted to a sparse array.
            slot_name: the name of the variable passed via the `slot` argument
        
        Returns:
            A length-5 tuple of (`X`, `obs`, `var`, `obsm`, `obsp`, `uns`).
        """
        from ryp import r, to_py
        if assay is None:
            assay = to_py(f'{seurat_object_name}@active.assay')
        elif to_py(f'{seurat_object_name}@{assay}') is None:
            error_message = (
                f'assay {assay!r} does not exist in '
                f'{seurat_object_name}@assays; specify a different assay than '
                f'{assay!r}')
            raise ValueError(error_message)
        assay_slot = f'{seurat_object_name}@assays${assay}'
        # If Seurat v5, merge layers if necessary, and use `$slot` instead of
        # `@slot` for `X` and `meta.data` instead of `meta.features` for `var`
        v5 = to_py(f'inherits({assay_slot}, "Assay5")')
        if v5:
            r(f'.SingleCell.layers = names({assay_slot}@layers)')
            try:
                r(f'.SingleCell.layers = '
                  f'.SingleCell.layers[grep("^{slot}", .SingleCell.layers)]')
                num_matching_layers = to_py('length(.SingleCell.layers)')
                if num_matching_layers == 0:
                    error_message = (
                        f'none of the layers in {assay_slot}@layers has a '
                        f'name starting with {slot_name} {slot!r}; specify a '
                        f'different assay than {assay!r} or a different '
                        f'{slot_name} than {slot!r}')
                    raise ValueError(error_message)
                elif num_matching_layers > 1:
                    r(f'{assay_slot} = JoinLayers('
                      f'{assay_slot}, layers=.SingleCell.layers)')
            finally:
                r('rm(".SingleCell.layers")')
            gene_names = to_py(f'{assay_slot}@features[[{slot!r}]]')\
                .rename('gene')
            var = to_py(f'{assay_slot}@meta.data', index=False)\
                .select(gene_names, pl.all())
            X_slot = f'{assay_slot}@layers${slot}'
        else:
            # unlike v5 objects, v3 objects indicate the absence of a slot with
            # a 0 x 0 matrix
            if not (to_py(f'"{slot}" %in% slotNames({assay_slot})') and
                    to_py(f'prod(dim({assay_slot}@{slot}))') > 0):
                error_message = (
                    f'{slot_name} {slot!r} does not exist in {assay_slot}; '
                    f'specify a different assay than {assay!r} or a different '
                    f'{slot_name} than {slot!r}')
                raise ValueError(error_message)
            X_slot = f'{assay_slot}@{slot}'
            var = to_py(f'{assay_slot}@meta.features')
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{slot_name} {slot!r} exists in {assay_slot} but is not a '
                f'dgCMatrix (column-oriented sparse matrix) or matrix, '
                f'instead having ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; specify a different assay than {assay!r} or a different '
                f'{slot_name} than {slot!r}')
            raise TypeError(error_message)
        obs_key = f'{seurat_object_name}@meta.data'
        obs = to_py(obs_key, index='_index' if to_py(
            f'"cell" %in% {obs_key}') else 'cell')
        if var is None:
            var = to_py(f'rownames({assay_slot}@{slot})').to_frame('gene')
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        reduction_names = to_py(f'names({seurat_object_name}@reductions)')
        obsm = {reduction_name: to_py(f'{seurat_object_name}@reductions$'
                                      f'{reduction_name}@cell.embeddings',
                                      format='numpy')
                for reduction_name in reduction_names
                if not to_py(f'is.null({seurat_object_name}@reductions$'
                             f'{reduction_name})') and
                to_py(f'{seurat_object_name}@reductions${reduction_name}'
                      f'@assay.used') == assay} \
            if reduction_names is not None else {}
        graph_names = to_py(f'names({seurat_object_name}@graphs)')
        obsp = {graph_name: csc_array((
            to_py(f'{seurat_object_name}${graph_name}@x', format='numpy'),
            to_py(f'{seurat_object_name}${graph_name}@i', format='numpy'),
            to_py(f'{seurat_object_name}${graph_name}@p', format='numpy')),
            shape=to_py(f'{seurat_object_name}${graph_name}@Dim',
                        format='numpy'))
            for graph_name in graph_names
            if to_py(f'length({seurat_object_name}${graph_name}@'
                     f'assay.used)') == 0 or
               to_py(f'{seurat_object_name}${graph_name}@assay.used') == assay
        } if graph_names is not None else {}
        uns = to_py(f'{seurat_object_name}@misc')
        if not uns:
            # uns may be an empty unnamed list in R, which ryp converts to a
            # Python list rather than a dictionary
            uns = {}
        return X, obs, var, obsm, obsp, uns
    
    @staticmethod
    def from_seurat(seurat_object_name: str, *,
                    assay: str | None = None,
                    slot: str = 'counts') -> SingleCell:
        """
        Create a SingleCell dataset from a Seurat object that has already been
        loaded into memory via the ryp Python-R bridge. To load a Seurat object
        from disk, use e.g. `SingleCell('filename.rds')`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to the Seurat object's
                   `active.assay` attribute (usually `'RNA'`)
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as `X`. Defaults
                  to `'counts'`. Set to `'data'` to load the normalized counts,
                  or `'scale.data'` to load the normalized and scaled counts,
                  if available. If dense, will be automatically converted to a
                  sparse array.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import to_py
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if assay is not None:
            check_type(assay, 'assay', str, 'a string')
        check_type(slot, 'slot', str, 'a string')
        if not to_py(f'inherits({seurat_object_name}, "Seurat")'):
            classes = to_py(f'class({seurat_object_name})', squeeze=False)
            error_message = (
                f'the R object named by seurat_object_name, '
                f'{seurat_object_name}, must be a Seurat object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, obsp, uns = \
            SingleCell._from_seurat(seurat_object_name, assay=assay, slot=slot,
                                    slot_name='slot')
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, obsp=obsp, uns=uns)
    
    def to_seurat(self,
                  seurat_object_name: str,
                  *,
                  QC_column: str | None = 'passed_QC',
                  assay: str = 'RNA',
                  X_key: str = 'counts') -> None:
        """
        Convert this SingleCell dataset to a Seurat object (version 3, not
        version 5) in the R workspace of the ryp Python-R bridge.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the Seurat object, set `QC_column` to `None`.
        
        When converting to Seurat, to match the requirements of Seurat objects,
        the `'X_'` prefix (often used by Scanpy) will be removed from each key
        of `obsm` where it is present (e.g. `'X_umap'` will become `'umap'`).
        Seurat will also add `'orig.ident'`, `'nCount_RNA'` and
        `'nFeature_RNA'` as gene-level metadata by default; you can disable
        the calculation of the latter two columns with:
        
        ```python
        from ryp import r
        r('options(Seurat.object.assay.calcn = FALSE)')
        ```
        
        varm` and DataFrame keys of `obsm` will not be converted.
        
        Args:
            seurat_object_name: the name of the R variable to assign the Seurat
                                object to
            QC_column: if not `None`, give an error if this column is present
                       in `obs` and not all cells pass QC
            assay: the name to use for the active assay
            X_key: the name of the slot within the active assay to save `X` to;
                   must be `'counts'` or `'data'`
        """
        if self._X is None:
            error_message = \
                'X is None, so converting to a Seurat object is not possible'
            raise TypeError(error_message)
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SeuratObject))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the Seurat object')
                    raise ValueError(error_message)
        check_type(assay, 'assay', str, 'a string')
        check_type(X_key, 'X_key', str, 'a string')
        if X_key != 'counts' and X_key != 'data':
            error_message = (
                f"when converting to a Seurat object, X_key must be 'counts' "
                f"or 'data', not {X_key!r}")
            raise ValueError(error_message)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a Seurat object')
                    raise TypeError(error_message)
        obs_names = self.obs_names
        var_names = self.var_names
        for names, names_name in (obs_names, 'obs_names'), \
                (var_names, 'var_names'):
            null_count = names.null_count()
            if null_count:
                error_message = (
                    f'{names_name} contains {null_count:,} null values, but '
                    f'must not contain any when converting to a Seurat object')
                raise ValueError(error_message)
        is_string = var_names.dtype == pl.String
        num_with_underscores = var_names.str.contains('_').sum() \
            if is_string else \
            var_names.cat.get_categories().str.contains('_').sum()
        if num_with_underscores:
            var_names_expression = f'pl.col.{var_names.name}' \
                if var_names.name.isidentifier() else \
                f'pl.col({var_names.name!r})'
            error_message = (
                f"var_names contains {num_with_underscores:,}"
                f"{'' if is_string else ' unique'} gene "
                f"{plural('name', num_with_underscores)} with "
                f"underscores, which are not supported by Seurat; Seurat "
                f"recommends changing the underscores to dashes, which you "
                f"can do with .with_columns_var({var_names_expression}"
                f"{'' if is_string else '.cast(pl.String)'}"
                f".str.replace_all('_', '-'))")
            raise ValueError(error_message)
        try:
            to_r(self._X.T, '.SingleCell.X.T', rownames=var_names,
                 colnames=obs_names)
            try:
                to_r(self._obs.drop(obs_names.name), '.SingleCell.obs',
                     rownames=obs_names)
                try:
                    r(f'{seurat_object_name} = CreateSeuratObject('
                      f'CreateAssayObject({X_key}=.SingleCell.X.T), '
                      f'meta.data=.SingleCell.obs, assay={assay})')
                    # Reverse the column name-mangling introduced by Seurat
                    r(f'.SingleCell.cols_to_ignore = '
                      f'c("orig.ident", "nCount_{assay}", "nFeature_{assay}")')
                    try:
                        r(f'names({seurat_object_name}@meta.data)['
                          f'!names({seurat_object_name}@meta.data) %in% '
                          f'.SingleCell.cols_to_ignore] = '
                          f'names(.SingleCell.obs)[!names(.SingleCell.obs) '
                          f'%in% .SingleCell.cols_to_ignore]')
                    finally:
                        r('rm(.SingleCell.cols_to_ignore)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            to_r(self._var.drop(var_names.name), '.SingleCell.var',
                 rownames=var_names)
            try:
                r(f'{seurat_object_name}@assays${assay}@meta.features = '
                  f'.SingleCell.var')
            finally:
                r('rm(.SingleCell.var)')
            if self._obsm:
                for original_key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    # Remove the initial X_ from the reduction name and suffix
                    # with `'_'` when creating the key, like
                    # mojaveazure.github.io/seurat-disk/reference/Convert.html
                    key = original_key.removeprefix('X_')
                    to_r(value, '.SingleCell.value', rownames=obs_names,
                         colnames=[f'{key}_{i}'
                                   for i in range(1, value.shape[1] + 1)])
                    try:
                        if value.dtype == np.int64 and \
                                to_py('class(.SingleCell.value)') == \
                                'integer64':
                            error_message = (
                                f'obsm[{original_key}] contains integers '
                                f'greater than INT32_MAX (2,147,483,647) '
                                f'or less than INT32_MIN - 1 '
                                f'(-2,147,483,648), which are not allowed '
                                f'when converting to a Seurat object')
                            raise ValueError(error_message)
                        r(f'{seurat_object_name}@reductions${key} = '
                          f'CreateDimReducObject(.SingleCell.value, '
                          f'key="{key}_", assay="{assay}")')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._obsp:
                to_r(self._obsp, '.SingleCell.obsp')
                try:
                    r(f'{seurat_object_name}@graphs = '
                      f'lapply(as.Graph, .SingleCell.obsp)')
                finally:
                    r('rm(.SingleCell.obsp)')
            if self._uns:
                to_r(self._uns, '.SingleCell.uns')
                try:
                    r(f'{seurat_object_name}@misc = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({seurat_object_name!r})'):
                r(f'rm({seurat_object_name})')
            raise
    
    @staticmethod
    def _get_DFrame(dframe: str, *, index: str) -> pl.DataFrame:
        """
        Convert a DFrame in the ryp R workspace to a Python DataFrame, raising
        an error if the DFrame contains any nested data stuctures.
        
        Args:
            dframe: the name of the DFrame object in the ryp R workspace
            index: the name of the column containing the rownames in the output
                   DataFrame; if this column name is already present in the
                   DFrame, fall back to calling the rownames column `'_index'`

        Returns:
            A polars DataFrame containing the data in `dframe`.
        """
        from ryp import to_py
        df = to_py(f'{dframe}@listData', index=False)
        if not all(isinstance(value, pl.Series) for value in df.values()):
            error_message = (
                f'{dframe} contains nested data; unnest before converting to '
                f'a SingleCell dataset')
            raise ValueError(error_message)
        if index in df.keys():
            index = '_index'
        df = pl.DataFrame({index: to_py(f'{dframe}@rownames')} | df)
        return df
    
    @staticmethod
    def _from_sce(sce_object_name: str,
                  *,
                  slot: str,
                  slot_name: str) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], NestedScalarOrArrayDict]:
        """
        Create a SingleCell dataset from an in-memory SingleCellExperiment
        object loaded with the ryp Python-R bridge. Used by `__init__()` and
        `from_sce()`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `sce_object@assays@data` to use as `X`.
                  Set to `'counts'` to load raw counts, or `'logcounts'` to
                  load the normalized counts if available. If dense, will be
                  automatically converted to a sparse array.
            slot_name: the name of the variable passed via the `slot` argument
        
        Returns:
            A length-5 tuple of (`X`, `obs`, `var`, `obsm`, `uns`).
        """
        from ryp import to_py
        X_slot = f'{sce_object_name}@assays@data${slot}'
        if not to_py(f'"{slot}" %in% names({sce_object_name}@assays@data)'):
            error_message = (
                f'{slot_name} {slot!r} does not exist in '
                f'{sce_object_name}@assays@data${slot}; specify a different '
                f'{slot_name} than {slot!r}')
            raise ValueError(error_message)
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{slot_name} {slot!r} exists in '
                f'{sce_object_name}@assays@data${slot} but is not a dgCMatrix '
                f'(column-oriented sparse matrix) or matrix, instead having ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; specify a different {slot_name} than {slot!r}')
            raise TypeError(error_message)
        obs = SingleCell._get_DFrame(f'colData({sce_object_name})',
                                     index='cell')
        var = SingleCell._get_DFrame(f'rowData({sce_object_name})',
                                     index='gene')
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        obsm = to_py(f'reducedDims({sce_object_name})@listData',
                     format='numpy')
        uns = to_py(f'{sce_object_name}@metadata', format='numpy')
        return X, obs, var, obsm, uns
    
    @staticmethod
    def from_sce(sce_object_name: str,
                 *,
                 slot: str = 'counts') -> SingleCell:
        """
        Create a SingleCell dataset from a SingleCellExperiment object that has
        already been loaded into memory via the ryp Python-R bridge. To load a
        SingleCellExperiment object from disk, use e.g.
        `SingleCell('filename.rds')`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `{sce_object_name}@assays@data` to use as
                  `X`. Defaults to `'counts'`. If available, set to
                  `'logcounts'` to load the normalized counts. If dense, will
                  be automatically converted to a sparse array.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import r, to_py
        r('suppressPackageStartupMessages(library(SingleCellExperiment))')
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        check_type(slot, 'slot', str, 'a string')
        if not to_py(f'inherits({sce_object_name}, "SingleCellExperiment")'):
            classes = to_py(f'class({sce_object_name})', squeeze=False)
            error_message = (
                f'the R object named by sce_object_name, {sce_object_name}, '
                f'must be a SingleCellExperiment object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, uns = \
            SingleCell._from_sce(sce_object_name, slot=slot, slot_name='slot')
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, uns=uns)
        
    def to_sce(self,
               sce_object_name: str,
               *,
               QC_column: str | None = 'passed_QC',
               X_key: str = 'counts') -> None:
        """
        Convert this SingleCell dataset to a SingleCellExperiment object in the
        R workspace of the ryp Python-R bridge.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the SingleCellExperiment object, set `QC_column` to
        `None`.
        
        `varm` and DataFrame keys of `obsm` will not be converted.
        
        Args:
            sce_object_name: the name of the R variable to assign the
                             SingleCellExperiment object to
            QC_column: if not `None`, give an error if this column is present
                       in `obs` and not all cells pass QC
            X_key: the name of the slot within `@assays@data` to save `X` to
        """
        if self._X is None:
            error_message = (
                'X is None, so converting to a SingleCellExperiment object is '
                'not possible')
            raise TypeError(error_message)
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SingleCellExperiment))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the SingleCellExperiment object')
                    raise ValueError(error_message)
        check_type(X_key, 'X_key', str, 'a string')
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a SingleCellExperiment object')
                    raise TypeError(error_message)
        try:
            obs_names = self.obs_names
            var_names = self.var_names
            for names, names_name in (obs_names, 'obs_names'), \
                    (var_names, 'var_names'):
                null_count = names.null_count()
                if null_count:
                    error_message = (
                        f'{names_name} contains {null_count:,} null values, '
                        f'but must not contain any when converting to a '
                        f'SingleCellExperiment object')
                    raise ValueError(error_message)
            to_r(self._X.T, '.SingleCell.X.T', rownames=var_names,
                 colnames=obs_names)
            try:
                to_r(self._obs.drop(obs_names.name), '.SingleCell.obs',
                     rownames=obs_names)
                try:
                    to_r(self._var.drop(var_names.name),
                         '.SingleCell.var', rownames=var_names)
                    try:
                        r(f'{sce_object_name} = SingleCellExperiment('
                          f'assays = list({X_key} = .SingleCell.X.T), '
                          f'colData = S4Vectors::DataFrame('
                          f'    .SingleCell.obs, check.names=FALSE), '
                          f'rowData = S4Vectors::DataFrame('
                          f'    .SingleCell.var, check.names=FALSE))')
                    finally:
                        r('rm(.SingleCell.var)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            if self._obsm:
                for key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    to_r(value, '.SingleCell.value', rownames=obs_names,
                             colnames=[f'{key}_{i}'
                                       for i in range(1, value.shape[1] + 1)])
                    try:
                        r(f'reducedDim({sce_object_name}, {key!r}) = '
                          f'.SingleCell.value')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._uns:
                to_r(self._uns, '.SingleCell.uns')
                try:
                    r(f'{sce_object_name}@metadata = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({sce_object_name!r})'):
                r(f'rm({sce_object_name})')
            raise
        
    def copy(self, deep: bool = False) -> SingleCell:
        """
        Make a copy of this SingleCell dataset.
        
        Args:
            deep: whether to perform a deep or shallow copy. Since polars
                  DataFrames are immutable, `obs` and `var` will always point
                  to the same underlying data as the original. The difference
                  when `deep=True` is that `X` and any NumPy arrays in `obsm`,
                  `varm`, `obsp`, `varp`, and `uns` will point to fresh copies
                  of the underlying data, instead of the same data as the
                  original SingleCell dataset. When `deep=False`, any
                  modifications to these NumPy arrays will modify both copies!

        Returns:
            A copy of the SingleCell dataset.
        """
        check_type(deep, 'deep', bool, 'Boolean')
        if deep:
            X = self._X.copy() if self._X is not None else None
            obsm = {key: value if isinstance(value, pl.DataFrame) else
                         value.copy() for key, value in self._obsm.items()}
            varm = {key: value if isinstance(value, pl.DataFrame) else
                         value.copy() for key, value in self._varm.items()}
            obsp = {key: value.copy() for key, value in self._obsp.items()}
            varp = {key: value.copy() for key, value in self._varp.items()}
            uns = SingleCell._copy_uns(self._uns, deep=True)
        else:
            X = self._X
            obsm = self._obsm.copy()
            varm = self._varm.copy()
            obsp = self._obsp.copy()
            varp = self._varp.copy()
            uns = SingleCell._copy_uns(self._uns)
        # noinspection PyTypeChecker
        return SingleCell(X=X, obs=self._obs, var=self._var, obsm=obsm,
                          varm=varm, obsp=obsp, varp=varp, uns=uns)
    
    def concat_obs(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False) -> SingleCell:
        """
        Concatenate one or more other SingleCell datasets with this one,
        cell-wise.
        
        By default, all datasets must have the same `var`, `varm`, `varp`, and
        `uns`. They must also have the same columns in `obs` and the same keys
        in `obsm` and `obsp`, with the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of `var`, i.e. the `var_names`) before
        concatenating. Subset to columns of `var` and keys of `varm`, `varp`,
        and `uns` that are identical in all datasets after this subsetting.
        Also, subset to columns of `obs` and keys of `obsm` and `obsp` that are
        present in all datasets, and have the same data types. All datasets'
        `obs_names` must have the same name and data type, and similarly for
        their `var_names`.
        
        The one exception to the `obs` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `obs`.
        
        If the datasets' `X` are a mix of CSR and CSC sparse arrays, they will
        all be coerced to CSR, and similarly for each key of `obsp` and `varp`.
        If all datasets' `X` are `None`, they can also be concatenated.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes, columns of `obs` and `var`,
                      and keys of `obsm`, `varm` and `uns` common to all
                      datasets before concatenating, rather than raising an
                      error on any mismatches
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        if self._X is not None:
            if all(dataset._X is not None for dataset in datasets):
                X_present = True
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        else:
            if all(dataset._X is None for dataset in datasets):
                X_present = False
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that `obs_names` and `var_names` have the same name and
            # data type across all datasets
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            # Subset to genes in common across all datasets
            genes_in_common = self.var_names\
                .filter(self.var_names
                        .is_in(pl.concat([dataset.var_names
                                          for dataset in datasets[1:]])))
            if len(genes_in_common) == 0:
                error_message = \
                    'no genes are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[:, genes_in_common] for dataset in datasets]
            # Subset to columns of `var` and keys of `varm`, `varp`, and `uns`
            # that are identical in all datasets after this subsetting
            var_columns_in_common = [
                column.name for column in datasets[0]._var[:, 1:]
                if all(column.name in dataset._var and
                       dataset._var[column.name].equals(column)
                       for dataset in datasets[1:])]
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in self._varm.items()
                if all(key in dataset._varm and
                       type(value) is type(dataset._varm[key]) and
                       (dataset._varm[key].dtype == value.dtype and
                        array_equal(dataset._varm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].equals(value))
                       for dataset in datasets[1:])]
            varp_keys_in_common = [
                key for key, value in self._varp.items()
                if all(key in dataset._varp and
                       dataset._varp[key].dtype == value.dtype and
                       sparse_equal(dataset._varp[key], value)
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and dataset._uns[key] == value
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._var = dataset._var.select(dataset.var_names,
                                                   *var_columns_in_common)
                dataset._varm = {key: dataset._varm[key]
                                 for key in varm_keys_in_common}
                dataset._varp = {key: dataset._varp[key]
                                 for key in varp_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            # Subset to columns of `obs` and keys of `obsm` and `obsp` that are
            # present in all datasets, and have the same data types. Also
            # include columns of `obs` that are Enum in some datasets and
            # Categorical in others, or Enum in all datasets but with different
            # categories in each dataset; cast these to Categorical.
            obs_mismatched_categoricals = {
                column for column, dtype in self._obs[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._obs and
                       dataset._obs[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._obs[column].dtype == dtype
                           for dataset in datasets[1:])}
            obs_columns_in_common = [
                column
                for column, dtype in islice(self._obs.schema.items(), 1, None)
                if column in obs_mismatched_categoricals or
                   all(column in dataset._obs and
                       dataset._obs[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._obs[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in obs_mismatched_categoricals}
            for dataset in datasets:
                # noinspection PyUnresolvedReferences
                dataset._obs = dataset._obs\
                    .select(dataset.obs_names, *obs_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in self._obsm.items()
                if all(key in dataset._obsm and
                       type(dataset._obsm[key]) is type(value) and
                       (dataset._obsm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            obsp_keys_in_common = [
                key for key, value in self._obsp.items()
                if all(key in dataset._obsp and
                       dataset._obsp[key].dtype == value.dtype and
                       sparse_equal(dataset._obsp[key], value)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obsm = {key: dataset._obsm[key]
                                 for key in obsm_keys_in_common}
                dataset._obsp = {key: dataset._obsp[key]
                                 for key in obsp_keys_in_common}
        else:  # non-flexible
            # Check that all `var`, `varm`, `varp`, and `uns` are identical
            var = self._var
            for dataset in datasets[1:]:
                if not dataset._var.equals(var):
                    error_message = (
                        'all SingleCell datasets must have the same var, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            varm = self._varm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._varm.keys() != varm.keys() or \
                        any(type(dataset._varm[key]) is not type(value) or
                            (dataset._varm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._varm[key].schema != value.schema)
                            for key, value in varm.items()) or not \
                        all(array_equal(dataset._varm[key], value)
                            for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same varm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            varp = self._varp
            for dataset in datasets[1:]:
                if dataset._varp.keys() != varp.keys() or \
                        any(type(dataset._varp[key]) is not type(value) or
                            dataset._varp[key].dtype != value.dtype
                            for key, value in varp.items()) or not \
                        all(sparse_equal(dataset._varp[key], value)
                            for key, value in varp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same varp, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            # Check that all `obs` have the same columns and data types
            schema = self._obs.schema
            for dataset in datasets[1:]:
                if dataset._obs.schema != schema:
                    if dataset._obs.columns != self._obs.columns:
                        error_message = (
                            'all SingleCell datasets must have the same '
                            'columns in obs, unless flexible=True')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            'all SingleCell datasets must have the same data '
                            'type for each column of obs, unless '
                            'flexible=True')
                        raise TypeError(error_message)
            # Check that all `obsm` and `obsp` have the same keys and data
            # types
            obsm = self._obsm
            for dataset in datasets[1:]:
                if dataset._obsm.keys() != obsm.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'obsm, unless flexible=True')
                    raise ValueError(error_message)
                # noinspection PyUnresolvedReferences
                if any(type(dataset._obsm[key]) is not type(value) or
                       (dataset._obsm[key].dtype != value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].schema != value.schema)
                       for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data '
                        'type for each key in obsm, unless flexible=True')
                    raise TypeError(error_message)
            obsp = self._obsp
            for dataset in datasets[1:]:
                if dataset._obsp.keys() != obsp.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'obsp, unless flexible=True')
                    raise ValueError(error_message)
                if any(dataset._obsp[key].dtype != value.dtype
                       for key, value in obsp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data type '
                        'for each key in obsp, unless flexible=True')
                    raise TypeError(error_message)
        # Concatenate; output should be CSR when there's a mix of inputs
        X = sparse.vstack([
            dataset._X for dataset in datasets], 
            format='csr' if any(isinstance(dataset._X, csr_array) 
                                for dataset in datasets) else 'csc') \
            if X_present else None
        obs = pl.concat([dataset._obs for dataset in datasets])
        # noinspection PyTypeChecker
        obsm = {key: np.concatenate([dataset._obsm[key]
                                     for dataset in datasets])
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._obsm[key] for dataset in datasets])
                for key, value in datasets[0]._obsm.items()}
        obsp = {key: sparse.hstack([
            dataset._obsp[key] for dataset in datasets],
            format='csr' if any(isinstance(dataset._obsp[key], csr_array)
                                for dataset in datasets) else 'csc') 
            for key in datasets[0]._obsp}
        return SingleCell(X=X, obs=obs, var=datasets[0]._var, obsm=obsm,
                          varm=datasets[0]._varm, obsp=obsp,
                          varp=datasets[0]._varp, uns=datasets[0]._uns)

    def concat_var(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False) -> SingleCell:
        """
        Concatenate one or more other SingleCell datasets with this one,
        gene-wise. This is much less common than the cell-wise concatenation
        provided by `concat_obs()`.
        
        By default, all datasets must have the same `obs`, `obsm`, `obsp`, and
        `uns`. They must also have the same columns in `var` and the same keys
        in `varm` and `varp`, with the same data types.
        
        Conversely, if `flexible=True`, subset to cells present in all datasets
        (according to the first column of `obs`, i.e. the `obs_names`) before
        concatenating. Subset to columns of `obs` and keys of `obsm`, `obsp`,
        and `uns` that are identical in all datasets after this subsetting.
        Also, subset to columns of `var` and keys of `varm` and `varp` that are
        present in all datasets, and have the same data types. All datasets'
        `obs_names` must have the same name and data type, and similarly for
        their `var_names`.
        
        The one exception to the `var` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `var`.
        
        If the datasets' `X` are a mix of CSR and CSC sparse arrays, they will
        all be coerced to CSR, and similarly for each key of `obsp` and `varp`.
        If all datasets' `X` are `None`, they can also be concatenated.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to cells, columns of `obs` and `var`,
                      and keys of `obsm`, `varm` and `uns` common to all
                      datasets before concatenating, rather than raising an
                      error on any mismatches
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        if self._X is not None:
            if all(dataset._X is not None for dataset in datasets):
                X_present = True
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        else:
            if all(dataset._X is None for dataset in datasets):
                X_present = False
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that `obs_names` and `var_names` have the same name and
            # data type across all datasets
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            # Subset to cells in common across all datasets
            cells_in_common = self.obs_names\
                .filter(self.obs_names
                        .is_in(pl.concat([dataset.obs_names
                                          for dataset in datasets[1:]])))
            if len(cells_in_common) == 0:
                error_message = \
                    'no cells are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[cells_in_common] for dataset in datasets]
            # Subset to columns of `obs` and keys of `obsm`, `obsp`, and `uns`
            # that are identical in all datasets after this subsetting
            obs_columns_in_common = [
                column.name for column in datasets[0]._obs[:, 1:]
                if all(column.name in dataset._obs and
                       dataset._obs[column.name].equals(column)
                       for dataset in datasets[1:])]
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in self._obsm.items()
                if all(key in dataset._obsm and
                       type(value) is type(dataset._obsm[key]) and
                       (dataset._obsm[key].dtype == value.dtype and
                        array_equal(dataset._obsm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].equals(value))
                       for dataset in datasets[1:])]
            obsp_keys_in_common = [
                key for key, value in self._obsp.items()
                if all(key in dataset._obsp and
                       dataset._obsp[key].dtype == value.dtype and
                       sparse_equal(dataset._obsp[key], value)
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and dataset._uns[key] == value
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obs = dataset._obs.select(dataset.obs_names,
                                                   *obs_columns_in_common)
                dataset._obsm = {key: dataset._obsm[key]
                                 for key in obsm_keys_in_common}
                dataset._obsp = {key: dataset._obsp[key]
                                 for key in obsp_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            # Subset to columns of `var` and keys of `varm` and `varp` that are
            # present in all datasets, and have the same data types. Also
            # include columns of `var` that are Enum in some datasets and
            # Categorical in others, or Enum in all datasets but with different
            # categories in each dataset; cast these to Categorical.
            var_mismatched_categoricals = {
                column for column, dtype in self._var[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._var and
                       dataset._var[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._var[column].dtype == dtype
                           for dataset in datasets[1:])}
            var_columns_in_common = [
                column
                for column, dtype in islice(self._var.schema.items(), 1, None)
                if column in var_mismatched_categoricals or
                   all(column in dataset._var and
                       dataset._var[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._var[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in var_mismatched_categoricals}
            for dataset in datasets:
                # noinspection PyUnresolvedReferences
                dataset._var = dataset._var\
                    .select(dataset.var_names, *var_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in self._varm.items()
                if all(key in dataset._varm and
                       type(dataset._varm[key]) is type(value) and
                       (dataset._varm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            varp_keys_in_common = [
                key for key, value in self._varp.items()
                if all(key in dataset._varp and
                       dataset._varp[key].dtype == value.dtype and
                       sparse_equal(dataset._varp[key], value)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._varm = {key: dataset._varm[key]
                                 for key in varm_keys_in_common}
                dataset._varp = {key: dataset._varp[key]
                                 for key in varp_keys_in_common}
        else:  # non-flexible
            # Check that all `obs`, `obsm`, `obsp`, and `uns` are identical
            obs = self._obs
            for dataset in datasets[1:]:
                if not dataset._obs.equals(obs):
                    error_message = (
                        'all SingleCell datasets must have the same obs, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            obsm = self._obsm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._obsm.keys() != obsm.keys() or \
                        any(type(dataset._obsm[key]) is not type(value) or
                            (dataset._obsm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._obsm[key].schema != value.schema)
                            for key, value in obsm.items()) or not \
                        all(array_equal(dataset._obsm[key], value)
                            for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same obsm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            obsp = self._obsp
            for dataset in datasets[1:]:
                if dataset._obsp.keys() != obsp.keys() or \
                        any(type(dataset._obsp[key]) is not type(value) or
                            dataset._obsp[key].dtype != value.dtype
                            for key, value in obsp.items()) or not \
                        all(sparse_equal(dataset._obsp[key], value)
                            for key, value in obsp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same obsp, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            # Check that all `var` have the same columns and data types
            schema = self._var.schema
            for dataset in datasets[1:]:
                if dataset._var.schema != schema:
                    if dataset._var.columns != self._var.columns:
                        error_message = (
                            'all SingleCell datasets must have the same '
                            'columns in var, unless flexible=True')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            'all SingleCell datasets must have the same data '
                            'type for each column of var, unless '
                            'flexible=True')
                        raise TypeError(error_message)
            # Check that all `varm` and `varp` have the same keys and data
            # types
            varm = self._varm
            for dataset in datasets[1:]:
                if dataset._varm.keys() != varm.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'varm, unless flexible=True')
                    raise ValueError(error_message)
                # noinspection PyUnresolvedReferences
                if any(type(dataset._varm[key]) is not type(value) or
                       (dataset._varm[key].dtype != value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema != value.schema)
                       for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data '
                        'type for each key in varm, unless flexible=True')
                    raise TypeError(error_message)
            varp = self._varp
            for dataset in datasets[1:]:
                if dataset._varp.keys() != varp.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'varp, unless flexible=True')
                    raise ValueError(error_message)
                if any(dataset._varp[key].dtype != value.dtype
                       for key, value in varp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data type '
                        'for each key in varp, unless flexible=True')
                    raise TypeError(error_message)
        # Concatenate; output should be CSR when there's a mix of inputs
        X = sparse.vstack([
            dataset._X for dataset in datasets], 
            format='csr' if any(isinstance(dataset._X, csr_array)
                                for dataset in datasets) else 'csc') \
            if X_present else None
        var = pl.concat([dataset._var for dataset in datasets])
        # noinspection PyTypeChecker
        varm = {key: np.concatenate([dataset._varm[key]
                                     for dataset in datasets])
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._varm[key] for dataset in datasets])
                for key, value in datasets[0]._varm.items()}
        varp = {key: sparse.hstack([
            dataset._varp[key] for dataset in datasets], 
            format='csr' if any(isinstance(dataset._varp[key], csr_array)
                                for dataset in datasets) else 'csc') 
            for key in datasets[0]._varp}
        return SingleCell(X=X, obs=datasets[0]._obs, var=var,
                          obsm=datasets[0]._obsm, varm=varm,
                          obsp=datasets[0]._obsp, varp=varp,
                          uns=datasets[0]._uns)

    def split_by_obs(self,
                     column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     sort: bool = False) -> Iterable[tuple[str, SingleCell]]:
        """
        The opposite of `concat_obs()`: splits a SingleCell dataset into a
        tuple of SingleCell datasets, one per unique value of a column of
        `obs`.

        Args:
            column: a String, Enum, or Categorical column of `obs` to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain `null` entries: the corresponding cells
                    will not be included in the result.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will not be
                       selected when subsampling.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
        
        Yields:
            For each unique value of `column`, a tuple of the value and a
            SingleCell dataset subset to cells where `column` has that value.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        column = self._get_column('obs', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  QC_column=QC_column, allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        values = column.value_counts(sort=True).to_series().drop_nulls() \
            if sort else column.unique(maintain_order=True)
        if QC_column is None:
            for value in values:
                yield value, self.filter_obs(column == value)
        else:
            for value in values:
                yield value, self.filter_obs(column.eq(value) & QC_column)
    
    def split_by_var(self,
                     column: SingleCellColumn,
                     *,
                     sort: bool = False) -> Iterable[str, SingleCell]:
        """
        The opposite of `concat_var()`: splits a SingleCell dataset into a
        tuple of SingleCell datasets, one per unique value of a column of
        `var`.

        Args:
            column: a String, Enum, or Categorical column of `var` to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain `null` entries: the corresponding genes
                    will not be included in the result.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
        
        Yields:
            For each unique value of `column`, a tuple of the value and a
            SingleCell dataset subset to genes where `column` has that value.
        """
        column = self._get_column('var', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        values = column.value_counts(sort=True).to_series().drop_nulls() \
            if sort else column.unique(maintain_order=True)
        for value in values:
            yield value, self.filter_var(column == value)
    
    def tocsr(self) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting `X` to a
        `csr_array`. Raise an error if `X` is already a `csr_array`.
        
        Returns:
            A copy of this SingleCell dataset, with `X` as a `csr_array`.
        """
        if self._X is None:
            error_message = 'X is None, so converting to CSR is not possible'
            raise TypeError(error_message)
        if isinstance(self._X, csr_array):
            error_message = 'X is already a csr_array'
            raise TypeError(error_message)
        return SingleCell(X=self._X.tocsr(), obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def tocsc(self) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting `X` to a csc_array.
        Raise an error if `X` is already a `csc_array`.
        
        This function is provided for completeness, but `csr_array` is a far
        better format than `csc_array` for cell-wise operations like
        pseudobulking, so using `tocsc()` is rarely advisable.
        
        Returns:
            A copy of this SingleCell dataset, with `X` as a `csc_array`.
        """
        if self._X is None:
            error_message = 'X is None, so converting to CSC is not possible'
            raise TypeError(error_message)
        if isinstance(self._X, csc_array):
            error_message = 'X is already a csc_array'
            raise TypeError(error_message)
        return SingleCell(X=self._X.tocsc(), obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def filter_obs(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both
        `obs`/`obsm` and `X`.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
            **constraints: column filters: `name=value` filters to cells
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to cells passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        obs = self._obs\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        mask = obs['_SingleCell_index'].to_numpy()
        if self._X is None:
            return SingleCell(X=None,
                              obs=obs.drop('_SingleCell_index'), var=self._var,
                              obsm={key: value[mask]
                                    for key, value in self._obsm.items()},
                              varm=self._varm,
                              obsp={key: value[np.ix_(mask, mask)]
                                    for key, value in self._obsp.items()},
                              varp=self._varp, uns=self._uns)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X[mask],
                              obs=obs.drop('_SingleCell_index'), var=self._var,
                              obsm={key: value[mask]
                                    for key, value in self._obsm.items()},
                              varm=self._varm,
                              obsp={key: value[np.ix_(mask, mask)]
                                    for key, value in self._obsp.items()},
                              varp=self._varp, uns=self._uns)
        finally:
            self._X._num_threads = original_num_threads
    
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both
        `var`/`varm` and `X`.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        var = self._var\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        mask = var['_SingleCell_index'].to_numpy()
        if self._X is None:
            return SingleCell(X=None,
                              obs=self._obs, var=var.drop('_SingleCell_index'),
                              obsm=self._obsm,
                              varm={key: value[:, mask]
                                    for key, value in self._varm.items()},
                              obsp=self._obsp,
                              varp={key: value[np.ix_(mask, mask)]
                                    for key, value in self._varp.items()},
                              uns=self._uns)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X[:, mask],
                              obs=self._obs, var=var.drop('_SingleCell_index'),
                              obsm=self._obsm,
                              varm={key: value[:, mask]
                                    for key, value in self._varm.items()},
                              obsp=self._obsp,
                              varp={key: value[np.ix_(mask, mask)]
                                    for key, value in self._varp.items()},
                              uns=self._uns)
        finally:
            self._X._num_threads = original_num_threads
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to `obs`.
        `obs_names` will be automatically included as the first column, if not
        included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.select(*exprs, **named_exprs)`, and `obs_names` as the
            first column unless already included explicitly.
        """
        obs = self._obs.select(*exprs, **named_exprs)
        if self.obs_names.name in obs:
            error_message = (
                f'one of the selected columns is the obs_names, '
                f'{self.obs_names.name!r}, but the obs_names will always be '
                f'selected automatically as the first column and thus should '
                f'not be specified explicitly')
            raise ValueError(error_message)
        obs = obs.select(self.obs_names, pl.all())
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to `var`.
        `var_names` will be automatically included as the first column, if not
        included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.select(*exprs, **named_exprs)`, and `var_names` as the
            first column unless already included explicitly.
        """
        var = self._var.select(*exprs, **named_exprs)
        if self.var_names.name in var:
            error_message = (
                f'one of the selected columns is the var_names, '
                f'{self.var_names.name!r}, but the var_names will always be '
                f'selected automatically as the first column and thus should '
                f'not be specified explicitly')
            raise ValueError(error_message)
        var = var.select(self.var_names, pl.all())
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)

    def select_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `obsm` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `obsm` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key in keys},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def select_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `varm` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `varm` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key in keys},
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def select_obsp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `obsp` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `obsp` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsp:
                error_message = \
                    f'tried to select {key!r}, which is not a key of obsp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp={key: value for key, value in self._obsp.items()
                                if key in keys},
                          varp=self._varp, uns=self._uns)
        
    def select_varp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `varp` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `varp` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varp:
                error_message = \
                    f'tried to select {key!r}, which is not a key of varp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp={key: value for key, value in self._varp.items()
                                if key in keys},
                          uns=self._uns)
    
    def select_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `uns` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `uns` subset to the specified key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to select {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns={key: value for key, value in self._uns.items()
                                if key in keys})
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to `obs`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(*exprs, **named_exprs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to `var`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.with_columns(*exprs, **named_exprs),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def with_obsm(self,
                  obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_obsm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to `obsm`, overwriting existing keys with the
        same names if present.
        
        Args:
            obsm: a dictionary of keys to add to (or overwrite in) `obsm`
            **more_obsm: additional keys to add to (or overwrite in) `obsm`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `obsm`.
        """
        check_type(obsm, 'obsm', dict, 'a dictionary')
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but new obsm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        obsm |= more_obsm
        if len(obsm) == 0:
            error_message = \
                'obsm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in obsm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def with_varm(self,
                  varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_varm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to `varm`, overwriting existing keys with the
        same names if present.
        
        Args:
            varm: a dictionary of keys to add to (or overwrite in) `varm`
            **more_varm: additional keys to add to (or overwrite in) `varm`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `varm`.
        """
        check_type(varm, 'varm', dict, 'a dictionary')
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but new varm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        varm |= more_varm
        if len(varm) == 0:
            error_message = \
                'varm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in varm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm | varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def with_obsp(self,
                  obsp: dict[str, csr_array | csc_array] = {},
                  **more_obsp: csr_array | csc_array) -> SingleCell:
        """
        Adds one or more keys to `obsp`, overwriting existing keys with the
        same names if present.
        
        Args:
            obsp: a dictionary of keys to add to (or overwrite in) `obsp`
            **more_obsp: additional keys to add to (or overwrite in) `obsp`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `obsp`.
        """
        check_type(obsp, 'obsp', dict, 'a dictionary')
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        obsp |= more_obsp
        if len(obsp) == 0:
            error_message = \
                'obsp is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        num_cells = self._X.shape[0]
        new_obsp = self._obsp.copy()
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of obsp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new obsp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[0] is '
                        f'{num_cells:,}')
                    raise ValueError(error_message)
            new_obsp[key] = value
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=new_obsp,
                          varp=self._varp, uns=self._uns)
    
    def with_varp(self,
                  varp: dict[str, sparse.csr_array | sparse.csc_array |
                                  sparse.csr_matrix | sparse.csc_matrix] = {},
                  **more_varp: sparse.csr_array | sparse.csc_array |
                               sparse.csr_matrix | sparse.csc_matrix) -> \
            SingleCell:
        """
        Adds one or more keys to `varp`, overwriting existing keys with the
        same names if present.
        
        Args:
            varp: a dictionary of keys to add to (or overwrite in) `varp`
            **more_varp: additional keys to add to (or overwrite in) `varp`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `varp`.
        """
        check_type(varp, 'varp', dict, 'a dictionary')
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        varp |= more_varp
        if len(varp) == 0:
            error_message = \
                'varp is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        num_genes = self._X.shape[1]
        new_varp = self._varp.copy()
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of varp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new varp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[1] is '
                        f'{num_genes:,}')
                    raise ValueError(error_message)
            new_varp[key] = value
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=new_varp, uns=self._uns)
    
    def with_uns(self,
                 uns: dict[str, NestedScalarOrArrayDict] = {},
                  **more_uns: NestedScalarOrArrayDict) -> SingleCell:
        """
        Adds one or more keys to `uns`, overwriting existing keys with the same
        names if present.
        
        Args:
            uns: a dictionary of keys to add to (or overwrite in) `uns`
            **more_uns: additional keys to add to (or overwrite in) `uns`,
                        specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `uns`.
        """
        check_type(uns, 'uns', dict, 'a dictionary')
        for key, value in uns.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of uns must be strings, but new uns contains a '
                    f'key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        uns |= more_uns
        if len(uns) == 0:
            error_message = \
                'uns is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns | uns)
    
    def drop_X(self):
        """
        Create a new SingleCell dataset with `X` removed, to reduce memory use.
        
        Returns:
            A new SingleCell dataset with `X` set to `None`.
        """
        if self._X is None:
            error_message = 'X is None, so it cannot be dropped'
            raise TypeError(error_message)
        return SingleCell(obs=self._obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from `obs`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                              positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs.drop(columns),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from `var`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.drop(columns), obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def drop_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `obsm`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            obsm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key not in keys},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def drop_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `varm`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            varm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key not in keys},
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def drop_obsp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `obsp`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            obsp.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsp:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of obsp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp={key: value for key, value in self._obsp.items()
                                if key not in keys},
                          varp=self._varp, uns=self._uns)
    
    def drop_varp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `varp`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            varp.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varp:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of varp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp={key: value for key, value in self._varp.items()
                                if key not in keys},
                          uns=self._uns)
    
    def drop_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `uns`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            uns.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns={key: value for key, value in self._uns.items()
                                if key not in keys})
    
    def rename_obs(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of `obs` renamed.
        
        Rename column(s) of `obs`.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of `obs` renamed.
        """
        return SingleCell(X=self._X, obs=self._obs.rename(mapping),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def rename_var(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of `var` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of `var` renamed.
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.rename(mapping), obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def rename_obsm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `obsm` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `obsm` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._obsm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of obsm'
                    raise ValueError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            obsm = {mapping.get(key, key): value
                    for key, value in self._obsm.items()}
        elif isinstance(mapping, Callable):
            obsm = {}
            for key, value in self._obsm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename obsm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                obsm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var, obsm=obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def rename_varm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `varm` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `varm` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._varm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of varm'
                    raise ValueError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            varm = {mapping.get(key, key): value
                    for key, value in self._varm.items()}
        elif isinstance(mapping, Callable):
            varm = {}
            for key, value in self._varm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename varm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                varm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def rename_obsp(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `obsp` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `obsp` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._obsp:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of obsp'
                    raise ValueError(error_message)
                if new_key in self._obsp:
                    error_message = (
                        f'tried to rename obsp[{key!r}] to obsp[{new_key!r}], '
                        f'but obsp[{new_key!r}] already exists')
                    raise ValueError(error_message)
            obsp = {mapping.get(key, key): value
                    for key, value in self._obsp.items()}
        elif isinstance(mapping, Callable):
            obsp = {}
            for key, value in self._obsp.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename obsp[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._obsp:
                    error_message = (
                        f'tried to rename obsp[{key!r}] to obsp[{new_key!r}], '
                        f'but obsp[{new_key!r}] already exists')
                    raise ValueError(error_message)
                obsp[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=obsp,
                          varp=self._varp, uns=self._uns)
    
    def rename_varp(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `varp` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `varp` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._varp:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of varp'
                    raise ValueError(error_message)
                if new_key in self._varp:
                    error_message = (
                        f'tried to rename varp[{key!r}] to varp[{new_key!r}], '
                        f'but varp[{new_key!r}] already exists')
                    raise ValueError(error_message)
            varp = {mapping.get(key, key): value
                    for key, value in self._varp.items()}
        elif isinstance(mapping, Callable):
            varp = {}
            for key, value in self._varp.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename varp[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._varp:
                    error_message = (
                        f'tried to rename varp[{key!r}] to varp[{new_key!r}], '
                        f'but varp[{new_key!r}] already exists')
                    raise ValueError(error_message)
                varp[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=varp, uns=self._uns)
    
    def rename_uns(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `uns` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `uns` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._uns:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of uns'
                    raise ValueError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
            uns = {mapping.get(key, key): value
                   for key, value in self._uns.items()}
        elif isinstance(mapping, Callable):
            uns = {}
            for key, value in self._uns.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename uns[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
                uns[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=uns)
    
    def cast_X(self, dtype: np._typing.DTypeLike) -> SingleCell:
        """
        Cast `X` to the specified data type.
        
        Args:
            dtype: a NumPy data type

        Returns:
            A new SingleCell dataset with `X` cast to the specified data type.
        """
        if self._X is None:
            error_message = 'X is None, so casting it is not possible'
            raise TypeError(error_message)
        return SingleCell(X=self._X.astype(dtype),
                          obs=self._obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of `obs` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of `obs` cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs.cast(dtypes, strict=strict),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of `var` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of `var` cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.cast(dtypes, strict=strict),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Left join `obs` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join `obs` with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in `obs`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `obs` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `obs`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            obs.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._obs
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'obs[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        obs = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(obs) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Join `var` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join `var` with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in `var`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `var` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `var`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            var.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._var
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'var[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        # noinspection PyTypeChecker
        var = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(var) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def peek_obs(self, row: int = 0) -> None:
        """
        Print a row of `obs` (the first row, by default) with each column on
        its own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[row].unpivot(variable_name='column'))
    
    def peek_var(self, row: int = 0) -> None:
        """
        Print a row of `var` (the first row, by default) with each column on
        its own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[row].unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer | None = None,
                      overwrite: bool = False) -> SingleCell:
        """
        Subsample a specific number or fraction of cells.
        
        Args:
            n: the number of cells to return; mutually exclusive with
               `fraction`
            fraction: the fraction of cells to return; mutually exclusive with
                      `n`
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will not be
                       selected when subsampling, and will not count towards
                       the denominator of `fraction`; QC_column will not appear
                       in the returned SingleCell object, since it would be
                       redundant.
            by_column: an optional String, Enum, Categorical, or integer column
                       of `obs` to subsample by. Can be a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of cells with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       `null` entries: the corresponding cells will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled cells; if `None`,
                              subset to these cells instead
            seed: the random seed to use when subsampling, or leave unset to
                  use `single_cell.options()['seed']` as the seed (0 by
                  default)
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `obs`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `obs`.
        """
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._obs:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of obs; did you already run subsample_obs()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        if by_column is not None:
            by_column = self._get_column(
                'obs', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column, allow_null=True)
            if QC_column is not None:
                by_column = by_column.filter(QC_column)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))\
                    .to_series()
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))\
                    .to_series()
        elif QC_column is not None:
            selected = pl.int_range(QC_column.sum(), dtype=pl.Int32,
                                    eager=True)\
                .shuffle(seed=seed)\
                .lt(n if fraction is None else (fraction * pl.len()).round())
        else:
            selected = self._obs\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))\
                .to_series()
        if QC_column is not None:
            # Back-project from QCed cells to all cells, filling with `null`
            selected = pl.when(QC_column)\
                .then(selected.gather(QC_column.cum_sum().cast(pl.Int32) - 1))
        sc = self.filter_obs(selected) if subsample_column is None else \
            self.with_columns_obs(selected.alias(subsample_column))
        if QC_column is not None:
            # noinspection PyTypeChecker
            sc._obs = sc._obs.drop(QC_column.name)
        return sc
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer | None = None,
                      overwrite: bool = False) -> SingleCell:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `var` to subsample by. Can be a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of genes with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       `null` entries: the corresponding genes will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling, or leave unset to
                  use `single_cell.options()['seed']` as the seed (0 by
                  default)
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `var`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `var`.
        """
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._var:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of var; did you already run subsample_var()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        if by_column is not None:
            by_column = self._get_column(
                'var', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                allow_null=True)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))
        else:
            selected = self._var\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))
        selected = selected.to_series()
        return self.filter_var(selected) if subsample_column is None else \
            self.with_columns_var(selected.alias(subsample_column))
    
    def pipe(self,
             function: Callable[[SingleCell, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a SingleCell dataset.
        
        Args:
            function: the function to apply to the SingleCell dataset. It must
                      take a SingleCell dataset as its first argument, and can
                      return any value. The function may also allow other
                      arguments after the count matrix, which can be specified
                      via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this SingleCell dataset.
        """
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[csr_array | csc_array, ...],
                                  csr_array | csc_array],
               *args: Any,
               **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `X`.
        
        Args:
            function: the function to apply to `X`. It must take the old `X` as
                      its first argument and return the new `X`. The function
                      may also take other arguments after `X`, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to `X`.
        """
        if self._X is None:
            error_message = 'X is None, so piping it is not possible'
            raise TypeError(error_message)
        return SingleCell(X=function(self._X, *args, **kwargs), obs=self._obs,
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def pipe_obs(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obs`.
        
        Args:
            function: the function to apply to `obs`. It must take the old
                      `obs` as its first argument and return the new `obs`. The
                      function may also take other arguments after `obs`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obs.
        """
        return SingleCell(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def pipe_var(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `var`.
        
        Args:
            function: the function to apply to `var`. It must take the old
                      `var` as its first argument and return the new `var`. The
                      function may also take other arguments after `var`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            var.
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=function(self._var, *args, **kwargs),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def pipe_obsm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obsm`.
        
        Args:
            function: the function to apply to `obsm`. It must take the old
                      `obsm` as its first argument and return the new `obsm`.
                      The function may also take other arguments after `obsm`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obsm.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=function(self._obsm, *args, **kwargs),
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def pipe_varm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `varm`.
        
        Args:
            function: the function to apply to `varm`. It must take the old
                      `varm` as its first argument and return the new `varm`.
                      The function may also take other arguments after `varm`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            varm.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm=function(self._varm, *args, **kwargs),
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def pipe_obsp(self,
                  function: Callable[[dict[str, csr_array | csc_array], ...],
                                     dict[str, csr_array | csc_array]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obsp`.
        
        Args:
            function: the function to apply to `obsp`. It must take the old
                      `obsp` as its first argument and return the new `obsp`.
                      The function may also take other arguments after `obsp`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obsp.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=function(self._obsp, *args, **kwargs),
                          varp=self._varp, uns=self._uns)
    
    def pipe_varp(self,
                  function: Callable[[dict[str, csr_array | csc_array], ...],
                                     dict[str, csr_array | csc_array]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `varp`.
        
        Args:
            function: the function to apply to `varp`. It must take the old
                      `varp` as its first argument and return the new `varp`.
                      The function may also take other arguments after `varp`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            varp.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=function(self._varp, *args, **kwargs),
                          uns=self._uns)
    
    def pipe_uns(self,
                 function: Callable[[NestedScalarOrArrayDict, ...],
                                    NestedScalarOrArrayDict],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `uns`.
        
        Args:
            function: the function to apply to `uns`. It must take the old
                      `uns` as its first argument and return the new `uns`. The
                      function may also take other arguments after `uns`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            uns.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns=function(self._uns, *args, **kwargs))
    
    def qc(self,
           custom_filter: SingleCellColumn | None = None,
           *,
           subset: bool = True,
           QC_column: str = 'passed_QC',
           max_mito_fraction: int | float | np.integer | np.floating |
                              None = 0.05,
           min_genes: int | np.integer | None = 100,
           nonzero_MALAT1: bool = True,
           remove_doublets: bool = False,
           batch_column: SingleCellColumn | None = None,
           doublet_fraction: float | np.floating | None = None,
           num_doublet_genes: int | np.integer = 500,
           allow_float: bool = False,
           allow_QCed: bool = False,
           overwrite: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Adds a Boolean column to `obs` indicating which cells passed quality
        control (QC), or subsets to these cells if `subset=True`.
        
        By default, filters to cells with a cell-type confidence of â‰¥90%, â‰¤10%
        mitochondrial reads, â‰¥100 genes detected, and nonzero MALAT1 or Malat1
        expression. Can also filter out doublets when `remove_doublets=True`.
        
        Raises an error if any gene names appear more than once in `var_names`
        (they can be deduplicated with `make_var_names_unique()`).
        
        Args:
            custom_filter: an optional Boolean column of `obs` containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be a column name,
                           a polars expression, a polars Series, a 1D NumPy
                           array, or a function that takes in this SingleCell
                           dataset and returns a polars Series or 1D NumPy
                           array.
            subset: whether to subset to cells passing QC, instead of merely
                    adding a `QC_column` to `obs`. This will roughly double
                    memory usage, but speed up subsequent operations.
            QC_column: the name of a Boolean column to add to `obs` indicating
                       which cells passed QC, if `subset=False`. Gives an error
                       if `obs` already has a column with this name, unless
                       `overwrite=True`.
            max_mito_fraction: if not `None`, filter to cells with <= this
                               fraction of mitochondrial counts. The default of
                               5% matches Seurat's recommended value.
            min_genes: if not `None`, filter to cells with >= this many genes
                       detected (with non-zero count). The default of 100
                       matches Scanpy's recommended value, while Seurat
                       recommends a minimum of 200.
            nonzero_MALAT1: if `True`, filter out cells with 0 expression of
                            the nuclear-expressed lncRNA MALAT1, which likely
                            represent empty droplets or poor-quality cells
                            (biorxiv.org/content/10.1101/2024.07.14.603469v1).
                            There must be exactly one gene in `obs_names` with
                            the name `'MALAT1'` or `'Malat1'` to use this
                            filter.
            remove_doublets: if `True`, remove predicted doublets (see
                             `find_doublets()`). Doublet detection uses the
                             cxds algorithm to score each cell, then thresholds
                             this continuous score to a binary one (doublet
                             versus non-doublet) using a threshold derived from
                             simulated doublets.
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Only used during doublet
                          detection; doublet detection will be performed
                          separately for each batch. Set to `None` if all cells
                          belong to the same sequencing batch. Must be `None`
                          when `remove_doublets=False`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described in
                              `find_doublets()`.
            num_doublet_genes: the number of highly variable genes, i.e. genes
                               expressed in as close to 50% of cells as
                               possible, to use during doublet detection. This
                               parameter usually has a minimal influence on
                               accuracy as long as it is sufficiently large (in
                               the hundreds), so increasing it further will
                               mainly just increase runtime. If
                               `num_doublet_genes` is greater than the number
                               of genes in the dataset, all genes will be used.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check.
                         Note that all steps except mitochondrial percent
                         filtering give the same result on normalized counts,
                         so as long as `max_mito_fraction=None` is specified
                         (not typically recommended), this function will give
                         the same result on raw and normalized counts.
            allow_QCed: if `False`, raise an error if `uns['QCed']` is `True`,
                        indicating that the dataset has already been QCed; if
                        `True`, disable this sanity check
            overwrite: if `True`, overwrite `QC_column` if already present in
                       obs, instead of raising an error. Must be `False` when
                       `subset=True`.
            verbose: whether to print how many cells were filtered out at each
                     step of the QC process
            num_threads: the number of threads to use when filtering based on
                         mitochondrial counts and MALAT1 expression, and for
                         doublet detection. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
                
        Returns:
            A new SingleCell dataset with `QC_column` added to `obs`, or subset
            to QCed cells if `subset=True`, and `uns['QCed']` set to `True`.
        """
        X = self._X
        if X is None:
            error_message = 'X is None, so QCing is not possible'
            raise TypeError(error_message)
        # If `allow_QCed=False`, check that `self` is not already QCed
        if not allow_QCed and self._uns['QCed']:
            error_message = (
                "uns['QCed'] is True; did you already run qc()? Specify "
                "allow_QCed=True, set uns['QCed'] = False, or run "
                "with_uns(QCed=False) to bypass this check.")
            raise ValueError(error_message)
        # Check inputs
        if self.var_names.n_unique() < len(self.var):
            error_message = (
                'var_names contains duplicates; deduplicate with '
                'make_var_names_unique()')
            raise ValueError(error_message)
        if custom_filter is not None:
            custom_filter = self._get_column(
                'obs', custom_filter, 'custom_filter', pl.Boolean)
        check_type(subset, 'subset', bool, 'Boolean')
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if not subset:
            check_type(QC_column, 'QC_column', str, 'a string')
            if not overwrite and QC_column in self._obs:
                error_message = (
                    f'QC_column {QC_column!r} is already a column of obs; did '
                    f'you already run qc()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = 'overwrite must be False when subset is True'
            raise ValueError(error_message)
        if max_mito_fraction is not None:
            check_type(max_mito_fraction, 'max_mito_fraction', (int, float),
                       'a number between 0 and 1, inclusive')
            check_bounds(max_mito_fraction, 'max_mito_fraction', 0, 1)
        if min_genes is not None:
            check_type(min_genes, 'min_genes', int, 'a non-negative integer')
            check_bounds(min_genes, 'min_genes', 0)
        check_type(nonzero_MALAT1, 'nonzero_MALAT1', bool, 'Boolean')
        check_type(remove_doublets, 'remove_doublets', bool, 'Boolean')
        if batch_column is not None:
            if not remove_doublets:
                error_message = \
                    'batch_column must be None when remove_doublets is False'
                raise ValueError(error_message)
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'))
        if doublet_fraction is not None:
            check_type(doublet_fraction, 'doublet_fraction', float,
                       'a number greater than 0 and less than 1')
            check_bounds(doublet_fraction, 'doublet_fraction', 0, 1,
                         left_open=True, right_open=True)
        check_type(num_doublet_genes, 'num_doublet_genes', int,
                   'a positive integer')
        check_bounds(num_doublet_genes, 'num_doublet_genes', 1)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'qc() requires raw counts but X has data type '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)
        # Apply the custom filter, if specified
        if verbose:
            print(f'Starting with {len(self):,} cells.')
        mask = None
        if custom_filter is not None:
            if verbose:
                print('Applying the custom filter...')
            mask = custom_filter
            if verbose:
                print(f'{mask.sum():,} cells remain after applying the custom '
                      f'filter.')
        # Filter to cells with â‰¤ `100 * max_mito_fraction`% mitochondrial
        # counts, if `max_mito_fraction` was specified
        if max_mito_fraction is not None:
            if verbose:
                print(f'Filtering to cells with â‰¤{100 * max_mito_fraction}% '
                      f'mitochondrial counts...')
            var_names = self.var_names
            if var_names.dtype != pl.String:
                var_names = var_names.cast(pl.String)
            mt_genes = var_names.str.to_uppercase().str.starts_with('MT-')
            if not mt_genes.any():
                error_message = (
                    'no genes are mitochondrial (start with "MT-", '
                    'case-insensitively); this may happen if your var_names '
                    'are Ensembl IDs (ENSG) rather than gene symbols (in '
                    'which case you should set the gene symbols as the '
                    'var_names with set_var_names()), or if mitochondrial '
                    'genes have already been filtered out (in which case you '
                    'can set max_mito_fraction to None)')
                raise ValueError(error_message)
            mito_mask = np.empty(X.shape[0], dtype=bool)
            if isinstance(X, csr_array):
                cython_inline(r'''
                    from cython.parallel cimport prange
                    
                    ctypedef fused numeric:
                        int
                        unsigned
                        long
                        unsigned long
                        float
                        double
                    
                    ctypedef fused signed_integer:
                        int
                        long
                    
                    def mito_mask(
                            const numeric[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            char[::1] mt_genes,
                            const double max_mito_fraction,
                            char[::1] mito_mask,
                            const unsigned num_threads):
                        
                        cdef unsigned row, row_sum, mt_sum, \
                            num_genes = indptr.shape[0] - 1
                        cdef unsigned long col
                        
                        if num_threads == 1:
                            for row in range(num_genes):
                                row_sum = mt_sum = 0
                                for col in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    row_sum = row_sum + <unsigned> data[col]
                                    if mt_genes[indices[col]]:
                                        mt_sum = mt_sum + <unsigned> data[col]
                                mito_mask[row] = \
                                    (<double> mt_sum / row_sum) <= \
                                    max_mito_fraction
                        else:
                            for row in prange(num_genes, nogil=True,
                                              num_threads=num_threads):
                                row_sum = mt_sum = 0
                                for col in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    row_sum = row_sum + <unsigned> data[col]
                                    if mt_genes[indices[col]]:
                                        mt_sum = mt_sum + <unsigned> data[col]
                                mito_mask[row] = \
                                    (<double> mt_sum / row_sum) <= \
                                    max_mito_fraction
                        ''')['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            mito_mask=mito_mask, num_threads=num_threads)
            else:
                row_sums = np.empty(X.shape[0], dtype=np.uint32)
                mt_sums = np.empty(X.shape[0], dtype=np.uint32)
                cython_inline(r'''
                    from cython.parallel cimport prange
                    from libcpp.vector cimport vector
                    
                    ctypedef fused numeric:
                        int
                        unsigned
                        long
                        unsigned long
                        float
                        double
                    
                    ctypedef fused signed_integer:
                        int
                        long
                    
                    def mito_mask(
                            const numeric[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            char[::1] mt_genes,
                            const double max_mito_fraction,
                            unsigned[::1] row_sums,
                            unsigned[::1] mt_sums,
                            char[::1] mito_mask,
                            const unsigned num_threads):
                        
                        cdef unsigned thread_index, gene, chunk_size, start, end, \
                            num_genes = indptr.shape[0] - 1, \
                            num_cells = mito_mask.shape[0]
                        cdef unsigned long cell
                        cdef vector[vector[unsigned]] thread_row_sums, thread_mt_sums
                        
                        if num_threads == 1:
                            row_sums[:] = 0
                            mt_sums[:] = 0
                            for gene in range(num_genes):
                                if mt_genes[gene]:
                                    for cell in range(<unsigned long> indptr[gene],
                                                      <unsigned long> indptr[gene + 1]):
                                        row_sums[indices[cell]] += <unsigned> data[cell]
                                        mt_sums[indices[cell]] += <unsigned> data[cell]
                                else:
                                    for cell in range(<unsigned long> indptr[gene],
                                                      <unsigned long> indptr[gene + 1]):
                                        row_sums[indices[cell]] += <unsigned> data[cell]
                            for cell in range(num_cells):
                                mito_mask[cell] = (
                                    <double> mt_sums[cell] / row_sums[cell]) <= \
                                    max_mito_fraction
                        else:
                            # Store total counts per cell and total
                            # mitochondrial counts per cell for each thread in
                            # a temporary buffer, then aggregate at the end. As
                            # an optimization, put the counts for the last
                            # thread (`thread_index == num_threads - 1`)
                            # directly into the final `row_sums` and `mt_sums`
                            # arrays.
                            
                            chunk_size = num_genes // num_threads
                            thread_row_sums.resize(num_threads - 1)
                            thread_mt_sums.resize(num_threads - 1)
                            with nogil:
                                for thread_index in prange(num_threads,
                                                           num_threads=num_threads,
                                                           schedule='static', chunksize=1):
                                    start = thread_index * chunk_size
                                    if thread_index == num_threads - 1:
                                        end = num_genes
                                        row_sums[:] = 0
                                        mt_sums[:] = 0
                                        for gene in range(start, end):
                                            if mt_genes[gene]:
                                                for cell in range(<unsigned long> indptr[gene],
                                                                  <unsigned long> indptr[gene + 1]):
                                                    row_sums[indices[cell]] += <unsigned> data[cell]
                                                    mt_sums[indices[cell]] += <unsigned> data[cell]
                                            else:
                                                for cell in range(<unsigned long> indptr[gene],
                                                                  <unsigned long> indptr[gene + 1]):
                                                    row_sums[indices[cell]] += <unsigned> data[cell]
                                    else:
                                        thread_row_sums[thread_index].resize(num_cells)
                                        thread_mt_sums[thread_index].resize(num_cells)
                                        end = start + chunk_size
                                        for gene in range(start, end):
                                            if mt_genes[gene]:
                                                for cell in range(<unsigned long> indptr[gene],
                                                                  <unsigned long> indptr[gene + 1]):
                                                    thread_row_sums[thread_index][indices[cell]] += \
                                                        <unsigned> data[cell]
                                                    thread_mt_sums[thread_index][indices[cell]] += \
                                                        <unsigned> data[cell]
                                            else:
                                                for cell in range(<unsigned long> indptr[gene],
                                                                  <unsigned long> indptr[gene + 1]):
                                                    thread_row_sums[thread_index][indices[cell]] += \
                                                        <unsigned> data[cell]
                                
                                # Aggregate counts from all threads except the last
                                
                                for thread_index in range(num_threads - 1):
                                    for cell in range(num_cells):
                                        row_sums[cell] += thread_row_sums[thread_index][cell]
                                        mt_sums[cell] += thread_mt_sums[thread_index][cell]
                            
                                # Populate the mask
                                
                                for cell in prange(num_cells,
                                                   num_threads=num_threads):
                                    mito_mask[cell] = \
                                        (<double> mt_sums[cell] /
                                        row_sums[cell]) <= max_mito_fraction
                        ''')['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            row_sums=row_sums, mt_sums=mt_sums,
                            mito_mask=mito_mask, num_threads=num_threads)
            mito_mask = pl.Series(mito_mask)
            if not mito_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'â‰¤{100 * max_mito_fraction}% mitochondrial counts')
                raise ValueError(error_message)
            if mask is None:
                mask = mito_mask
            else:
                mask &= mito_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with â‰¤{100 * max_mito_fraction}% mitochondrial '
                      f'counts.')
        # Filter to cells with â‰¥ `min_genes` genes detected, if specified
        if min_genes is not None:
            if verbose:
                print(f'Filtering to cells with â‰¥{min_genes:,} genes '
                      f'detected (with non-zero count)...')
            gene_mask = pl.Series(getnnz(X, axis=1, num_threads=num_threads) >=
                                  min_genes)
            if not gene_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'â‰¥{min_genes:,} genes detected')
                raise ValueError(error_message)
            if mask is None:
                mask = gene_mask
            else:
                mask &= gene_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with â‰¥{min_genes:,} genes detected.')
        # Filter to cells with non-zero MALAT1 expression, if
        # `nonzero_MALAT1=True`
        if nonzero_MALAT1:
            if verbose:
                print(f'Filtering to cells with non-zero MALAT1 expression...')
            MALAT1_index = self._var\
                .select(pl.arg_where(pl.col(self.var_names.name)
                                     .is_in(('MALAT1', 'Malat1'))))
            if len(MALAT1_index) == 0:
                error_message = (
                    f"neither 'MALAT1' nor 'Malat1' was found in var_names; "
                    f"this may happen if your var_names are Ensembl IDs "
                    f"(ENSG) rather than gene symbols (in which case you "
                    f"should set the gene symbols as the var_names with "
                    f"set_var_names()). Alternatively, set "
                    f"nonzero_MALAT1=False to disable filtering on MALAT1 "
                    f"expression.")
                raise ValueError(error_message)
            if len(MALAT1_index) == 2:
                error_message = (
                    "both 'MALAT1' and 'Malat1' were found in var_names; if "
                    "this is intentional, rename one of them before running "
                    "qc(), or set nonzero_MALAT1=False to disable filtering "
                    "on MALAT1 expression")
                raise ValueError(error_message)
            MALAT1_index = MALAT1_index.item()
            # The code below is a faster version of:
            # `MALAT1_mask = (X[:, [MALAT1_index]] != 0).toarray().squeeze()`
            # More specifically,
            MALAT1_mask = np.zeros(X.shape[0], dtype=bool)
            has_sorted_indices = getattr(X, '_has_sorted_indices', None)
            if isinstance(X, csr_array):
                if has_sorted_indices:
                    # Use binary search
                    cython_inline(r'''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def get_MALAT1_mask_csr(
                                const numeric[::1] data,
                                const signed_integer[::1] indices,
                                const signed_integer[::1] indptr,
                                const unsigned MALAT1_index,
                                char[::1] MALAT1_mask,
                                const unsigned num_threads):
                            
                            cdef unsigned row, num_genes = indptr.shape[0] - 1
                            cdef unsigned long low, high, mid
                            
                            if num_threads == 1:
                                for row in range(num_genes):
                                    low = indptr[row]
                                    high = indptr[row + 1] - 1
                                    while True:
                                        mid = (low + high) // 2
                                        if indices[mid] < MALAT1_index:
                                            low = mid + 1
                                        else:
                                            high = mid
                                        if low >= high:
                                            break
                                    MALAT1_mask[row] = \
                                        indices[low] == MALAT1_index
                            else:
                                for row in prange(num_genes, nogil=True,
                                                  num_threads=num_threads):
                                    low = indptr[row]
                                    high = indptr[row + 1] - 1
                                    while True:
                                        mid = (low + high) // 2
                                        if indices[mid] < MALAT1_index:
                                            low = mid + 1
                                        else:
                                            high = mid
                                        if low >= high:
                                            break
                                    MALAT1_mask[row] = \
                                        indices[low] == MALAT1_index
                    ''')['get_MALAT1_mask_csr'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
                else:
                    if verbose and has_sorted_indices is False:
                        print('Warning: X does not have sorted indices, so '
                              'some operations may be slower. You may want to '
                              'sort indices with `X.sort_indices()` (an '
                              'in-place operation) as the first step after '
                              'loading, though be aware that this may take a '
                              'while.')
                    # Use brute-force search
                    cython_inline(r'''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def get_MALAT1_mask_csr(
                                const numeric[::1] data,
                                const signed_integer[::1] indices,
                                const signed_integer[::1] indptr,
                                const unsigned MALAT1_index,
                                char[::1] MALAT1_mask,
                                const unsigned num_threads):
                            
                            cdef unsigned row, num_genes = indptr.shape[0] - 1
                            cdef unsigned long col
                            
                            if num_threads == 1:
                                for row in range(num_genes):
                                    for col in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                        if <unsigned> indices[col] == MALAT1_index:
                                            MALAT1_mask[row] = True
                                            break
                            else:
                                for row in prange(num_genes, nogil=True,
                                                  num_threads=num_threads):
                                    for col in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                        if <unsigned> indices[col] == MALAT1_index:
                                            MALAT1_mask[row] = True
                                            break
                    ''')['get_MALAT1_mask_csr'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
            else:
                start = X.indptr[MALAT1_index]
                end = X.indptr[MALAT1_index + 1]
                MALAT1_mask[X.indices[start:end]] = True
            MALAT1_mask = pl.Series(MALAT1_mask)
            if mask is None:
                mask = MALAT1_mask
            else:
                mask &= MALAT1_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with non-zero MALAT1 expression.')
        # Remove predicted doublets, if `remove_doublets=True`
        if remove_doublets:
            if verbose:
                print('Removing predicted doublets...')
            singlets = ~pl.Series(SingleCell._find_doublets(
                X=self._X, batch_column=batch_column, QC_column=None,
                doublet_fraction=doublet_fraction, num_genes=num_doublet_genes,
                num_threads=num_threads, verbose=False)[0])
            if mask is None:
                mask = singlets
            else:
                mask &= singlets
            if verbose:
                print(f'{mask.sum():,} cells remain after removing predicted '
                      f'doublets.')
        # Add the mask of QCed cells as a column, or subset if `subset=True`
        if mask is None:
            error_message = 'no QC filters were specified'
            raise ValueError(error_message)
        if subset:
            if verbose:
                print(f'Subsetting to cells passing QC (note: you can reduce '
                      f'memory usage by specifying subset=False)...')
            sc = self.filter_obs(mask)
        else:
            if verbose:
                print(f'Adding a Boolean column, obs[{QC_column!r}], '
                      f'indicating which cells passed QC...')
            sc = SingleCell(X=X, obs=self._obs.with_columns(
                pl.lit(mask).alias(QC_column)), var=self._var, obsm=self._obsm,
                              varm=self._varm, uns=self._uns)
        sc._uns['QCed'] = True
        return sc
        
    @staticmethod
    def _find_doublets(X: csr_array | csc_array,
                       batch_column: SingleCellColumn | None,
                       QC_column: SingleCellColumn | None,
                       doublet_fraction: float | np.floating | None,
                       num_genes: int | np.integer,
                       verbose: bool,
                       num_threads: int | np.integer | None) -> \
            tuple[np.ndarray[1, np.dtype[np.bool_]],
                  np.ndarray[1, np.dtype[np.float64]]]:
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507). Used by
        `qc()` (when `remove_doublets=True`) and `find_doublets()`.
        
        Args:
            X: the count matrix; may be either normalized or unnormalized
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described in
                              `find_doublets()`.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            verbose: whether to print debug information about timings at each
                     step (will be removed for the final version)
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).

        Returns:
            A tuple of two NumPy arrays with the binary doublet calls and
            doublet scores.
        """
        # Define Cython functions
        is_csr = isinstance(X, csr_array)
        cython_functions = cython_inline(r'''
            from cython.parallel cimport prange
            from libcpp.cmath cimport erfc, exp, fabs, floor, log, log1p, sqrt
            
            ctypedef fused numeric:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            ctypedef fused signed_integer:
                int
                long
            
            cdef extern from *:
                """
                #define atomic_add(x, y) _Pragma("omp atomic") x += y
                """
                void atomic_add(unsigned &x, unsigned y) nogil
            
            cdef double log_one_half = -0.6931471805599453
            cdef double log_sqrt_2_pi = 0.91893853320467274
            cdef double one_over_sqrt_2 = 0.70710678118654752
            cdef double one_over_sqrt_pi = 0.5641895835477563
            
            cdef double[5] gamma_A = [
                8.11614167470508450300E-4, -5.95061904284301438324E-4,
                7.93650340457716943945E-4, -2.77777777730099687205E-3,
                8.33333333333331927722E-2]
            
            cdef double[6] gamma_B = [
                -1.37825152569120859100E3, -3.88016315134637840924E4,
                -3.31612992738871184744E5, -1.16237097492762307383E6,
                -1.72173700820839662146E6, -8.53555664245765465627E5]
            
            cdef double[6] gamma_C = [
                -3.51815701436523470549E2, -1.70642106651881159223E4,
                -2.20528590553854454839E5, -1.13933444367982507207E6,
                -2.53252307177582951285E6, -2.01889141433532773231E6]
            
            cdef double[6] P = [
                0.5641895835477550741253201704,
                1.275366644729965952479585264,
                5.019049726784267463450058,
                6.1602098531096305440906,
                7.409740605964741794425,
                2.97886562639399288862]
            
            cdef double[6] Q = [
                2.260528520767326969591866945,
                9.396034016235054150430579648,
                12.0489519278551290360340491,
                17.08144074746600431571095,
                9.608965327192787870698,
                3.3690752069827527677]
            
            cdef inline double p1evl(const double x,
                                     const double* coef,
                                     const unsigned N) noexcept nogil:
                cdef double ans
                cdef unsigned i
                
                ans = x + coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef inline double polevl(const double x,
                                      const double* coef,
                                      const unsigned N) noexcept nogil:
                cdef double ans
                cdef unsigned i
                
                ans = coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef inline double log_erfc(const double x) noexcept nogil:
                # Based on GSL's gsl_sf_log_erfc_e at
                # github.com/ampl/gsl/blob/master/specfunc/erfc.c#L306
                
                cdef double y, series
                
                if x * x < 0.02460783300575925:
                    y = x * one_over_sqrt_pi
                    series = 0.00048204
                    series = y * series - 0.00142906
                    series = y * series + 0.0013200243174
                    series = y * series + 0.0009461589032
                    series = y * series - 0.0045563339802
                    series = y * series + 0.00556964649138
                    series = y * series + 0.00125993961762116
                    series = y * series - 0.01621575378835404
                    series = y * series + 0.02629651521057465
                    series = y * series - 0.001829764677455021
                    series = y * series - 0.09439510239319526
                    series = y * series + 0.28613578213673563
                    series = y * series + 1
                    series = y * series + 1
                    return -2 * y * series
                elif x > 8:
                    return log(polevl(x, &P[0], 6) / p1evl(x, &Q[0], 6)) - \
                        x * x
                else:
                    return log(erfc(x))
            
            cdef inline double gammaln(double x) noexcept nogil:
                # Simplified from github.com/scipy/scipy/blob/main/scipy/
                # special/xsf/cephes/gamma.h, based on the knowledge that `x`
                # will always be positive and finite when calculating terms in
                # the binomial distribution
                
                cdef double p, q, u, z
                
                if x < 13:
                    z = 1
                    p = 0
                    u = x
                    while u >= 3:
                        p -= 1
                        u = x + p
                        z *= u
                    while u < 2:
                        z /= u
                        p += 1
                        u = x + p
                    z = fabs(z)
                    if u == 2:
                        return log(z)
                    p -= 2
                    x = x + p
                    p = x * polevl(x, &gamma_B[0], 6) / \
                        p1evl(x, &gamma_C[0], 6)
                    return log(z) + p
                elif x >= 1000:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    if x > 1e8:
                        return q
                    p = 1.0 / (x * x)
                    p = ((7.9365079365079365079365e-4 * p -
                          2.7777777777777777777778e-3) *
                         p + 0.0833333333333333333333) / x
                    return q + p
                else:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    p = 1.0 / (x * x)
                    return q + polevl(p, &gamma_A[0], 5) / x
            
            cdef inline double binom_logsf_term(
                    const unsigned j,
                    const unsigned n,
                    const double log_p,
                    const double log1p_q,
                    const double gammaln_n_plus_1) noexcept nogil:
                return gammaln_n_plus_1 - gammaln(j + 1) - \
                    gammaln(n - j + 1) + j * log_p + (n - j) * log1p_q
            
            cdef inline double binom_logsf(const unsigned k,
                                           const unsigned n,
                                           const double p) noexcept nogil:
                cdef unsigned j, j_max
                cdef double mu, sigma, z
                cdef double sum_exp, max_term, term, log_p, log1p_q, \
                    gammaln_n_plus_1
                
                # Use the normal approximation when n * p and n * (1 - p) are
                # both greater than 500; add 0.5 for continuity correction
                
                if n * p > 500 and n * (1 - p) > 500:
                    mu = n * p
                    sigma = sqrt(mu * (1 - p))
                    z = (k + 0.5 - mu) / sigma
                    return log_erfc(z * one_over_sqrt_2) + log_one_half
                
                # Otherwise, compute the exact binomial p-value
            
                log_p = log(p)
                log1p_q = log1p(-p)
                gammaln_n_plus_1 = gammaln(n + 1)
            
                # Find `j_max`, the value of `j` with the largest binomial term
                # (the `term` variabel below). For a binomial distribution, the
                # mode (maximum probability) occurs at `floor((n + 1) * p)`.
                # However, since we are summing from `k + 1` to `n`, not 0 to
                # `n`, we need to ensure `j_max` is at least `k + 1`.
                
                j_max = <unsigned> floor((n + 1) * p)
                if j_max <= k:
                    j_max = k + 1
                
                max_term = binom_logsf_term(j_max, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                
                # Sum the terms of the binomial via the logsumexp trick. This
                # improves numerical stability by subtracting off the max term
                # from each term before exponentiation, then adding back the
                # max term at the end.
                
                sum_exp = 0
                for j in range(k + 1, n + 1):
                    term = binom_logsf_term(j, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                    sum_exp += exp(term - max_term)
            
                return log(sum_exp) + max_term
            
            def compute_obs(
                    const unsigned[::1] detection_count,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    unsigned[:, ::1] obs,
                    const unsigned num_threads):
                
                # obs[i, j] is the number of cells in which exactly one of the
                # two genes i and j is expressed. If we define:
                # - (1) as the number of cells where gene i is expressed
                # - (2) as the number of cells where gene j is expressed
                # - (3) as the number of cells where both genes i and j are
                #   expressed
                # then obs[i, j] = (1) + (2) - 2 * (3)
                
                cdef unsigned cell, gene_i, gene_j, \
                    num_genes = detection_count.shape[0], \
                    num_cells = indptr.shape[0] - 1
                cdef unsigned long i, j, i_start, i_end, j_end
                
                if num_threads == 1:
                    # Initialize the upper diagonal of obs to (1) + (2)
                    
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            obs[i, j] = detection_count[i] + detection_count[j]
                    
                    # Now subtract off 2 * (3) for the upper diagonal: iterate
                    # over all pairs of genes i and j within each cell, and
                    # subtract 2 from `obs[i, j]` for each pair
                    
                    for cell in range(num_cells):
                        for i in range(<unsigned long> indptr[cell],
                                       <unsigned long> indptr[cell + 1]):
                            for j in range(i + 1, indptr[cell + 1]):
                                obs[indices[i], indices[j]] -= 2
                else:
                    with nogil:
                        for i in prange(num_genes, num_threads=num_threads):
                            for j in range(i + 1, num_genes):
                                obs[i, j] = \
                                    detection_count[i] + detection_count[j]
                        
                        for cell in prange(num_cells, num_threads=num_threads):
                            for i in range(<unsigned long> indptr[cell],
                                           <unsigned long> indptr[cell + 1]):
                                for j in range(i + 1, indptr[cell + 1]):
                                    atomic_add(obs[indices[i], indices[j]], -2)
            
            def compute_S(const unsigned[:, ::1] obs,
                          const double[::1] p,
                          const unsigned num_cells,
                          double[:, ::1] S,
                          const unsigned num_threads):
                
                cdef unsigned num_genes = p.shape[0]
                cdef unsigned i, j
                
                if num_threads == 1:
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
                else:
                    for i in prange(num_genes, nogil=True,
                                    num_threads=num_threads):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
            
            def compute_cxds(
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    const double[:, ::1] S,
                    double[::1] cxds_scores,
                    const unsigned num_threads):
                
                cdef unsigned cell, gene_i, gene_j, num_cells = indptr.shape[0] - 1
                cdef unsigned long i, j, i_start, i_end, j_end
                
                # Iterate over all pairs of genes i and j within each cell, and
                # subtract `S[i, j]` from the cell's cxds score for each pair
                
                if num_threads == 1:
                    for cell in range(num_cells):
                        for gene_i in range(<unsigned long> indptr[cell],
                                            <unsigned long> indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1, indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
                else:
                    for cell in prange(num_cells, nogil=True,
                                       num_threads=num_threads):
                        for gene_i in range(<unsigned long> indptr[cell],
                                            <unsigned long> indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1, indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            def simulate_doublets(
                    const numeric[::1] data,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    integer[::1] sim_indices,
                    integer[::1] sim_indptr,
                    const unsigned num_cells,
                    const unsigned long seed):
                
                cdef unsigned long i, j, i_end, j_end, nnz = 0, state = srand(seed)
                cdef unsigned sim_index, cell_i, cell_j
                
                sim_indptr[0] = 0
                for sim_index in range(1, num_cells + 1):
                    cell_i = randint(num_cells, &state)
                    cell_j = randint(num_cells, &state)
                    i = indptr[cell_i]
                    i_end = indptr[cell_i + 1]
                    j = indptr[cell_j]
                    j_end = indptr[cell_j + 1]
                    while True:
                        if indices[i] == indices[j]:
                            sim_indices[nnz] = indices[i]
                            nnz = nnz + 1
                            i = i + 1
                            j = j + 1
                            if i == i_end or j == j_end:
                                break
                        elif indices[i] < indices[j]:
                            # `rand(&state) & 1` gives a random Boolean; only
                            # coin-flip if `data[i] == 1`
                            
                            if data[i] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[i]
                                nnz = nnz + 1
                            i = i + 1
                            if i == i_end:
                                break
                        else:
                            if data[j] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[j]
                                nnz = nnz + 1
                            j = j + 1
                            if j == j_end:
                                break
                    sim_indptr[sim_index] = nnz
        ''')
        compute_obs = cython_functions['compute_obs']
        compute_S = cython_functions['compute_S']
        compute_cxds = cython_functions['compute_cxds']
        simulate_doublets = cython_functions['simulate_doublets']
        
        # If `batch_column` is not `None`, get the row indices of each batch,
        # ignoring cells failing QC when `QC_column` is present in `obs`. If
        # no batches were specified but `QC_column` is present, use `QC_column`
        # as the batch labels.
        if QC_column is not None and batch_column is None:
            batch_column = QC_column
        if batch_column is not None:
            batch_column_name = batch_column.name
            # noinspection PyTypeChecker
            batches = (batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      if QC_column is None else
                      batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      .filter(QC_column.name))\
                .group_by(batch_column_name, maintain_order=True)\
                .agg('_SingleCell_batch_indices')\
                .select('_SingleCell_batch_indices')\
                .collect()\
                .to_series()
        else:
            batches = None,
        
        # Preallocate
        if batch_column is not None:
            doublets = np.empty(X.shape[0], dtype=bool)
            doublet_scores = np.empty(X.shape[0])
        obs = np.empty((num_genes, num_genes), dtype=np.uint32)
        S = np.empty((num_genes, num_genes))
        
        # noinspection PyUnresolvedReferences
        original_num_threads = X._num_threads
        try:
            X._num_threads = num_threads
            # For each batch...
            for batch_index, batch_indices in enumerate(batches):
                # Subset to cells in that batch
                with Timer('subset to batch', verbose=verbose):
                    X_batch = \
                        X[batch_indices] if batch_column is not None else X
                if X_batch.shape[0] == 1:
                    if verbose:
                        print('Skipping batch due to only having one cell')
                    continue
                # Get the detection count of each gene
                with Timer('getnnz', verbose=verbose):
                    detection_count = \
                        getnnz(X_batch, axis=0, num_threads=num_threads)
                # Subset to the `num_genes` genes with detection rates closest
                # to 50%. Exclude genes with detection rates of 0% or 100%.
                with Timer('hvg_indices and misc subsetting', verbose=verbose):
                    num_cells = X_batch.shape[0]
                    p = detection_count / num_cells
                    hvg_indices = \
                        np.argsort(p * (1 - p), kind='stable')[-num_genes:]
                    least_variable_p = p[hvg_indices[-num_genes]]
                    if least_variable_p == 0 or least_variable_p == 1:
                        hvg_p = p[hvg_indices]
                        hvg_indices = hvg_indices[(hvg_p > 0) & (hvg_p < 1)]
                    hvg_indices.sort()
                    detection_count = detection_count[hvg_indices]
                    p = p[hvg_indices]
                with Timer('subsetting X_batch to hvgs', verbose=verbose):
                    X_batch = X_batch[:, hvg_indices]
                # Convert `X_batch` to CSR, if CSC
                if not is_csr:
                    with Timer('converting to CSR', verbose=verbose):
                        X_batch = X_batch.tocsr()
                # Sort indices, if not already sorted (necessary for
                # `simulate_doublets`)
                with Timer('sorting indices if not already sorted',
                           verbose=verbose):
                    if not X.has_sorted_indices:
                        X_batch.sort_indices()
                # Get `obs`, where `obs[i, j]` is the number of cells that
                # express exactly one of genes `i` and `j`
                with Timer('compute obs', verbose=verbose):
                    compute_obs(detection_count=detection_count,
                                indices=X_batch.indices, indptr=X_batch.indptr,
                                obs=obs, num_threads=num_threads)
                # Get `S`, the upper-tail log binomial p-values of `obs`
                with Timer('compute S', verbose=verbose):
                    compute_S(obs=obs, p=p, num_cells=num_cells, S=S,
                              num_threads=num_threads)
                # Calculate each cell's cxds score: the sum of `-S[i, j]`
                # across all gene pairs `i` and `j` that are both expressed by
                # the cell
                with Timer('compute cxds', verbose=verbose):
                    cxds_scores_batch = np.zeros(num_cells)
                    compute_cxds(indices=X_batch.indices,
                                 indptr=X_batch.indptr, S=S,
                                 cxds_scores=cxds_scores_batch,
                                 num_threads=num_threads)
                # Now simulate doublets within the batch and compute their cxds
                # scores, using the original `S` matrix derived from the real
                # data
                with Timer('simulating doublets', verbose=verbose):
                    sim_indptr = np.empty_like(X_batch.indptr)
                    # conservatively allocate twice as much memory for the
                    # indices as the original indices, since in the worst-case
                    # scenario none of the indices will match up and all
                    # coinflips will be 1
                    sim_indices = np.empty(2 * len(X_batch.indices),
                                           dtype=X_batch.indices.dtype)
                    simulate_doublets(
                        data=X_batch.data, indices=X_batch.indices,
                        indptr=X_batch.indptr, sim_indices=sim_indices,
                        sim_indptr=sim_indptr, num_cells=num_cells,
                        seed=batch_index)
                    sim_indices = sim_indices[:sim_indptr[-1]]
                with Timer('compute cxds simulated', verbose=verbose):
                    cxds_scores_sim = np.zeros(num_cells)
                    compute_cxds(indices=sim_indices, indptr=sim_indptr, S=S,
                                 cxds_scores=cxds_scores_sim,
                                 num_threads=num_threads)
                # Call doublets
                with Timer('calling doublets', verbose=verbose):
                    if doublet_fraction is None:
                        # Call doublets based on whether their cxds score is
                        # above the median cxds score for simulated doublets
                        doublets_batch = \
                            cxds_scores_batch >= np.median(cxds_scores_sim)
                    else:
                        # Call a fixed fraction of cells as doublets
                        rank = rankdata(cxds_scores_batch, method='ordinal')
                        doublets_batch = \
                            rank > (1 - doublet_fraction) * len(rank)
                # If using multiple batches, map doublet labels and scores back
                # to the full dataset
                with Timer('mapping back to full dataset', verbose=verbose):
                    if batch_column is not None:
                        doublets[batch_indices] = doublets_batch
                        doublet_scores[batch_indices] = cxds_scores_batch
                    else:
                        doublets = doublets_batch
                        doublet_scores = cxds_scores_batch
        finally:
            X._num_threads = original_num_threads
        return doublets, doublet_scores
    
    def find_doublets(self,
                      batch_column: SingleCellColumn | None,
                      *,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      doublet_fraction: float | np.floating | None = None,
                      num_genes: int | np.integer = 500,
                      doublet_column: str = 'doublet',
                      doublet_score_column: str = 'doublet_score',
                      overwrite: bool = False,
                      verbose: bool = False,
                      num_threads: int | np.integer | None = None):
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507).
        
        Doublets can also be filtered out with `qc(..., remove_doublets=True)`.
        If `remove_doublets=True` was specified in `qc()`, this function should
        not also be used.
        
        This function gives the same result regardless of whether it is run
        before or after normalization. The actual expression value does not
        matter, only whether or not it is zero.
        
        Doublets cannot occur across sequencing batches, so make sure to
        specify `batch_column` if your dataset has multiple batches! Doublet
        detection will be done independently within each batch.
        
        Since the cxds score is continuous, it needs to be converted into a
        binary classification of doublets versus non-doublets. This problem can
        be framed as finding a cxds score threshold above which a cell is
        deemed to be a doublet. To determine this threshold, we simulate
        doublets by combining the counts from randomly selected pairs of cells,
        via the following steps:
        
        1) Sample as many random pairs of cells (with replacement) as there are
           real cells.
        2) Combine the counts from each pair of cells into a simulated doublet.
           Because cxds operates on binarized count matrices, we average the
           two cells' count matrices in a binary sense: if a gene is expressed
           in either cell, it is deemed to be expressed in the simulated
           doublet, but if it has a count of 1 in one cell and 0 in the other,
           it is randomly chosen to be either expressed or not expressed with
           equal probability (since the average count would be 0.5).
        3) Calculate cxds scores for these simulated doublets, based on the
           coexpression patterns (the `S` matrix from cxds) learned from the
           real data.
        4) Take the median cxds score of the simulated doublets as the
           threshold. In other words, if a real cell has a higher doublet score
           than the average simulated doublet, we call it a doublet.
           
        Alternatively, specify `doublet_fraction` to force a specific fraction
        of cells to be classified as doublets.
           
        Args:
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described above.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            doublet_column: the name of a Boolean column to be added to `obs`
                            containing the doublet labels, i.e. whether each
                            cell is predicted to be a doublet
            doublet_score_column: the name of a column to be added to `obs`
                                  containing each cell's doublet score. Higher
                                  scores indicate greater likelihood of being a
                                  doublet. Scores are not normalized and are
                                  not comparable across datasets or batches,
                                  but are guaranteed to be positive (since they
                                  are sums of log p-values).
            overwrite: if `True`, overwrite `doublet_column` and/or
                       `doublet_score_column` if already present in `obs`,
                       instead of raising an error
            verbose: whether to print debug information about timings at each
                     step (will be removed for the final version)
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).

        Returns:
            A new SingleCell dataset where `var` contains two additional
            columns, `doublet_column` (default: `doublet`), indicating whether
            each cell is predicted to be a doublet, and `doublet_score_column`
            (default: `'doublet_score'`), containing each cell's doublet score.
        
        Note:
            This function's cxds scores are almost exactly half the original
            implementation's, because it avoids double-counting the two genes
            in each gene pair. Slight deviations from this one-half (usually
            by less than one part in a million) may occur because this function
            uses a normal approximation to the binomial p-value to avoid long
            runtimes on large datasets.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        if self._X is None:
            error_message = 'X is None, so doublet finding is not possible'
            raise TypeError(error_message)
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_doublets()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get `QC_column` and `batch_column`, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if batch_column is not None:
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column)
        # Check that `doublet_fraction`, if specified, is > 0 and < 1
        if doublet_fraction is not None:
            check_type(doublet_fraction, 'doublet_fraction', float,
                       'a number greater than 0 and less than 1')
            check_bounds(doublet_fraction, 'doublet_fraction', 0, 1,
                         left_open=True, right_open=True)
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `doublet_column` and `doublet_score_column` are strings
        # and, unless `overwrite=True`, not already in `obs`
        for column, column_name in (
                (doublet_column, 'doublet_column'),
                (doublet_score_column, 'doublet_score_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column of obs; '
                    f'did you already run find_doublets()? Set overwrite=True '
                    f'to overwrite.')
                raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Run doublet detection
        doublets, doublet_scores = SingleCell._find_doublets(
            X=self._X, batch_column=batch_column, QC_column=QC_column,
            doublet_fraction=doublet_fraction, num_genes=num_genes,
            num_threads=num_threads, verbose=verbose)
        # Convert doublet labels and scores to polars Series to add to `obs`.
        # If `QC_column` exists, set doublet labels and scores to `null` for
        # cells failing QC.
        doublet_column = pl.Series(doublet_column, doublets)
        doublet_score_column = pl.Series(doublet_score_column, doublet_scores)
        if QC_column is None:
            return self.with_columns_obs(doublet_column, doublet_score_column)
        else:
            return self.with_columns_obs(
                pl.when(QC_column).then(doublet_column),
                pl.when(QC_column).then(doublet_score_column))
    
    def make_obs_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make `obs_names` unique by appending `'-1'` to the second occurence of
        a given name, `'-2'` to the third occurrence, and so on, where `'-'`
        can be switched to a different string via the `separator` argument.
        Raises an error if any `obs_names` already contain `separator`.
        
        Args:
            separator: the string connecting the original name and the integer
                       suffix

        Returns:
            A new SingleCell dataset with the `obs_names` made unique.
        """
        check_type(separator, 'separator', str, 'a string')
        if self.obs_names.str.contains(separator).any():
            error_message = (
                f'some obs_names already contain the separator {separator!r}; '
                f'did you already run make_obs_names_unique()? If not, set '
                f'the separator argument to a different string.')
            raise ValueError(error_message)
        obs_names = pl.col(self.obs_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(obs_names)
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(obs_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(obs_names)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def make_var_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make `var_names` unique by appending `'-1'` to the second occurence of
        a given name, `'-2'` to the third occurrence, and so on, where `'-'`
        can be switched to a different string via the `separator` argument.
        Raises an error if any `var_names` already contain `separator`.
        
        Args:
            separator: the string connecting the original name and the integer
                       suffix

        Returns:
            A new SingleCell dataset with the `var_names` made unique.
        """
        var_names = pl.col(self.var_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(var_names)
        return SingleCell(X=self._X,
                          obs=self._obs,
                          var=self._var.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(var_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(var_names)),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns)
    
    def get_sample_covariates(self,
                              ID_column: SingleCellColumn,
                              *,
                              QC_column: SingleCellColumn |
                                         None = 'passed_QC') -> pl.DataFrame:
        """
        Get a DataFrame of sample-level covariates, i.e. the columns of `obs`
        that are the same for all cells within each sample.
        
        Args:
            ID_column: a column of `obs` containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
        
        Returns:
            A DataFrame of the sample-level covariates, with ID_column (sorted)
            as the first column.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column)
        ID_column_name = ID_column.name
        obs = self._obs
        if QC_column is not None:
            obs = obs.filter(QC_column)
            ID_column = ID_column.filter(QC_column)
        return obs\
            .select(ID_column,
                    *obs
                    .group_by(ID_column)
                    .n_unique()
                    .pipe(filter_columns,
                          (pl.exclude(ID_column_name)
                           if ID_column_name in obs else pl.all()).max().eq(1))
                    .columns)\
            .unique(ID_column_name)\
            .sort(ID_column_name)
    
    def pseudobulk(self,
                   ID_column: SingleCellColumn,
                   cell_type_column: SingleCellColumn,
                   *,
                   QC_column: SingleCellColumn | None = 'passed_QC',
                   additional_obs: pl.DataFrame | None = None,
                   include_nulls: bool = False,
                   sort_genes: bool = False,
                   num_threads: int | np.integer | None = None,
                   verbose: bool = True) -> Pseudobulk:
        """
        Pseudobulk a SingleCell dataset with sample ID and cell type columns.
        
        Counts from cells with the same pair of values in `ID_column` and
        `cell_type_column` will be summed to a single value. Cells with `null`
        in either column are excluded, unless `include_nulls=True`.
        
        You can run this function multiple times at different cell type
        resolutions by setting a different `cell_type_column` each time, then
        combining the results after with the `|` operator:
        
        ```
        pb_broad = sc.pseudobulk('ID', 'broad_cell_type')
        pb_fine = sc.pseudobulk('ID', 'fine_grained_cell_type')
        pb = pb_broad | pb_fine
        ```
        
        Args:
            ID_column: a column of `obs` containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be excluded
                       from the pseudobulk.
            additional_obs: an optional DataFrame of additional sample-level
                            covariates, which will be joined to the
                            pseudobulk's `obs` for each cell type
            include_nulls: whether to exclude cells with `null` values in
                           `ID_column` and/or `cell_type_column` from the
                           pseudobulk. If `include_nulls=True`, `null` will be
                           treated just like any other value. This means that,
                           for instance, all cells from a given cell type that
                           have `null` as the sample ID will be pseudobulked
                           together, as will all cells from a given sample ID
                           that have `null` as the cell type.
            sort_genes: whether to sort genes in alphabetical order in the
                        pseudobulk; by default, genes appear in the same order
                        as in the SingleCell dataset
            num_threads: the number of threads to use when pseudobulking;
                         parallelism happens across {sample, cell type} pairs
                         (or just samples, if `cell_type_column` is `None`).
                         Set `num_threads=-1` to use all available cores
                         (as determined by `os.cpu_count()`), or leave unset to
                         use `single_cell.options()['num_threads']` cores (1 by
                         default). For count matrices stored in the usual CSR
                         format, parallelization takes place across cell types
                         and samples, so specifying more cores than the number
                         of cell type-sample pairs may not improve performance.
            verbose: whether to print the number of cells excluded when
                     `include_nulls=False`
        
        Returns:
            A Pseudobulk dataset with `X` (the pseudobulked counts), `obs`
            (metadata per sample), and `var` (metadata per gene) fields, each
            of which are dictionaries across cell types. The columns of each
            cell type's `obs` will be:
            - `ID_column`
            - `'num_cells'` (the number of cells for that sample and cell type)
            followed by whichever columns of the SingleCell dataset's `obs` are
            constant across samples. `var` will be identical to the SingleCell
            dataset's `var`.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        X = self._X
        if X is None:
            error_message = 'X is None, so pseudobulking is not possible'
            raise TypeError(error_message)
        # Check that `self` is QCed and not normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "pseudobulk()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = (
                "uns['normalized'] is True; did you already run normalize()?")
            raise ValueError(error_message)
        # Check inputs
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        original_ID_column = ID_column
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column,
                                     allow_null=True)
        ID_column_name = ID_column.name
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column,
                             allow_null=True)
        cell_type_column_name = cell_type_column.name
        num_cells_column_name = 'num_cells'
        for column_description, column_name in ('ID_column', ID_column_name), \
                ('cell_type_column', cell_type_column_name):
            if column_name == num_cells_column_name:
                error_message = (
                    f'{column_description} has the name '
                    f'{num_cells_column_name!r}, which conflicts with the '
                    f'name of the column to be added to the Pseudobulk '
                    f'dataset containing the number of cells of each cell '
                    f'type')
                raise ValueError(error_message)
        check_type(include_nulls, 'include_nulls', bool, 'Boolean')
        check_type(sort_genes, 'sort_genes', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        if additional_obs is not None:
            check_type(additional_obs, 'additional_obs', pl.DataFrame,
                       'a polars DataFrame')
            if ID_column_name not in additional_obs:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f'{ID_column_description} is not a column of '
                    f'additional_obs')
                raise ValueError(error_message)
            if ID_column.dtype != additional_obs[ID_column_name].dtype:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f"{ID_column_description} has a different data type in "
                    f"additional_obs than in this SingleCell dataset's obs")
                raise TypeError(error_message)
        # Check that the first column of `var` is String, Enum, or Categorical:
        # this is a requirement of the Pseudobulk class. (The first column of
        # `obs` must be as well, but this will always be true by construction,
        # since it will always be the sample ID.)
        if self.var_names.dtype not in (pl.Categorical, pl.Enum, pl.String):
            error_message = (
                f'the first column of var (var_names) has data type '
                f'{self.obs_names.dtype!r}, but must be String, Enum, '
                f'or Categorical')
            raise ValueError(error_message)
        # Get the row indices that will be pseudobulked across for each group
        # (cell type-sample pair), ignoring cells failing QC when `QC_column`
        # is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (pl.LazyFrame((cell_type_column, ID_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, ID_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name, ID_column_name)\
            .agg('_SingleCell_group_indices',
                 pl.len().alias(num_cells_column_name))\
            .sort(cell_type_column_name, ID_column_name)\
            .collect()
        # Exclude cells with null values in `ID_column` and/or
        # `cell_type_column`, if `include_nulls=False`
        if not include_nulls:
            if verbose:
                excluded = groups\
                    .filter(pl.any_horizontal(pl.col(
                        cell_type_column_name, ID_column_name).is_null()))
                num_ID_null_only = excluded\
                    .filter(pl.col(ID_column_name).is_null(),
                            pl.col(cell_type_column_name).is_not_null())\
                    [num_cells_column_name]\
                    .sum()
                num_cell_type_null_only = excluded\
                    .filter(pl.col(ID_column_name).is_not_null(),
                            pl.col(cell_type_column_name).is_null())\
                    [num_cells_column_name]\
                    .sum()
                num_excluded = excluded[num_cells_column_name].sum()
                num_both_null = \
                    num_excluded - num_ID_null_only - num_cell_type_null_only
                if num_excluded > 0:
                    print(f'Excluding {num_excluded:,} cells when '
                          f'pseudobulking: {num_both_null:,} with nulls in '
                          f'both the ID ({ID_column!r}) and cell-type '
                          f'({cell_type_column!r}) columns, '
                          f'{num_ID_null_only:,} with nulls in just the ID '
                          f'column, and {num_cell_type_null_only:,} with '
                          f'nulls in just the cell-type column.')
                    groups = groups.drop_nulls()
            else:
                groups = groups.drop_nulls()
        # Pseudobulk, storing the result in a preallocated NumPy array
        result = np.zeros((len(groups), X.shape[1]), dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = groups[num_cells_column_name].cum_sum().to_numpy()
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_sum_csr(
                        const numeric[::1] data,
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned group, cell, num_groups = group_ends.shape[0]
                    cdef int row
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type-sample pair)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse
                                # matrix
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for
                                # this cell...
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    # Add the value at this cell and gene to
                                    # the total for this group and gene
                                    result[group, indices[gene]] += \
                                        <unsigned> data[gene]
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    result[group, indices[gene]] += \
                                        <unsigned> data[gene]
                ''')['groupby_sum_csr'](
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    group_indices=group_indices, group_ends=group_ends,
                    result=result, num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = QC_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_sum_csc(
                        const numeric[::1] data,
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_map,
                        const bint has_missing,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned gene, num_genes = result.shape[1]
                    cdef int group
                    cdef unsigned long cell
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene (column of the sparse array)...
                            for gene in range(num_genes):
                                # For each cell (row) that's non-zero for this
                                # gene...
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    # Get the group index for this cell (-1 if it
                                    # failed QC)
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    # Add the value at this cell and gene to the
                                    # total for this group and gene
                                    result[group, gene] += <unsigned> data[cell]
                        else:
                            for gene in range(num_genes):
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, gene] += <unsigned> data[cell]
                    else:
                        if has_missing:
                            for gene in prange(num_genes, nogil=True,
                                               num_threads=num_threads):
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    result[group, gene] += <unsigned> data[cell]
                        else:
                            for gene in prange(num_genes, nogil=True,
                                               num_threads=num_threads):
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, gene] += <unsigned> data[cell]
                    ''')['groupby_sum_csc'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=result, num_threads=num_threads)
        # Sort genes, if `sort_genes=True`
        cell_type_var = self._var
        if sort_genes:
            result = result[:, cell_type_var[:, 0].arg_sort().to_numpy()]
            cell_type_var = cell_type_var.sort(cell_type_var.columns[0])
        # Break up the results by cell type
        sample_covariates = self.get_sample_covariates(ID_column,
                                                       QC_column=QC_column)
        X, obs, var = {}, {}, {}
        start_index = 0
        for cell_type, count in groups[cell_type_column_name]\
                .value_counts().sort(cell_type_column_name).iter_rows():
            end_index = start_index + count
            X[cell_type] = result[start_index:end_index]
            obs[cell_type] = groups.lazy()\
                .select(ID_column_name, num_cells_column_name)\
                .slice(start_index, count)\
                .join(sample_covariates.lazy(), on=ID_column_name, how='left')\
                .pipe(lambda df: df.join(additional_obs.lazy(),
                                         on=ID_column_name, how='left')
                      if additional_obs is not None else df)\
                .pipe(lambda df: df if QC_column is None else
                                 df.drop(QC_column.name))\
                .collect()
            var[cell_type] = cell_type_var
            start_index = end_index
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def hvg(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            batch_column: SingleCellColumn | None |
                          Sequence[SingleCellColumn | None] = None,
            num_genes: int | np.integer = 2000,
            min_cells: int | np.integer = 3,
            exclude: str | Sequence[str] | None = None,
            flavor: Literal['seurat_v3', 'seurat_v3_paper'] = 'seurat_v3',
            span: int | float | np.integer | np.floating = 0.3,
            hvg_column: str = 'highly_variable',
            rank_column: str = 'highly_variable_rank',
            allow_float: bool = False,
            overwrite: bool = False,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Select highly variable genes using Seurat's algorithm. Operates on
        raw counts.
        
        By default, uses the same approach as Scanpy's
        `scanpy.pp.highly_variable_genes` function with the `flavor` argument
        set to the non-default value `'seurat_v3'`, and Seurat's
        `FindVariableFeatures` function with the `selection.method` argument
        set to the default value `'vst'`.

        Requires the scikit-misc package; install with:
        pip install --no-deps --no-build-isolation scikit-misc
        
        The general idea is that since genes with higher mean expression tend
        to have higher variance in expression (because they have more non-zero
        values), we want to select genes that have a high variance *relative to
        their mean expression*. Otherwise, we'd only be picking highly
        expressed genes! To correct for the mean-variance relationship, fit a
        LOESS curve fit to the mean-variance trend.
        
        Args:
            others: optional SingleCell datasets to jointly compute highly
                    variable genes across, alongside this one. Each dataset
                    will be treated as a separate batch. If `batch_column` is
                    not `None`, each dataset AND each distinct value of
                    `batch_column` within each dataset will be treated as a
                    separate batch. Variances will be computed per batch and
                    then aggregated (see `flavor`) across batches.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
                       When `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `hvg()`, except that the
                          `min_cells` filter will always be calculated
                          per-dataset rather than per-batch. Variances will be
                          computed per batch and then aggregated (see `flavor`)
                          across batches. Set to `None` to treat each dataset
                          as having a single batch. When `others` is specified,
                          `batch_column` can be a length-`1 + len(others)`
                          sequence of columns, expressions, Series, functions,
                          or `None` for each dataset (for `self`, followed by
                          each dataset in `others`).
            num_genes: the number of highly variable genes to select. The
                       default of 2000 matches Seurat and Scanpy's recommended
                       value. Fewer than `num_genes` genes will be selected if
                       not enough genes have non-zero count in >= `min_cells`
                       cells (or when `min_cells` is `None`, if not enough
                       genes are present).
            min_cells: if not `None`, filter to genes detected (with non-zero
                       count) in >= this many cells in every dataset, before
                       calculating highly variable genes. The default value of
                       3 matches Seurat and Scanpy's recommended value. Note
                       that genes with zero variance in any dataset will always
                       be filtered out, even if `min_cells` is 0.
            exclude: one or more optional case-insensitive regular expressions
                     matching genes to exclude from the highly variable gene
                     calculation. For instance, to exclude mitochondrial genes
                     (starting with `'MT-'`) and ribosomal genes (starting with
                     `'RPL-'`, `'RPS'`, `'MRPL'`, or `'MRPS'`), specify
                     `exclude=('^MT-', '^RPL', '^RPS', '^MRPL', '^MRPS')`.
            flavor: the highly variable gene algorithm to use. Must be one of
                    `seurat_v3` and `seurat_v3_paper`, both of which match the
                    algorithms with the same name in Scanpy. Both algorithms
                    select genes based on two criteria: 1) which genes are
                    ranked as most variable (taking the median of the ranks
                    across batches where the gene is among the top `num_genes`
                    highly variable genes) and 2) the number of batches in
                    which a gene is ranked in among the top `num_genes` in
                    variability. `seurat_v3` ranks genes by 1) and uses 2) to
                    tiebreak, whereas `seurat_v3_paper` ranks genes by 2) and
                    uses 1) to tiebreak. When there is only one batch, both
                    algorithms are the same and only rank based on 1).
            span: the span of the LOESS fit; higher values will lead to more
                  smoothing
            hvg_column: the name of a Boolean column to be added to (each
                        dataset's) `var` indicating the highly variable genes
            rank_column: the name of an integer column to be added to (each
                         dataset's) `var` with the rank of each highly variable
                         gene's variance (1 = highest variance, 2 =
                         next-highest, etc.); will be `null` for non-highly
                         variable genes. In the very unlikely event of ties,
                         the gene that appears first in `var` will get the
                         lowest rank.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            overwrite: if `True`, overwrite `hvg_column` and/or `rank_column`
                       if already present in `var`, instead of raising an error
            num_threads: the number of threads to use when finding highly
                         variable genes. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A new SingleCell dataset where `var` contains an additional Boolean
            column, `hvg_column` (default: `'highly_variable'`), indicating the
            `num_genes` most highly variable genes, and `rank_column` (default:
            'highly_variable_rank') indicating the (one-based) rank of each
            highly variable gene's variance, with `null` values for non-highly
            variable genes. Or, if additional SingleCell dataset(s) are
            specified via the `others` argument, a length-`1 + len(others)`
            tuple of SingleCell datasets with these two columns added: `self`,
            followed by each dataset in `others`.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        
        Note:
            This function may not give identical results to Seurat and Scanpy.
            It avoids floating-point summation, which is more numerically
            stable than Scanpy and Seurat's calculations. If multiple genes are
            tied as the `num_genes`-th most highly variable gene in a batch or
            dataset, this function includes all of them, whereas Seurat and
            Scanpy arbitrarily pick one (or a subset) of them. Also, this
            function uses the ordering from a stable sort to break ties when
            selecting the final list of highly variable genes, instead of the
            unstable sort used by Seurat and Scanpy.
        """
        # noinspection PyUnresolvedReferences
        from skmisc.loess import loess
        if self._X is None:
            error_message = \
                'X is None, so highly variable gene finding is not possible'
            raise TypeError(error_message)
        # Check that all elements of `others` are SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Check that all datasets are QCed and not normalized
        if not all(dataset._uns['QCed'] for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f"uns['QCed'] is False{suffix}; did you forget to run qc() "
                f"before hvg()? Set uns['QCed'] = True or run "
                f"with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if any(dataset._uns['normalized'] for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f"hvg() requires raw counts but uns['normalized'] is "
                f"True{suffix}; did you already run normalize()?")
            raise ValueError(error_message)
        # Check that there are at least three cells in each dataset (since
        # LOESS seems to need at least three observations to converge)
        if any(len(dataset._obs) < 3 for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f'there are fewer than three cells{suffix}, so '
                f'highly variable genes cannot be calculated')
            raise ValueError(error_message)
        # Get `QC_column` and `batch_column` from every dataset, if not `None`
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        # Check that `min_cells` is a positive integer and at least as large as
        # the number of cells in each dataset
        check_type(min_cells, 'min_cells', int, 'a non-negative integer')
        check_bounds(min_cells, 'min_cells', 0)
        if any(len(dataset._obs) < min_cells for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f'the number of cells in this dataset ({len(self._obs)}) '
                f'is less than min_cells ({min_cells}){suffix}; increase '
                f'min_cells')
            raise ValueError(error_message)
        # Check that `exclude`, if specified, is a string or sequence thereof
        if exclude is not None:
            exclude = to_tuple_checked(exclude, 'exclude', str, 'strings')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `hvg_column` and `rank_column` are strings and, unless
        # `overwrite=True`, not already in `var` for any dataset
        for column, column_name in (hvg_column, 'hvg_column'), \
                (rank_column, 'rank_column'):
            check_type(column, column_name, str, 'a string')
            if not overwrite and \
                    any(column in dataset._var for dataset in datasets):
                suffix = ' for at least one dataset' if others else ''
                error_message = (
                    f'{column_name} {column!r} is already a column of '
                    f'var{suffix}; did you already run hvg()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # If `allow_float=False`, raise an error if `X` is floating-point
        # for any dataset
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float:
            for dataset in datasets:
                X = dataset._X
                if np.issubdtype(X.dtype, np.floating):
                    error_message = (
                        f'hvg() requires raw counts but X has data type '
                        f'{str(X.dtype)!r}, a floating-point data type. If '
                        f'you are sure that all values are raw integer '
                        f'counts, i.e. that (X.data == X.data.astype(int))'
                        f'.all(), then set allow_float=True.')
                    raise TypeError(error_message)
        # Get the universe of genes we'll be considering: those present in any
        # dataset, and not matching `exclude` (if specified). If there are
        # multiple datasets, also get the indices of these genes in each
        # dataset (with `null` for genes not present in that particular
        # dataset).
        if others:
            # The use of `align_frames` here is a bit wasteful memory-wise,
            # because it creates an identical `'gene'` column for every
            # DataFrame in `genes_and_indices`. Fortunately, it's only one
            # small string column per dataset.
            genes_and_indices = pl.align_frames(*(
                dataset.var[:, 0]
                .to_frame('gene')
                .pipe(lambda df: df.filter(~pl.col.gene.str.contains(
                    '(?i)' + '|'.join(exclude)))  # case-insensitive
                    if exclude is not None else df)
                .with_columns(index=pl.int_range(pl.len(), dtype=pl.Int32))
                for dataset in datasets), on='gene')
            # noinspection PyTypeChecker
            genes_in_any_dataset = genes_and_indices[0]['gene']
            # noinspection PyTypeChecker
            dataset_gene_indices = [df['index'] for df in genes_and_indices]
            del genes_and_indices
        else:
            genes_in_any_dataset = self.var_names\
                .rename('gene')\
                .to_frame()\
                .pipe(lambda df: df.filter(~pl.col.gene.str.contains(
                    '(?i)' + '|'.join(exclude)))  # case-insensitive
                    if exclude is not None else df)\
                .to_series()
        # Get the batches to calculate variance across (datasets + batches
        # within each dataset)
        if others:
            if batch_column is None:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X, QC_column.to_numpy()
                            if QC_column is not None else None, gene_indices)
                           for dataset, QC_column, gene_indices in
                           zip(datasets, QC_columns, dataset_gene_indices))
            else:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X,
                            (batch_column.eq(batch) if QC_column is None else
                             batch_column.eq(batch) & QC_column).to_numpy()
                            if batch is not None else
                            (QC_column.to_numpy() if QC_column is not None else
                             None), gene_indices)
                           for dataset, QC_column, batch_column, gene_indices
                           in zip(datasets, QC_columns, batch_columns,
                                  dataset_gene_indices)
                           for batch in ((None,) if batch_column is None else
                                         batch_column.unique()))
        else:
            X = self._X
            batch_column = batch_columns[0]
            if batch_column is None:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = (X, QC_columns[0].to_numpy(), None),
                else:
                    batches = (X, None, None),
            else:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = ((X, (batch_column.eq(batch) & QC_columns[0])
                                   .to_numpy(), None)
                               for batch in batch_column.unique())
                else:
                    batches = ((X, batch_column.eq(batch).to_numpy(), None)
                               for batch in batch_column.unique())
        # Get the variance of each gene in each batch across cells passing QC
        norm_gene_vars = []
        for X, cell_mask, gene_indices in batches:
            num_dataset_genes = X.shape[1]
            mean = np.empty(num_dataset_genes)
            var = np.empty(num_dataset_genes)
            nonzero_count = np.empty(num_dataset_genes, dtype=np.uint32)
            is_CSR = isinstance(X, csr_array)
            if is_CSR:
                if cell_mask is None:
                    cell_indices = np.array([], dtype=np.int64)
                    num_cells = X.shape[0]
                else:
                    cell_indices = np.flatnonzero(cell_mask)
                    num_cells = len(cell_indices)
                # noinspection PyShadowingBuiltins
                sum = np.empty(num_dataset_genes * num_threads, dtype=np.uint64)
                sum_of_squares = \
                    np.empty(num_dataset_genes * num_threads, dtype=np.uint64)
                cython_inline(r'''
                from cython.parallel cimport prange
                from libcpp.vector cimport vector
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def sparse_mean_var_minor_axis(
                        const numeric[::1] data,
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const long[::1] cell_indices,
                        const unsigned long num_cells,
                        const unsigned num_dataset_genes,
                        unsigned long[::1] sum,
                        unsigned long[::1] sum_of_squares,
                        double[::1] mean,
                        double[::1] var,
                        unsigned[::1] nonzero_count,
                        const unsigned num_threads):
                        
                    cdef unsigned long num_elements, i, j, value, start, end, \
                        chunk_size, total_sum, total_sum_of_squares, \
                        total_nonzero_count
                    cdef unsigned gene, cell, dest_start, dest_end, thread_index
                    cdef double inv_num_cells = 1.0 / num_cells, \
                        inv_num_pairs_of_cells = \
                        1.0 / (num_cells * (num_cells - 1))
                    cdef vector[vector[unsigned]] thread_nonzero_counts
                    
                    if num_threads == 1:
                        sum[:] = 0
                        sum_of_squares[:] = 0
                        nonzero_count[:] = 0
                        if cell_indices.shape[0] == 0:
                            # Iterate over all elements of the count
                            # matrix, ignoring which cell they're from
                            
                            num_elements = indices.shape[0]
                            for i in range(num_elements):
                                gene = indices[i]
                                value = <unsigned long> data[i]
                                sum[gene] += value
                                sum_of_squares[gene] += value * value
                                nonzero_count[gene] += 1
                        else:
                            # Only iterate over the elements from cells
                            # in `cell_indices` (i.e. cells in this
                            # batch, and/or passing QC)
                            
                            for j in range(num_cells):
                                cell = cell_indices[j]
                                for i in range(<unsigned long> indptr[cell],
                                               <unsigned long> indptr[cell + 1]):
                                    gene = indices[i]
                                    value = <unsigned long> data[i]
                                    sum[gene] += value
                                    sum_of_squares[gene] += \
                                        value * value
                                    nonzero_count[gene] += 1
                        
                        # Calculate means and variances from the sums
                        # and squared sums, including the contribution
                        # from zero elements
                        
                        for gene in range(num_dataset_genes):
                            mean[gene] = sum[gene] * inv_num_cells
                            var[gene] = inv_num_pairs_of_cells * (
                                num_cells * sum_of_squares[gene] -
                                sum[gene] * sum[gene])
                    else:
                        thread_nonzero_counts.resize(num_threads)
                        with nogil:
                            if cell_indices.shape[0] == 0:
                                # Partition the work by elements, not
                                # cells, for better load-balancing in
                                # case cells have substantially
                                # different library sizes
                                
                                num_elements = indices.shape[0]
                                chunk_size = \
                                    num_elements // num_threads
                                for thread_index in prange(num_threads,
                                                           num_threads=num_threads,
                                                           schedule='static',
                                                           chunksize=1):
                                    thread_nonzero_counts[thread_index].resize(num_dataset_genes)
                                    start = thread_index * chunk_size
                                    end = num_elements if thread_index == \
                                        num_threads - 1 else start + chunk_size
                                    dest_start = thread_index * num_dataset_genes
                                    dest_end = dest_start + num_dataset_genes
                                    sum[dest_start:dest_end] = 0
                                    sum_of_squares[dest_start:dest_end] = 0
                                    for i in range(start, end):
                                        gene = indices[i]
                                        value = <unsigned long> data[i]
                                        sum[dest_start + gene] += value
                                        sum_of_squares[dest_start + gene] += \
                                            value * value
                                        thread_nonzero_counts[thread_index][gene] += 1
                            else:
                                # Partition the work by cells
                                
                                chunk_size = num_cells // num_threads
                                for thread_index in prange(num_threads,
                                                           num_threads=num_threads,
                                                           schedule='static',
                                                           chunksize=1):
                                    thread_nonzero_counts[thread_index].resize(num_dataset_genes)
                                    start = thread_index * chunk_size
                                    end = num_cells if thread_index == \
                                        num_threads - 1 else start + chunk_size
                                    dest_start = thread_index * num_dataset_genes
                                    dest_end = dest_start + num_dataset_genes
                                    sum[dest_start:dest_end] = 0
                                    sum_of_squares[dest_start:dest_end] = 0
                                    for j in range(start, end):
                                        cell = cell_indices[j]
                                        for i in range(<unsigned long> indptr[cell],
                                                       <unsigned long> indptr[cell + 1]):
                                            gene = indices[i]
                                            value = <unsigned long> data[i]
                                            sum[dest_start + gene] += value
                                            sum_of_squares[dest_start + gene] += \
                                                value * value
                                            thread_nonzero_counts[thread_index][gene] += 1
                            
                            # Calculate means and variances by
                            # aggregating the sums and squared sums
                            # across threads
                            
                            for gene in prange(num_dataset_genes,
                                               num_threads=num_threads):
                                total_sum = 0
                                total_sum_of_squares = 0
                                total_nonzero_count = 0
                                for thread_index in range(num_threads):
                                    total_sum = total_sum + \
                                        sum[thread_index * num_dataset_genes + gene]
                                    total_sum_of_squares = total_sum_of_squares + \
                                        sum_of_squares[thread_index * num_dataset_genes + gene]
                                    total_nonzero_count = total_nonzero_count + \
                                        thread_nonzero_counts[thread_index][gene]
                                mean[gene] = total_sum * inv_num_cells
                                var[gene] = inv_num_pairs_of_cells * (
                                    num_cells * total_sum_of_squares -
                                    total_sum * total_sum)
                                nonzero_count[gene] = total_nonzero_count
                    ''')['sparse_mean_var_minor_axis'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_indices=cell_indices, num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes, sum=sum,
                        sum_of_squares=sum_of_squares, mean=mean, var=var,
                        nonzero_count=nonzero_count, num_threads=num_threads)
            else:
                if cell_mask is None:
                    cell_mask = np.array([], dtype=bool)
                    num_cells = X.shape[0]
                else:
                    num_cells = cell_mask.sum()
                cython_inline(r'''
                    from cython.parallel cimport prange
                    
                    ctypedef fused numeric:
                        int
                        unsigned
                        long
                        unsigned long
                        float
                        double
                    
                    ctypedef fused signed_integer:
                        int
                        long
                    
                    def sparse_mean_var_major_axis(
                            const numeric[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            char[::1] cell_mask,
                            const unsigned long num_cells,  # avoid overflow on (num_cells * (num_cells - 1))
                            const unsigned num_dataset_genes,
                            double[::1] mean,
                            double[::1] var,
                            unsigned[::1] nonzero_count,
                            const unsigned num_threads):
                    
                        cdef unsigned long i, value, sum, sum_of_squares, gene
                        cdef double inv_num_cells = 1.0 / num_cells, \
                            inv_num_pairs_of_cells = \
                            1.0 / (num_cells * (num_cells - 1))
                        
                        if num_threads == 1:
                            if cell_mask.shape[0] == 0:
                                for gene in range(num_dataset_genes):
                                    # Calculate the sum and squared sum for
                                    # this gene, across cells with non-zero
                                    # counts for the gene
                                    
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        value = <unsigned long> data[i]
                                        sum += value
                                        sum_of_squares += value * value
                                        nonzero_count[gene] += 1
                                    # Calculate the mean and variance from the
                                    # sum and squared sum, including the
                                    # contribution from zero elements
                                    
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                            else:
                                # As above, but only include cells where
                                # `cell_mask` is `True`
                                
                                for gene in range(num_dataset_genes):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        if cell_mask[indices[i]]:
                                            value = <unsigned long> data[i]
                                            sum += value
                                            sum_of_squares += value * value
                                            nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                        else:
                            if cell_mask.shape[0] == 0:
                                for gene in prange(num_dataset_genes,
                                                   nogil=True,
                                                   num_threads=num_threads):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        value = <unsigned long> data[i]
                                        sum = sum + value
                                        sum_of_squares = \
                                            sum_of_squares + value * value
                                        nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                            else:
                                for gene in prange(num_dataset_genes,
                                                   nogil=True,
                                                   num_threads=num_threads):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        if cell_mask[indices[i]]:
                                            value = <unsigned long> data[i]
                                            sum = sum + value
                                            sum_of_squares = \
                                                sum_of_squares + value * value
                                            nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                    ''')['sparse_mean_var_major_axis'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_mask=cell_mask, num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes, mean=mean,
                        var=var, nonzero_count=nonzero_count,
                        num_threads=num_threads)
            
            not_constant = var > 0
            y = np.log10(var[not_constant])
            x = np.log10(mean[not_constant])
            model = loess(x, y, span=span)
            try:
                model.fit()
            except ValueError as e:
                error_message = (
                    f'LOESS model fitting failed; this is unusual and tends '
                    f'to only happen when there are very few cells'
                    f' (e.g. under 500), which '
                    f'{"is" if num_cells < 500 else "is not"} the case here')
                raise ValueError(error_message) from e
            
            estimated_variance = np.empty(num_dataset_genes)
            estimated_variance[not_constant] = model.outputs.fitted_values
            estimated_variance[~not_constant] = 0
            estimated_stddev = np.sqrt(10 ** estimated_variance)
            clip_val = mean + estimated_stddev * np.sqrt(num_cells)
            
            batch_counts_sum = np.zeros(num_dataset_genes)
            squared_batch_counts_sum = np.zeros(num_dataset_genes)
            if is_CSR:
                batch_counts_sum = np.empty(num_dataset_genes)
                squared_batch_counts_sum = np.empty(num_dataset_genes)
                batch_counts_sum_int = \
                    np.empty(num_dataset_genes * num_threads, dtype=np.uint64)
                squared_batch_counts_sum_int = \
                    np.empty(num_dataset_genes * num_threads, dtype=np.uint64)
                num_out_of_range = \
                    np.empty(num_dataset_genes * num_threads, dtype=np.uint64)
                # noinspection PyUnboundLocalVariable
                cython_inline(r'''
                    from cython.parallel cimport prange
                    
                    ctypedef fused numeric:
                        int
                        unsigned
                        long
                        unsigned long
                        float
                        double
                    
                    ctypedef fused signed_integer:
                        int
                        long
                    
                    def clipped_sum(const numeric[::1] data,
                                    const signed_integer[::1] indices,
                                    const signed_integer[::1] indptr,
                                    const unsigned num_cells,
                                    const unsigned num_dataset_genes,
                                    const long[::1] cell_indices,
                                    const double[::1] clip_val,
                                    unsigned long[::1] batch_counts_sum_int,
                                    unsigned long[::1] squared_batch_counts_sum_int,
                                    unsigned long[::1] num_out_of_range,
                                    double[::1] batch_counts_sum,
                                    double[::1] squared_batch_counts_sum,
                                    const unsigned num_threads):
                        cdef unsigned long i, j, value, start, end, chunk_size, \
                            total_batch_counts_sum, \
                            total_squared_batch_counts_sum, \
                            total_num_out_of_range
                        cdef unsigned thread_index, cell, gene, dest_start, \
                            dest_end, gene_index
                        
                        # Key insight: the things we're summing are integers
                        # except when `value > clip_val[gene]`, in which case
                        # we are adding a (floating-point) constant, so keep
                        # track of this case separately and add it at the end,
                        # to minimize floating-point error
                        
                        if num_threads == 1:
                            batch_counts_sum_int[:] = 0
                            squared_batch_counts_sum_int[:] = 0
                            num_out_of_range[:] = 0
                            if cell_indices.shape[0] == 0:
                                num_cells = indptr.shape[0] - 1
                                for cell in range(num_cells):
                                    for i in range(<unsigned long> indptr[cell], <unsigned long> indptr[cell + 1]):
                                        gene = indices[i]
                                        value = <unsigned long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range[gene] += 1
                                        else:
                                            batch_counts_sum_int[gene] += value
                                            squared_batch_counts_sum_int[gene] += \
                                                value ** 2
                            else:
                                for j in range(<unsigned long> cell_indices.shape[0]):
                                    cell = cell_indices[j]
                                    for i in range(<unsigned long> indptr[cell], <unsigned long> indptr[cell + 1]):
                                        gene = indices[i]
                                        value = <unsigned long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range[gene] += 1
                                        else:
                                            batch_counts_sum_int[gene] += value
                                            squared_batch_counts_sum_int[gene] += \
                                                value ** 2
                            for gene in range(num_dataset_genes):
                                batch_counts_sum[gene] = \
                                    batch_counts_sum_int[gene] + \
                                    num_out_of_range[gene] * clip_val[gene]
                                squared_batch_counts_sum[gene] = \
                                    squared_batch_counts_sum_int[gene] + \
                                    num_out_of_range[gene] * clip_val[gene] * \
                                    clip_val[gene]
                        else:
                            chunk_size = num_cells // num_threads
                            with nogil:
                                if cell_indices.shape[0] == 0:
                                    for thread_index in prange(
                                            num_threads, num_threads=num_threads,
                                            schedule='static', chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_cells if thread_index == num_threads - 1 \
                                            else start + chunk_size
                                        dest_start = thread_index * num_dataset_genes
                                        dest_end = dest_start + num_dataset_genes
                                        batch_counts_sum_int[dest_start:dest_end] = 0
                                        squared_batch_counts_sum_int[dest_start:dest_end] = 0
                                        num_out_of_range[dest_start:dest_end] = 0
                                        for cell in range(start, end):
                                            for i in range(<unsigned long> indptr[cell], <unsigned long> indptr[cell + 1]):
                                                gene = indices[i]
                                                gene_index = dest_start + gene
                                                value = <unsigned long> data[i]
                                                if value > clip_val[gene]:
                                                    num_out_of_range[gene_index] += 1
                                                else:
                                                    batch_counts_sum_int[gene_index] += value
                                                    squared_batch_counts_sum_int[gene_index] += value ** 2
                                else:
                                    for thread_index in prange(
                                            num_threads, num_threads=num_threads,
                                            schedule='static', chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_cells if thread_index == num_threads - 1 \
                                            else start + chunk_size
                                        dest_start = thread_index * num_dataset_genes
                                        dest_end = dest_start + num_dataset_genes
                                        batch_counts_sum_int[dest_start:dest_end] = 0
                                        squared_batch_counts_sum_int[dest_start:dest_end] = 0
                                        num_out_of_range[dest_start:dest_end] = 0
                                        for j in range(start, end):
                                            cell = cell_indices[j]
                                            for i in range(<unsigned long> indptr[cell], <unsigned long> indptr[cell + 1]):
                                                gene = indices[i]
                                                gene_index = dest_start + gene
                                                value = <unsigned long> data[i]
                                                if value > clip_val[gene]:
                                                    num_out_of_range[gene_index] += 1
                                                else:
                                                    batch_counts_sum_int[gene_index] += value
                                                    squared_batch_counts_sum_int[gene_index] += value ** 2
                                
                                for gene in prange(num_dataset_genes,
                                                   num_threads=num_threads):
                                    total_batch_counts_sum = 0
                                    total_squared_batch_counts_sum = 0
                                    total_num_out_of_range = 0
                                    for thread_index in range(num_threads):
                                        gene_index = thread_index * \
                                            num_dataset_genes + gene
                                        total_batch_counts_sum = \
                                            total_batch_counts_sum + \
                                            batch_counts_sum_int[gene_index]
                                        total_squared_batch_counts_sum = \
                                            total_squared_batch_counts_sum + \
                                            squared_batch_counts_sum_int[gene_index]
                                        total_num_out_of_range = \
                                            total_num_out_of_range + \
                                            num_out_of_range[gene_index]
                                    batch_counts_sum[gene] = \
                                        total_batch_counts_sum + \
                                        total_num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        total_squared_batch_counts_sum + \
                                        total_num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                    ''')['clipped_sum'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes,
                        cell_indices=cell_indices, clip_val=clip_val,
                        batch_counts_sum_int=batch_counts_sum_int,
                        squared_batch_counts_sum_int=squared_batch_counts_sum_int,
                        num_out_of_range=num_out_of_range,
                        batch_counts_sum=batch_counts_sum,
                        squared_batch_counts_sum=squared_batch_counts_sum,
                        num_threads=num_threads)
            else:
                cython_inline(r'''
                    from cython.parallel cimport prange
                    
                    ctypedef fused numeric:
                        int
                        unsigned
                        long
                        unsigned long
                        float
                        double
                    
                    ctypedef fused signed_integer:
                        int
                        long
                    
                    def clipped_sum(const numeric[::1] data,
                                    const signed_integer[::1] indices,
                                    const signed_integer[::1] indptr,
                                    char[::1] cell_mask,
                                    const double[::1] clip_val,
                                    double[::1] batch_counts_sum,
                                    double[::1] squared_batch_counts_sum,
                                    const unsigned num_threads):
                        cdef unsigned cell, gene, num_genes = indptr.shape[0] - 1
                        cdef unsigned long i, value, batch_counts_sum_int, \
                            squared_batch_counts_sum_int, num_out_of_range
                        
                        if num_threads == 1:
                            if cell_mask.shape[0] == 0:
                                for gene in range(num_genes):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        value = <unsigned long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range += 1
                                        else:
                                            batch_counts_sum_int += value
                                            squared_batch_counts_sum_int += \
                                                value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                            else:
                                for gene in range(num_genes):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        cell = indices[i]
                                        if cell_mask[cell]:
                                            value = <unsigned long> data[i]
                                            if value > clip_val[gene]:
                                                num_out_of_range += 1
                                            else:
                                                batch_counts_sum_int += value
                                                squared_batch_counts_sum_int += \
                                                    value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                        else:
                            if cell_mask.shape[0] == 0:
                                for gene in prange(num_genes, nogil=True,
                                                   num_threads=num_threads):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        value = <unsigned long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range = num_out_of_range + 1
                                        else:
                                            batch_counts_sum_int = \
                                                batch_counts_sum_int + value
                                            squared_batch_counts_sum_int = \
                                                squared_batch_counts_sum_int + \
                                                value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                            else:
                                for gene in prange(num_genes, nogil=True,
                                                   num_threads=num_threads):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                        cell = indices[i]
                                        if cell_mask[cell]:
                                            value = <unsigned long> data[i]
                                            if value > clip_val[gene]:
                                                num_out_of_range = num_out_of_range + 1
                                            else:
                                                batch_counts_sum_int = \
                                                    batch_counts_sum_int + value
                                                squared_batch_counts_sum_int = \
                                                    squared_batch_counts_sum_int + \
                                                    value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                    ''')['clipped_sum'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_mask=cell_mask, clip_val=clip_val,
                        batch_counts_sum=batch_counts_sum,
                        squared_batch_counts_sum=squared_batch_counts_sum,
                        num_threads=num_threads)
            norm_gene_var = pl.Series(
                (1 / ((num_cells - 1) * np.square(estimated_stddev))) *
                ((num_cells * np.square(mean)) + squared_batch_counts_sum -
                 2 * batch_counts_sum * mean))
            # If `min_cells` is non-zero, set variances to `null` for genes
            # with a non-zero count less than `min_cells`
            if min_cells:
                norm_gene_var = norm_gene_var\
                    .set(pl.Series(nonzero_count < min_cells), None)
            # If there are multiple datasets, `norm_gene_var` is currently with
            # respect to the genes in `dataset.var_names`; map back to the
            # genes in `genes_in_any_dataset`, filling with `null`
            if others:
                norm_gene_var = norm_gene_var[gene_indices]
            norm_gene_vars.append(norm_gene_var)
        
        rank = pl.exclude('gene').rank('min', descending=True)
        final_rank = pl.struct(
            ('median_rank', 'nbatches') if flavor == 'seurat_v3' else
            ('nbatches', 'median_rank')).rank('ordinal')
        # note: the expression for `median_rank` can be replaced by
        # `pl.median_horizontal(pl.exclude('gene'))` once polars implements it
        hvgs = pl.DataFrame([genes_in_any_dataset] + norm_gene_vars)\
            .lazy()\
            .pipe(lambda df: df.drop_nulls(pl.selectors.exclude('gene'))
                             if min_cells or others else df)\
            .with_columns(pl.when(rank <= num_genes).then(rank))\
            .with_columns(nbatches=pl.sum_horizontal(pl.exclude('gene')
                                                     .is_null()),
                          median_rank=pl.concat_list(pl.exclude('gene'))
                                      .explode()
                                      .median().over(pl.int_range(
                                          pl.len(), dtype=pl.Int32)))\
            .select('gene', (final_rank <= num_genes).alias(hvg_column),
                    pl.when(final_rank <= num_genes).then(final_rank)
                    .alias(rank_column))\
            .collect()
        # Return a new SingleCell dataset (or a tuple of datasets, if others
        # is non-empty) containing the highly variable genes
        for dataset_index, dataset in enumerate(datasets):
            new_var = dataset._var\
                .join(hvgs.rename({'gene': dataset.var_names.name}),
                      on=dataset.var_names.name, how='left')\
                .with_columns(pl.col(hvg_column).fill_null(False))
            datasets[dataset_index] = \
                SingleCell(X=dataset._X, obs=dataset._obs, var=new_var,
                           obsm=dataset._obsm, varm=dataset._varm,
                           uns=dataset._uns)
        return tuple(datasets) if others else datasets[0]
    
    def normalize(self,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  method: Literal['PFlog1pPF', 'log1pPF',
                                  'logCP10k'] = 'PFlog1pPF',
                  allow_float: bool = False,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Normalize this SingleCell dataset's counts using one of three methods.
        All three methods normalize each cell independently of the rest and
        log-transform the counts in some way, but differ in the details.
        
        By default, uses the PFlog1pPF method introduced in Booeshaghi et al.
        2022 (biorxiv.org/content/10.1101/2022.05.06.490859v1.full). With
        `method='logCP10k'`, it matches the default settings of Seurat's
        `NormalizeData` function, aside from differences in floating-point
        error.
        
        PFlog1pPF is a three-step process:
        1. Divide each cell's counts by a "size factor", namely the total
        number of counts for that cell, divided by the mean number of counts
        across all cells. Booeshaghi et al. call this process "proportional
        fitting" (PF). In NumPy, proportional fitting can be implemented as:
        ```
        total_counts_per_cell = X.sum(axis=1)
        size_factor = total_counts_per_cell / total_counts_per_cell.mean()
        X = X / size_factor
        ```
        2. Take the logarithm of each entry plus 1, i.e. `log1p()`.
        3. Run an additional round of proportional fitting.
        
        If `method='log1pPF'`, only performs steps 1 and 2 and leaves out step
        3. Booeshaghi et al. call this method "log1pPF". Ahlmann-Eltze and
        Huber 2023 (nature.com/articles/s41592-023-01814-1) recommend this
        method and argue that it outperforms log(CPM) normalization. However,
        Booeshaghi et al. note that log1pPF does not fully normalize for read
        depth, because the log transform of step 2 partially undoes the
        normalization introduced by step 1. This is the reasoning behind their
        use of step 3: to restore full depth normalization. By default,
        Scanpy's `normalize_total()` uses a variation of proportional fitting
        that divides by the median instead of the mean, so it's closest to
        `method='log1pPF'`.
        
        If `method='logCP10k'`, uses 10,000 for the denominator of the size
        factors instead of `X.sum(axis=1).mean()`, and leaves out step 3. This
        method is not recommended because it implicitly assumes an
        unrealistically large amount of overdispersion, and performs worse than
        log1pPF and PFlog1pPF in Ahlmann-Eltze and Huber and Booeshaghi et
        al.'s benchmarks. Seurat's `NormalizeData()` uses logCP10k
        normalization.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will still be
                       normalized, but will not count towards the calculation
                       of the mean total count across cells when `method` is
                       `'PFlog1pPF'` or `'log1pPF'`. Has no effect when
                       `method` is `'logCP10k'`.
            method: the normalization method to use (see above)
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            num_threads: the number of threads to use when normalizing. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A new SingleCell dataset with the normalized counts, and
            `uns['normalized']` set to `True`.
        """
        if self._X is None:
            error_message = 'X is None, so normalizing is not possible'
            raise TypeError(error_message)
        # Check that `self` is QCed and not already normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() (and "
                "possibly hvg()) before normalize()? Set uns['QCed'] = True "
                "or run with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = \
                "uns['normalized'] is True; did you already run normalize()?"
            raise ValueError(error_message)
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `method` is one of the three valid methods
        if method not in ('PFlog1pPF', 'log1pPF', 'logCP10k'):
            error_message = \
                "method must be one of 'PFlog1pPF', 'log1pPF', or 'logCP10k'"
            raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # If `allow_float=False`, raise an error if `X` is floating-point
        X = self._X
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'normalize() requires raw counts but X has data type '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)
        # Step 1
        rowsums = X.sum(axis=1)
        inverse_size_factors = np.empty_like(rowsums, dtype=float) \
            if np.issubdtype(rowsums.dtype, np.integer) else rowsums
        # Note: QCed cells will have `null` as the batch, and over() treats
        # `null` as its own category, so effectively all cells failing QC
        # will be treated as their own batch. This doesn't matter since we
        # never use the counts for these cells anyway.
        np.divide(10_000 if method == 'logCP10k' else
                  rowsums.mean() if QC_column is None else
                  rowsums[QC_column].mean(), rowsums, inverse_size_factors)
        X = sparse_matrix_vector_multiply(X, inverse_size_factors, axis=0,
                                          inplace=False,
                                          num_threads=num_threads)
        # Step 2
        cython_inline('''
            from cython.parallel cimport prange
            from libc.math cimport log1p
            
            def log1p_inplace(double[::1] data, const unsigned num_threads):
                cdef unsigned long i
                
                if num_threads == 1:
                    for i in range(<unsigned long> data.shape[0]):
                        data[i] = log1p(data[i])
                else:
                    for i in prange(<unsigned long> data.shape[0],
                                    nogil=True, num_threads=num_threads):
                        data[i] = log1p(data[i])
            ''')['log1p_inplace'](X.data, num_threads)
        # Step 3
        if method == 'PFlog1pPF':
            rowsums = X.sum(axis=1)
            inverse_size_factors = rowsums
            np.divide(rowsums.mean() if QC_column is None else
                      rowsums[QC_column].mean(), rowsums, inverse_size_factors)
            sparse_matrix_vector_multiply(X, inverse_size_factors, axis=0,
                                          inplace=True,
                                          num_threads=num_threads)
        sc = SingleCell(X=X, obs=self._obs, var=self._var, obsm=self._obsm,
                        varm=self._varm, uns=self._uns)
        sc._uns['normalized'] = True
        return sc
    
    def PCA(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            seed: int | np.integer | None = None,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components using irlba, the package used by Seurat.
        Operates on normalized counts (see `normalize()`).
        
        Install irlba with:
        
        from ryp import r
        r('install.packages("irlba", type="source")')
        
        IMPORTANT: if you already have a copy of irlba from CRAN (e.g.
        installed with Seurat), you will get the error:
        
        ```
        RuntimeError: in irlba(X, 50, verbose = FALSE) :
          function 'as_cholmod_sparse' not provided by package 'Matrix'
        ```
        
        This error will go away if you install irlba from source as described
        above.
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their PCs set to `NaN`. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            hvg_column: a Boolean column of `var` indicating the highly
                        variable genes. Can be a column name, a polars
                        expression, a polars Series, a 1D NumPy array, or a
                        function that takes in this SingleCell dataset and
                        returns a polars Series or 1D NumPy array. Set to
                        `None` to use all genes. When `others` is specified,
                        `hvg_column` can be a length-`1 + len(others)` sequence
                        of columns, expressions, Series, functions, or `None`
                        for each dataset (for `self`, followed by each dataset
                        in `others`).
            PC_key: the key of `obsm` where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            seed: the random seed to use for irlba when computing PCs, via R's
                  `set.seed()` function, or leave unset to use
                  `single_cell.options()['seed']` as the seed (0 by default)
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to set the verbose flag in irlba
            num_threads: the number of threads to use when subsetting the count
                         matrix/matrices prior to PCA. PCA will run
                         single-threaded regardless of the value of
                         `num_threads`, because it does not parallelize
                         efficiently. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A new SingleCell dataset where `obsm` contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA` function, which requires `ScaleData` to be
            run first, this function does not require the data to be scaled
            beforehand. Instead, it scales the data implicitly. It does this by
            providing the standard deviation and mean of the data to `irlba()`
            via its `scale` and `center` arguments, respectively. This approach
            is much more computationally efficient than explicit scaling, and
            is also taken by Seurat's internal (and currently unused)
            `RunPCA_Sparse` function, which this function is based on.
        """
        from ryp import r, to_py, to_r
        from sklearn.utils.sparsefuncs import mean_variance_axis
        r('suppressPackageStartupMessages(library(irlba))')
        if self._X is None:
            error_message = \
                'X is None, so finding principal components is not possible'
            raise TypeError(error_message)
        # Check that all elements of `others` are SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Check that all datasets are normalized
        suffix = ' for at least one dataset' if others else ''
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        # Raise an error if `X` has an integer data type for any dataset
        for dataset in datasets:
            if np.issubdtype(dataset._X.dtype, np.integer):
                error_message = (
                    f'PCA() requires raw counts, but X has data type '
                    f'{str(dataset._X.dtype)!r}, an integer data type'
                    f'{suffix}; did you forget to run normalize() before '
                    f'PCA()?')
                raise TypeError(error_message)
        # Get `QC_column` (if not `None`) and `hvg_column` from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            allow_None=False,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and normalize()) '
                         f'before PCA()?')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `PC_key` is not already in `obsm`, unless `overwrite=True`
        check_type(PC_key, 'PC_key', str, 'a string')
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Get the matrix to compute PCA across: a CSC array of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use `X[np.ix_(rows, columns)]` as a faster,
        # more memory-efficient alternative to `X[rows][:, columns]`. Use CSC
        # rather than CSR because irlba has a fast C-based implementation for
        # CSC.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = self._var.filter(hvg_columns[0]).to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [dataset._X[np.ix_(QC_col.to_numpy(),
                                                hvg_col.to_numpy())]
                              if QC_col is not None else
                              dataset._X[:, hvg_col.to_numpy()]
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        X = sparse.vstack(Xs, format='csc')
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        del Xs
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        # Run PCA with irlba (github.com/bwlewis/irlba/blob/master/R/irlba.R)
        # This section is adapted from
        # github.com/satijalab/seurat/blob/master/R/integration.R#L7276-L7317
        # Note: totalvar doesn't seem to be used by irlba, maybe a Seurat bug?
        # Note: mean_variance_axis() can be replaced by Welford's algorithm
        center, feature_var = mean_variance_axis(X, axis=0)
        scale = np.sqrt(feature_var)
        scale.clip(min=1e-8, out=scale)
        to_r(X, '.SingleCell.X')
        try:
            to_r(center, '.SingleCell.center')
            try:
                to_r(scale, '.SingleCell.scale')
                try:
                    r(f'set.seed({seed})')
                    r(f'.SingleCell.PCs = irlba(.SingleCell.X, {num_PCs}, '
                      f'verbose={str(verbose).upper()}, '
                      f'scale=.SingleCell.scale, '
                      f'center=.SingleCell.center)')
                    try:
                        PCs = to_py('.SingleCell.PCs$u', format='numpy') * \
                              to_py('.SingleCell.PCs$d', format='numpy')
                    finally:
                        r('rm(.SingleCell.PCs)')
                finally:
                    r('rm(.SingleCell.scale)')
            finally:
                r('rm(.SingleCell.center)')
        finally:
            r('rm(.SingleCell.X)')
        # Store each dataset's PCs in its `obsm`
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            else:
                dataset_PCs = np.ascontiguousarray(dataset_PCs)
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns)
        return tuple(datasets) if others else datasets[0]
    
    def neighbors(self,
                  *,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  PC_key: str = 'PCs',
                  neighbors_key: str = 'neighbors',
                  num_neighbors: int | np.integer = 20,
                  num_clusters: int | np.integer | None = None,
                  num_probes: int | np.integer | None = None,
                  num_clustering_iterations: int | np.integer = 10,
                  seed: int | np.integer | None = None,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate the `num_neighbors`-nearest neighbors of each cell.
        
        This function is intended to be run after `PCA()`; by default, it uses
        `obsm['PCs']` as the input to the nearest-neighbors calculation. It
        uses an approximate algorithm based on an inverted file (IVF), as
        implemented in the Facebook AI Similarity Search (FAISS) library.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their nearest neighbors set to `-1`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input for the
                    nearest-neighbors calculation
            neighbors_key: the key of `obsm` where the nearest neighbors will
                           be stored
            num_neighbors: the number of nearest neighbors to report for each
                           cell
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be positive and less than the number of
                          cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            num_clustering_iterations: the maximum number of iterations of
                                       k-means clustering to perform before
                                       starting the nearest-neighbors search,
                                       stopping early if convergence is reached
            seed: the random seed to use when finding nearest neighbors, or
                  leave unset to use `single_cell.options()['seed']` as the
                  seed (0 by default)
            overwrite: if `True`, overwrite `neighbors_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the nearest-neighbors search
            num_threads: the number of threads to use when finding nearest
                         neighbors. Set `num_threads=-1` to use all available
                         cores (as determined by `os.cpu_count()`), or leave
                         unset to use `single_cell.options()['num_threads']`
                         cores (1 by default).
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsm[neighbors_key]` as a `len(obs)` Ã—
            `num_neighbors` NumPy array, where `obsm[neighbors_key][i, j]`
            stores the index of the `i`th cell's `j + 1`th nearest neighbor.
            (This differs from Scanpy and Seurat, which use a less compact
            sparse matrix representation instead.) For instance, if
            `num_neighbors=2` and the 0th cell's nearest neighbors are the 4th
            cell and the 6th cell, then `obsm[neighbors_key][0]` will be
            `np.array([4, 6])`. Note that if `QC_column` is not `None`, these
            integer indices are with respect to QCed cells, not all cells.
        """
        with ignore_sigint():
            import faiss
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `PC_key` is the name of a key in `obsm`
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += \
                    '; did you forget to run PCA() before neighbors()?'
            raise ValueError(error_message)
        # Get PCs, and check that they are float64 and C-contiguous
        PCs = self._obsm[PC_key]
        if PCs.dtype != float:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float64'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
            raise ValueError(error_message)
        # Subset PCs to QCed cells only, if `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `neighbors_key` is a string and, unless `overwrite=True`,
        # not already a key in `obsm`
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if not overwrite and neighbors_key in self._obsm:
            error_message = (
                f'neighbors_key {neighbors_key!r} is already a key of obsm; '
                f'did you already run neighbors()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Check that `num_neighbors` is between 1 and the number of cells
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        num_cells = len(PCs)
        if not 1 <= num_neighbors <= num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be â‰¥ 1 and â‰¤ '
                f'the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells),
                                           num_cells / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters < num_cells:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be â‰¥ 1 and '
                    f'less than the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be â‰¥ 1 and â‰¤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = SingleCell._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        # Calculate each cell's `num_neighbors + 1`-nearest neighbors with
        # faiss, where the `+ 1` is for the cell itself
        dim = PCs.shape[1]
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        index.cp.niter = num_clustering_iterations
        # noinspection PyArgumentList
        index.train(PCs)
        # noinspection PyArgumentList
        index.add(PCs)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = index.search(PCs, num_neighbors + 1)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = PCs[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} cells '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) did '
                          f'not have enough neighbors with {index.nprobe:,} '
                          f'probes; re-running nearest-neighbors finding for '
                          f'these cells with {num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = \
                    index.search(needs_update_X, num_neighbors + 1)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        if verbose:
            # noinspection PyUnresolvedReferences
            percent = \
                (nearest_neighbor_indices[:, 0] == range(num_cells)).mean()
            print(f'{100 * percent:.3f}% of cells are correctly detected as '
                  f'their own nearest neighbors (a measure of the quality of '
                  f'the k-nearest neighbors search)')
        # Remove self-neighbors from each cell's list of nearest neighbors.
        # These are almost always in the 0th column, but occasionally later due
        # to the inaccuracy of the nearest-neighbors search. This leaves us
        # with `num_neighbors + num_extra_neighbors` nearest neighbors.
        remove_self_neighbors = cython_inline(r'''
            from cython.parallel cimport prange
        
            def remove_self_neighbors(long[:, ::1] neighbors,
                                      const unsigned num_threads):
                cdef unsigned i, j, num_cells = neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1]
                
                if num_threads == 1:
                    for i in range(num_cells):
                        # If the cell is its own nearest neighbor (almost always), skip
                        
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                        
                        # Find the position where the cell is listed as its own
                        # self-neighbor
                        
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                        
                        # Shift all neighbors before it to the right, overwriting it
                        
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                else:
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                ''')['remove_self_neighbors']
        remove_self_neighbors(nearest_neighbor_indices, num_threads)
        nearest_neighbor_indices = nearest_neighbor_indices[:, 1:]
        # If `QC_column` is not `None`, back-project from QCed cells to all
        # cells, filling with -1
        if QC_column is not None:
            nearest_neighbor_indices_QCed = nearest_neighbor_indices
            nearest_neighbor_indices = np.full(
                (len(self), nearest_neighbor_indices_QCed.shape[1]), -1)
            # noinspection PyUnboundLocalVariable
            nearest_neighbor_indices[QCed_NumPy] = \
                nearest_neighbor_indices_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm |
                               {neighbors_key: nearest_neighbor_indices},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def neighbors_new(self,
                  *,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  PC_key: str = 'PCs',
                  neighbors_key: str = 'neighbors',
                  num_neighbors: int | np.integer = 20,
                  num_clusters: int | np.integer | None = None,
                  num_probes: int | np.integer | None = None,
                  num_clustering_iterations: int | np.integer = 10,
                  seed: int | np.integer | None = None,
                  random_init: bool = False,  # TODO
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate the `num_neighbors`-nearest neighbors of each cell.
        
        This function is intended to be run after `PCA()`; by default, it uses
        `obsm['PCs']` as the input to the nearest-neighbors calculation. It
        uses an approximate algorithm based on an inverted file (IVF), as
        implemented in the Facebook AI Similarity Search (FAISS) library.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their nearest neighbors set to `-1`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input for the
                    nearest-neighbors calculation
            neighbors_key: the key of `obsm` where the nearest neighbors will
                           be stored
            num_neighbors: the number of nearest neighbors to report for each
                           cell
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be positive and less than the number of
                          cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            num_clustering_iterations: the maximum number of iterations of
                                       k-means clustering to perform before
                                       starting the nearest-neighbors search,
                                       stopping early if convergence is reached
            seed: the random seed to use when finding nearest neighbors, or
                  leave unset to use `single_cell.options()['seed']` as the
                  seed (0 by default)
            overwrite: if `True`, overwrite `neighbors_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the nearest-neighbors search
            num_threads: the number of threads to use when finding nearest
                         neighbors. Set `num_threads=-1` to use all available
                         cores (as determined by `os.cpu_count()`), or leave
                         unset to use `single_cell.options()['num_threads']`
                         cores (1 by default).
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsm[neighbors_key]` as a `len(obs)` Ã—
            `num_neighbors` NumPy array, where `obsm[neighbors_key][i, j]`
            stores the index of the `i`th cell's `j + 1`th nearest neighbor.
            (This differs from Scanpy and Seurat, which use a less compact
            sparse matrix representation instead.) For instance, if
            `num_neighbors=2` and the 0th cell's nearest neighbors are the 4th
            cell and the 6th cell, then `obsm[neighbors_key][0]` will be
            `np.array([4, 6])`. Note that if `QC_column` is not `None`, these
            integer indices are with respect to QCed cells, not all cells.
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `PC_key` is the name of a key in `obsm`
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += \
                    '; did you forget to run PCA() before neighbors()?'
            raise ValueError(error_message)
        # Get PCs, and check that they are float64 and C-contiguous
        PCs = self._obsm[PC_key]
        if PCs.dtype != float:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float64'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
            raise ValueError(error_message)
        # Subset PCs to QCed cells only, if `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `neighbors_key` is a string and, unless `overwrite=True`,
        # not already a key in `obsm`
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if not overwrite and neighbors_key in self._obsm:
            error_message = (
                f'neighbors_key {neighbors_key!r} is already a key of obsm; '
                f'did you already run neighbors()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Check that `num_neighbors` is between 1 and the number of cells
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        num_cells = len(PCs)
        if not 1 <= num_neighbors <= num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be â‰¥ 1 and â‰¤ '
                f'the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells),
                                           num_cells / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters < num_cells:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be â‰¥ 1 and '
                    f'less than the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be â‰¥ 1 and â‰¤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = SingleCell._process_num_threads(num_threads)
        cython_functions = cython_inline(r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport prange
        from libcpp.algorithm cimport fill
        from libcpp.vector cimport vector
        
        # TODO
        from cython.parallel import parallel
        from libc.stdlib cimport malloc, calloc, free
        from libc.string cimport memset
        from scipy.linalg.cython_blas cimport dgemm
        
        cdef extern from "omp.h":
            ctypedef struct omp_lock_t:
                pass
            void omp_init_lock(omp_lock_t *) noexcept nogil
            void omp_destroy_lock(omp_lock_t *) noexcept nogil
            void omp_set_lock(omp_lock_t *) noexcept nogil
            void omp_unset_lock(omp_lock_t *) noexcept nogil
            int omp_get_thread_num() noexcept nogil
            int omp_get_max_threads() noexcept nogil

        cdef extern from "float.h":
            cdef double DBL_MAX

        cdef extern from "limits.h":
            cdef unsigned UINT_MAX

        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))

        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state

        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef inline double random_probability(unsigned long* state) noexcept nogil:
            # Returns a random probability, i.e. a random number in U(0, 1)
            return <double> rand(state) / UINT_MAX
        
        cdef inline void kmeans_random_init(const double[:, ::1] X,
                                            double[:, ::1] centroids,
                                            const unsigned num_cells,
                                            const unsigned num_clusters,
                                            const unsigned long seed):
            # Initialize centroids with random cells. Use a bitmap to efficiently keep
            # track of which cells have been sampled already, to avoid sampling the
            # same cell twice.
            
            cdef unsigned i, j
            cdef unsigned long word_index, bit_index, state = srand(seed)
            cdef vector[unsigned long] bitmap
            bitmap.resize((num_cells + 63) // 64)
            for i in range(num_clusters):
                while True:
                    j = randint(num_cells, &state)
                    word_index = j >> 6
                    bit_index = j & 63
                    if not bitmap[word_index] & (1 << bit_index):
                        bitmap[word_index] |= 1 << bit_index
                        centroids[i] = X[j]
                        break
        
        cdef inline void kmeans_barbar(const double[:, ::1] X,
                                       double[:, ::1] centroids,
                                       const unsigned num_init_iterations,
                                       const double oversampling_factor,
                                       const unsigned num_cells,
                                       const unsigned num_clusters,
                                       const unsigned num_dimensions,
                                       const unsigned long seed):
            cdef unsigned random_cell, iteration, i, j, k, c0, c1, \
                selected_cell, best_selected_cell, num_selected_cells, cluster_index, \
                selected_centroid
            cdef unsigned long state
            cdef double l = oversampling_factor * num_clusters, cost, difference, \
                distance, l_over_cost, min_distance, inverse_cost, probability
            cdef vector[double] min_distances
            cdef vector[unsigned] selected_cells, selected_cell_weights, \
                centroid_indices
            min_distances.resize(num_cells)
            # reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
        
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # These will constitute a shortlist from which we will select the final
            # centroids to start off k-means with.
        
            state = srand(seed)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
        
            # For the first iteration, calculate the (squared Euclidean) distance from
            # each cell to this random cell, storing it in `min_distances`. Also
            # calculate the sum of the `min_distances`, i.e. the `cost`.
    
            cost = 0
            for i in range(num_cells):
                distance = 0
                for j in range(num_dimensions):
                    difference = X[i, j] - X[random_cell, j]
                    distance += difference * difference
                min_distances[i] = distance
                cost += distance
    
            # Sample each cell with probability `l * min_distances[i] / cost`
            
            l_over_cost = l / cost
            for i in range(num_cells):
                state = srand(seed + i)
                if l_over_cost * min_distances[i] >= random_probability(&state):
                    selected_cells.push_back(i)
            
            PyErr_CheckSignals()
        
            # For each remaining iteration...
        
            for iteration in range(1, num_init_iterations):
                # Find each cell's distance to its nearest candidate centroid
                # (selected cell), storing it in `min_distances`. Also calculate
                # the sum of the `min_distances`, i.e. the `cost`.
    
                cost = 0
                for i in range(num_cells):
                    min_distance = DBL_MAX
                    for selected_cell in selected_cells:
                        distance = 0
                        for j in range(num_dimensions):
                            difference = X[i, j] - X[selected_cell, j]
                            distance = distance + difference * difference
                        if distance < min_distance:
                            min_distance = distance
                    min_distances[i] = min_distance
                    cost += min_distance
    
                # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                # Note that `min_distances` will be 0 for cells that have already been
                # sampled, so we will automatically avoid sampling them twice.
    
                l_over_cost = l / cost
                for i in range(num_cells):
                    state = srand(seed + i)
                    if l_over_cost * min_distances[i] >= random_probability(&state):
                        selected_cells.push_back(i)
                    
                PyErr_CheckSignals()
        
            # Get the weight for each selected cell: the number of cells that are
            # closer to the selected cell than to any other selected cell
        
            num_selected_cells = selected_cells.size()
            selected_cell_weights.resize(num_selected_cells)
            for i in range(num_cells):
                min_distance = DBL_MAX
                for j in range(num_selected_cells):
                    selected_cell = selected_cells[j]
                    distance = 0
                    for k in range(num_dimensions):
                        difference = X[i, k] - X[selected_cell, k]
                        distance = distance + difference * difference
                    if distance < min_distance:
                        min_distance = distance
                        best_selected_cell = j
                selected_cell_weights[best_selected_cell] += 1
        
            PyErr_CheckSignals()
            
            # Run k-means++ to select `num_clusters` of the selected cells as the
            # centroids, using `selected_cell_weights` as weights. Start by selecting a
            # random cell from our selected cells as the first centroid.
        
            state = srand(seed + 1)
            random_cell = selected_cells[randint(num_selected_cells, &state)]
            centroid_indices.resize(num_clusters)
            centroid_indices[0] = random_cell
            centroids[0] = X[random_cell]
        
            # Iteratively select the remaining centroids
        
            for cluster_index in range(1, num_clusters):
    
                # Find each selected cell's distance to its nearest centroid. Multiply
                # this distance by the selected cell's weight (from
                # `selected_cell_weights`), and store it in `min_distances`. Also
                # calculate the sum of the weighted `min_distances`, i.e. the `cost`.
    
                cost = 0
                for i in range(num_selected_cells):
                    selected_cell = selected_cells[i]
                    min_distance = DBL_MAX
                    for j in range(cluster_index):
                        selected_centroid = centroid_indices[j]
                        distance = 0
                        for k in range(num_dimensions):
                            difference = X[selected_cell, k] - X[selected_centroid, k]
                            distance = distance + difference * difference
                        if distance < min_distance:
                            min_distance = distance
                    min_distance = min_distance * selected_cell_weights[i]
                    min_distances[i] = min_distance
                    cost += min_distance
    
                # Sample a single cell `i` with probability
                # `min_distances[i] / cost`. Note that `min_distances` will be 0
                # for cells that have already been sampled, so we will
                # automatically avoid sampling them twice.
    
                inverse_cost = 1 / cost
                probability = random_probability(&state)
                i = 0
                while True:
                    probability -= min_distances[i] * inverse_cost
                    if probability < 0:
                        break
                    i += 1
                centroid_indices[cluster_index] = selected_cells[i]
                centroids[cluster_index] = X[selected_cells[i]]
        
            PyErr_CheckSignals()
        
        cdef inline void kmeans_barbar_parallel(const double[:, ::1] X,
                                                double[:, ::1] centroids,
                                                const unsigned num_init_iterations,
                                                const double oversampling_factor,
                                                const unsigned num_cells,
                                                const unsigned num_clusters,
                                                const unsigned num_dimensions,
                                                const unsigned long seed,
                                                const unsigned num_threads):
            cdef unsigned random_cell, iteration, i, j, k, thread_index, c0, c1, \
                selected_cell, best_selected_cell, num_selected_cells, cluster_index, \
                selected_centroid
            cdef unsigned long state
            cdef double l = oversampling_factor * num_clusters, cost, difference, \
                distance, l_over_cost, min_distance, inverse_cost, probability
            cdef vector[double] min_distances
            cdef vector[unsigned] selected_cells, selected_cell_weights, \
                centroid_indices
            cdef vector[vector[unsigned]] thread_selected_cells
            min_distances.resize(num_cells)
            # reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
        
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # These will constitute a shortlist from which we will select the final
            # centroids to start off k-means with.
        
            state = srand(seed)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
        
            with nogil:
                # For the first iteration, calculate the (squared Euclidean) distance from
                # each cell to this random cell, storing it in `min_distances`. Also
                # calculate the sum of the `min_distances`, i.e. the `cost`.
        
                cost = 0
                for i in prange(num_cells, num_threads=num_threads):
                    distance = 0
                    for j in range(num_dimensions):
                        difference = X[i, j] - X[random_cell, j]
                        distance = distance + difference * difference
                    min_distances[i] = distance
                    cost += distance
        
                # Sample each cell with probability `l * min_distances[i] / cost`
        
                thread_selected_cells.resize(num_threads)
                l_over_cost = l / cost
                for thread_index in prange(num_threads, num_threads=num_threads,
                                           chunksize=1, schedule='static'):
                    thread_selected_cells[thread_index].reserve(
                        <unsigned> (1.25 * l / num_threads))
                    c0 = num_cells * thread_index / num_threads
                    c1 = num_cells * (thread_index + 1) / num_threads
                    for i in range(c0, c1):
                        state = srand(seed + i)
                        if l_over_cost * min_distances[i] >= random_probability(&state):
                            thread_selected_cells[thread_index].push_back(i)
        
                # Aggregate each thread's selected cells into a single vector
        
                for thread_index in range(num_threads):
                    selected_cells.insert(
                        selected_cells.end(),
                        thread_selected_cells[thread_index].begin(),
                        thread_selected_cells[thread_index].end())
        
            PyErr_CheckSignals()
        
            # For each remaining iteration...
        
            for iteration in range(1, num_init_iterations):
                with nogil:
                    # Find each cell's distance to its nearest candidate centroid
                    # (selected cell), storing it in `min_distances`. Also calculate
                    # the sum of the `min_distances`, i.e. the `cost`.
        
                    cost = 0
                    for i in prange(num_cells, num_threads=num_threads):
                        min_distance = DBL_MAX
                        for selected_cell in selected_cells:
                            distance = 0
                            for j in range(num_dimensions):
                                difference = X[i, j] - X[selected_cell, j]
                                distance = distance + difference * difference
                            if distance < min_distance:
                                min_distance = distance
                        min_distances[i] = min_distance
                        cost += min_distance
        
                    # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                    # Note that `min_distances` will be 0 for cells that have already been
                    # sampled, so we will automatically avoid sampling them twice.
        
                    l_over_cost = l / cost
                    for thread_index in prange(num_threads, num_threads=num_threads,
                                               chunksize=1, schedule='static'):
                        thread_selected_cells[thread_index].clear()
                        c0 = num_cells * thread_index / num_threads
                        c1 = num_cells * (thread_index + 1) / num_threads
                        for i in range(c0, c1):
                            state = srand(seed + i)
                            if l_over_cost * min_distances[i] >= random_probability(&state):
                                thread_selected_cells[thread_index].push_back(i)
        
                    # Aggregate each thread's selected cells into a single vector
        
                    for thread_index in range(num_threads):
                        selected_cells.insert(
                            selected_cells.end(),
                            thread_selected_cells[thread_index].begin(),
                            thread_selected_cells[thread_index].end())
        
                PyErr_CheckSignals()
        
            # Get the weight for each selected cell: the number of cells that are
            # closer to the selected cell than to any other selected cell
        
            num_selected_cells = selected_cells.size()
            selected_cell_weights.resize(num_selected_cells)
            for i in prange(num_cells, nogil=True, num_threads=num_threads):
                min_distance = DBL_MAX
                for j in range(num_selected_cells):
                    selected_cell = selected_cells[j]
                    distance = 0
                    for k in range(num_dimensions):
                        difference = X[i, k] - X[selected_cell, k]
                        distance = distance + difference * difference
                    if distance < min_distance:
                        min_distance = distance
                        best_selected_cell = j
                selected_cell_weights[best_selected_cell] += 1
        
            PyErr_CheckSignals()
            
            # Run k-means++ to select `num_clusters` of the selected cells as the
            # centroids, using `selected_cell_weights` as weights. Start by selecting a
            # random cell from our selected cells as the first centroid.
        
            state = srand(seed + 1)
            random_cell = selected_cells[randint(num_selected_cells, &state)]
            centroid_indices.resize(num_clusters)
            centroid_indices[0] = random_cell
            centroids[0] = X[random_cell]
        
            # Iteratively select the remaining centroids
        
            with nogil:
                for cluster_index in range(1, num_clusters):
        
                    # Find each selected cell's distance to its nearest centroid. Multiply
                    # this distance by the selected cell's weight (from
                    # `selected_cell_weights`), and store it in `min_distances`. Also
                    # calculate the sum of the weighted `min_distances`, i.e. the `cost`.
        
                    cost = 0
                    for i in prange(num_selected_cells, num_threads=num_threads):
                        selected_cell = selected_cells[i]
                        min_distance = DBL_MAX
                        for j in range(cluster_index):
                            selected_centroid = centroid_indices[j]
                            distance = 0
                            for k in range(num_dimensions):
                                difference = X[selected_cell, k] - X[selected_centroid, k]
                                distance = distance + difference * difference
                            if distance < min_distance:
                                min_distance = distance
                        min_distance = min_distance * selected_cell_weights[i]
                        min_distances[i] = min_distance
                        cost += min_distance
        
                    # Sample a single cell `i` with probability
                    # `min_distances[i] / cost`. Note that `min_distances` will be 0
                    # for cells that have already been sampled, so we will
                    # automatically avoid sampling them twice.
        
                    inverse_cost = 1 / cost
                    probability = random_probability(&state)
                    i = 0
                    while True:
                        probability -= min_distances[i] * inverse_cost
                        if probability < 0:
                            break
                        i += 1
                    centroid_indices[cluster_index] = selected_cells[i]
                    centroids[cluster_index] = X[selected_cells[i]]
        
            PyErr_CheckSignals()
        
        def kmeans(const double[:, ::1] X,
                   unsigned[::1] cluster_labels,
                   double[:, ::1] centroids,
                   double[:, ::1] centroids_new,  # TODO
                   double[::1] centroid_norms,  # TODO
                   unsigned[::1] num_cells_per_cluster,
                   const bint random_init,
                   const unsigned num_init_iterations,
                   const double oversampling_factor,
                   const unsigned num_kmeans_iterations,
                   const unsigned long seed,
                   const unsigned num_threads):
            cdef unsigned i, j, k, l, random_cell, best_cluster, thread_index, \
                c0, c1, num_cells = X.shape[0], \
                num_clusters = centroids.shape[0], num_dimensions = centroids.shape[1]
            cdef double difference, distance, min_distance, norm, \
                inv_cells_minus_clusters = 1.0 / (num_cells - num_clusters), \
                one_plus_eps = 1025.0 / 1024, one_minus_eps = 1023.0 / 1024
            cdef unsigned long state = srand(seed)
            
            # TODO
            cdef omp_lock_t lock
            cdef unsigned i_, j_, chunk_index, start, end, chunk_num_cells, \
                num_chunks = (num_cells + 255) / 256
            cdef double alpha = -2, beta = 1
            cdef char transA = b'T', transB = b'N'
            cdef double* centroids_new_chunk
            cdef unsigned* num_cells_per_cluster_chunk
            cdef double* distances
            cdef double[:, ::1] temp
            
            if num_threads == 1:
                if random_init:
                    # Initialize centroids with random points
                    kmeans_random_init(X, centroids, num_cells, num_clusters, seed)
                else:
                    # Initialize centroids with k-means|| (Bahmani et al. 2012)
                    kmeans_barbar(X, centroids, num_init_iterations,
                                  oversampling_factor, num_cells, num_clusters,
                                  num_dimensions, seed)
                
                # Run k-means for `num_kmeans_iterations` iterations
                
                for iteration in range(num_kmeans_iterations):
                    # Find each cell's nearest centroid, stored in `cluster_labels`
        
                    for i in range(num_cells):
                        min_distance = DBL_MAX
                        for j in range(num_clusters):
                            distance = 0
                            for k in range(num_dimensions):
                                difference = X[i, k] - centroids[j, k]
                                distance += difference * difference
                            if distance < min_distance:
                                min_distance = distance
                                best_cluster = j
                        cluster_labels[i] = best_cluster
        
                    PyErr_CheckSignals()
        
                    # Find the new centroids, based on these cluster assignments
        
                    centroids[:] = 0
                    num_cells_per_cluster[:] = 0
                    for i in range(num_cells):
                        ci = cluster_labels[i]
                        num_cells_per_cluster[ci] += 1
                        for j in range(num_dimensions):
                            centroids[ci, j] += X[i, j]
                    for ci in range(num_clusters):
                        if num_cells_per_cluster[ci] > 0:
                            norm = 1.0 / num_cells_per_cluster[ci]
                            for j in range(num_dimensions):
                                centroids[ci, j] *= norm
        
                    # Handle empty clusters by randomly picking a larger cluster, with
                    # probability proportional to its size, and splitting it in two. Set
                    # the centroids of these two clusters by taking the original cluster's
                    # centroid and applying small, opposite perturbations. Update
                    # `num_cells_per_cluster` (used to make future splits) by heuristically
                    # assuming the cells split evenly between the two clusters; it's not
                    # important enough to bother recalculating the exact split.
        
                    for i in range(num_clusters):
                        if num_cells_per_cluster[i] == 0:
                            j = 0
                            while inv_cells_minus_clusters * \
                                    (num_cells_per_cluster[j] - 1) <= \
                                    random_probability(&state):
                                j = (j + 1) % num_clusters
                            centroids[i, :] = centroids[j, :]
                            for k in range(0, num_dimensions, 2):
                                centroids[i, k] *= one_plus_eps
                                centroids[j, k] *= one_minus_eps
                                centroids[i, k + 1] *= one_minus_eps
                                centroids[j, k + 1] *= one_plus_eps
                            num_cells_per_cluster[i] = num_cells_per_cluster[j] / 2
                            num_cells_per_cluster[j] -= num_cells_per_cluster[i]
        
                    PyErr_CheckSignals()
            else:
                # Same as the single-threaded case, but the centroid-finding step needs
                # each thread to scan through every cell and only process the cells
                # that match a particular cluster, to avoid expensive synchronization.
                # The k-means|| has similar additional complexity.
                
                if random_init:
                    # Initialize centroids with random points
                    kmeans_random_init(X, centroids, num_cells, num_clusters, seed)
                else:
                    # Initialize centroids with k-means|| (Bahmani et al. 2012)
                    kmeans_barbar_parallel(X, centroids, num_init_iterations,
                                           oversampling_factor, num_cells,
                                           num_clusters, num_dimensions, seed,
                                           num_threads)
                
                # Swap `centroids` and `centroids_new` if doing an odd number
                # of k-means iterations, so the array passed in as `centroids`
                # always ends up with the final centroids
                
                if num_kmeans_iterations & 1:
                    temp = centroids
                    centroids = centroids_new
                    centroids_new = temp
                
                for iteration in range(num_kmeans_iterations):
                    # # Find each cell's nearest centroid, stored in `cluster_labels`
                    #
                    # for i in prange(num_cells, nogil=True, num_threads=num_threads):
                    #     min_distance = DBL_MAX
                    #     for j in range(num_clusters):
                    #         distance = 0
                    #         for k in range(num_dimensions):
                    #             difference = X[i, k] - centroids[j, k]
                    #             distance = distance + difference * difference
                    #         if distance < min_distance:
                    #             min_distance = distance
                    #             best_cluster = j
                    #     cluster_labels[i] = best_cluster
                    #
                    # PyErr_CheckSignals()
                    #
                    # # Find the new centroids, based on these cluster assignments
                    #
                    # centroids[:] = 0
                    # num_cells_per_cluster[:] = 0
                    # with nogil:
                    #     for thread_index in prange(num_threads, num_threads=num_threads,
                    #                                schedule='static', chunksize=1):
                    #         # Each thread calculates centroids `c0` to `c1 - 1`
                    #         c0 = num_clusters * thread_index / num_threads
                    #         c1 = num_clusters * (thread_index + 1) / num_threads
                    #         for i in range(num_cells):
                    #             ci = cluster_labels[i]
                    #             if c0 <= ci < c1:
                    #                 num_cells_per_cluster[ci] += 1
                    #                 for j in range(num_dimensions):
                    #                     centroids[ci, j] += X[i, j]
                    #     for ci in prange(num_clusters, num_threads=num_threads):
                    #         if num_cells_per_cluster[ci] > 0:
                    #             norm = 1.0 / num_cells_per_cluster[ci]
                    #             for j in range(num_dimensions):
                    #                 centroids[ci, j] *= norm
                    centroids_new[:] = 0
                    num_cells_per_cluster[:] = 0
                    omp_init_lock(&lock)
                    
                    for i in prange(num_clusters, nogil=True, num_threads=num_threads):
                        norm = 0
                        for j in range(num_dimensions):
                            norm = norm + centroids[i, j] * centroids[i, j]
                        centroid_norms[i] = norm
                
                    with nogil, parallel(num_threads=min(num_threads, num_chunks)):
                        # thread-local buffers
                        centroids_new_chunk = <double*> calloc(num_clusters * num_dimensions, sizeof(double))
                        num_cells_per_cluster_chunk = <unsigned*> calloc(num_clusters, sizeof(unsigned))
                        distances = <double*> malloc(256 * num_clusters * sizeof(double))
                
                        for chunk_index in prange(num_chunks):
                            start = chunk_index * 256
                            end = num_cells if chunk_index == num_chunks - 1 else start + 256
                            chunk_num_cells = end - start
                            
                            # Use the identity ||X - C||Â² = ||X||Â² - 2 * X.dot(C.T) + ||C||Â²,
                            # but skip calculating the ||X||Â² term since the best cluster for a
                            # given cell only depends on the centroids
                            for i in range(chunk_num_cells):
                                for j in range(num_clusters):
                                    # distances = ||C||Â²
                                    distances[i * num_clusters + j] = centroid_norms[j]
                            # distances -= 2 * X.dot(C.T)
                            dgemm(&transA, &transB, <int*> &num_clusters, <int*> &chunk_num_cells,
                                  <int*> &num_dimensions, <double*> &alpha, <double*> &X[start, 0],
                                  <int*> &num_dimensions, <double*> &centroids[0, 0],
                                  <int*> &num_dimensions, <double*> &beta, &distances[0],
                                  <int*> &num_clusters)
                            for i in range(chunk_num_cells):
                                min_distance = distances[i * num_clusters]
                                best_cluster = 0
                                for j in range(1, num_clusters):
                                    distance = distances[i * num_clusters + j]
                                    if distance < min_distance:
                                        min_distance = distance
                                        best_cluster = j
                                cluster_labels[start + i] = best_cluster
                                num_cells_per_cluster_chunk[best_cluster] += 1
                                for k in range(num_dimensions):
                                    centroids_new_chunk[best_cluster * num_dimensions + k] += X[start + i, k]
                
                        omp_set_lock(&lock)
                        for i_ in range(num_clusters):
                            num_cells_per_cluster[i_] += num_cells_per_cluster_chunk[i_]
                            for j_ in range(num_dimensions):
                                centroids_new[i_, j_] += centroids_new_chunk[i_ * num_dimensions + j_]
                        omp_unset_lock(&lock)
                
                        free(centroids_new_chunk)
                        free(num_cells_per_cluster_chunk)
                        free(distances)
                    
                        for i in prange(num_clusters):
                            if num_cells_per_cluster[i] > 0:
                                norm = 1.0 / num_cells_per_cluster[i]
                                for j in range(num_dimensions):
                                    centroids_new[i, j] *= norm
                    
                    # Handle empty clusters, as described in the single-threaded case
        
                    for i in range(num_clusters):
                        if num_cells_per_cluster[i] == 0:
                            j = 0
                            while inv_cells_minus_clusters * \
                                    (num_cells_per_cluster[j] - 1) <= \
                                    random_probability(&state):
                                j = (j + 1) % num_clusters
                            centroids[i, :] = centroids[j, :]
                            for k in range(0, num_dimensions, 2):
                                centroids[i, k] *= one_plus_eps
                                centroids[j, k] *= one_minus_eps
                                centroids[i, k + 1] *= one_minus_eps
                                centroids[j, k + 1] *= one_plus_eps
                            num_cells_per_cluster[i] = num_cells_per_cluster[j] / 2
                            num_cells_per_cluster[j] -= num_cells_per_cluster[i]
        
                    PyErr_CheckSignals()
                    
                    # Swap `centroids` and `centroids_new` after each k-means
                    # iteration
                    
                    temp = centroids
                    centroids = centroids_new
                    centroids_new = temp
                
        cdef inline void heap_replace_top(unsigned* neighbors_i,
                                          double* distances_i,
                                          const unsigned label,
                                          const double distance,
                                          const unsigned k) noexcept nogil:
            # Replaces the top element from the heap defined by `distances_i[0..k-1]`
            # and `neighbors_i[0..k-1]`. Equivalent to `std::pop_heap` followed by
            # `std::push_heap`, but done more efficiently as a single operation.

            cdef unsigned j = 1, child
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            neighbors_i -= 1
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] <= distances_i[child + 1]
                if distance > distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                neighbors_i[j] = neighbors_i[child]
                j = child
            distances_i[j] = distance
            neighbors_i[j] = label

        cdef inline void heap_pop(unsigned* neighbors_i,
                                  double* distances_i,
                                  const unsigned k) noexcept nogil:
            # Pops the top element from the heap defined by `distances_i[0..k-1]` and
            # `neighbors_i[0..k-1]`. On output the `k-1`th element is undefined.

            cdef unsigned label, j = 1, child
            cdef double distance
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            neighbors_i -= 1
            distance = distances_i[k]
            label = neighbors_i[k]
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] <= distances_i[child + 1]
                if distance > distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                neighbors_i[j] = neighbors_i[child]
                j = child
            distances_i[j] = distance
            neighbors_i[j] = label

        cdef inline void heap_sort(unsigned* neighbors_i,
                                   double* distances_i,
                                   const unsigned k) noexcept nogil:
            cdef unsigned j, label
            cdef double distance
            for j in range(k):
                # Save the root (maximum element)
                distance = distances_i[0]
                label = neighbors_i[0]
                # Restore the heap property with reduced size `k - i`
                heap_pop(neighbors_i, distances_i, k - j)
                # Place the maximum element after the end of the heap
                distances_i[k - j - 1] = distance
                neighbors_i[k - j - 1] = label

        def knn(const double[:, ::1] Y,
                const double[:, ::1] X,
                const unsigned[::1] cluster_labels,
                const double[:, ::1] centroids,
                const unsigned[::1] num_cells_per_cluster,
                unsigned[:, ::1] neighbors,
                double[:, ::1] distances,
                unsigned[:, ::1] nearest_clusters,
                double[:, ::1] centroid_distances,
                const unsigned num_neighbors,
                const unsigned num_probes,
                const unsigned num_candidates_per_neighbor,
                const unsigned num_threads):
            # Find the `num_neighbors`-nearest neighbors of each cell in `Y` among the
            # cells in `X`, which have k-means cluster labels `cluster_labels`. Store
            # the nearest-neighbor indices in `neighbors` and distances in `distances`.

            cdef unsigned i, j, k, thread_index, cluster_label, num_searched, iprobe, \
                cluster_num_cells, neighbor, num_Y = Y.shape[0], num_X = X.shape[0], \
                num_dimensions = X.shape[1], num_clusters = centroids.shape[0], \
                num_candidates = num_neighbors * num_candidates_per_neighbor
            cdef double difference, distance
            cdef vector[vector[unsigned]] index

            # Create the inverted file index: a mapping from cluster labels to the
            # cells from `X` in the cluster. This is just an inversion of
            # `cluster_labels`.

            index.resize(num_clusters)
            for cluster_label in range(num_clusters):
                index[cluster_label].reserve(num_cells_per_cluster[cluster_label])
            for thread_index in prange(num_threads, nogil=True, num_threads=num_threads,
                                       chunksize=1, schedule='static'):
                for i in range(num_X):
                    cluster_label = cluster_labels[i]
                    if cluster_label % num_threads == thread_index:
                        index[cluster_label].push_back(i)
            
            PyErr_CheckSignals()

            # Find the `num_probes` nearest centroids of each cell in `Y`,
            # storing their indices in `nearest_clusters`

            for i in prange(num_Y, nogil=True, num_threads=num_threads):
                for j in range(num_probes):
                    nearest_clusters[i, j] = -1
                    centroid_distances[i, j] = DBL_MAX
                for cluster_label in range(num_clusters):
                    distance = 0
                    for j in range(num_dimensions):
                        difference = Y[i, j] - centroids[cluster_label, j]
                        distance = distance + difference * difference
                    # If this centroid is one of the `num_probes` closest centroids
                    # found so far, add it to the heap, and remove the formerly
                    # `num_probes`th-closest centroid (which is now not in the top
                    # `num_probes` centroids anymore)
                    if distance < centroid_distances[i, 0]:
                        heap_replace_top(&nearest_clusters[i, 0], &centroid_distances[i, 0],
                                         cluster_label, distance, num_probes)
                heap_sort(&nearest_clusters[i, 0], &centroid_distances[i, 0], num_neighbors)

            PyErr_CheckSignals()

            # Search each cell's `num_probes` nearest clusters for nearest-neighbor
            # candidates, stopping early (at the end of fully searching a cluster) if
            # more than `num_candidates` cells have been considered.

            for i in prange(num_Y, nogil=True, num_threads=num_threads):
                for j in range(num_neighbors):
                    neighbors[i, j] = -1
                    distances[i, j] = DBL_MAX
                num_searched = 0
                for iprobe in range(num_probes):
                    cluster_label = nearest_clusters[i, iprobe]
                    cluster_num_cells = num_cells_per_cluster[cluster_label]
                    for j in range(cluster_num_cells):
                        neighbor = index[cluster_label][j]
                        distance = 0
                        for k in range(num_dimensions):
                            difference = X[neighbor, k] - Y[i, k]
                            distance = distance + difference * difference
                        if distance < distances[i, 0]:
                            heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                             neighbor, distance, num_neighbors)
                    num_searched = num_searched + cluster_num_cells
                    if num_searched >= num_candidates:
                        break
                heap_sort(&neighbors[i, 0], &distances[i, 0], num_neighbors)
        ''')
        kmeans = cython_functions['kmeans']
        knn = cython_functions['knn']
        
        # faiss.omp_set_num_threads(num_threads)
        # # Calculate each cell's `num_neighbors + 1`-nearest neighbors with
        # # faiss, where the `+ 1` is for the cell itself
        # dim = PCs.shape[1]
        # quantizer = faiss.IndexFlatL2(dim)
        # quantizer.verbose = verbose
        # index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        # index.cp.seed = seed
        # index.verbose = verbose
        # index.cp.verbose = verbose
        # index.cp.niter = num_clustering_iterations
        # # noinspection PyArgumentList
        # index.train(PCs)
        # # noinspection PyArgumentList
        # index.add(PCs)
        # index.nprobe = num_probes
        # # noinspection PyArgumentList
        # neighbors = index.search(PCs, num_neighbors + 1)[1]
        # # Sometimes there aren't enough nearest neighbors for certain cells
        # # with `num_probes` probes; if so, double `num_probes` (and threshold
        # # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # # those cells
        # needs_update = neighbors[:, -1] == -1
        # # noinspection PyUnresolvedReferences
        # if needs_update.any():
        #     needs_update_X = PCs[needs_update]
        #     while True:
        #         num_probes = min(num_probes * 2, num_clusters)
        #         if verbose:
        #             print(f'{len(needs_update_X):,} cells '
        #                   f'({len(needs_update_X) / len(self._obs):.2f}%) did '
        #                   f'not have enough neighbors with {index.nprobe:,} '
        #                   f'probes; re-running nearest-neighbors finding for '
        #                   f'these cells with {num_probes:,} probes')
        #         index.nprobe = num_probes
        #         # noinspection PyArgumentList
        #         new_indices = \
        #             index.search(needs_update_X, num_neighbors + 1)[1]
        #         neighbors[needs_update] = new_indices
        #         still_needs_update = new_indices[:, -1] == -1
        #         # noinspection PyUnresolvedReferences
        #         if not still_needs_update.any():
        #             break
        #         # noinspection PyUnresolvedReferences
        #         needs_update[needs_update] = still_needs_update
        #         needs_update_X = needs_update_X[still_needs_update]
        
        num_PCs = PCs.shape[1]
        cluster_labels = np.empty(num_cells, dtype=np.uint32)
        centroids = np.empty((num_clusters, num_PCs), dtype=float)
        num_cells_per_cluster = np.empty(num_clusters, dtype=np.uint32)
        kmeans(X=PCs, cluster_labels=cluster_labels, centroids=centroids,
               centroids_new=np.empty((num_clusters, num_PCs), dtype=float),  # TODO
               centroid_norms=np.empty(num_clusters, dtype=float),  # TODO
               num_cells_per_cluster=num_cells_per_cluster,
               random_init=random_init, num_init_iterations=5, oversampling_factor=1,
               num_kmeans_iterations=25, seed=seed, num_threads=num_threads)
        neighbors = np.empty((num_cells, num_neighbors), dtype=np.uint32)
        distances = np.empty((num_cells, num_neighbors), dtype=float)
        num_probes = num_neighbors  # TODO
        knn(Y=PCs, X=PCs, cluster_labels=cluster_labels, centroids=centroids,
            num_cells_per_cluster=num_cells_per_cluster, neighbors=neighbors,
            distances=distances,
            nearest_clusters=np.empty((num_cells, num_probes),
                                      dtype=np.uint32),
            centroid_distances=np.empty((num_cells, num_probes), dtype=float),
            num_neighbors=num_neighbors, num_probes=num_probes,
            num_candidates_per_neighbor=10, num_threads=num_threads)
        if verbose:
            # noinspection PyUnresolvedReferences
            percent = (neighbors[:, 0] == range(num_cells)).mean()
            print(f'{100 * percent:.3f}% of cells are correctly detected as '
                  f'their own nearest neighbors (a measure of the quality of '
                  f'the k-nearest neighbors search)')
        # Remove self-neighbors from each cell's list of nearest neighbors.
        # These are almost always in the 0th column, but occasionally later due
        # to the inaccuracy of the nearest-neighbors search. This leaves us
        # with `num_neighbors + num_extra_neighbors` nearest neighbors.
        remove_self_neighbors = cython_inline(r'''
            from cython.parallel cimport prange
        
            def remove_self_neighbors(long[:, ::1] neighbors,
                                      const unsigned num_threads):
                cdef unsigned i, j, num_cells = neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1]
                
                if num_threads == 1:
                    for i in range(num_cells):
                    
                        # If the cell is its own nearest neighbor (almost always), skip
                        
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                            
                        # Find the position where the cell is listed as its own
                        # self-neighbor
                        
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                                
                        # Shift all neighbors before it to the right, overwriting it
                        
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                else:
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                ''')['remove_self_neighbors']
        remove_self_neighbors(neighbors, num_threads)
        neighbors = neighbors[:, 1:]
        # If `QC_column` is not `None`, back-project from QCed cells to all
        # cells, filling with -1
        if QC_column is not None:
            neighbors_QCed = neighbors
            neighbors = np.full((len(self), neighbors_QCed.shape[1]), -1)
            # noinspection PyUnboundLocalVariable
            neighbors[QCed_NumPy] = neighbors_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {neighbors_key: neighbors},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    def shared_neighbors(self,
                         *,
                         QC_column: SingleCellColumn | None |
                                    Sequence[SingleCellColumn |
                                             None] = 'passed_QC',
                         neighbors_key: str = 'neighbors',
                         shared_neighbors_key: str = 'shared_neighbors',
                         min_shared_neighbors: int | np.integer = 3,
                         overwrite: bool = False,
                         num_threads: int | np.integer | None = None) -> \
            SingleCell:
        """
        Calculate the shared nearest neighbor graph of this dataset's cells.
        
        This function is intended to be run after `neighbors()`; by default, it
        uses `obsm['neighbors']` as the input to the shared nearest-neighbors
        calculation.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and excluded from the shared nearest neighbor graph.
            neighbors_key: the key of `obsm` containing the nearest neighbors
                           of each cell calculated with `neighbors()`, to use
                           as the input for the shared nearest neighbor graph
                           calculation
            shared_neighbors_key: the key of `obsp` where the shared nearest
                                  neighbor graph will be stored
            min_shared_neighbors: the minimum number of neighbors a pair of
                                  cells must share to include an edge between
                                  them in the shared nearest neighbor graph. 
                                  With 20 nearest neighbors (the default
                                  `num_neighbors` in `neighbors()`), the
                                  default value of `min_shared_neighbors=3`
                                  corresponds to the default value of
                                  `prune.SNN = 1 / 15` in Seurat's
                                  `FindNeighbors()` function: with 3 shared
                                  neighbors, the shared nearest neighbor weight
                                  is `3 / (40 - 3)` or about 0.08, which is
                                  greater than `1 / 15`, but when there are 2,
                                  the weight is only `2 / (40 - 2)` or about
                                  0.05, which is less than `1 / 15`.
            overwrite: if `True`, overwrite `shared_neighbors_key` if already
                       present in `obsp`, instead of raising an error
            num_threads: the number of threads to use when finding shared
                         nearest neighbors. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsp[shared_neighbors_key]` as a symmetric
            `len(obs)` Ã— `len(obs)` sparse array, where
            `obsp[shared_neighbors_key][i, j]` stores the Jaccard index of the
            `i`th and `j`th cell's nearest neighbors: the number of cells that
            are neighbors of both `i` and `j`, divided by the number of cells
            that are neighbors of at least one of `i` and `j`. For instance, if
            20 nearest neighbors have been calculated (i.e.
            `obsm[neighbors_key].shape[1] == 20`) and 8 of the 20 cells in
            `obsm[neighbors_key][i]` are also found in
            `obsm[neighbors_key][j]`, then `obsp[shared_neighbors_key][i, j]`
            will be 0.25 (`8 / (20 + 20 - 8)`).
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += (
                    '; did you forget to run neighbors() before '
                    'shared_neighbors()?')
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}] must have integer data type, but '
                f'has data type {str(nearest_neighbor_indices.dtype)!r}')
            raise TypeError(error_message)
        # Check that `min_shared_neighbors` is less than the number of
        # neighbors
        check_type(min_shared_neighbors, 'min_shared_neighbors', int,
                   'a non-negative integer')
        num_cells, num_neighbors = nearest_neighbor_indices.shape
        if not 0 <= min_shared_neighbors < num_neighbors:
            error_message = (
                f'min_shared_neighbors is {min_shared_neighbors:,}, but must '
                f'be â‰¥ 0 and less than the number of neighbors in '
                f'obsm[{neighbors_key!r}] ({num_neighbors:,})')
            raise ValueError(error_message)
        # Subset neighbor indices to QCed cells only, if `QC_column` is not
        # `None`; also map each cell's row index in the new neighbors array to
        # its row index in the original neighbors array
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
            QCed_to_full_map = np.flatnonzero(QCed_NumPy)
            num_QCed_cells = len(nearest_neighbor_indices)
        else:
            QCed_to_full_map = np.array([], dtype=np.int64)
            num_QCed_cells = num_cells
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `shared_neighbors_key` is a string and, unless
        # `overwrite=True`, not already in `obsp`
        check_type(shared_neighbors_key, 'shared_neighbors_key', str,
                   'a string')
        if not overwrite and shared_neighbors_key in self._obsp:
            error_message = (
                f'shared_neighbors_key {shared_neighbors_key!r} is already a '
                f'key of obsp; did you already run shared_neighbors()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Compute the shared nearest neighbor graph
        indptr = np.empty(num_cells + 1, dtype=np.uint64)
        num_shared_neighbors = np.zeros((num_threads, num_QCed_cells),
                                        dtype=np.uint32)
        indices, data = cython_inline(r'''
            import numpy as np
            cimport numpy as np
            from cpython.exc cimport PyErr_CheckSignals
            from cython.parallel cimport threadid, prange
            from libc.string cimport memcpy
            from libcpp.vector cimport vector
            
            # Loosely based on the build_snn_graph function from
            # github.com/libscran/scran_graph_cluster
            
            def compute_SNN(const long[:, :] neighbors,
                            const long[::1] QCed_to_full_map,
                            unsigned long[::1] indptr,
                            unsigned[:, ::1] num_shared_neighbors,
                            const unsigned min_shared_neighbors,
                            str neighbors_key,
                            const unsigned num_threads):
                cdef bint has_QC_column = QCed_to_full_map.shape[0] != 0
                cdef unsigned i, j, k, thread_index, num_unique_ks, \
                    num_unique_pruned_ks, num_shared, size, \
                    num_cells = QCed_to_full_map.shape[0] \
                        if has_QC_column else neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1], \
                    twice_num_neighbors = 2 * num_neighbors
                cdef unsigned long nnz, start
                cdef vector[vector[unsigned]] reverse_neighbors, shared_neighbors
                cdef vector[vector[double]] shared_neighbor_weights
                cdef np.ndarray[np.uint32_t, ndim=1] indices
                cdef np.ndarray[np.float64_t, ndim=1] data
                cdef unsigned[::1] indices_view
                cdef double[::1] data_view
                
                # Allocate space for shared nearest neighbors and weights
                
                shared_neighbors.resize(num_cells)
                shared_neighbor_weights.resize(num_cells)
                
                # Build the "reverse" mapping: which cells have each cell as a neighbor.
                # Also check that all neighbor indices are in range.
                
                reverse_neighbors.resize(num_cells)
                for i in range(num_cells):
                    for j in range(num_neighbors):
                        neighbor = neighbors[i, j]
                        if neighbor < 0:
                            error_message = (
                                f'some nearest-neighbor indices in '
                                f'obsm[{neighbors_key!r}] are negative')
                            raise ValueError(error_message)
                        if neighbor >= num_cells:
                            error_message = (
                                f'some nearest-neighbor indices in '
                                f'obsm[{neighbors_key!r}] are >= the total '
                                f'number of cells, {num_cells:,}. This may '
                                f'happen if you subset this SingleCell '
                                f'dataset between neighbors() and '
                                f'shared_neighbors(); if so, make sure to run '
                                f'neighbors() after, not before, subsetting.')
                            raise ValueError(error_message)
                        reverse_neighbors[neighbor].push_back(i)
                
                PyErr_CheckSignals()
                
                # Calculate the number of shared neigbors for each pair of cells
                
                if not has_QC_column:
                    if num_threads == 1:
                        thread_index = 0
                        for i in range(num_cells):
                            # For this cell `i`, find all pairs of cells `j` and `k`
                            # where `j` is a shared nearest neighbor of `i` and `k`.
                            # Do this by looking up all neighbors (`j`) of this cell
                            # (`i`), then looking up each of these neighbors' reverse
                            # neighbors (`k`).
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    # Make a list of the unique `k`s...
                                    if num_shared_neighbors[thread_index, k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    # ...and the number of times each `k` has a shared
                                    # nearest neighbor with `i`. Note:
                                    # `num_shared_neighbors[thread_index]` has one element
                                    # per cell, and almost all elements are 0!
                                    num_shared_neighbors[thread_index, k] += 1
                            
                            # Calculate the SNN weight between `i` and each `k`: the
                            # number of shared neighbors, divided by the total number
                            # of unique cells in `i` and `k`'s neighbor lists (which is
                            # twice the number of total neighbors, minus the number of
                            # shared neighbors). Note that since `shared_neighbors` is
                            # sorted, the elements of `shared_neighbor_weights` will
                            # correspond to the unique `k`s (after pruning) in sorted order.
                            # Preallocate enough space for all `num_unique_ks` to avoid the
                            # need for dynamic resizing, even though the true required size
                            # (which we call `num_unique_pruned_ks`) is less due to the pruning.
                            
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[thread_index, k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <double> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k  # move left
                                    num_unique_pruned_ks += 1
                                num_shared_neighbors[thread_index, k] = 0  # reset
                            
                            # Store the number of unique `k`s after pruning in
                            # `indptr` (we will take the cumsum later)
                            
                            indptr[i + 1] = num_unique_pruned_ks
                    else:
                        # Same code as the serial version aside from the
                        # `prange()`; note that each thread has its own
                        # independent workspace within `num_shared_neighbors`
                        
                        for i in prange(num_cells, num_threads=num_threads,
                                        nogil=True):
                            thread_index = threadid()
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if num_shared_neighbors[thread_index, k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    num_shared_neighbors[thread_index, k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[thread_index, k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <double> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks = num_unique_pruned_ks + 1
                                num_shared_neighbors[thread_index, k] = 0
                            indptr[i + 1] = num_unique_pruned_ks
                else:
                    # Same code as above, but store in `indptr[QCed_to_full_map[i] + 1]`
                    # instead of `indptr[i + 1]`. Also, initialize `indptr` to 0 so
                    # that cells failing QC are not left uninitialized.
                    
                    indptr[:] = 0
                    if num_threads == 1:
                        thread_index = 0
                        for i in range(num_cells):
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if num_shared_neighbors[thread_index, k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    num_shared_neighbors[thread_index, k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[thread_index, k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <double> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks += 1
                                num_shared_neighbors[thread_index, k] = 0
                            indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                        
                    else:
                        for i in prange(num_cells, num_threads=num_threads,
                                        nogil=True):
                            thread_index = threadid()
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if num_shared_neighbors[thread_index, k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    num_shared_neighbors[thread_index, k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[thread_index, k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <double> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks = num_unique_pruned_ks + 1
                                num_shared_neighbors[thread_index, k] = 0
                            indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                
                PyErr_CheckSignals()
                
                # Take the cumulative sum of the values in `indptr`; initialize the
                # first element to 0
                
                indptr[0] = 0
                for i in range(2, indptr.shape[0]):
                    indptr[i] += indptr[i - 1]
                
                # Allocate `indices` and `data`: their length is the sum of the
                # numbers of unique `k`s across all cells
                
                nnz = indptr[indptr.shape[0] - 1]
                indices = np.empty(nnz, dtype=np.uint32)
                data = np.empty(nnz, dtype=np.float64)
                
                # Populate `indices` and `data` with `shared_neighbors` and
                # `shared_neighbor_weights`, respectively. Access the arrays via
                # memoryviews with C-contiguity specified to force Cython to avoid
                # generating slower code that accounts for stride (not sure if this
                # is necessary). If `QC_column` is not `None`, map each element
                # of `indices` through `QCed_to_full_map` so the indices are
                # with respect to all cells, not just QCed cells.
                
                indices_view = indices
                data_view = data
                if not has_QC_column:
                    if num_threads == 1:
                        for i in range(num_cells):
                            start = indptr[i]
                            size = indptr[i + 1] - start
                            # indices_view[start:start + size] = shared_neighbors[i][:]
                            memcpy(&indices_view[start], shared_neighbors[i].data(),
                                   size * sizeof(unsigned))
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(double))
                    else:
                        for i in prange(num_cells, num_threads=num_threads, nogil=True):
                            start = indptr[i]
                            size = indptr[i + 1] - start
                            # indices_view[start:start + size] = shared_neighbors[i][:]
                            memcpy(&indices_view[start], shared_neighbors[i].data(),
                                   size * sizeof(unsigned))
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(double))
                else:
                    if num_threads == 1:
                        for i in range(num_cells):
                            start = indptr[QCed_to_full_map[i]]
                            size = indptr[QCed_to_full_map[i] + 1] - start
                            # indices_view[start:start + size] = QCed_to_full_map[shared_neighbors[i][:]]
                            for j in range(size):
                                indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(double))
                    else:
                        for i in prange(num_cells, num_threads=num_threads, nogil=True):
                            start = indptr[QCed_to_full_map[i]]
                            size = indptr[QCed_to_full_map[i] + 1] - start
                            # indices_view[start:start + size] = QCed_to_full_map[shared_neighbors[i][:]]
                            for j in range(size):
                                indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(double))
                
                return indices, data
            ''', warn_undeclared=False)['compute_SNN'](
                neighbors=nearest_neighbor_indices,
                QCed_to_full_map=QCed_to_full_map, indptr=indptr,
                num_shared_neighbors=num_shared_neighbors,
                min_shared_neighbors=min_shared_neighbors,
                neighbors_key=neighbors_key, num_threads=num_threads)
        snn_graph = csr_array((data, indices, indptr),
                              shape=(num_cells, num_cells))
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp | {shared_neighbors_key: snn_graph},
                          varp=self._varp, uns=self._uns)
    
    def cluster(self,
                *,
                QC_column: SingleCellColumn | None |
                           Sequence[SingleCellColumn | None] = 'passed_QC',
                shared_neighbors_key: str = 'shared_neighbors',
                cell_type_column: str = 'cell_type',
                overwrite: bool = False,
                verbose: bool = True,
                num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Cluster cells into cell types using hierarchical clustering.
        
        Hierarchical clustering is performed via the Paris algorithm
        (arxiv.org/abs/1806.01664; github.com/tbonald/paris), as originally
        applied to single-cell data in the Hierarchical Graph Clustering (HGC)
        package (academic.oup.com/bioinformatics/article/37/21/3964/6294402;
        github.com/XuegongLab/HGC).
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their cell-type labels set to `NaN`.
            shared_neighbors_key: the key of `obsp` containing the shared
                                  nearest neighbor graph calculated with
                                  `shared_neighbors()`, to use as the input for
                                  cell-type clustering. Must be symmetric,
                                  although this is not checked for, due to
                                  speed considerations.
            cell_type_column: the name of an integer column to be added to
                              obs indicating the cell-type labels
            overwrite: if `True`, overwrite `cell_type_column` if already
                       present in `obs`, instead of raising an error
            verbose: whether to print details of the cell-type clustering
            num_threads: the number of threads to use when clustering. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). Note that only the initial graph
                         construction is parallelized, so the speedup from
                         parallelism will likely be modest (e.g. ~10% faster
                         with a few dozen threads compared to a single thread).
        
        Returns:
            A new SingleCell dataset where `obs[cell_type_column]` contains an
            integer cell-type label for each cell.
        
        Note:
            This function may give an incorrect output if you specified a
            custom shared nearest-neighbor graph that a) is non-symmetric (i.e.
            `(obsp[shared_neighbors_key] != obsp[shared_neighbors_key].T).nnz`
            is non-zero), b) contains explicit zeros (i.e. if
            `(obsp[shared_neighbors_key].data == 0).any()`), or c) contains
            negative values: these are not checked for, due to speed
            considerations. In the unlikely event that your custom shared
            nearest-neighbor graph contains explicit zeros, remove them by
            running `obsp[shared_neighbors_key].eliminate_zeros()` (an in-place
            operation) first.
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the shared nearest neighbor graph, and check that it is
        # floating-point
        check_type(shared_neighbors_key, 'shared_neighbors_key', str,
                   'a string')
        if shared_neighbors_key not in self._obsp:
            error_message = (
                f'shared_neighbors_key {shared_neighbors_key!r} is not a key '
                f'of obsp')
            if shared_neighbors_key == 'shared_neighbors':
                error_message += (
                    '; did you forget to run shared_neighbors() before '
                    'cluster()?')
            raise ValueError(error_message)
        snn_graph = self._obsp[shared_neighbors_key]
        if not np.issubdtype(snn_graph.dtype, np.floating):
            error_message = (
                f'obsp[{shared_neighbors_key!r}] must have floating-point '
                f'data type, but has data type {str(snn_graph.dtype)!r}')
            raise TypeError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `cell_type_column` is a string and, unless
        # `overwrite=True`, not already in `obs`
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        if not overwrite and 'cell_type_column' in self._obs:
            error_message = (
                f'cell_type_column {cell_type_column!r} is already a column '
                f'of obs; did you already run cluster()? Set overwrite=True '
                f'to overwrite.')
            raise ValueError(error_message)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Perform hierarchical clustering with the Paris algorithm. Store the
        # result in three arrays, sorted by increasing distance:
        # - `left_nodes[i]` is the first node in merge `i`
        # - `right_nodes[i]` is the second node in merge `i`
        # - `distances[i]` is the distance at which merge `i` occurs
        # Note: there will always be `num_nodes - 1` merges, where `num_nodes`
        #       is the number of cells/nodes in the graph.
        # Note: because `snn_graph` is symmetric, we don't have to worry about
        #       whether it's CSR or CSC.
        num_nodes = snn_graph.shape[0]
        num_nodes_minus_1 = num_nodes - 1
        left_nodes = np.empty(num_nodes_minus_1, dtype=np.uint32)
        right_nodes = np.empty(num_nodes_minus_1, dtype=np.uint32)
        distances = np.empty(num_nodes_minus_1, dtype=float)
        cython_inline(r'''
        # This code generally follows the original Paris clustering
        # implementation (github.com/tbonald/paris/blob/master/paris.py), but
        # ignores self edges (the diagonal of the sparse matrix) instead of
        # counting them twice towards `w` and once towards `wtot`. For
        # computational efficiency, it also differs from the original by not
        # keeping track of cluster sizes, and not relabelling non-leaf nodes in
        # order of increasing distance when sorting by distance.
        
        from cpython.exc cimport PyErr_CheckSignals
        from cython.operator cimport dereference
        from cython.parallel cimport prange
        from libc.math cimport INFINITY, isinf
        from libcpp.pair cimport pair
        from libcpp.vector cimport vector
        
        cdef extern from "limits.h":
            cdef unsigned UINT_MAX
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        ctypedef fused signed_integer:
            int
            long
        
        cdef extern from *:
            """
            // From github.com/ktprime/emhash/blob/master/thirdparty/emilib/emilib2o.hpp,
            // as benchmarked at jacksonallan.github.io/c_cpp_hash_tables_benchmark.
            // LICENSE:
            //   This software is dual-licensed to the public domain and under the following
            //   license: you are granted a perpetual, irrevocable license to copy, modify,
            //   publish, and distribute this file as you see fit.
        
            #include <cstdlib>
            #include <cstring>
            #include <iterator>
            #include <utility>
            #include <cassert>
        
            #ifdef _WIN32
            #  include <intrin.h>
            #ifndef __clang__
            //#  include <zmmintrin.h>
            #endif
            #elif __x86_64__
            #  include <x86intrin.h>
            #else
            # include "sse2neon.h"
            #endif
        
            #undef EMH_LIKELY
            #undef EMH_UNLIKELY
        
            // likely/unlikely
            #if (__GNUC__ >= 4 || __clang__) && _MSC_VER == 0
            #    define EMH_LIKELY(condition)   __builtin_expect(condition, 1)
            #    define EMH_UNLIKELY(condition) __builtin_expect(condition, 0)
            #else
            #    define EMH_LIKELY(condition)   condition
            #    define EMH_UNLIKELY(condition) condition
            #endif
        
            // namespace emilib2 {
        
                enum State : int8_t
                {
                    EFILLED  = -126, EDELETE = -127, EEMPTY = -128,
                    SENTINEL = 127,
                };
        
                constexpr static uint8_t EMPTY_OFFSET = 0;
                constexpr static uint8_t OFFSET_STEP  = 4;
        
            #define EMH_ITERATOR_BITS simd_bytes
        
            #ifndef AVX2_EHASH
                const static auto simd_empty  = _mm_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm_set1_epi8(EDELETE);
                const static auto simd_filled = _mm_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm_set1_epi8
                #define LOAD_UEPI8     _mm_loadu_si128
                #define MOVEMASK_EPI8  _mm_movemask_epi8
                #define CMPEQ_EPI8     _mm_cmpeq_epi8
                #define CMPGT_EPI8     _mm_cmpgt_epi8
            #elif SSE2_EMHASH == 0
                const static auto simd_empty  = _mm256_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm256_set1_epi8(EDELETE);
                const static auto simd_filled = _mm256_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm256_set1_epi8
                #define LOAD_UEPI8     _mm256_loadu_si256
                #define MOVEMASK_EPI8  _mm256_movemask_epi8
                #define CMPEQ_EPI8     _mm256_cmpeq_epi8
                #define CMPGT_EPI8     _mm256_cmpgt_epi8
        
            #elif AVX512_EHASH
                const static auto simd_empty  = _mm512_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm512_set1_epi8(EDELETE);
                const static auto simd_filled = _mm512_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm512_set1_epi8
                #define LOAD_UEPI8     _mm512_loadu_si512
                #define MOVEMASK_EPI8  _mm512_movemask_epi8 //avx512 error
                #define CMPEQ_EPI8     _mm512_test_epi8_mask
                #define CMPGT_EPI8     _mm512_cmpgt_epi8
            #endif
        
            inline static uint32_t CTZ(uint64_t n)
            {
            #if defined(__x86_64__) || defined(_WIN32) || (__BYTE_ORDER__ && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
        
            #elif __BIG_ENDIAN__ || (__BYTE_ORDER__ && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)
                n = __builtin_bswap64(n);
            #else
                static uint32_t endianness = 0x12345678;
                const auto is_big = *(const char *)&endianness == 0x12;
                if (is_big)
                n = __builtin_bswap64(n);
            #endif
        
            #if _WIN32
                unsigned long index;
                #if defined(_WIN64)
                    _BitScanForward64(&index, n);
                #else
                if ((uint32_t)n)
                    _BitScanForward(&index, (uint32_t)n);
                else
                    {_BitScanForward(&index, (uint32_t)(n >> 32)); index += 32; }
                #endif
            #elif defined (__LP64__) || (SIZE_MAX == UINT64_MAX) || defined (__x86_64__)
                uint32_t index = __builtin_ctzll(n);
            #elif 1
                uint32_t index = __builtin_ctzl(n);
            #endif
        
                return (uint32_t)index;
            }
        
            /// A cache-friendly hash table with open addressing, linear probing and power-of-two capacity
            template <typename KeyT, typename ValueT, typename HashT = std::hash<KeyT>, typename EqT = std::equal_to<KeyT>>
            class HashMap
            {
            private:
                using htype = HashMap<KeyT, ValueT, HashT, EqT>;
        
                using PairT = std::pair<KeyT, ValueT>;
        
            public:
                using size_t          = uint32_t;
                using value_type      = PairT;
                using reference       = PairT&;
                using const_reference = const PairT&;
        
                using mapped_type     = ValueT;
                using val_type        = ValueT;
                using key_type        = KeyT;
                using hasher          = HashT;
                using key_equal       = EqT;
        
                template<typename UType, typename std::enable_if<!std::is_integral<UType>::value, int8_t>::type = 0>
                inline int8_t hash_key2(size_t& main_bucket, const UType& key) const
                {
                    const auto key_hash = _hasher(key);
                    main_bucket = (size_t)key_hash & _mask;
                    return (int8_t)((uint64_t)key_hash % 253 + EFILLED);
                }
        
                template<typename UType, typename std::enable_if<std::is_integral<UType>::value, int8_t>::type = 0>
                inline int8_t hash_key2(size_t& main_bucket, const UType& key) const
                {
                    const auto key_hash = _hasher(key);
                    main_bucket = (size_t)key_hash & _mask;
            //        return (int8_t)((key * 0x9FB21C651E98DF25ull % 251) - 125);
                    return (int8_t)((size_t)key % 253 + EFILLED);
                }
        
                class const_iterator;
                class iterator
                {
                public:
                    using iterator_category = std::forward_iterator_tag;
                    using difference_type   = std::ptrdiff_t;
                    using value_type        = std::pair<KeyT, ValueT>;
                    using pointer           = value_type*;
                    using reference         = value_type&;
        
                    iterator() {}
                    iterator(const const_iterator& it)
                        : _map(it._map), _bucket(it._bucket), _bmask(it._bmask), _from(it._from) {}
                    iterator(const htype* hash_map, size_t bucket) : _map(hash_map), _bucket(bucket) { init(); }
            #if EMH_ITER_SAFE
                    iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { init(); }
            #else
                    iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { _bmask = 0; _from = size_t(- 1); }
            #endif
        
                    void init()
                    {
                        _from = (_bucket / EMH_ITERATOR_BITS) * EMH_ITERATOR_BITS;
                        const auto bucket_count = _map->bucket_count();
                        if (_bucket < bucket_count) {
                            _bmask = _map->filled_mask(_from);
                            _bmask &= ~((1ull << (_bucket % EMH_ITERATOR_BITS)) - 1);
                        } else {
                            _bmask = 0;
                        }
                    }
        
                    size_t operator - (const iterator& r) const
                    {
                        return _bucket - r._bucket;
                    }
        
                    size_t bucket() const
                    {
                        return _bucket;
                    }
        
                    iterator& operator++()
                    {
            #ifndef EMH_ITER_SAFE
                        if (_from == (size_t)-1) init();
            #endif
                        goto_next_element();
                        return *this;
                    }
        
                    iterator operator++(int)
                    {
            #ifndef EMH_ITER_SAFE
                        if (_from == (size_t)-1) init();
            #endif
                        iterator old(*this);
                        goto_next_element();
                        return old;
                    }
        
                    reference operator*() const { return _map->_pairs[_bucket]; }
                    pointer operator->() const { return _map->_pairs + _bucket; }
        
                    bool operator==(const iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const iterator& rhs) const { return _bucket != rhs._bucket; }
                    bool operator==(const const_iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const const_iterator& rhs) const { return _bucket != rhs._bucket; }
        
                private:
                    void goto_next_element()
                    {
                        _bmask &= _bmask - 1;
                        if (_bmask != 0) {
                            _bucket = _from + CTZ(_bmask);
                            return;
                        }
        
                        do {
                            _bmask = _map->filled_mask(_from += EMH_ITERATOR_BITS);
                        } while (_bmask == 0);
        
                        _bucket = _from + CTZ(_bmask);
                    }
        
                public:
                    const htype*  _map;
                    size_t        _bmask;
                    size_t        _bucket;
                    size_t        _from;
                };
        
                class const_iterator
                {
                public:
                    using iterator_category = std::forward_iterator_tag;
                    using difference_type   = std::ptrdiff_t;
                    using value_type        = const std::pair<KeyT, ValueT>;
                    using pointer           = value_type*;
                    using reference         = value_type&;
        
                    explicit const_iterator(const iterator& it)
                        : _map(it._map), _bucket(it._bucket), _bmask(it._bmask), _from(it._from) { init(); }
                    const_iterator(const htype* hash_map, size_t bucket) : _map(hash_map), _bucket(bucket) { init(); }
            //        const_iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { init(); }
        
                    void init()
                    {
                        _from = (_bucket / EMH_ITERATOR_BITS) * EMH_ITERATOR_BITS;
                        const auto bucket_count = _map->bucket_count();
                        if (_bucket < bucket_count) {
                            _bmask = _map->filled_mask(_from);
                            _bmask &= ~((1ull << (_bucket % EMH_ITERATOR_BITS)) - 1);
                        } else {
                            _bmask = 0;
                        }
                    }
        
                    size_t bucket() const
                    {
                        return _bucket;
                    }
        
                    size_t operator - (const const_iterator& r) const
                    {
                        return _bucket - r._bucket;
                    }
        
                    const_iterator& operator++()
                    {
                        goto_next_element();
                        return *this;
                    }
        
                    const_iterator operator++(int)
                    {
                        const_iterator old(*this);
                        goto_next_element();
                        return old;
                    }
        
                    reference operator*() const { return _map->_pairs[_bucket]; }
                    pointer operator->() const { return _map->_pairs + _bucket; }
        
                    bool operator==(const iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const iterator& rhs) const { return _bucket != rhs._bucket; }
                    bool operator==(const const_iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const const_iterator& rhs) const { return _bucket != rhs._bucket; }
        
                private:
                    void goto_next_element()
                    {
                        _bmask &= _bmask - 1;
                        if (_bmask != 0) {
                            _bucket = _from + CTZ(_bmask);
                            return;
                        }
        
                        do {
                            _bmask = _map->filled_mask(_from += EMH_ITERATOR_BITS);
                        } while (_bmask == 0);
        
                        _bucket = _from + CTZ(_bmask);
                    }
        
                public:
                    const htype*  _map;
                    size_t        _bmask;
                    size_t        _bucket;
                    size_t        _from;
                };
        
                // ------------------------------------------------------------------------
        
                HashMap(size_t n = 4) noexcept
                {
                    rehash(n);
                }
        
                HashMap(const HashMap& other) noexcept
                {
                    clone(other);
                }
        
                HashMap(HashMap&& other) noexcept
                {
                    rehash(1);
                    if (this != &other) {
                        swap(other);
                    }
                }
        
                HashMap(std::initializer_list<value_type> il) noexcept
                {
                    reserve((size_t)il.size());
                    for (auto it = il.begin(); it != il.end(); ++it)
                        insert(*it);
                }
        
                template<class InputIt>
                HashMap(InputIt first, InputIt last, size_t bucket_count = 4) noexcept
                {
                    reserve(std::distance(first, last) + bucket_count);
                    for (; first != last; ++first)
                        insert(*first);
                }
        
                HashMap& operator=(const HashMap& other) noexcept
                {
                    if (this != &other)
                        clone(other);
                    return *this;
                }
        
                HashMap& operator=(HashMap&& other) noexcept
                {
                    if (this != &other) {
                        swap(other);
                        other.clear();
                    }
                    return *this;
                }
        
                ~HashMap() noexcept
                {
                    clear_data();
                    _num_filled = 0;
                    _pairs[_num_buckets].~PairT();
                    free(_pairs);
                }
        
                void clone(const HashMap& other) noexcept
                {
                    if (other.size() == 0) {
                        clear();
                        return;
                    }
        
                    clear_data();
        
                    if (other._num_buckets != _num_buckets) {
                        _num_filled = _num_buckets = 0;
                        rehash(other._num_buckets);
                    }
        
                    if (is_copy_trivially()) {
                        const auto pairs_size = (_num_buckets + 1) * sizeof(PairT);
                        memcpy((char*)_pairs, other._pairs, pairs_size);
                    } else {
                        for (auto it = other.cbegin(); it.bucket() <= _num_buckets; ++it)
                            new(_pairs + it.bucket()) PairT(*it);
                    }
        
                    //assert(_num_buckets == other._num_buckets);
                    _num_filled = other._num_filled;
                    const auto state_size = simd_bytes + _num_buckets;
                    memcpy(_states, other._states, state_size * sizeof(_states[0]));
                    memcpy(_offset, other._offset, _num_buckets * sizeof(_offset[0]) / OFFSET_STEP + 1);
                }
        
                void swap(HashMap& other) noexcept
                {
                    std::swap(_hasher,      other._hasher);
                    std::swap(_eq,          other._eq);
                    std::swap(_states,      other._states);
                    std::swap(_pairs,       other._pairs);
                    std::swap(_num_buckets, other._num_buckets);
                    std::swap(_num_filled,  other._num_filled);
                    std::swap(_offset,      other._offset);
                    std::swap(_mask,        other._mask);
                }
        
                // -------------------------------------------------------------
        
                iterator begin() noexcept
                {
                    if (_num_filled == 0)
                        return {this, _num_buckets, false};
                    return {this, find_first_slot(0), false};
                }
        
                const_iterator cbegin() const noexcept
                {
                    if (_num_filled == 0)
                        return {this, _num_buckets};
                    return {this, find_first_slot(0)};
                }
        
                const_iterator begin() const noexcept
                {
                    return cbegin();
                }
        
                iterator end() noexcept
                {
                    return {this, _num_buckets, false};
                }
        
                const_iterator cend() const noexcept
                {
                    return {this, _num_buckets};
                }
        
                const_iterator end() const noexcept
                {
                    return cend();
                }
        
                size_t size() const
                {
                    return _num_filled;
                }
        
                bool empty() const
                {
                    return _num_filled == 0;
                }
        
                // Returns the number of buckets.
                size_t bucket_count() const
                {
                    return _num_buckets;
                }
        
                /// Returns average number of elements per bucket.
                float load_factor() const
                {
                    return _num_filled / static_cast<float>(_num_buckets);
                }
        
                float max_load_factor(float lf = 7.0f / 8)
                {
                    (void)lf;
                    return 7.0f / 8;
                }
        
                // ------------------------------------------------------------
        
                template<typename K>
                iterator find(const K& key) noexcept
                {
                    return {this, find_filled_bucket(key), false};
                }
        
                template<typename K>
                const_iterator find(const K& key) const noexcept
                {
                    return {this, find_filled_bucket(key)};
                }
        
                template<typename K>
                bool contains(const K& k) const noexcept
                {
                    return find_filled_bucket(k) != _num_buckets;
                }
        
                template<typename K>
                size_t count(const K& k) const noexcept
                {
                    return find_filled_bucket(k) != _num_buckets;
                }
        
                template<typename Key = KeyT>
                ValueT& at(const KeyT& key)
                {
                    const auto bucket = find_filled_bucket(key);
                    return _pairs[bucket].second;
                }
        
                template<typename Key = KeyT>
                const ValueT& at(const KeyT& key) const
                {
                    const auto bucket = find_filled_bucket(key);
                    return _pairs[bucket].second;
                }
        
                /// Returns the matching ValueT or nullptr if k isn't found.
                template<typename K>
                ValueT* try_get(const K& k)
                {
                    auto bucket = find_filled_bucket(k);
                    return &_pairs[bucket].second;
                }
        
                /// Const version of the above
                template<typename K>
                ValueT* try_get(const K& k) const
                {
                    auto bucket = find_filled_bucket(k);
                    return &_pairs[bucket].second;
                }
        
                template<typename Con>
                bool operator == (const Con& rhs) const
                {
                    if (size() != rhs.size())
                        return false;
        
                    for (auto it = begin(), last = end(); it != last; ++it) {
                        auto oi = rhs.find(it->first);
                        if (oi == rhs.end() || it->second != oi->second)
                            return false;
                    }
                    return true;
                }
        
                template<typename Con>
                bool operator != (const Con& rhs) const { return !(*this == rhs); }
        
                void merge(HashMap& rhs)
                {
                    if (empty()) {
                        *this = std::move(rhs);
                        return;
                    }
        
                    for (auto rit = rhs.begin(); rit != rhs.end(); ) {
                        auto fit = find(rit->first);
                        if (fit.bucket() > _mask) {
                            insert_unique(rit->first, std::move(rit->second));
                            rhs.erase(rit++);
                        } else {
                            ++rit;
                        }
                    }
                }
        
                // -----------------------------------------------------
        
                /// Returns a pair consisting of an iterator to the inserted element
                /// (or to the element that prevented the insertion)
                /// and a bool denoting whether the insertion took place.
                template<typename K, typename V>
                std::pair<iterator, bool> do_insert(K&& key, V&& val) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
        
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                std::pair<iterator, bool> do_insert(const value_type& value) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(value.first, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(value); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                std::pair<iterator, bool> do_insert(value_type&& value) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(value.first, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::move(value)); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                template <class... Args>
                inline std::pair<iterator, bool> emplace(Args&&... args) noexcept
                {
                    return do_insert(std::forward<Args>(args)...);
                }
        
                std::pair<iterator, bool> insert(value_type&& value) noexcept
                {
                    return do_insert(std::move(value));
                }
        
                std::pair<iterator, bool> insert(const value_type& value) noexcept
                {
                    return do_insert(value);
                }
        
            #if 0
                iterator insert(iterator hint, const value_type& value) noexcept
                {
                    (void)hint;
                    return do_insert(value).first;
                }
            #endif
        
                template <typename Iter>
                void insert(Iter beginc, Iter endc)
                {
                    reserve(endc - beginc + _num_filled);
                    for (; beginc != endc; ++beginc)
                        do_insert(beginc->first, beginc->second);
                }
        
                template<class... Args>
                std::pair<iterator, bool> try_emplace(const KeyT& key, Args&&... args)
                {
                    //check_expand_need();
                    return do_insert(key, std::forward<Args>(args)...);
                }
        
                template<class... Args>
                std::pair<iterator, bool> try_emplace(KeyT&& key, Args&&... args)
                {
                    //check_expand_need();
                    return do_insert(std::forward<KeyT>(key), std::forward<Args>(args)...);
                }
        
                void insert(std::initializer_list<value_type> ilist) noexcept
                {
                    reserve(ilist.size() + _num_filled);
                    for (auto it = ilist.begin(); it != ilist.end(); ++it)
                        do_insert(*it);
                }
        
                template<typename K, typename V>
                size_t insert_unique(K&& key, V&& val) noexcept
                {
                    check_expand_need();
        
                    size_t main_bucket;
                    const auto key_h2 = hash_key2(main_bucket, key);
                    const auto bucket = find_empty_slot(main_bucket, main_bucket, 0);
        
                    set_states(bucket, key_h2);
                    new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    return bucket;
                }
        
                template <class M>
                std::pair<iterator, bool> insert_or_assign(const KeyT& key, M&& val) { return do_assign(key, std::forward<M>(val)); }
                template <class M>
                std::pair<iterator, bool> insert_or_assign(KeyT&& key, M&& val) { return do_assign(std::move(key), std::forward<M>(val)); }
        
                template<typename K, typename V>
                std::pair<iterator, bool> do_assign(K&& key, V&& val)
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
        
                    // Check if inserting a new val rather than overwriting an old entry
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    } else {
                        _pairs[bucket].second = std::forward<V>(val);
                    }
        
                    return { {this, bucket, false}, bempty };
                }
        
                bool set_get(const KeyT& key, const ValueT& val, ValueT& oldv)
                {
                    //check_expand_need();
        
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    /* Check if inserting a new value rather than overwriting an old entry */
                    if (bempty) {
                        new(_pairs + bucket) PairT(key,val); _num_filled++;
                    } else
                        oldv = _pairs[bucket].second;
                    return bempty;
                }
        
                ValueT& operator[](const KeyT& key) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    /* Check if inserting a new value rather than overwriting an old entry */
                    if (bempty) {
                        new(_pairs + bucket) PairT(key, std::move(ValueT())); _num_filled++;
                    }
        
                    return _pairs[bucket].second;
                }
        
                ValueT& operator[](KeyT&& key) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::move(key), std::move(ValueT())); _num_filled++;
                    }
        
                    return _pairs[bucket].second;
                }
        
                // -------------------------------------------------------
        
                /// Erase an element from the hash table.
                /// return false if element was not found
                size_t erase(const KeyT& key) noexcept
                {
                    auto bucket = find_filled_bucket(key);
                    if (bucket == _num_buckets)
                        return 0;
        
                    _erase(bucket);
                    return 1;
                }
        
                void erase(const const_iterator& cit) noexcept
                {
                    _erase(cit._bucket);
                }
        
                void erase(iterator it) noexcept
                {
                    _erase(it._bucket);
                }
        
                void _erase(size_t bucket) noexcept
                {
                    _num_filled -= 1;
                    if (is_triviall_destructable())
                        _pairs[bucket].~PairT();
            #if EMH_PSL_LINEAR
                    set_states(bucket, _states[bucket + 1] == State::EEMPTY ? State::EEMPTY : State::EDELETE);
            #else
                    set_states(bucket, State::EDELETE);
            #endif
        
            #if EMH_PSL_ERASE && EMH_PSL_LINEAR
                    if (_states[bucket] == State::EEMPTY) {
                        _offset[bucket] = EMPTY_OFFSET; bucket = (bucket - 1) & _mask;
                        while (_states[bucket] == State::EDELETE) {
                            set_states(bucket, State::EEMPTY); _offset[bucket] = EMPTY_OFFSET;
                            bucket = (bucket - 1) & _mask;
                        }
                    }
            #elif 0
                    if (EMH_UNLIKELY(_num_filled == 0)) {
                        std::fill_n(_states, _num_buckets, State::EEMPTY);
                        std::fill_n(_offset, _num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
                    }
            #endif
                }
        
                iterator erase(const_iterator first, const_iterator last)
                {
                    auto iend = cend();
                    auto next = first;
                    for (; next != last && next != iend; )
                        erase(next++);
        
                    return {this, next.bucket()};
                }
        
                template<typename Pred>
                size_t erase_if(Pred pred)
                {
                    auto old_size = size();
                    for (auto it = begin(), last = end(); it != last; ) {
                        if (pred(*it))
                            erase(it);
                        ++it;
                    }
                    return old_size - size();
                }
        
                static constexpr bool is_triviall_destructable()
                {
            #if __cplusplus >= 201402L || _MSC_VER > 1600
                    return !(std::is_trivially_destructible<KeyT>::value && std::is_trivially_destructible<ValueT>::value);
            #else
                    return !(std::is_pod<KeyT>::value && std::is_pod<ValueT>::value);
            #endif
                }
        
                static constexpr bool is_copy_trivially()
                {
            #if __cplusplus >= 201402L || _MSC_VER > 1600
                    return (std::is_trivially_copyable<KeyT>::value && std::is_trivially_copyable<ValueT>::value);
            #else
                    return (std::is_pod<KeyT>::value && std::is_pod<ValueT>::value);
            #endif
                }
        
                void clear_data()
                {
                    if (is_triviall_destructable()) {
                        for (auto it = begin(); _num_filled; ++it) {
                            const auto bucket = it.bucket();
                            _pairs[bucket].~PairT();
                            _num_filled -= 1;
                        }
                    }
                }
        
                /// Remove all elements, keeping full capacity.
                void clear() noexcept
                {
                    if (_num_filled) {
                        clear_data();
                        std::fill_n(_states, _num_buckets, State::EEMPTY);
                        std::fill_n(_offset, _num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
                    }
                    _num_filled = 0;
                }
        
                void shrink_to_fit()
                {
                    rehash(_num_filled + 1);
                }
        
                bool reserve(size_t num_elems) noexcept
                {
                    size_t required_buckets = num_elems + num_elems / 6;
                    if (EMH_LIKELY(required_buckets < _num_buckets))
                        return false;
        
                    rehash(required_buckets + 2);
                    return true;
                }
        
                /// Make room for this many elements
                void rehash(size_t num_elems) noexcept
                {
                    const size_t required_buckets = num_elems;
                    if (required_buckets < _num_filled)
                        return;
        
                    auto num_buckets = _num_filled > (1u << 16) ? (1u << 16) : simd_bytes;
                    while (num_buckets < required_buckets) { num_buckets *= 2; }
        
                    const auto pairs_size = (num_buckets + 1) * sizeof(PairT);
                    const auto state_size = (simd_bytes + num_buckets);
                    //assert(state_size % 8 == 0);
        
                    const auto* new_data = (char*)malloc(pairs_size + state_size * sizeof(_states[0]) + (state_size / OFFSET_STEP) * sizeof(_offset[0]));
                    auto old_states      = _states;
        
                    auto* new_pairs = (decltype(_pairs)) new_data;
                    _states         = (decltype(_states))(new_data + pairs_size);
                    _offset         = (decltype(_offset))(_states + state_size);
        
                    auto old_num_filled  = _num_filled;
                    auto old_pairs       = _pairs;
                    auto old_buckets     = _num_buckets;
        
                    _num_filled  = 0;
                    _num_buckets = num_buckets;
                    _mask        = num_buckets - 1;
                    _pairs       = new_pairs;
        
                    //init empty
                    std::fill_n(_states, num_buckets, State::EEMPTY);
                    //set sentinel tombstone
                    std::fill_n(_states + num_buckets, simd_bytes, State::SENTINEL);
                    //fill offset to 0
                    std::fill_n(_offset, num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
        
                    {
                        //set last packet tombstone. not equal key h2
                        new(_pairs + num_buckets) PairT(KeyT(), ValueT());
                        //size_t main_bucket;
                        //_states[num_buckets] = hash_key2(main_bucket, _pairs[num_buckets].first) + 2; //iterator end tombstone:
                        if (old_buckets && is_triviall_destructable())
                            old_pairs[old_buckets].~PairT();
                    }
        
                    for (size_t src_bucket = old_buckets - 1; _num_filled < old_num_filled; --src_bucket) {
                        if (old_states[src_bucket] >= State::EFILLED) {
                            auto& src_pair = old_pairs[src_bucket];
                            size_t main_bucket;
                            const auto key_h2 = hash_key2(main_bucket, src_pair.first);
                            const auto bucket = find_empty_slot(main_bucket, main_bucket, 0);
        
                            set_states(bucket, key_h2);
                            new(_pairs + bucket) PairT(std::move(src_pair));
                            _num_filled ++;
                            if (is_triviall_destructable())
                                src_pair.~PairT();
                        }
                    }
                    free(old_pairs);
                }
        
            private:
                // Can we fit another element?
                void check_expand_need()
                {
                    reserve(_num_filled);
                }
        
                static void prefetch_heap_block(char* ctrl)
                {
                    // Prefetch the heap-allocated memory region to resolve potential TLB
                    // misses.  This is intended to overlap with execution of calculating the hash for a key.
            #if defined(_MSC_VER) && (defined(_M_X64) || defined(_M_IX86))
                    _mm_prefetch((const char*)ctrl, _MM_HINT_T0);
            #elif defined(__GNUC__)
                    __builtin_prefetch(static_cast<const void*>(ctrl));
            #endif
                }
        
                inline uint32_t get_offset(size_t main_bucket) const
                {
            #if EMH_SAFE_PSL
                    if (EMH_UNLIKELY(_offset[main_bucket / OFFSET_STEP] > 128))
                        return (_offset[main_bucket / OFFSET_STEP] - 127) * 128;
            #endif
                    return _offset[main_bucket / OFFSET_STEP];
                }
        
                inline void set_offset(size_t main_bucket, uint32_t off)
                {
            #if EMH_SAFE_PSL
                    _offset[main_bucket / OFFSET_STEP] = off <= 128 ? off : 128 + off / 128;
            #else
                    _offset[main_bucket / OFFSET_STEP] = (uint8_t)off;
            #endif
                }
        
                inline void set_states(size_t ebucket, int8_t key_h2)
                {
                    _states[ebucket] = key_h2;
                }
        
                inline size_t get_next_bucket(size_t next_bucket, size_t offset) const
                {
            #if EMH_PSL_LINEAR == 0
                    next_bucket += offset < 6 ? simd_bytes * offset + 5 : _num_buckets / 15 + 1;
                    //next_bucket += simd_bytes * offset + 1;
            #elif EMH_PSL_LINEAR == 1
                    if (offset < 8)
                        next_bucket += simd_bytes * 2 + offset;
                    else
                        next_bucket += _num_buckets / 32 + 1;
            #else
                    next_bucket += simd_bytes;
                    if (next_bucket >= _num_buckets)
                        next_bucket = offset;
            #endif
                    return next_bucket & _mask;
                }
        
                bool is_empty(size_t bucket) const
                {
                    return _states[bucket] == State::EEMPTY;
                }
        
                // Find the main_bucket with this key, or return (size_t)-1
                template<typename K>
                size_t find_filled_bucket(const K& key) const noexcept
                {
                    size_t main_bucket;
                    const auto filled = SET1_EPI8(hash_key2(main_bucket, key));
                    auto next_bucket = main_bucket;
                    size_t offset = 0;
        
                    if (1)
                    {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
                        if (maskf) {
                            prefetch_heap_block((char*)&_pairs[next_bucket]);
                            do {
                                const auto fbucket = next_bucket + CTZ(maskf);
                                if (EMH_LIKELY(_eq(_pairs[fbucket].first, key)))
                                    return fbucket;
                                maskf &= maskf - 1;
                            } while (maskf != 0);
                        }
        
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0)
                            return _num_buckets;
                        else if (0 == get_offset(main_bucket))
                            return _num_buckets;
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                    }
        
                    while (true) {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
                        if (maskf) {
                            prefetch_heap_block((char*)&_pairs[next_bucket]);
                            do {
                                const auto fbucket = next_bucket + CTZ(maskf);
                                if (EMH_LIKELY(_eq(_pairs[fbucket].first, key)))
                                    return fbucket;
                                maskf &= maskf - 1;
                            } while (maskf != 0);
                        }
            #if 0
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0)
                            break;
            #endif
                        if (++offset > get_offset(main_bucket))
                            break;
                        next_bucket = get_next_bucket(next_bucket, offset);
                    }
        
                    return _num_buckets;
                }
        
                // Find the main_bucket with this key, or return a good empty main_bucket to place the key in.
                // In the later case, the main_bucket is expected to be filled.
                template<typename K>
                size_t find_or_allocate(const K& key, bool& bnew) noexcept
                {
                    reserve(_num_filled);
        
                    size_t main_bucket;
                    const auto key_h2 = hash_key2(main_bucket, key);
                    prefetch_heap_block((char*)&_pairs[main_bucket]);
                    const auto filled = SET1_EPI8(key_h2);
                    auto next_bucket = main_bucket, offset = 0u;
                    constexpr size_t chole = (size_t)-1;
                    size_t hole = chole;
        
                    while (true) {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf  = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
        
                        //1. find filled
                        while (maskf != 0) {
                            const auto fbucket = next_bucket + CTZ(maskf);
                            if (_eq(_pairs[fbucket].first, key)) {
                                bnew = false;
                                return fbucket;
                            }
                            maskf &= maskf - 1;
                        }
        
                        //2. find empty
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0) {
                            auto ebucket = next_bucket + CTZ(maske);
                            if (EMH_UNLIKELY(hole != chole))
                                ebucket = hole;
                            set_states(ebucket, key_h2);
                            return ebucket;
                        }
        
                        //3. find erased
                        else if (hole == chole) {
                            const auto maskd = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_delete));
                            if (maskd != 0)
                                hole = next_bucket + CTZ(maskd);
                        }
        
                        //4. next round
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                        if (offset > get_offset(main_bucket))
                            break;
                    }
        
                    if (hole != chole) {
                        //set_offset(main_bucket, offset - 1);
                        set_states(hole, key_h2);
                        return hole;
                    }
        
                    const auto ebucket = find_empty_slot(main_bucket, next_bucket, offset);
                    set_states(ebucket, key_h2);
                    return ebucket;
                }
        
                inline uint32_t empty_delete(size_t gbucket) const noexcept
                {
                    const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[gbucket]));
                    return MOVEMASK_EPI8(CMPGT_EPI8(simd_filled, vec));
                }
        
                inline uint32_t filled_mask(size_t gbucket) const noexcept
                {
            #if EMH_ITERATOR_BITS == 160
                    const auto vec = _mm_slli_epi16(_mm_loadu_si128((__m128i const*) & _states[gbucket]), 7);
                    return (uint32_t)~_mm_movemask_epi8(vec) & 0xFFFF;
            #elif EMH_ITERATOR_BITS == 320
                    const auto vec = _mm256_slli_epi32(_mm256_loadu_si256((__m256i const*) & _states[gbucket]), 7);
                    return (uint32_t)~_mm256_movemask_epi8(vec);
            #else
                    const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[gbucket]));
                    return MOVEMASK_EPI8(CMPGT_EPI8(vec, simd_delete));
            #endif
                }
        
            #if EMH_PSL
                //unlike robin hood, only move large offset once
                size_t update_offset(size_t gbucket, size_t new_bucket, size_t offset) noexcept
                {
                    const auto kdiff  = offset / 2;
                    const auto kprobe = offset - kdiff;
                    const auto kgroup = (new_bucket - kdiff * simd_bytes) & _mask;
                    for (int i = 0; i < 8 * simd_bytes; i++) {
                        const auto kbucket = (kgroup + i) & _mask;
                        if (_offset[kbucket] == 0)
                            continue;
        
                        size_t kmain_bucket;
                        hash_key2(kmain_bucket, _pairs[kbucket].first);
                        if (kmain_bucket != kbucket)
                            continue;
        
                        //move kbucket to new_bucket, update offset
                        set_states(new_bucket, _states[kbucket]);
                        new(_pairs + new_bucket) PairT(std::move(_pairs[kbucket]));
                        _pairs[kbucket].~PairT();
        
                        if (kdiff + 1 - i / simd_bytes > get_offset(kmain_bucket))
                            set_offset(kmain_bucket, kdiff + 1 - i / simd_bytes);
                        if (kprobe + i / simd_bytes >= get_offset(gbucket))
                            set_offset(gbucket, kprobe + 1 + i / simd_bytes);
                        return kbucket;
                    }
        
                    return -1;
                }
            #endif
        
                size_t find_empty_slot(size_t main_bucket, size_t next_bucket, size_t offset) noexcept
                {
                    while (true) {
                        const auto maske = empty_delete(next_bucket);
                        if (maske != 0) {
                            const auto ebucket = CTZ(maske) + next_bucket;
                            prefetch_heap_block((char*)&_pairs[ebucket]);
                            if (offset > get_offset(main_bucket))
                                set_offset(main_bucket, offset);
                            return ebucket;
                        }
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                    }
        
                    return 0;
                }
        
                size_t find_first_slot(size_t next_bucket) const noexcept
                {
                    while (true) {
                        const auto maske = filled_mask(next_bucket);
                        if (maske != 0)
                            return next_bucket + CTZ(maske);
                        next_bucket += simd_bytes;
                    }
                    return 0;
                }
        
            private:
        
                HashT   _hasher;
                EqT     _eq;
                int8_t* _states           = nullptr;
                uint8_t*_offset           = nullptr;
                PairT*  _pairs            = nullptr;
                size_t  _num_buckets      = 0;
                size_t  _mask             = 0; // _num_buckets minus one
                size_t  _num_filled       = 0;
            };
        
            // } // namespace emilib
            #undef LOAD_UEPI8
            """
            cdef cppclass UnsignedDoubleHashMapIterator "HashMap<unsigned, double>::iterator":
                UnsignedDoubleHashMapIterator() noexcept nogil
                pair[unsigned, double]& operator*() noexcept nogil const
                UnsignedDoubleHashMapIterator operator++() noexcept nogil
                UnsignedDoubleHashMapIterator operator++(int) noexcept nogil
                bint operator==(const UnsignedDoubleHashMapIterator&) noexcept nogil const
                bint operator!=(const UnsignedDoubleHashMapIterator&) noexcept nogil const
        
            cdef cppclass UnsignedDoubleHashMap "HashMap<unsigned, double>":
                UnsignedDoubleHashMap() noexcept nogil
                UnsignedDoubleHashMap(size_t n) noexcept nogil
                UnsignedDoubleHashMapIterator begin() noexcept nogil const
                UnsignedDoubleHashMapIterator end() noexcept nogil const
                size_t erase(const unsigned& key) noexcept nogil
                void clear() noexcept nogil
                bint empty() noexcept nogil const
                size_t size() noexcept nogil const
                size_t find_first_slot(size_t next_bucket) noexcept nogil const
                double& operator[](const unsigned& key) noexcept nogil
        
        def paris(const numeric[::1] data,
                  const signed_integer[::1] indices,
                  const signed_integer[::1] indptr,
                  unsigned[::1] left_nodes,
                  unsigned[::1] right_nodes,
                  double[::1] distances,
                  const unsigned long num_nodes,
                  const unsigned num_threads):
            
            cdef unsigned a, b, c, u, v, start_node, num_merges = 0, \
                n = num_nodes, num_dendrogram_nodes = 2 * num_nodes - 1
            cdef unsigned long i
            cdef double weight, d, dmin, wtot = 0, inv_wtot
            cdef pair[unsigned, double] neighbor
            
            # SNN graph
            cdef vector[UnsignedDoubleHashMap] graph = \
                vector[UnsignedDoubleHashMap](num_dendrogram_nodes)
            # node weights
            cdef vector[double] w
            # current (merged) node index for each leaf node
            cdef vector[unsigned] node_map
            # nearest-neighbor chain
            cdef vector[unsigned] chain
            # connected components
            cdef vector[unsigned] cc
            
            # Build graph and calculate weights
            
            w.resize(num_dendrogram_nodes)
            node_map.resize(num_nodes)
            if num_threads == 1:
                for u in range(num_nodes):
                    for i in range(<unsigned long> indptr[u],
                                   <unsigned long> indptr[u + 1]):
                        v = indices[i]
                        if u != v:
                            weight = data[i]
                            graph[u][v] = weight
                            w[u] += weight
                    node_map[u] = u
                    wtot += w[u]
            else:
                for u in prange(num_nodes, num_threads=num_threads, nogil=True):
                    for i in range(<unsigned long> indptr[u],
                                   <unsigned long> indptr[u + 1]):
                        v = indices[i]
                        if u != v:
                            weight = data[i]
                            graph[u][v] = weight
                            w[u] = w[u] + weight
                for u in range(num_nodes):
                    node_map[u] = u
                    wtot += w[u]
            inv_wtot = 1 / wtot
            
            # Cluster
            
            u = num_nodes
            chain.reserve(64)
            start_node = 0
            while n > 0:
                PyErr_CheckSignals()
                
                # Pick an arbitrary (non-empty) node, and add it to the chain
                
                while True:
                    if node_map[start_node] != UINT_MAX:
                        chain.push_back(start_node)
                        break
                    start_node += 1
                while not chain.empty():
                    # Pop the last node, `a`, from the chain
                    
                    a = chain.back()
                    chain.pop_back()
                    
                    # Find `a`'s nearest neighbor, `b`
                    
                    dmin = INFINITY
                    for neighbor in graph[a]:
                        v = neighbor.first
                        weight = neighbor.second
                        d = w[v] / weight
                        if d < dmin:
                            b = v
                            dmin = d
                        elif d == dmin and v < b:
                            b = v
                    
                    # If the chain is still not empty after popping `a`...
                    
                    if not chain.empty():
                        # Pop the second-last node, `c`, from the chain
                        
                        c = chain.back()
                        chain.pop_back()
                        if b == c:
                            # `a`'s nearest neighbor `b` was the second-last
                            # node in the chain, meaning that `a` and `b` are
                            # mutual nearest neighbors. Merge `a` and `b` (or
                            # more specifically, `node_map[a]` and
                            # `node_map[b]`).
                            
                            left_nodes[num_merges] = node_map[a]
                            right_nodes[num_merges] = node_map[b]
                            distances[num_merges] = dmin * w[a] * inv_wtot
                            num_merges += 1
                            
                            # Update graph
                            
                            if graph[a].size() < graph[b].size():
                                # Merge `a` into `b`, since `a` has fewer
                                # neighbors than `b` so this should be faster
                                # than merging `b` into `a`. For each neighbor
                                # `v` of `a`, assign `graph[a][v]`'s weight to
                                # `graph[b][v]` and `graph[v][b]`, or add its
                                # weight if `graph[b][v]` and `graph[b][v]`
                                # already exist. Then, delete `graph[a][v]` (by
                                # calling `graph[a].clear()` at the end) and
                                # `graph[v][a]`. To avoid creating self-loops,
                                # delete `graph[a][b]` and `graph[b][a]` at the
                                # start.
                                
                                graph[a].erase(b)
                                graph[b].erase(a)
                                for neighbor in graph[a]:
                                    v = neighbor.first
                                    weight = neighbor.second
                                    graph[b][v] += weight
                                    graph[v][b] += weight
                                    graph[v].erase(a)
                                graph[a].clear()
                                
                                # Set `node_map[a]` to an invalid value (`a`
                                # will never be used again), and update
                                # `node_map[b]` to `u`, the index of the merged
                                # node we just created
                                
                                node_map[a] = UINT_MAX
                                node_map[b] = u
                                
                                # Update the weight of `b` to include the
                                # contribution from `a`
                                
                                w[b] += w[a]
                            else:
                                # Merge `b` into `a`: the reverse of the code
                                # above
                                
                                graph[b].erase(a)
                                graph[a].erase(b)
                                for neighbor in graph[b]:
                                    v = neighbor.first
                                    weight = neighbor.second
                                    graph[a][v] += weight
                                    graph[v][a] += weight
                                    graph[v].erase(b)
                                graph[b].clear()
                                node_map[b] = UINT_MAX
                                node_map[a] = u
                                w[a] += w[b]
                            n -= 1
                            u += 1
                        else:
                            # `a`'s nearest neighbor is `b`, but `b`'s nearest
                            # neighbor is another node that's not `a`. Put
                            # `c` and `a` back into the chain, and also add
                            # `b` so we can find which node is its nearest
                            # neighbor, and continue the chain until we find a
                            # pair of nodes that are mutual nearest neighbors.
                            
                            chain.push_back(c)
                            chain.push_back(a)
                            chain.push_back(b)
                    elif not isinf(dmin):
                        # `a` was the first node we added to the chain. Add it
                        # and its nearest neighbor `b` to the chain, and keep
                        # going.
                        
                        chain.push_back(a)
                        chain.push_back(b)
                    else:
                        # `a` has no neighbors, meaning that it comprises an
                        # entire connected component. Remove this connected
                        # component from the graph and store it in `cc`.
                        
                        cc.push_back(node_map[a])
                        for neighbor in graph[a]:
                            graph[neighbor.first].erase(a)
                        graph[a].clear()
                        node_map[a] = UINT_MAX
                        n -= 1

            # Add connected components to the dendrogram, with a distance of
            # infinity from each other
            
            if not cc.empty():
                a = cc.back()
                cc.pop_back()
                for b in cc:
                    left_nodes[num_merges] = a
                    right_nodes[num_merges] = b
                    distances[num_merges] = INFINITY
                    num_merges += 1
                    a = u
                    u += 1
        ''')['paris'](
            data=snn_graph.data, indices=snn_graph.indices,
            indptr=snn_graph.indptr, left_nodes=left_nodes,
            right_nodes=right_nodes, distances=distances, num_nodes=num_nodes,
            num_threads=num_threads)
        cell_type_labels = (lambda a, b, c: pl.Series(
            np.zeros(len(self._obs if QC_column is None else QC_column))))(
            left_nodes, right_nodes, distances)  # not currently using `verbose`
        if QC_column is not None:
            # Back-project from QCed cells to all cells, filling with `null`
            cell_type_labels = pl.when(QC_column)\
                .then(cell_type_labels
                      .gather(QC_column.cum_sum().cast(pl.Int32) - 1))
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(cell_type_labels),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns)
    
    def harmonize(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  max_harmony_iterations: int | np.integer = 10,
                  max_clustering_iterations: int | np.integer | None = 20,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tol_harmony: int | float | np.integer | np.floating = 1e-4,
                  tol_clustering: int | float | np.integer |
                                  np.floating = 1e-5,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  theta: int | float | np.integer | np.floating = 2,
                  tau: int | float | np.integer | np.floating = 0,
                  seed: int | np.integer | None = None,
                  overwrite: bool = False,
                  verbose: bool = True) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0). Harmony was originally written
        in R (github.com/immunogenomics/harmony) but has two Python ports,
        harmony-pytorch (github.com/lilab-bcb/harmony-pytorch), which our
        implementation is based on, and harmonypy
        (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm.
                          If not specified, take the minimum of 100 and
                          floor(number of cells / 30).
            max_harmony_iterations: the maximum number of iterations to run Harmony
                              for, if convergence is not achieved. Defaults to
                              10, like the original harmony package,
                              harmony-pytorch, and harmonypy. Set to `None` to
                              use as many iterations as necessary to achieve
                              convergence.
            max_clustering_iterations: the maximum number of iterations to run the
                                 clustering step within each Harmony iteration
                                 for, if convergence is not achieved. Defaults
                                 to 20 iterations, like the original harmony
                                 package and harmonypy; this differs from
                                 the default of 200 iterations used by
                                 harmony-pytorch. Set to `None` to use as many
                                 iterations as necessary to achieve
                                 convergence.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tol_harmony: the relative tolerance used to determine whether to
                         stop Harmony before `max_harmony_iterations` iterations;
                         must be positive
            tol_clustering: the relative tolerance used to determine whether to
                            stop clustering before `max_clustering_iterations`
                            iterations; must be positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            theta: the weight of the diversity penalty term in the Harmony
                   objective function; must be non-negative
            tau: the discounting factor on theta; must be non-negative. By
                 default, `tau = 0`, so there is no discounting.
            seed: the random seed for Harmony, or leave unset to use
                  `single_cell.options()['seed']` as the seed (0 by default)
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        with ignore_sigint():
            import faiss
        # Check `others`
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `Harmony_key` is a string and, unless `overwrite=True`,
        # not already in `obsm` for any dataset
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Check that `num_clusters`, `max_harmony_iterations`, and
        # `max_clustering_iterations` are `None` or a positive integer; if either max
        # iter argument is `None`, set it to `INT32_MAX`
        for parameter, parameter_name in (
                (num_clusters, 'num_clusters'),
                (max_harmony_iterations, 'max_harmony_iterations'),
                (max_clustering_iterations, 'max_clustering_iterations')):
            if parameter is not None:
                check_type(parameter, parameter_name, int,
                           'a positive integer')
                check_bounds(parameter, parameter_name, 1)
        if max_harmony_iterations is None:
            max_harmony_iterations = 2147483647
        if max_clustering_iterations is None:
            max_clustering_iterations = 2147483647
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        # Check that `tol_harmony` and `tol_clustering` are positive numbers,
        # and that `ridge_lambda`, `sigma`, `theta`, and `tau` are non-negative
        # numbers. If any is an integer, cast it to a float.
        for parameter, parameter_name in (
                (tol_harmony, 'tol_harmony'),
                (tol_clustering, 'tol_clustering')):
            check_type(parameter, parameter_name, (int, float),
                       'a positive number')
            check_bounds(parameter, parameter_name, 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma'),
                (theta, 'theta'), (tau, 'tau')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tol_harmony = float(tol_harmony)
        tol_clustering = float(tol_clustering)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        theta = float(theta)
        tau = float(tau)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Concatenate PCs (`Z`) across datasets; get labels indicating which
        # rows of these concatenated PCs come from each dataset or batch.
        # Check that the PCs are float64 and C-contiguous.
        Z = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in Z:
            dtype = PCs.dtype
            if dtype != float:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float64')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
                raise ValueError(error_message)
        if QC_column is not None:
            Z = [PCs[QCed] if QCed is not None else PCs
                 for PCs, QCed in zip(Z, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, Z)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum()))
                    batch_index += 1
            batch_labels = np.concatenate(batch_labels)
        Z = np.concatenate(Z)
        
        # Run Harmony
        
        cython_functions = cython_inline(r'''
            from cpython.exc cimport PyErr_CheckSignals
            from libcpp.cmath cimport abs, exp, pow, log, sqrt
            from scipy.linalg.cython_blas cimport dgemm, dgemv
            
            cdef inline void matrix_multiply(const double[:, ::1] A,
                                             const double[:, ::1] B,
                                             double[:, ::1] C,
                                             const bint transpose_A,
                                             const bint transpose_B,
                                             const double alpha,
                                             const double beta) noexcept nogil:
                # Flip `A` <-> `B` and `shape[0]` <-> `shape[1]` since our
                # matrices are C-major, whereas BLAS expects Fortran-major
                
                cdef int m, n, k, lda, ldb
                cdef char transA, transB
                if transpose_B:
                    m = B.shape[0]
                    k = B.shape[1]
                    lda = k
                    transA = b'T'
                else:
                    m = B.shape[1]
                    k = B.shape[0]
                    lda = m
                    transA = b'N'
                if transpose_A:
                    n = A.shape[1]
                    ldb = n
                    transB = b'T'
                else:
                    n = A.shape[0]
                    ldb = k
                    transB = b'N'
                dgemm(&transA, &transB, &m, &n, &k, <double*> &alpha,
                      <double*> &B[0, 0], &lda, <double*> &A[0, 0], &ldb,
                      <double*> &beta, &C[0, 0], &m)
            
            cdef inline void matrix_vector_multiply(
                    const double[:, ::1] A,
                    const double[::1] X,
                    double[::1] Y,
                    const bint transpose,
                    const double alpha,
                    const double beta) noexcept nogil:
                # Flip `trans` since our matrix is C-major, whereas BLAS
                # expects Fortran-major
                
                cdef int m = A.shape[1], n = A.shape[0], incx = 1, incy = 1
                cdef char trans = b'N' if transpose else b'T'
                dgemv(&trans, &m, &n, <double*> &alpha, <double*> &A[0,0],
                      &m, <double*> &X[0], &incx, <double*> &beta, &Y[0],
                      &incy)
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            cdef inline void shuffle_array(unsigned[::1] arr, unsigned long* state) noexcept nogil:
                cdef unsigned i, j, temp
                for i in range(arr.shape[0] - 1, 0, -1):
                    j = randint(i + 1, state)
                    temp = arr[i]
                    arr[i] = arr[j]
                    arr[j] = temp
            
            cdef inline double compute_objective(
                    double[:, ::1] Z_norm_times_Y_norm,
                    const double[:, ::1] R,
                    const double[:, ::1] E,
                    const double[:, ::1] O,
                    double[:, ::1] ratio,
                    const double[::1] theta,
                    double[::1] theta_times_ratio,
                    const double sigma,
                    const unsigned num_cells,
                    const unsigned num_clusters,
                    const unsigned num_batches) noexcept nogil:
                cdef unsigned i, j
                cdef double kmeans_error, entropy_term, diversity_penalty
                kmeans_error = entropy_term = diversity_penalty = 0
                for i in range(num_cells):
                    for j in range(num_clusters):
                        kmeans_error += \
                            R[i, j] * (1 - Z_norm_times_Y_norm[i, j])
                        entropy_term += R[i, j] * log(R[i, j])
                kmeans_error *= 2
                entropy_term *= sigma
                for i in range(num_batches):
                    for j in range(num_clusters):
                        ratio[i, j] = O[i, j] * log(
                            (O[i, j] + 1) / (E[i, j] + 1))
                matrix_vector_multiply(ratio, theta, theta_times_ratio,
                                       transpose=True, alpha=1, beta=0)
                for i in range(num_clusters):
                    diversity_penalty += theta_times_ratio[i]
                diversity_penalty *= sigma
                return kmeans_error + entropy_term + diversity_penalty
            
            def initialize(const double[:, ::1] Z_norm,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const unsigned[::1] N_b,
                           double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[::1] R_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           const double sigma,
                           double[:, ::1] ratio,
                           double[::1] theta,
                           double[::1] theta_times_ratio,
                           const double tau):
                cdef unsigned i, j, batch_label, num_cells = Z_norm.shape[0], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef double norm, objective, base, two_over_sigma = 2 / sigma
                
                # Initialize `Pr_b`
                
                for i in range(num_batches):
                    Pr_b[i] = <double> N_b[i] / num_cells
                
                # Initialize `R` (and `R_sum`) and `O`
                
                matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                transpose_A=False, transpose_B=True, alpha=1,
                                beta=0)
                for i in range(num_cells):
                    batch_label = batch_labels[i]
                    norm = 0
                    for j in range(num_clusters):
                        R[i, j] = exp(two_over_sigma *
                                      (Z_norm_times_Y_norm[i, j] - 1))
                        norm += R[i, j]
                    norm = 1 / norm
                    for j in range(num_clusters):
                        R[i, j] *= norm
                        O[batch_label, j] += R[i, j]
                        R_sum[j] += R[i, j]
                
                # Initialize `E`
                
                for i in range(num_batches):
                    for j in range(num_clusters):
                        E[i, j] = Pr_b[i] * R_sum[j]
                
                # Apply discounting to `theta`, if specified
                
                if tau > 0:
                    for i in range(num_batches):
                        base = exp(-N_b[i] / (num_clusters * tau))
                        theta[i] = theta[i] * (1 - base * base)
                
                # Compute and return the initial value of the objective
                # function
                
                objective = compute_objective(
                    Z_norm_times_Y_norm, R, E, O, ratio, theta,
                    theta_times_ratio, sigma, num_cells, num_clusters,
                    num_batches)
                
                return objective
            
            def clustering(const double[:, ::1] Z_norm,
                           double[:, ::1] Z_norm_in,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[:, ::1] R_in,
                           double[::1] R_in_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           double[:, ::1] ratio,
                           const double[::1] theta,
                           double[::1] theta_times_ratio,
                           unsigned[::1] idx_list,
                           const double tol,
                           const unsigned max_iter,
                           const double sigma,
                           const unsigned block_size):
                cdef unsigned i, j, k, iter, num_cells_in_block, pos, batch_label, \
                    num_cells = Z_norm.shape[0], num_PCs = Z_norm.shape[1], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef unsigned long state
                cdef double norm, old, new, objective, \
                    two_over_sigma = 2 / sigma
                cdef double exp_neg_two_over_sigma = exp(-two_over_sigma)
                cdef double[:, ::1] Z_norm_block, R_block
                cdef double past_clustering_objectives[3]
                
                for iter in range(max_iter):
                    # Compute cluster centroids
                    
                    matrix_multiply(R, Z_norm, Y_norm, transpose_A=True,
                                    transpose_B=False, alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm = norm + Y_norm[i, j] * Y_norm[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y_norm[i, j] = Y_norm[i, j] * norm
                    for i in range(num_cells):
                        idx_list[i] = i
                    state = srand(iter)
                    shuffle_array(idx_list, &state)
                    
                    # Update cells blockwise
                    
                    pos = 0
                    Z_norm_block = Z_norm_in
                    R_block = R_in
                    while pos < num_cells:
                        if pos + block_size > num_cells:
                            num_cells_in_block = num_cells - pos
                            Z_norm_block = Z_norm_block[:num_cells_in_block]
                            R_block = R_block[:num_cells_in_block]
                        else:
                            num_cells_in_block = block_size
                        
                        # Remove the cells in this block from `E` and `O`
                        
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[pos + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_in_sum[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_norm_block[i, j] = Z_norm[k, j]
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] -= Pr_b[i] * R_in_sum[j]
                        
                        # Recompute `R` for the removed cells
                        # Note: the original formula is
                        # `exp(-2 / sigma * (1 - Z_norm_block @ Y_norm.T))`,
                        # which expands to `exp(-2 / sigma) *
                        # exp(2 / sigma * Z_norm_block @ Y_norm.T))`. Since
                        # `exp(-2 / sigma)` is a constant, we fold it into
                        # `ratio`.
                        
                        matrix_multiply(Z_norm_block, Y_norm, R_block,
                                        transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    pow((E[i, j] + 1) / (O[i, j] + 1),
                                        theta[i])
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[i + pos]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                R[k, j] = exp(R_block[i, j]) * \
                                          ratio[batch_label, j]
                                norm += R[k, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[k, j] *= norm
                                R_in_sum[j] += R[k, j]
                                
                                # Add the removed cells back into `O`
                                
                                O[batch_label, j] += R[k, j]
                        
                        # Add the removed cells back into `E`
                        
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] += Pr_b[i] * R_in_sum[j]
                        
                        # Move to the next block
                        
                        pos += block_size
                    
                    # Compute the objective and decide whether we've converged
                    
                    matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                    transpose_A=False, transpose_B=True,
                                    alpha=1, beta=0)
                    objective = compute_objective(
                        Z_norm_times_Y_norm, R, E, O, ratio, theta,
                        theta_times_ratio, sigma, num_cells, num_clusters,
                        num_batches)
                    if iter < 3:
                        past_clustering_objectives[iter] = objective
                    else:
                        old = past_clustering_objectives[0] + \
                            past_clustering_objectives[1] + \
                            past_clustering_objectives[2]
                        new = past_clustering_objectives[1] + \
                            past_clustering_objectives[2] + objective
                        if old - new < tol * abs(old):
                            break
                        else:
                            past_clustering_objectives[0] = \
                                past_clustering_objectives[1]
                            past_clustering_objectives[1] = \
                                past_clustering_objectives[2]
                            past_clustering_objectives[2] = objective
                    PyErr_CheckSignals()
                return objective
            
            def correction(const double[:, ::1] Z,
                           double[:, ::1] Z_hat,
                           const double[:, ::1] R,
                           const double[:, ::1] O,
                           const double ridge_lambda,
                           const unsigned[::1] batch_labels,
                           double[::1] factor,
                           double[:, ::1] P,
                           double[:, ::1] P_t_B_inv,
                           double[:, ::1] inv_mat,
                           double[:, ::1] Phi_t_diag_R_by_X,
                           double[:, ::1] W):
                cdef unsigned i, j, k, batch_label, num_cells = Z.shape[0], \
                    num_PCs = Z.shape[1], num_batches = O.shape[0], \
                    num_clusters = O.shape[1]
                cdef double c, c_inv
                
                Z_hat[:] = Z[:]
                
                # Initialize `P` to the identity matrix
                
                P[:] = 0
                for i in range(num_batches + 1):
                    P[i, i] = 1
            
                for k in range(num_clusters):
                    # Compute `factor`, `c_inv` and `P`
                    
                    c = 0
                    for i in range(num_batches):
                        factor[i] = 1 / (O[i, k] + ridge_lambda)
                        c += O[i, k] * (1 - factor[i] * O[i, k])
                        P[0, i + 1] = -factor[i] * O[i, k]
                    c_inv = 1 / c
                    
                    # Compute `P_t_B_inv`
                    
                    P_t_B_inv[:] = 0
                    P_t_B_inv[0, 0] = c_inv
                    for i in range(1, num_batches + 1):
                        P_t_B_inv[i, i] = factor[i - 1]
                        P_t_B_inv[i, 0] = P[0, i] * c_inv
                    
                    # Compute `inv_mat`
                    
                    matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                    transpose_B=False, alpha=1, beta=0)
                    
                    # Compute `Phi_t_diag_R @ X`
                    
                    Phi_t_diag_R_by_X[:] = 0
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Phi_t_diag_R_by_X[0, j] += Z[i, j] * R[i, k]
                            Phi_t_diag_R_by_X[batch_label + 1, j] += \
                                Z[i, j] * R[i, k]
                            
                    # Compute `W`
                    
                    matrix_multiply(inv_mat, Phi_t_diag_R_by_X, W,
                                    transpose_A=False, transpose_B=False,
                                    alpha=1, beta=0)
                    
                    # Update `Z_hat`
                    
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Z_hat[i, j] = Z_hat[i, j] - \
                                W[batch_label + 1, j] * R[i, k]
                        
            def normalize_rows(const double[:, ::1] arr, double[:, ::1] out):
                cdef unsigned i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

            def normalize_rows_inplace(double[:, ::1] arr):
                cdef unsigned i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        arr[i, j] = arr[i, j] * norm
        ''')
        initialize = cython_functions['initialize']
        clustering = cython_functions['clustering']
        correction = cython_functions['correction']
        normalize_rows = cython_functions['normalize_rows']
        normalize_rows_inplace = cython_functions['normalize_rows_inplace']
        
        # Get dimensions of everything
        num_cells, num_PCs = Z.shape
        block_size = int(num_cells * block_proportion)
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        N_b = bincount(batch_labels, num_bins=batch_labels[-1] + 1,
                       num_threads=1)
        num_batches = len(N_b)
        
        # Allocate arrays
        Z_norm = np.empty((num_cells, num_PCs))
        Z_norm_in = np.empty((block_size, num_PCs))
        Z_norm_times_Y_norm = np.empty((num_cells, num_clusters))
        Pr_b = np.empty(num_batches)
        R = np.empty((num_cells, num_clusters))
        R_in = np.empty((block_size, num_clusters))
        R_in_sum = np.zeros(num_clusters)
        E = np.empty((num_batches, num_clusters))
        O = np.zeros((num_batches, num_clusters))
        ratio = np.empty((num_batches, num_clusters))
        theta = np.repeat(theta, num_batches)
        theta_times_ratio = np.empty(num_clusters)
        idx_list = np.empty(num_cells, dtype=np.uint32)
        factor = np.empty(num_cells)
        P = np.empty((num_batches + 1, num_batches + 1))
        P_t_B_inv = np.empty((num_batches + 1, num_batches + 1))
        inv_mat = np.empty((num_batches + 1, num_batches + 1))
        Phi_t_diag_R_by_X = np.empty((num_batches + 1, num_PCs))
        W = np.empty((num_batches + 1, num_PCs))
        
        # Run k-means
        normalize_rows(Z, Z_norm)
        kmeans = faiss.Kmeans(num_PCs, num_clusters, seed=seed)
        kmeans.train(Z_norm)
        Y_norm = kmeans.centroids.astype(float)
        normalize_rows_inplace(Y_norm)
        
        # Complete initialization in Cython
        objective = initialize(
            Z_norm=Z_norm, Y_norm=Y_norm,
            Z_norm_times_Y_norm=Z_norm_times_Y_norm, N_b=N_b, Pr_b=Pr_b,
            batch_labels=batch_labels, R=R, R_sum=R_in_sum, E=E, O=O,
            sigma=sigma, ratio=ratio, theta=theta,
            theta_times_ratio=theta_times_ratio, tau=tau)
        
        if verbose:
            print(f'Initialization is complete: objective = {objective:.2f}')
        
        iteration_string = plural('iteration', max_harmony_iterations)
        
        for i in range(1, max_harmony_iterations + 1):
            prev_objective = objective
            objective = clustering(
                Z_norm=Z_norm, Z_norm_in=Z_norm_in, Y_norm=Y_norm,
                Z_norm_times_Y_norm=Z_norm_times_Y_norm, Pr_b=Pr_b,
                batch_labels=batch_labels, R=R, R_in=R_in, R_in_sum=R_in_sum,
                E=E, O=O, ratio=ratio, theta=theta,
                theta_times_ratio=theta_times_ratio, idx_list=idx_list,
                tol=tol_clustering, max_iter=max_clustering_iterations, sigma=sigma,
                block_size=block_size)
            correction(Z=Z, Z_hat=Z_norm, R=R, O=O, ridge_lambda=ridge_lambda,
                       batch_labels=batch_labels, factor=factor, P=P,
                       P_t_B_inv=P_t_B_inv, inv_mat=inv_mat,
                       Phi_t_diag_R_by_X=Phi_t_diag_R_by_X, W=W)
            
            if verbose:
                print(f'Completed {i} of {max_harmony_iterations} '
                      f'{iteration_string}: objective = {objective:.2f}')
            
            if prev_objective - objective < tol_harmony * abs(prev_objective):
                if verbose:
                    print(f'Reached convergence after {i} {iteration_string}')
                break
            
            if i == max_harmony_iterations:
                if verbose:
                    print(f'Failed to converge after {i} {iteration_string}')
                break
            
            normalize_rows_inplace(Z_norm)
        
        del batch_labels, Z, Pr_b, theta, R, E, O, Z_norm_in, Y_norm, \
            Z_norm_times_Y_norm, R_in, R_in_sum, ratio, theta_times_ratio, \
            idx_list, factor, P, P_t_B_inv, inv_mat, Phi_t_diag_R_by_X, W
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z_norm[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns)
        return tuple(datasets) if others else datasets[0]
    
    def harmonize_new(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  num_init_iterations: int | np.integer = 5,
                  oversampling_factor: int | np.integer | float |
                                       np.floating = 1,
                  num_kmeans_iterations: int | np.integer = 25,
                  max_harmony_iterations: int | np.integer = 10,
                  max_clustering_iterations: int | np.integer | None = 20,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tol_harmony: int | float | np.integer | np.floating = 1e-4,
                  tol_clustering: int | float | np.integer |
                                  np.floating = 1e-5,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  theta: int | float | np.integer | np.floating = 2,
                  tau: int | float | np.integer | np.floating = 0,
                  seed: int | np.integer | None = None,
                  random_init: bool = False,  # TODO
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0). Harmony was originally written
        in R (github.com/immunogenomics/harmony) but has two Python ports,
        harmony-pytorch (github.com/lilab-bcb/harmony-pytorch), which our
        implementation is based on, and harmonypy
        (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm,
                          including in the initial k-means clustering. If not
                          specified, take the minimum of 100 and
                          floor(number of cells / 30).
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402).
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++.
            num_kmeans_iterations: the number of iterations of k-means
                                   clustering to run, as the first step of
                                   Harmony. Defaults to 25, like the original
                                   Harmony R package, harmony-pytorch, and
                                   harmonypy. However, unlike these packages,
                                   only one initialization is tried rather than
                                   10 to reduce runtime.
            max_harmony_iterations: the maximum number of iterations to run
                                    Harmony for, if convergence is not
                                    achieved. Defaults to 10, like the original
                                    Harmony R package, harmony-pytorch, and
                                    harmonypy. Set to `None` to use as many
                                    iterations as necessary to achieve
                                    convergence.
            max_clustering_iterations: the maximum number of iterations to run
                                       the clustering step within each Harmony
                                       iteration for, if convergence is not
                                       achieved. Defaults to 20 iterations,
                                       like the original harmony R package and
                                       harmonypy; this differs from the default
                                       of 200 iterations used by
                                       harmony-pytorch. Set to `None` to use as
                                       many iterations as necessary to achieve
                                       convergence.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tol_harmony: the relative tolerance used to determine whether to
                         stop Harmony before `max_harmony_iterations`
                         iterations; must be positive
            tol_clustering: the relative tolerance used to determine whether to
                            stop clustering before `max_clustering_iterations`
                            iterations; must be positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            theta: the weight of the diversity penalty term in the Harmony
                   objective function; must be non-negative
            tau: the discounting factor on theta; must be non-negative. By
                 default, `tau = 0`, so there is no discounting.
            seed: the random seed to use for the initial k-means clustering, or
                  leave unset to use `single_cell.options()['seed']` as the
                  seed (0 by default)
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
            num_threads: the number of threads to use for the initial k-means
                         clustering and for the matrix and matrix-vector
                         multiplications within Harmony. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        # Check `others`
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `Harmony_key` is a string and, unless `overwrite=True`,
        # not already in `obsm` for any dataset
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Check that `num_clusters`, `max_harmony_iterations`, and
        # `max_clustering_iterations` are `None` or a positive integer; if
        # either of the latter two arguments is `None`, set it to `INT32_MAX`
        for parameter, parameter_name in (
                (num_clusters, 'num_clusters'),
                (max_harmony_iterations, 'max_harmony_iterations'),
                (max_clustering_iterations, 'max_clustering_iterations')):
            if parameter is not None:
                check_type(parameter, parameter_name, int,
                           'a positive integer')
                check_bounds(parameter, parameter_name, 1)
        if max_harmony_iterations is None:
            max_harmony_iterations = 2147483647
        if max_clustering_iterations is None:
            max_clustering_iterations = 2147483647
        # Check that `num_init_iterations` and `num_kmeans_iterations` are
        # positive integers
        for parameter, parameter_name in (
                (num_init_iterations, 'num_init_iterations'),
                (num_kmeans_iterations, 'num_kmeans_iterations')):
            check_type(parameter, parameter_name, int, 'a positive integer')
            check_bounds(parameter, parameter_name, 1)
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        # Check that `tol_harmony` and `tol_clustering` are positive numbers,
        # and that `ridge_lambda`, `sigma`, `theta`, and `tau` are non-negative
        # numbers. If any is an integer, cast it to a float.
        for parameter, parameter_name in (
                (tol_harmony, 'tol_harmony'),
                (tol_clustering, 'tol_clustering')):
            check_type(parameter, parameter_name, (int, float),
                       'a positive number')
            check_bounds(parameter, parameter_name, 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma'),
                (theta, 'theta'), (tau, 'tau')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tol_harmony = float(tol_harmony)
        tol_clustering = float(tol_clustering)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        theta = float(theta)
        tau = float(tau)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Concatenate PCs (`Z`) across datasets; get labels indicating which
        # rows of these concatenated PCs come from each dataset or batch.
        # Check that the PCs are float64 and C-contiguous.
        Z = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in Z:
            dtype = PCs.dtype
            if dtype != float:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float64')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
                raise ValueError(error_message)
        if QC_column is not None:
            Z = [PCs[QCed] if QCed is not None else PCs
                 for PCs, QCed in zip(Z, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, Z)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum()))
                    batch_index += 1
            batch_labels = np.concatenate(batch_labels)
        Z = np.concatenate(Z)
        
        # Run Harmony
        
        cython_functions = cython_inline(r'''
            from cpython.exc cimport PyErr_CheckSignals
            from libcpp.cmath cimport abs, exp, pow, log, sqrt
            from scipy.linalg.cython_blas cimport dgemm, dgemv
            
            cdef inline void matrix_multiply(const double[:, ::1] A,
                                             const double[:, ::1] B,
                                             double[:, ::1] C,
                                             const bint transpose_A,
                                             const bint transpose_B,
                                             const double alpha,
                                             const double beta) noexcept nogil:
                # Flip `A` <-> `B` and `shape[0]` <-> `shape[1]` since our
                # matrices are C-major, whereas BLAS expects Fortran-major
                
                cdef int m, n, k, lda, ldb
                cdef char transA, transB
                if transpose_B:
                    m = B.shape[0]
                    k = B.shape[1]
                    lda = k
                    transA = b'T'
                else:
                    m = B.shape[1]
                    k = B.shape[0]
                    lda = m
                    transA = b'N'
                if transpose_A:
                    n = A.shape[1]
                    ldb = n
                    transB = b'T'
                else:
                    n = A.shape[0]
                    ldb = k
                    transB = b'N'
                dgemm(&transA, &transB, &m, &n, &k, <double*> &alpha,
                      <double*> &B[0, 0], &lda, <double*> &A[0, 0], &ldb,
                      <double*> &beta, &C[0, 0], &m)
            
            cdef inline void matrix_vector_multiply(
                    const double[:, ::1] A,
                    const double[::1] X,
                    double[::1] Y,
                    const bint transpose,
                    const double alpha,
                    const double beta) noexcept nogil:
                # Flip `trans` since our matrix is C-major, whereas BLAS
                # expects Fortran-major
                
                cdef int m = A.shape[1], n = A.shape[0], incx = 1, incy = 1
                cdef char trans = b'N' if transpose else b'T'
                dgemv(&trans, &m, &n, <double*> &alpha, <double*> &A[0,0],
                      &m, <double*> &X[0], &incx, <double*> &beta, &Y[0],
                      &incy)
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            cdef inline void shuffle_array(unsigned[::1] arr, unsigned long* state) noexcept nogil:
                cdef unsigned i, j, temp
                for i in range(arr.shape[0] - 1, 0, -1):
                    j = randint(i + 1, state)
                    temp = arr[i]
                    arr[i] = arr[j]
                    arr[j] = temp
            
            cdef inline double compute_objective(
                    double[:, ::1] Z_norm_times_Y_norm,
                    const double[:, ::1] R,
                    const double[:, ::1] E,
                    const double[:, ::1] O,
                    double[:, ::1] ratio,
                    const double[::1] theta,
                    double[::1] theta_times_ratio,
                    const double sigma,
                    const unsigned num_cells,
                    const unsigned num_clusters,
                    const unsigned num_batches) noexcept nogil:
                cdef unsigned i, j
                cdef double kmeans_error, entropy_term, diversity_penalty
                kmeans_error = entropy_term = diversity_penalty = 0
                for i in range(num_cells):
                    for j in range(num_clusters):
                        kmeans_error += \
                            R[i, j] * (1 - Z_norm_times_Y_norm[i, j])
                        entropy_term += R[i, j] * log(R[i, j])
                kmeans_error *= 2
                entropy_term *= sigma
                for i in range(num_batches):
                    for j in range(num_clusters):
                        ratio[i, j] = O[i, j] * log(
                            (O[i, j] + 1) / (E[i, j] + 1))
                matrix_vector_multiply(ratio, theta, theta_times_ratio,
                                       transpose=True, alpha=1, beta=0)
                for i in range(num_clusters):
                    diversity_penalty += theta_times_ratio[i]
                diversity_penalty *= sigma
                return kmeans_error + entropy_term + diversity_penalty
            
            def initialize(const double[:, ::1] Z_norm,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const unsigned[::1] N_b,
                           double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[::1] R_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           const double sigma,
                           double[:, ::1] ratio,
                           double[::1] theta,
                           double[::1] theta_times_ratio,
                           const double tau):
                cdef unsigned i, j, batch_label, num_cells = Z_norm.shape[0], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef double norm, objective, base, two_over_sigma = 2 / sigma
                
                # Initialize `Pr_b`
                
                for i in range(num_batches):
                    Pr_b[i] = <double> N_b[i] / num_cells
                
                # Initialize `R` (and `R_sum`) and `O`
                
                matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                transpose_A=False, transpose_B=True, alpha=1,
                                beta=0)
                for i in range(num_cells):
                    batch_label = batch_labels[i]
                    norm = 0
                    for j in range(num_clusters):
                        R[i, j] = exp(two_over_sigma *
                                      (Z_norm_times_Y_norm[i, j] - 1))
                        norm += R[i, j]
                    norm = 1 / norm
                    for j in range(num_clusters):
                        R[i, j] *= norm
                        O[batch_label, j] += R[i, j]
                        R_sum[j] += R[i, j]
                
                # Initialize `E`
                
                for i in range(num_batches):
                    for j in range(num_clusters):
                        E[i, j] = Pr_b[i] * R_sum[j]
                
                # Apply discounting to `theta`, if specified
                
                if tau > 0:
                    for i in range(num_batches):
                        base = exp(-N_b[i] / (num_clusters * tau))
                        theta[i] = theta[i] * (1 - base * base)
                
                # Compute and return the initial value of the objective
                # function
                
                objective = compute_objective(
                    Z_norm_times_Y_norm, R, E, O, ratio, theta,
                    theta_times_ratio, sigma, num_cells, num_clusters,
                    num_batches)
                
                return objective
            
            def clustering(const double[:, ::1] Z_norm,
                           double[:, ::1] Z_norm_in,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[:, ::1] R_in,
                           double[::1] R_in_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           double[:, ::1] ratio,
                           const double[::1] theta,
                           double[::1] theta_times_ratio,
                           unsigned[::1] idx_list,
                           const double tol,
                           const unsigned max_iter,
                           const double sigma,
                           const unsigned block_size):
                cdef unsigned i, j, k, iter, num_cells_in_block, pos, batch_label, \
                    num_cells = Z_norm.shape[0], num_PCs = Z_norm.shape[1], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef unsigned long state
                cdef double norm, old, new, objective, \
                    two_over_sigma = 2 / sigma
                cdef double exp_neg_two_over_sigma = exp(-two_over_sigma)
                cdef double[:, ::1] Z_norm_block, R_block
                cdef double past_clustering_objectives[3]
                
                for iter in range(max_iter):
                    # Compute cluster centroids
                    
                    matrix_multiply(R, Z_norm, Y_norm, transpose_A=True,
                                    transpose_B=False, alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm = norm + Y_norm[i, j] * Y_norm[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y_norm[i, j] = Y_norm[i, j] * norm
                    for i in range(num_cells):
                        idx_list[i] = i
                    state = srand(iter)
                    shuffle_array(idx_list, &state)
                    
                    # Update cells blockwise
                    
                    pos = 0
                    Z_norm_block = Z_norm_in
                    R_block = R_in
                    while pos < num_cells:
                        if pos + block_size > num_cells:
                            num_cells_in_block = num_cells - pos
                            Z_norm_block = Z_norm_block[:num_cells_in_block]
                            R_block = R_block[:num_cells_in_block]
                        else:
                            num_cells_in_block = block_size
                        
                        # Remove the cells in this block from `E` and `O`
                        
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[pos + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_in_sum[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_norm_block[i, j] = Z_norm[k, j]
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] -= Pr_b[i] * R_in_sum[j]
                        
                        # Recompute `R` for the removed cells
                        # Note: the original formula is
                        # `exp(-2 / sigma * (1 - Z_norm_block @ Y_norm.T))`,
                        # which expands to `exp(-2 / sigma) *
                        # exp(2 / sigma * Z_norm_block @ Y_norm.T))`. Since
                        # `exp(-2 / sigma)` is a constant, we fold it into
                        # `ratio`.
                        
                        matrix_multiply(Z_norm_block, Y_norm, R_block,
                                        transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    pow((E[i, j] + 1) / (O[i, j] + 1),
                                        theta[i])
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[i + pos]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                R[k, j] = exp(R_block[i, j]) * \
                                          ratio[batch_label, j]
                                norm += R[k, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[k, j] *= norm
                                R_in_sum[j] += R[k, j]
                                
                                # Add the removed cells back into `O`
                                
                                O[batch_label, j] += R[k, j]
                        
                        # Add the removed cells back into `E`
                        
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] += Pr_b[i] * R_in_sum[j]
                        
                        # Move to the next block
                        
                        pos += block_size
                    
                    # Compute the objective and decide whether we've converged
                    
                    matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                    transpose_A=False, transpose_B=True,
                                    alpha=1, beta=0)
                    objective = compute_objective(
                        Z_norm_times_Y_norm, R, E, O, ratio, theta,
                        theta_times_ratio, sigma, num_cells, num_clusters,
                        num_batches)
                    if iter < 3:
                        past_clustering_objectives[iter] = objective
                    else:
                        old = past_clustering_objectives[0] + \
                            past_clustering_objectives[1] + \
                            past_clustering_objectives[2]
                        new = past_clustering_objectives[1] + \
                            past_clustering_objectives[2] + objective
                        if old - new < tol * abs(old):
                            break
                        else:
                            past_clustering_objectives[0] = \
                                past_clustering_objectives[1]
                            past_clustering_objectives[1] = \
                                past_clustering_objectives[2]
                            past_clustering_objectives[2] = objective
                    PyErr_CheckSignals()
                return objective
            
            def correction(const double[:, ::1] Z,
                           double[:, ::1] Z_hat,
                           const double[:, ::1] R,
                           const double[:, ::1] O,
                           const double ridge_lambda,
                           const unsigned[::1] batch_labels,
                           double[::1] factor,
                           double[:, ::1] P,
                           double[:, ::1] P_t_B_inv,
                           double[:, ::1] inv_mat,
                           double[:, ::1] Phi_t_diag_R_by_X,
                           double[:, ::1] W):
                cdef unsigned i, j, k, batch_label, num_cells = Z.shape[0], \
                    num_PCs = Z.shape[1], num_batches = O.shape[0], \
                    num_clusters = O.shape[1]
                cdef double c, c_inv
                
                Z_hat[:] = Z[:]
                
                # Initialize `P` to the identity matrix
                
                P[:] = 0
                for i in range(num_batches + 1):
                    P[i, i] = 1
            
                for k in range(num_clusters):
                    # Compute `factor`, `c_inv` and `P`
                    
                    c = 0
                    for i in range(num_batches):
                        factor[i] = 1 / (O[i, k] + ridge_lambda)
                        c += O[i, k] * (1 - factor[i] * O[i, k])
                        P[0, i + 1] = -factor[i] * O[i, k]
                    c_inv = 1 / c
                    
                    # Compute `P_t_B_inv`
                    
                    P_t_B_inv[:] = 0
                    P_t_B_inv[0, 0] = c_inv
                    for i in range(1, num_batches + 1):
                        P_t_B_inv[i, i] = factor[i - 1]
                        P_t_B_inv[i, 0] = P[0, i] * c_inv
                    
                    # Compute `inv_mat`
                    
                    matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                    transpose_B=False, alpha=1, beta=0)
                    
                    # Compute `Phi_t_diag_R @ X`
                    
                    Phi_t_diag_R_by_X[:] = 0
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Phi_t_diag_R_by_X[0, j] += Z[i, j] * R[i, k]
                            Phi_t_diag_R_by_X[batch_label + 1, j] += \
                                Z[i, j] * R[i, k]
                            
                    # Compute `W`
                    
                    matrix_multiply(inv_mat, Phi_t_diag_R_by_X, W,
                                    transpose_A=False, transpose_B=False,
                                    alpha=1, beta=0)
                    
                    # Update `Z_hat`
                    
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Z_hat[i, j] = Z_hat[i, j] - \
                                W[batch_label + 1, j] * R[i, k]
                        
            def normalize_rows(const double[:, ::1] arr, double[:, ::1] out):
                cdef unsigned i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

            def normalize_rows_inplace(double[:, ::1] arr):
                cdef unsigned i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        arr[i, j] = arr[i, j] * norm
        ''')
        initialize = cython_functions['initialize']
        clustering = cython_functions['clustering']
        correction = cython_functions['correction']
        normalize_rows = cython_functions['normalize_rows']
        normalize_rows_inplace = cython_functions['normalize_rows_inplace']
        
        # Get dimensions of everything
        num_cells, num_PCs = Z.shape
        block_size = int(num_cells * block_proportion)
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        N_b = bincount(batch_labels, num_bins=batch_labels[-1] + 1,
                       num_threads=num_threads)
        num_batches = len(N_b)
        
        # Allocate arrays
        Z_norm = np.empty((num_cells, num_PCs))
        Z_norm_in = np.empty((block_size, num_PCs))
        Z_norm_times_Y_norm = np.empty((num_cells, num_clusters))
        Pr_b = np.empty(num_batches)
        R = np.empty((num_cells, num_clusters))
        R_in = np.empty((block_size, num_clusters))
        R_in_sum = np.zeros(num_clusters)
        E = np.empty((num_batches, num_clusters))
        O = np.zeros((num_batches, num_clusters))
        ratio = np.empty((num_batches, num_clusters))
        theta = np.repeat(theta, num_batches)
        theta_times_ratio = np.empty(num_clusters)
        idx_list = np.empty(num_cells, dtype=np.uint32)
        factor = np.empty(num_cells)
        P = np.empty((num_batches + 1, num_batches + 1))
        P_t_B_inv = np.empty((num_batches + 1, num_batches + 1))
        inv_mat = np.empty((num_batches + 1, num_batches + 1))
        Phi_t_diag_R_by_X = np.empty((num_batches + 1, num_PCs))
        W = np.empty((num_batches + 1, num_PCs))
        
        # Run k-means
        kmeans = cython_inline(r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport prange
        from libcpp.algorithm cimport fill
        from libcpp.vector cimport vector
        
        cdef extern from "float.h":
            cdef double DBL_MAX
        
        cdef extern from "limits.h":
            cdef unsigned UINT_MAX
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef inline double random_probability(unsigned long* state) noexcept nogil:
            # Returns a random probability, i.e. a random number in U(0, 1)
            return <double> rand(state) / UINT_MAX
        
        cdef inline void kmeans_random_init(const double[:, ::1] X,
                                            double[:, ::1] centroids,
                                            const unsigned num_cells,
                                            const unsigned num_clusters, 
                                            const unsigned long seed):
            # Initialize centroids with random cells. Use a bitmap to efficiently keep
            # track of which cells have been sampled already, to avoid sampling the
            # same cell twice.
            
            cdef unsigned i, j
            cdef unsigned long word_index, bit_index, state = srand(seed)
            cdef vector[unsigned long] bitmap
            bitmap.resize((num_cells + 63) // 64)
            for i in range(num_clusters):
                while True:
                    j = randint(num_cells, &state)
                    word_index = j >> 6
                    bit_index = j & 63
                    if not bitmap[word_index] & (1 << bit_index):
                        bitmap[word_index] |= 1 << bit_index
                        centroids[i] = X[j]
                        break
        
        cdef inline void kmeans_barbar(const double[:, ::1] X,
                                       double[:, ::1] centroids,
                                       const unsigned num_init_iterations,
                                       const double oversampling_factor,
                                       const unsigned num_cells,
                                       const unsigned num_clusters,
                                       const unsigned num_dimensions,
                                       const unsigned long seed):
            cdef unsigned random_cell, iteration, i, j, k, c0, c1, \
                selected_cell, best_selected_cell, num_selected_cells, cluster_index, \
                selected_centroid
            cdef unsigned long state
            cdef double l = oversampling_factor * num_clusters, cost, difference, \
                distance, l_over_cost, min_distance, inverse_cost, probability
            cdef vector[double] min_distances
            cdef vector[unsigned] selected_cells, selected_cell_weights, \
                centroid_indices
            min_distances.resize(num_cells)
            # reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
        
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # These will constitute a shortlist from which we will select the final
            # centroids to start off k-means with.
        
            state = srand(seed)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
        
            # For the first iteration, calculate the (squared Euclidean) distance from
            # each cell to this random cell, storing it in `min_distances`. Also
            # calculate the sum of the `min_distances`, i.e. the `cost`.
    
            cost = 0
            for i in range(num_cells):
                distance = 0
                for j in range(num_dimensions):
                    difference = X[i, j] - X[random_cell, j]
                    distance += difference * difference
                min_distances[i] = distance
                cost += distance
    
            # Sample each cell with probability `l * min_distances[i] / cost`
            
            l_over_cost = l / cost
            for i in range(num_cells):
                state = srand(seed + i)
                if l_over_cost * min_distances[i] >= random_probability(&state):
                    selected_cells.push_back(i)
            
            PyErr_CheckSignals()
        
            # For each remaining iteration...
        
            for iteration in range(1, num_init_iterations):
                # Find each cell's distance to its nearest candidate centroid
                # (selected cell), storing it in `min_distances`. Also calculate
                # the sum of the `min_distances`, i.e. the `cost`.
    
                cost = 0
                for i in range(num_cells):
                    min_distance = DBL_MAX
                    for selected_cell in selected_cells:
                        distance = 0
                        for j in range(num_dimensions):
                            difference = X[i, j] - X[selected_cell, j]
                            distance = distance + difference * difference
                        if distance < min_distance:
                            min_distance = distance
                    min_distances[i] = min_distance
                    cost += min_distance
    
                # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                # Note that `min_distances` will be 0 for cells that have already been
                # sampled, so we will automatically avoid sampling them twice.
    
                l_over_cost = l / cost
                for i in range(num_cells):
                    state = srand(seed + i)
                    if l_over_cost * min_distances[i] >= random_probability(&state):
                        selected_cells.push_back(i)
                    
                PyErr_CheckSignals()
        
            # Get the weight for each selected cell: the number of cells that are
            # closer to the selected cell than to any other selected cell
        
            num_selected_cells = selected_cells.size()
            selected_cell_weights.resize(num_selected_cells)
            for i in range(num_cells):
                min_distance = DBL_MAX
                for j in range(num_selected_cells):
                    selected_cell = selected_cells[j]
                    distance = 0
                    for k in range(num_dimensions):
                        difference = X[i, k] - X[selected_cell, k]
                        distance = distance + difference * difference
                    if distance < min_distance:
                        min_distance = distance
                        best_selected_cell = j
                selected_cell_weights[best_selected_cell] += 1
        
            PyErr_CheckSignals()
            
            # Run k-means++ to select `num_clusters` of the selected cells as the
            # centroids, using `selected_cell_weights` as weights. Start by selecting a
            # random cell from our selected cells as the first centroid.
        
            state = srand(seed + 1)
            random_cell = selected_cells[randint(num_selected_cells, &state)]
            centroid_indices.resize(num_clusters)
            centroid_indices[0] = random_cell
            centroids[0] = X[random_cell]
        
            # Iteratively select the remaining centroids
        
            for cluster_index in range(1, num_clusters):
    
                # Find each selected cell's distance to its nearest centroid. Multiply
                # this distance by the selected cell's weight (from
                # `selected_cell_weights`), and store it in `min_distances`. Also
                # calculate the sum of the weighted `min_distances`, i.e. the `cost`.
    
                cost = 0
                for i in range(num_selected_cells):
                    selected_cell = selected_cells[i]
                    min_distance = DBL_MAX
                    for j in range(cluster_index):
                        selected_centroid = centroid_indices[j]
                        distance = 0
                        for k in range(num_dimensions):
                            difference = X[selected_cell, k] - X[selected_centroid, k]
                            distance = distance + difference * difference
                        if distance < min_distance:
                            min_distance = distance
                    min_distance = min_distance * selected_cell_weights[i]
                    min_distances[i] = min_distance
                    cost += min_distance
    
                # Sample a single cell `i` with probability
                # `min_distances[i] / cost`. Note that `min_distances` will be 0
                # for cells that have already been sampled, so we will
                # automatically avoid sampling them twice.
    
                inverse_cost = 1 / cost
                probability = random_probability(&state)
                i = 0
                while True:
                    probability -= min_distances[i] * inverse_cost
                    if probability < 0:
                        break
                    i += 1
                centroid_indices[cluster_index] = selected_cells[i]
                centroids[cluster_index] = X[selected_cells[i]]
        
            PyErr_CheckSignals()
        
        cdef inline void kmeans_barbar_parallel(const double[:, ::1] X,
                                                double[:, ::1] centroids,
                                                const unsigned num_init_iterations,
                                                const double oversampling_factor,
                                                const unsigned num_cells,
                                                const unsigned num_clusters,
                                                const unsigned num_dimensions,
                                                const unsigned long seed,
                                                const unsigned num_threads):
            cdef unsigned random_cell, iteration, i, j, k, thread_index, c0, c1, \
                selected_cell, best_selected_cell, num_selected_cells, cluster_index, \
                selected_centroid
            cdef unsigned long state
            cdef double l = oversampling_factor * num_clusters, cost, difference, \
                distance, l_over_cost, min_distance, inverse_cost, probability
            cdef vector[double] min_distances
            cdef vector[unsigned] selected_cells, selected_cell_weights, \
                centroid_indices
            cdef vector[vector[unsigned]] thread_selected_cells
            min_distances.resize(num_cells)
            # reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
        
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # These will constitute a shortlist from which we will select the final
            # centroids to start off k-means with.
        
            state = srand(seed)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
        
            with nogil:
                # For the first iteration, calculate the (squared Euclidean) distance from
                # each cell to this random cell, storing it in `min_distances`. Also
                # calculate the sum of the `min_distances`, i.e. the `cost`.
        
                cost = 0
                for i in prange(num_cells, num_threads=num_threads):
                    distance = 0
                    for j in range(num_dimensions):
                        difference = X[i, j] - X[random_cell, j]
                        distance = distance + difference * difference
                    min_distances[i] = distance
                    cost += distance
        
                # Sample each cell with probability `l * min_distances[i] / cost`
        
                thread_selected_cells.resize(num_threads)
                l_over_cost = l / cost
                for thread_index in prange(num_threads, num_threads=num_threads,
                                           chunksize=1, schedule='static'):
                    thread_selected_cells[thread_index].reserve(
                        <unsigned> (1.25 * l / num_threads))
                    c0 = num_cells * thread_index / num_threads
                    c1 = num_cells * (thread_index + 1) / num_threads
                    for i in range(c0, c1):
                        state = srand(seed + i)
                        if l_over_cost * min_distances[i] >= random_probability(&state):
                            thread_selected_cells[thread_index].push_back(i)
        
                # Aggregate each thread's selected cells into a single vector
        
                for thread_index in range(num_threads):
                    selected_cells.insert(
                        selected_cells.end(),
                        thread_selected_cells[thread_index].begin(),
                        thread_selected_cells[thread_index].end())
        
            PyErr_CheckSignals()
        
            # For each remaining iteration...
        
            for iteration in range(1, num_init_iterations):
                with nogil:
                    # Find each cell's distance to its nearest candidate centroid
                    # (selected cell), storing it in `min_distances`. Also calculate
                    # the sum of the `min_distances`, i.e. the `cost`.
        
                    cost = 0
                    for i in prange(num_cells, num_threads=num_threads):
                        min_distance = DBL_MAX
                        for selected_cell in selected_cells:
                            distance = 0
                            for j in range(num_dimensions):
                                difference = X[i, j] - X[selected_cell, j]
                                distance = distance + difference * difference
                            if distance < min_distance:
                                min_distance = distance
                        min_distances[i] = min_distance
                        cost += min_distance
        
                    # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                    # Note that `min_distances` will be 0 for cells that have already been
                    # sampled, so we will automatically avoid sampling them twice.
        
                    l_over_cost = l / cost
                    for thread_index in prange(num_threads, num_threads=num_threads,
                                               chunksize=1, schedule='static'):
                        thread_selected_cells[thread_index].clear()
                        c0 = num_cells * thread_index / num_threads
                        c1 = num_cells * (thread_index + 1) / num_threads
                        for i in range(c0, c1):
                            state = srand(seed + i)
                            if l_over_cost * min_distances[i] >= random_probability(&state):
                                thread_selected_cells[thread_index].push_back(i)
        
                    # Aggregate each thread's selected cells into a single vector
        
                    for thread_index in range(num_threads):
                        selected_cells.insert(
                            selected_cells.end(),
                            thread_selected_cells[thread_index].begin(),
                            thread_selected_cells[thread_index].end())
        
                PyErr_CheckSignals()
        
            # Get the weight for each selected cell: the number of cells that are
            # closer to the selected cell than to any other selected cell
        
            num_selected_cells = selected_cells.size()
            selected_cell_weights.resize(num_selected_cells)
            for i in prange(num_cells, nogil=True, num_threads=num_threads):
                min_distance = DBL_MAX
                for j in range(num_selected_cells):
                    selected_cell = selected_cells[j]
                    distance = 0
                    for k in range(num_dimensions):
                        difference = X[i, k] - X[selected_cell, k]
                        distance = distance + difference * difference
                    if distance < min_distance:
                        min_distance = distance
                        best_selected_cell = j
                selected_cell_weights[best_selected_cell] += 1
        
            PyErr_CheckSignals()
            
            # Run k-means++ to select `num_clusters` of the selected cells as the
            # centroids, using `selected_cell_weights` as weights. Start by selecting a
            # random cell from our selected cells as the first centroid.
        
            state = srand(seed + 1)
            random_cell = selected_cells[randint(num_selected_cells, &state)]
            centroid_indices.resize(num_clusters)
            centroid_indices[0] = random_cell
            centroids[0] = X[random_cell]
        
            # Iteratively select the remaining centroids
        
            with nogil:
                for cluster_index in range(1, num_clusters):
        
                    # Find each selected cell's distance to its nearest centroid. Multiply
                    # this distance by the selected cell's weight (from
                    # `selected_cell_weights`), and store it in `min_distances`. Also
                    # calculate the sum of the weighted `min_distances`, i.e. the `cost`.
        
                    cost = 0
                    for i in prange(num_selected_cells, num_threads=num_threads):
                        selected_cell = selected_cells[i]
                        min_distance = DBL_MAX
                        for j in range(cluster_index):
                            selected_centroid = centroid_indices[j]
                            distance = 0
                            for k in range(num_dimensions):
                                difference = X[selected_cell, k] - X[selected_centroid, k]
                                distance = distance + difference * difference
                            if distance < min_distance:
                                min_distance = distance
                        min_distance = min_distance * selected_cell_weights[i]
                        min_distances[i] = min_distance
                        cost += min_distance
        
                    # Sample a single cell `i` with probability
                    # `min_distances[i] / cost`. Note that `min_distances` will be 0
                    # for cells that have already been sampled, so we will
                    # automatically avoid sampling them twice.
        
                    inverse_cost = 1 / cost
                    probability = random_probability(&state)
                    i = 0
                    while True:
                        probability -= min_distances[i] * inverse_cost
                        if probability < 0:
                            break
                        i += 1
                    centroid_indices[cluster_index] = selected_cells[i]
                    centroids[cluster_index] = X[selected_cells[i]]
        
            PyErr_CheckSignals()
        
        ''')['kmeans']
        normalize_rows(Z, Z_norm)
        Y_norm = np.empty((num_clusters, num_PCs), dtype=float)
        kmeans(X=Z_norm, cluster_labels=np.empty(num_cells, dtype=np.uint32),
               centroids=Y_norm,
               num_cells_per_cluster=np.empty(num_clusters, dtype=np.uint32),
               random_init=random_init, num_init_iterations=num_init_iterations,
               oversampling_factor=oversampling_factor,
               num_kmeans_iterations=num_kmeans_iterations, seed=seed,
               num_threads=num_threads)
        normalize_rows_inplace(Y_norm)
        
        with threadpool_limits(limits=num_threads):
            # Complete initialization in Cython
            objective = initialize(
                Z_norm=Z_norm, Y_norm=Y_norm,
                Z_norm_times_Y_norm=Z_norm_times_Y_norm, N_b=N_b, Pr_b=Pr_b,
                batch_labels=batch_labels, R=R, R_sum=R_in_sum, E=E, O=O,
                sigma=sigma, ratio=ratio, theta=theta,
                theta_times_ratio=theta_times_ratio, tau=tau)
            
            if verbose:
                print(f'Initialization is complete: objective = '
                      f'{objective:.2f}')
            
            iteration_string = plural('iteration', max_harmony_iterations)
            
            for i in range(1, max_harmony_iterations + 1):
                prev_objective = objective
                objective = clustering(
                    Z_norm=Z_norm, Z_norm_in=Z_norm_in, Y_norm=Y_norm,
                    Z_norm_times_Y_norm=Z_norm_times_Y_norm, Pr_b=Pr_b,
                    batch_labels=batch_labels, R=R, R_in=R_in,
                    R_in_sum=R_in_sum, E=E, O=O, ratio=ratio, theta=theta,
                    theta_times_ratio=theta_times_ratio, idx_list=idx_list,
                    tol=tol_clustering, max_iter=max_clustering_iterations,
                    sigma=sigma, block_size=block_size)
                correction(Z=Z, Z_hat=Z_norm, R=R, O=O,
                           ridge_lambda=ridge_lambda,
                           batch_labels=batch_labels, factor=factor, P=P,
                           P_t_B_inv=P_t_B_inv, inv_mat=inv_mat,
                           Phi_t_diag_R_by_X=Phi_t_diag_R_by_X, W=W)
                
                if verbose:
                    print(f'Completed {i} of {max_harmony_iterations} '
                          f'{iteration_string}: objective = {objective:.2f}')
                
                if prev_objective - objective < \
                        tol_harmony * abs(prev_objective):
                    if verbose:
                        print(f'Reached convergence after {i} '
                              f'{iteration_string}')
                    break
                
                if i == max_harmony_iterations:
                    if verbose:
                        print(f'Failed to converge after {i} '
                              f'{iteration_string}')
                    break
                
                normalize_rows_inplace(Z_norm)
        
        del batch_labels, Z, Pr_b, theta, R, E, O, Z_norm_in, Y_norm, \
            Z_norm_times_Y_norm, R_in, R_in_sum, ratio, theta_times_ratio, \
            idx_list, factor, P, P_t_B_inv, inv_mat, Phi_t_diag_R_by_X, W
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z_norm[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns)
        return tuple(datasets) if others else datasets[0]
    
    def label_transfer_from(
            self,
            other: SingleCell,
            original_cell_type_column: SingleCellColumn,
            *,
            QC_column: SingleCellColumn | None = 'passed_QC',
            other_QC_column: SingleCellColumn | None = 'passed_QC',
            Harmony_key: str = 'Harmony_PCs',
            cell_type_column: str = 'cell_type',
            confidence_column: str = 'cell_type_confidence',
            next_best_cell_type_column: str = 'next_best_cell_type',
            next_best_confidence_column: str =
                'next_best_cell_type_confidence',
            num_neighbors: int | np.integer = 20,
            num_clusters: int | np.integer | None = None,
            num_probes: int | np.integer | None = None,
            num_clustering_iterations: int | np.integer = 10,
            seed: int | np.integer | None = None,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Transfer cell-type labels from another dataset to this one, by running
        approximate k-nearest neighbors on the Harmony embeddings from
        `harmonize()`.
        
        For each cell in `self`, the transferred cell-type label is the most
        common cell-type label among the `num_neighbors` cells in `other` with
        the nearest Harmony embeddings. The cell-type confidence is the
        fraction of these neighbors that share this most common cell-type
        label.
        
        Args:
            other: the dataset to transfer cell-type labels from
            original_cell_type_column: a column of `other.obs` containing
                                       cell-type labels. Can be a column name,
                                       a polars expression, a polars Series, a
                                       1D NumPy array, or a function that takes
                                       in `other` and returns a polars Series
                                       or 1D NumPy array.
            QC_column: an optional Boolean column of `self.obs` indicating
                       which cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in `self` and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will have their cell-type labels
                       and confidences set to `null`.
            other_QC_column: an optional Boolean column of `other.obs`
                             indicating which cells passed QC. Can be a column
                             name, a polars expression, a polars Series, a 1D
                             NumPy array, or a function that takes in `other`
                             and returns a polars Series or 1D NumPy array. Set
                             to `None` to include all cells. Cells failing QC
                             will be ignored during the label transfer.
            Harmony_key: the key of `self.obsm` and `other.obsm` containing the
                         Harmony embeddings for each dataset
            cell_type_column: the name of a column to be added to `self.obs`
                              indicating each cell's most likely cell type,
                              i.e. the most common cell-type label among the
                              cell's `num_neighbors` nearest neighbors in
                              `other`
            confidence_column: the name of a column to be added to `self.obs`
                               indicating each cell's cell-type confidence,
                               i.e. the fraction of the cell's `num_neighbors`
                               nearest neighbors in `other` that share the most
                               common cell-type label. If multiple cell types
                               are equally common among the nearest neighbors,
                               tiebreak based on which of them is most common
                               in `original_cell_type_column`.
            next_best_cell_type_column: the name of a column to be added to
                                        `self.obs` indicating each cell's
                                        second-most likely cell type, i.e. the
                                        second-most common cell-type label
                                        among the cell's `num_neighbors`
                                        nearest neighbors in
                                        `original_cell_type_column`
            next_best_confidence_column: the name of a column to be added to
                                         `self.obs` indicating each cell's
                                         cell-type confidence, i.e. the
                                         fraction of the cell's `num_neighbors`
                                         nearest neighbors in
                                         `original_cell_type_column` that share
                                         the most common cell-type label. If
                                         multiple cell types are equally common
                                         among the nearest neighbors, tiebreak
                                         based on which of them is most common
                                         in `other`.
            num_neighbors: the number of nearest neighbors to use when
                           determining a cell's label. All cell-type
                           confidences will be multiples of
                           `1 / num_neighbors`.
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be positive and less than the number of
                          cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            num_clustering_iterations: the maximum number of iterations of
                                       k-means clustering to perform before
                                       starting the nearest-neighbors search,
                                       stopping early if convergence is reached
            seed: the random seed to use when finding nearest neighbors, or
                  leave unset to use `single_cell.options()['seed']` as the
                  seed (0 by default)
            overwrite: if `True`, overwrite `cell_type_column` and/or
                       `confidence_column` if already present in this dataset's
                       obs, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use for the nearest-neighbors
                         tree construction and search. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            `self`, but with two additional columns: `cell_type_column`,
            containing the transferred cell-type labels, and
            `confidence_column`, containing the cell-type confidences.
        """
        with ignore_sigint():
            import faiss
        # Check that `other` is a SingleCell dataset
        check_type(other, 'other', SingleCell, 'a SingleCell dataset')
        # Get `QC_column` from `self` and `other_QC_column` from `other`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if other_QC_column is not None:
            other_QC_column = other._get_column(
                'obs', other_QC_column, 'other_QC_column', pl.Boolean,
                allow_missing=other_QC_column == 'passed_QC')
        # Get the number of cells in `self` and `other`
        num_cells_in_self = len(self._obs) if QC_column is None else \
            QC_column.sum()
        num_cells_in_other = len(other._obs) if other_QC_column is None else \
            other_QC_column.sum()
        # Get `original_cell_type_column` from `other`
        original_original_cell_type_column = original_cell_type_column
        original_cell_type_column = other._get_column(
            'obs', original_cell_type_column, 'original_cell_type_column',
            (pl.Categorical, pl.Enum, pl.String), QC_column=other_QC_column)
        # If `other_QC_column` is not `None`, filter the cell type labels in
        # `original_cell_type_column` to cells passing QC
        if other_QC_column is not None:
            original_cell_type_column = \
                original_cell_type_column.filter(other_QC_column)
        # Check that `original_cell_type_column` has at least two distinct cell
        # types
        most_common_cell_types = \
            original_cell_type_column.value_counts(sort=True).to_series()
        if len(most_common_cell_types) == 1:
            original_cell_type_column_description = \
                SingleCell._describe_column('original_cell_type_column',
                                            original_original_cell_type_column)
            error_message = (
                f'{original_cell_type_column_description} must have at least '
                f'two distinct cell types')
            if other_QC_column is not None:
                error_message += ' after filtering to cells passing QC'
            raise ValueError(error_message)
        # Check that `Harmony_key` is a string and in both `self.obsm` and
        # `other.obsm`
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        datasets = (self, 'self'), (other, 'other')
        for dataset, dataset_name in datasets:
            if Harmony_key not in dataset._obsm:
                error_message = (
                    f'Harmony_key {Harmony_key!r} is not a column of '
                    f'{dataset_name}.obs; did you forget to run harmonize() '
                    f'before label_transfer_from()? Set overwrite=True to '
                    f'overwrite.')
                raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `cell_type_column`, `confidence_column`,
        # `next_best_cell_type_column` and `next_best_confidence_column` are
        # strings and, unless `overwrite=True`, not already columns of
        # `self.obs`
        for column, column_name in (
                (cell_type_column, 'cell_type_column'),
                (confidence_column, 'confidence_column'),
                (next_best_cell_type_column, 'next_best_cell_type_column'),
                (next_best_confidence_column, 'next_best_confidence_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column '
                    f'of obs; did you already run label_transfer_from()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_neighbors` is a positive integer
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        check_bounds(num_neighbors, 'num_neighbors', 1)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells_in_other),
                                           num_cells_in_other / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters < num_cells_in_other:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be â‰¥ 1 and '
                    f'less than the number of cells in other '
                    f'({num_cells_in_other:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be â‰¥ 1 and â‰¤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = SingleCell._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Recode cell types so the most common is 0, the next-most common 1,
        # etc. This has the effect of breaking ties by taking the most common
        # cell type: we pick the first element in case of ties.
        cell_type_to_code = dict(zip(most_common_cell_types, range(
            len(most_common_cell_types))))
        original_cell_type_column = original_cell_type_column\
            .replace_strict(cell_type_to_code, return_dtype=pl.UInt32)
        # Get the Harmony embeddings for self and other
        self_Harmony_embeddings = self._obsm[Harmony_key] \
            if QC_column is None else \
            self._obsm[Harmony_key][QC_column.to_numpy()]
        other_Harmony_embeddings = other._obsm[Harmony_key] \
            if other_QC_column is None else \
            other._obsm[Harmony_key][other_QC_column.to_numpy()]
        dim = self_Harmony_embeddings.shape[1]
        if other_Harmony_embeddings.shape[1] != dim:
            error_message = (
                f"the two datasets' Harmony embeddings have different numbers "
                f"of dimensions ({dim:,} vs "
                f"{other_Harmony_embeddings.shape[1]:,}")
            raise ValueError(error_message)
        # Use faiss to get the indices of the num_neighbors nearest
        # neighbors in other for each cell in self
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        index.cp.niter = num_clustering_iterations
        # noinspection PyArgumentList
        index.train(other_Harmony_embeddings)
        # noinspection PyArgumentList
        index.add(other_Harmony_embeddings)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = \
            index.search(self_Harmony_embeddings, num_neighbors)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = self_Harmony_embeddings[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} cells '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) '
                          f'did not have enough neighbors with '
                          f'{index.nprobe:,} probes; re-running '
                          f'nearest-neighbors finding for these cells with '
                          f'{num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = index.search(needs_update_X, num_neighbors)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        # Get the cell-type labels of these nearest neighbors (using our
        # integer encoding where the most common cell type is 0, the next-most
        # common 1, etc.)
        nearest_neighbor_cell_types = \
            original_cell_type_column.to_numpy()[nearest_neighbor_indices]
        # Get the two most common cell types for each cell in `self` among its
        # `num_neighbors` nearest neighbors in `other`. Pick the first element
        # in case of ties, which according to our encoding is the most common
        # cell type. Also get the cell-type confidence of these two cell types,
        # i.e. their frequency among the nearest neighbors.
        cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        confidences = np.empty(num_cells_in_self, dtype=float)
        next_best_cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        next_best_confidences = np.empty(num_cells_in_self, dtype=float)
        cython_inline(r'''
            from cython.parallel cimport threadid, prange
            from libcpp.algorithm cimport fill
            from libcpp.vector cimport vector
            
            def label_transfer(const unsigned[:, ::1] nearest_neighbor_cell_types,
                               const unsigned num_cell_types,
                               unsigned[::1] cell_types,
                               double[::1] confidences,
                               unsigned[::1] next_best_cell_types,
                               double[::1] next_best_confidences,
                               const unsigned num_threads):
                cdef unsigned i, j, thread_index, cell_type, count, \
                    most_common_cell_type, second_most_common_cell_type, \
                    max_count, second_max_count, \
                    num_cells = nearest_neighbor_cell_types.shape[0], \
                    num_neighbors = nearest_neighbor_cell_types.shape[1]
                cdef double inv_num_neighbors = 1.0 / num_neighbors
                cdef vector[vector[unsigned]] thread_counts
                
                if num_threads == 1:
                    thread_counts.resize(1)
                    thread_counts[0].resize(num_neighbors)
                    for i in range(num_cells):
                        fill(thread_counts[0].begin(),
                             thread_counts[0].end(), 0)
                        for j in range(num_neighbors):
                            cell_type = nearest_neighbor_cell_types[i, j]
                            thread_counts[0][cell_type] += 1
                        if thread_counts[0][0] >= thread_counts[0][1]:
                            max_count = thread_counts[0][0]
                            second_max_count = thread_counts[0][1]
                            most_common_cell_type = 0
                            second_most_common_cell_type = 1
                        else:
                            max_count = thread_counts[0][1]
                            second_max_count = thread_counts[0][0]
                            most_common_cell_type = 1
                            second_most_common_cell_type = 0
                        for cell_type in range(2, num_cell_types):
                            count = thread_counts[0][cell_type]
                            if count > max_count:
                                second_max_count = max_count
                                second_most_common_cell_type = \
                                    most_common_cell_type
                                max_count = count
                                most_common_cell_type = cell_type
                            elif count > second_max_count:
                                second_max_count = count
                                second_most_common_cell_type = cell_type
                        cell_types[i] = most_common_cell_type
                        confidences[i] = max_count * inv_num_neighbors
                        next_best_cell_types[i] = \
                            second_most_common_cell_type
                        next_best_confidences[i] = \
                            second_max_count * inv_num_neighbors
                else:
                    thread_counts.resize(num_threads)
                    for thread_index in range(num_threads):
                        thread_counts[thread_index].resize(num_neighbors)
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        thread_index = threadid()
                        fill(thread_counts[thread_index].begin(),
                             thread_counts[thread_index].end(), 0)
                        for j in range(num_neighbors):
                            cell_type = nearest_neighbor_cell_types[i, j]
                            thread_counts[thread_index][cell_type] += 1
                        if thread_counts[thread_index][0] >= \
                                thread_counts[thread_index][1]:
                            max_count = thread_counts[thread_index][0]
                            second_max_count = thread_counts[thread_index][1]
                            most_common_cell_type = 0
                            second_most_common_cell_type = 1
                        else:
                            max_count = thread_counts[thread_index][1]
                            second_max_count = thread_counts[thread_index][0]
                            most_common_cell_type = 1
                            second_most_common_cell_type = 0
                        for cell_type in range(2, num_cell_types):
                            count = thread_counts[thread_index][cell_type]
                            if count > max_count:
                                second_max_count = max_count
                                second_most_common_cell_type = \
                                    most_common_cell_type
                                max_count = count
                                most_common_cell_type = cell_type
                            elif count > second_max_count:
                                second_max_count = count
                                second_most_common_cell_type = cell_type
                        cell_types[i] = most_common_cell_type
                        confidences[i] = max_count * inv_num_neighbors
                        next_best_cell_types[i] = \
                            second_most_common_cell_type
                        next_best_confidences[i] = \
                            second_max_count * inv_num_neighbors
            ''')['label_transfer'](
                nearest_neighbor_cell_types=nearest_neighbor_cell_types,
                num_cell_types=len(cell_type_to_code), cell_types=cell_types,
                confidences=confidences,
                next_best_cell_types=next_best_cell_types,
                next_best_confidences=next_best_confidences,
                num_threads=num_threads)
        # Map the cell-type codes back to their labels by constructing a polars
        # Series from the codes, then casting it to an Enum. Also convert
        # cell-type confidences to Series.
        cell_types = pl.Series(cell_type_column, cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        confidences = pl.Series(confidence_column, confidences)
        next_best_cell_types = \
            pl.Series(next_best_cell_type_column, next_best_cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        next_best_confidences = \
            pl.Series(next_best_confidence_column, next_best_confidences)
        # Add the four columns to `self.obs`. If `QC_column` is not `None`,
        # back-project from QCed cells to all cells, filling with `null`.
        columns = cell_types, confidences, next_best_cell_types, \
            next_best_confidences
        if QC_column is None:
            obs = self._obs.with_columns(columns)
        else:
            # noinspection PyTypeChecker
            expand = lambda series: pl.when(QC_column.name)\
                .then(pl.lit(series).gather(pl.col(QC_column.name).cum_sum()
                                            .cast(pl.Int32) - 1))
            obs = self._obs.with_columns(map(expand, columns))
        # Return a new SingleCell dataset containing the cell-type labels and
        # confidences
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    # noinspection PyUnresolvedReferences
    @staticmethod
    def _get_rocket_r() -> 'LinearSegmentedColormap':
        """
        Define Seaborn's rocket_r colormap using base Matplotlib.
        
        Returns:

        """
        import matplotlib.pyplot as plt
        rocket_colors = [
            [0.01060815, 0.01808215, 0.10018654],
            [0.01428972, 0.02048237, 0.10374486],
            [0.01831941, 0.0229766, 0.10738511],
            [0.02275049, 0.02554464, 0.11108639],
            [0.02759119, 0.02818316, 0.11483751],
            [0.03285175, 0.03088792, 0.11863035],
            [0.03853466, 0.03365771, 0.12245873],
            [0.04447016, 0.03648425, 0.12631831],
            [0.05032105, 0.03936808, 0.13020508],
            [0.05611171, 0.04224835, 0.13411624],
            [0.0618531, 0.04504866, 0.13804929],
            [0.06755457, 0.04778179, 0.14200206],
            [0.0732236, 0.05045047, 0.14597263],
            [0.0788708, 0.05305461, 0.14995981],
            [0.08450105, 0.05559631, 0.15396203],
            [0.09011319, 0.05808059, 0.15797687],
            [0.09572396, 0.06050127, 0.16200507],
            [0.10132312, 0.06286782, 0.16604287],
            [0.10692823, 0.06517224, 0.17009175],
            [0.1125315, 0.06742194, 0.17414848],
            [0.11813947, 0.06961499, 0.17821272],
            [0.12375803, 0.07174938, 0.18228425],
            [0.12938228, 0.07383015, 0.18636053],
            [0.13501631, 0.07585609, 0.19044109],
            [0.14066867, 0.0778224, 0.19452676],
            [0.14633406, 0.07973393, 0.1986151],
            [0.15201338, 0.08159108, 0.20270523],
            [0.15770877, 0.08339312, 0.20679668],
            [0.16342174, 0.0851396, 0.21088893],
            [0.16915387, 0.08682996, 0.21498104],
            [0.17489524, 0.08848235, 0.2190294],
            [0.18065495, 0.09009031, 0.22303512],
            [0.18643324, 0.09165431, 0.22699705],
            [0.19223028, 0.09317479, 0.23091409],
            [0.19804623, 0.09465217, 0.23478512],
            [0.20388117, 0.09608689, 0.23860907],
            [0.20973515, 0.09747934, 0.24238489],
            [0.21560818, 0.09882993, 0.24611154],
            [0.22150014, 0.10013944, 0.2497868],
            [0.22741085, 0.10140876, 0.25340813],
            [0.23334047, 0.10263737, 0.25697736],
            [0.23928891, 0.10382562, 0.2604936],
            [0.24525608, 0.10497384, 0.26395596],
            [0.25124182, 0.10608236, 0.26736359],
            [0.25724602, 0.10715148, 0.27071569],
            [0.26326851, 0.1081815, 0.27401148],
            [0.26930915, 0.1091727, 0.2772502],
            [0.27536766, 0.11012568, 0.28043021],
            [0.28144375, 0.11104133, 0.2835489],
            [0.2875374, 0.11191896, 0.28660853],
            [0.29364846, 0.11275876, 0.2896085],
            [0.29977678, 0.11356089, 0.29254823],
            [0.30592213, 0.11432553, 0.29542718],
            [0.31208435, 0.11505284, 0.29824485],
            [0.31826327, 0.1157429, 0.30100076],
            [0.32445869, 0.11639585, 0.30369448],
            [0.33067031, 0.11701189, 0.30632563],
            [0.33689808, 0.11759095, 0.3088938],
            [0.34314168, 0.11813362, 0.31139721],
            [0.34940101, 0.11863987, 0.3138355],
            [0.355676, 0.11910909, 0.31620996],
            [0.36196644, 0.1195413, 0.31852037],
            [0.36827206, 0.11993653, 0.32076656],
            [0.37459292, 0.12029443, 0.32294825],
            [0.38092887, 0.12061482, 0.32506528],
            [0.38727975, 0.12089756, 0.3271175],
            [0.39364518, 0.12114272, 0.32910494],
            [0.40002537, 0.12134964, 0.33102734],
            [0.40642019, 0.12151801, 0.33288464],
            [0.41282936, 0.12164769, 0.33467689],
            [0.41925278, 0.12173833, 0.33640407],
            [0.42569057, 0.12178916, 0.33806605],
            [0.43214263, 0.12179973, 0.33966284],
            [0.43860848, 0.12177004, 0.34119475],
            [0.44508855, 0.12169883, 0.34266151],
            [0.45158266, 0.12158557, 0.34406324],
            [0.45809049, 0.12142996, 0.34540024],
            [0.46461238, 0.12123063, 0.34667231],
            [0.47114798, 0.12098721, 0.34787978],
            [0.47769736, 0.12069864, 0.34902273],
            [0.48426077, 0.12036349, 0.35010104],
            [0.49083761, 0.11998161, 0.35111537],
            [0.49742847, 0.11955087, 0.35206533],
            [0.50403286, 0.11907081, 0.35295152],
            [0.51065109, 0.11853959, 0.35377385],
            [0.51728314, 0.1179558, 0.35453252],
            [0.52392883, 0.11731817, 0.35522789],
            [0.53058853, 0.11662445, 0.35585982],
            [0.53726173, 0.11587369, 0.35642903],
            [0.54394898, 0.11506307, 0.35693521],
            [0.5506426, 0.11420757, 0.35737863],
            [0.55734473, 0.11330456, 0.35775059],
            [0.56405586, 0.11235265, 0.35804813],
            [0.57077365, 0.11135597, 0.35827146],
            [0.5774991, 0.11031233, 0.35841679],
            [0.58422945, 0.10922707, 0.35848469],
            [0.59096382, 0.10810205, 0.35847347],
            [0.59770215, 0.10693774, 0.35838029],
            [0.60444226, 0.10573912, 0.35820487],
            [0.61118304, 0.10450943, 0.35794557],
            [0.61792306, 0.10325288, 0.35760108],
            [0.62466162, 0.10197244, 0.35716891],
            [0.63139686, 0.10067417, 0.35664819],
            [0.63812122, 0.09938212, 0.35603757],
            [0.64483795, 0.0980891, 0.35533555],
            [0.65154562, 0.09680192, 0.35454107],
            [0.65824241, 0.09552918, 0.3536529],
            [0.66492652, 0.09428017, 0.3526697],
            [0.67159578, 0.09306598, 0.35159077],
            [0.67824099, 0.09192342, 0.3504148],
            [0.684863, 0.09085633, 0.34914061],
            [0.69146268, 0.0898675, 0.34776864],
            [0.69803757, 0.08897226, 0.3462986],
            [0.70457834, 0.0882129, 0.34473046],
            [0.71108138, 0.08761223, 0.3430635],
            [0.7175507, 0.08716212, 0.34129974],
            [0.72398193, 0.08688725, 0.33943958],
            [0.73035829, 0.0868623, 0.33748452],
            [0.73669146, 0.08704683, 0.33543669],
            [0.74297501, 0.08747196, 0.33329799],
            [0.74919318, 0.08820542, 0.33107204],
            [0.75535825, 0.08919792, 0.32876184],
            [0.76145589, 0.09050716, 0.32637117],
            [0.76748424, 0.09213602, 0.32390525],
            [0.77344838, 0.09405684, 0.32136808],
            [0.77932641, 0.09634794, 0.31876642],
            [0.78513609, 0.09892473, 0.31610488],
            [0.79085854, 0.10184672, 0.313391],
            [0.7965014, 0.10506637, 0.31063031],
            [0.80205987, 0.10858333, 0.30783],
            [0.80752799, 0.11239964, 0.30499738],
            [0.81291606, 0.11645784, 0.30213802],
            [0.81820481, 0.12080606, 0.29926105],
            [0.82341472, 0.12535343, 0.2963705],
            [0.82852822, 0.13014118, 0.29347474],
            [0.83355779, 0.13511035, 0.29057852],
            [0.83850183, 0.14025098, 0.2876878],
            [0.84335441, 0.14556683, 0.28480819],
            [0.84813096, 0.15099892, 0.281943],
            [0.85281737, 0.15657772, 0.27909826],
            [0.85742602, 0.1622583, 0.27627462],
            [0.86196552, 0.16801239, 0.27346473],
            [0.86641628, 0.17387796, 0.27070818],
            [0.87079129, 0.17982114, 0.26797378],
            [0.87507281, 0.18587368, 0.26529697],
            [0.87925878, 0.19203259, 0.26268136],
            [0.8833417, 0.19830556, 0.26014181],
            [0.88731387, 0.20469941, 0.25769539],
            [0.89116859, 0.21121788, 0.2553592],
            [0.89490337, 0.21785614, 0.25314362],
            [0.8985026, 0.22463251, 0.25108745],
            [0.90197527, 0.23152063, 0.24918223],
            [0.90530097, 0.23854541, 0.24748098],
            [0.90848638, 0.24568473, 0.24598324],
            [0.911533, 0.25292623, 0.24470258],
            [0.9144225, 0.26028902, 0.24369359],
            [0.91717106, 0.26773821, 0.24294137],
            [0.91978131, 0.27526191, 0.24245973],
            [0.92223947, 0.28287251, 0.24229568],
            [0.92456587, 0.29053388, 0.24242622],
            [0.92676657, 0.29823282, 0.24285536],
            [0.92882964, 0.30598085, 0.24362274],
            [0.93078135, 0.31373977, 0.24468803],
            [0.93262051, 0.3215093, 0.24606461],
            [0.93435067, 0.32928362, 0.24775328],
            [0.93599076, 0.33703942, 0.24972157],
            [0.93752831, 0.34479177, 0.25199928],
            [0.93899289, 0.35250734, 0.25452808],
            [0.94036561, 0.36020899, 0.25734661],
            [0.94167588, 0.36786594, 0.2603949],
            [0.94291042, 0.37549479, 0.26369821],
            [0.94408513, 0.3830811, 0.26722004],
            [0.94520419, 0.39062329, 0.27094924],
            [0.94625977, 0.39813168, 0.27489742],
            [0.94727016, 0.4055909, 0.27902322],
            [0.94823505, 0.41300424, 0.28332283],
            [0.94914549, 0.42038251, 0.28780969],
            [0.95001704, 0.42771398, 0.29244728],
            [0.95085121, 0.43500005, 0.29722817],
            [0.95165009, 0.44224144, 0.30214494],
            [0.9524044, 0.44944853, 0.3072105],
            [0.95312556, 0.45661389, 0.31239776],
            [0.95381595, 0.46373781, 0.31769923],
            [0.95447591, 0.47082238, 0.32310953],
            [0.95510255, 0.47787236, 0.32862553],
            [0.95569679, 0.48489115, 0.33421404],
            [0.95626788, 0.49187351, 0.33985601],
            [0.95681685, 0.49882008, 0.34555431],
            [0.9573439, 0.50573243, 0.35130912],
            [0.95784842, 0.51261283, 0.35711942],
            [0.95833051, 0.51946267, 0.36298589],
            [0.95879054, 0.52628305, 0.36890904],
            [0.95922872, 0.53307513, 0.3748895],
            [0.95964538, 0.53983991, 0.38092784],
            [0.96004345, 0.54657593, 0.3870292],
            [0.96042097, 0.55328624, 0.39319057],
            [0.96077819, 0.55997184, 0.39941173],
            [0.9611152, 0.5666337, 0.40569343],
            [0.96143273, 0.57327231, 0.41203603],
            [0.96173392, 0.57988594, 0.41844491],
            [0.96201757, 0.58647675, 0.42491751],
            [0.96228344, 0.59304598, 0.43145271],
            [0.96253168, 0.5995944, 0.43805131],
            [0.96276513, 0.60612062, 0.44471698],
            [0.96298491, 0.6126247, 0.45145074],
            [0.96318967, 0.61910879, 0.45824902],
            [0.96337949, 0.6255736, 0.46511271],
            [0.96355923, 0.63201624, 0.47204746],
            [0.96372785, 0.63843852, 0.47905028],
            [0.96388426, 0.64484214, 0.4861196],
            [0.96403203, 0.65122535, 0.4932578],
            [0.96417332, 0.65758729, 0.50046894],
            [0.9643063, 0.66393045, 0.5077467],
            [0.96443322, 0.67025402, 0.51509334],
            [0.96455845, 0.67655564, 0.52251447],
            [0.96467922, 0.68283846, 0.53000231],
            [0.96479861, 0.68910113, 0.53756026],
            [0.96492035, 0.69534192, 0.5451917],
            [0.96504223, 0.7015636, 0.5528892],
            [0.96516917, 0.70776351, 0.5606593],
            [0.96530224, 0.71394212, 0.56849894],
            [0.96544032, 0.72010124, 0.57640375],
            [0.96559206, 0.72623592, 0.58438387],
            [0.96575293, 0.73235058, 0.59242739],
            [0.96592829, 0.73844258, 0.60053991],
            [0.96612013, 0.74451182, 0.60871954],
            [0.96632832, 0.75055966, 0.61696136],
            [0.96656022, 0.75658231, 0.62527295],
            [0.96681185, 0.76258381, 0.63364277],
            [0.96709183, 0.76855969, 0.64207921],
            [0.96739773, 0.77451297, 0.65057302],
            [0.96773482, 0.78044149, 0.65912731],
            [0.96810471, 0.78634563, 0.66773889],
            [0.96850919, 0.79222565, 0.6764046],
            [0.96893132, 0.79809112, 0.68512266],
            [0.96935926, 0.80395415, 0.69383201],
            [0.9698028, 0.80981139, 0.70252255],
            [0.97025511, 0.81566605, 0.71120296],
            [0.97071849, 0.82151775, 0.71987163],
            [0.97120159, 0.82736371, 0.72851999],
            [0.97169389, 0.83320847, 0.73716071],
            [0.97220061, 0.83905052, 0.74578903],
            [0.97272597, 0.84488881, 0.75440141],
            [0.97327085, 0.85072354, 0.76299805],
            [0.97383206, 0.85655639, 0.77158353],
            [0.97441222, 0.86238689, 0.78015619],
            [0.97501782, 0.86821321, 0.78871034],
            [0.97564391, 0.87403763, 0.79725261],
            [0.97628674, 0.87986189, 0.8057883],
            [0.97696114, 0.88568129, 0.81430324],
            [0.97765722, 0.89149971, 0.82280948],
            [0.97837585, 0.89731727, 0.83130786],
            [0.97912374, 0.90313207, 0.83979337],
            [0.979891, 0.90894778, 0.84827858],
            [0.98067764, 0.91476465, 0.85676611],
            [0.98137749, 0.92061729, 0.86536915]]
        return plt.matplotlib.colors.LinearSegmentedColormap.from_list(
            'rocket_r', rocket_colors[::-1])
    
    # noinspection PyUnresolvedReferences
    def plot_heatmap(self,
                     x: SingleCellColumn,
                     y: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     cells_to_plot_column: SingleCellColumn |
                                           None = 'passed_QC',
                     normalize_rows: bool = False,
                     normalize_columns: bool = False,
                     ax: 'Axes' | None = None,
                     figure_kwargs: dict[str, Any] | None = None,
                     colormap: str | 'Colormap' | None = None,
                     heatmap_kwargs: dict[str, Any] | None = None,
                     label: bool = False,
                     label_format: str | None = None,
                     label_kwargs: dict[str, Any] | None = None,
                     colorbar: bool = True,
                     colorbar_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | Literal[True] | None = True,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | Literal[True] | None = True,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot a heatmap of the count of each combination of two categorical
        columns, `x` and `y`. If `normalize_rows` or `normalize_columns` is
        specified, plot percentages instead of counts, so that each row or
        column sums to 100%.
        
        Args:
            x: the first column; must be String, Enum or Categorical
            y: the second column; must be String, Enum or Categorical
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            normalize_rows: whether to plot percentages instead of counts, so
                            that each row sums to 100%. Mutually exclusive with
                            `normalize_columns`.
            normalize_columns: whether to plot percentages instead of counts,
                               so that each column sums to 100%. Mutually
                               exclusive with `normalize_rows`.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. The default is a
                             complicated formula based on the number of genes
                             and cell types being plotted, unlike Matplotlib's
                             default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in the heatmap, or `None` to use
                      Seaborn's `'rocket_r'` colormap.
            heatmap_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.pcolormesh()` when generating the heatmap, such
                            as:
                            - `rasterized`: whether to convert the heatmap
                              cells to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `norm`, `vmin`, and `vmax`: control how the
                              colormap maps counts or percentages to heatmap
                              colors
                            - `edgecolors`: the border color of each heatmap
                              cell; defaults to `'none'`, meaning no borders.
                              Specifying `cmap` will raise an error, since it
                              conflicts with the `colormap` argument.
            label: whether to label each cell of the heatmap with its count
                   (or percentage, if `normalize_rows=True` or
                    `normalize_columns=True`)
            label_format: a format string to apply to the label for each count
                          or percentage. If `None`, use `'{:,}'` for counts and
                          `'{:.2f}%'` for percentages. Can only be specified
                          when `label=True`.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size.
                             By default, the color is dark gray for
                             light-colored cells, and white for dark-colored
                             ones.
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                          Can only be specified when `label=True`.
            colorbar: whether to add a colorbar
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, `True` to use the name of `x` as the
                    x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, `True` to use the name of `y` as the
                    y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the spines (borders of the plot area)
                     from the plot; unlike the other plotting functions in this
                     library, this also removes the left and bottom spines
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Get `cells_to_plot_column`, if not `None`
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        # Get `x` and `y`, and check that they are String, Enum, or Categorical
        x = self._get_column('obs', x, 'x',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        y = self._get_column('obs', y, 'y',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        if cells_to_plot_column is not None:
            x = x.filter(cells_to_plot_column)
            y = y.filter(cells_to_plot_column)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `normalize_rows`, `normalize_columns`, `label`,
        # `colorbar`, and `despine` are Boolean
        check_type(normalize_rows, 'normalize_rows', bool, 'Boolean')
        check_type(normalize_columns, 'normalize_columns', bool, 'Boolean')
        check_type(label, 'label', bool, 'Boolean')
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # Check that `normalize_rows` and `normalize_columns` are mutually
        # exclusive
        if normalize_rows and normalize_columns:
            error_message = \
                'only one of normalize_rows and normalize_columns can be True'
            raise ValueError(error_message)
        # If `figure_kwargs` is not `None`, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        # Check that `colormap` is a string in `plt.colormaps`, a Colormap
        # object, or `None`; if `None`, default to Seaborn's rocket_r colormap
        if colormap is None:
            colormap = SingleCell._get_rocket_r()
        else:
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap),
                       'a string or matplotlib Colormap object')
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
        # If `label=False`, check that `label_format` and `label_kwargs` are
        # `None`.
        if not label:
            if label_format is not None:
                error_message = 'label_format must be None when label=False'
                raise ValueError(error_message)
            if label_kwargs is not None:
                error_message = 'label_kwargs must be None when label=False'
                raise ValueError(error_message)
        # If not `None`, check that `label_format` is a valid format string.
        # For simplicity, just check that it has curly braces and that all
        # braces are matched. If `label_format` is `None`, use `'{:,}'` for
        # counts or, if `normalize_rows=True` or `normalize_columns=True`,
        # `'{:.2f}%'` for percentages.
        if label_format is None:
            label_format = \
                '{:.2f}%' if normalize_rows or normalize_columns else '{:,}'
        else:
            check_type(label_format, 'label_format', str, 'a string')
            open_braces = 0
            has_braces = False
            for char in label_format:
                if char == '{':
                    has_braces = True
                    open_braces += 1
                elif char == '}':
                    open_braces -= 1
                if open_braces < 0:
                    error_message = \
                        'label_format contains mismatched curly braces'
                    raise ValueError(error_message)
            if open_braces == 0:
                error_message = 'label_format contains mismatched curly braces'
                raise ValueError(error_message)
            if not has_braces:
                error_message = 'label_format must contain curly braces'
                raise ValueError(error_message)
        # If `colorbar=False`, check that `colorbar_kwargs` is None
        if not colorbar and colorbar_kwargs is not None:
            error_message = 'colorbar_kwargs must be None when colorbar=False'
            raise ValueError(error_message)
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well.
        if title is not None:
            check_type(title, 'title', str, 'a string')
        elif title_kwargs is not None:
            error_message = 'title_kwargs must be None when title is None'
            raise ValueError(error_message)
        # Check that `xlabel` is a string, `True` (in which case set it to
        # `x.name`), or `None`; if `None`, check that `xlabel_kwargs` is `None`
        # as well. Ditto for `ylabel`.
        if xlabel is not None:
            if xlabel is True:
                xlabel = x.name
            else:
                check_type(xlabel, 'xlabel', str, 'a string')
        elif xlabel_kwargs is not None:
            error_message = 'xlabel_kwargs must be None when xlabel is None'
            raise ValueError(error_message)
        if ylabel is not None:
            if ylabel is True:
                ylabel = y.name
            else:
                check_type(ylabel, 'ylabel', str, 'a string')
        elif ylabel_kwargs is not None:
            error_message = 'ylabel_kwargs must be None when ylabel is None'
            raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (heatmap_kwargs, 'heatmap_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Override the defaults for certain values of `heatmap_kwargs`; if
        # specified, check that `heatmap_kwargs` does not contain the `cmap`
        # argument
        default_heatmap_kwargs = dict(rasterized=True)
        if heatmap_kwargs is None:
            heatmap_kwargs = default_heatmap_kwargs
        else:
            if 'cmap' in heatmap_kwargs:
                error_message = (
                    f"'cmap' cannot be specified as a key in heatmap_kwargs; "
                    f"specify the colormap argument instead")
                raise ValueError(error_message)
            heatmap_kwargs = heatmap_kwargs | default_heatmap_kwargs
        # Get the heatmap data
        count = pl.DataFrame((x, y))\
            .group_by(pl.all(), maintain_order=True)\
            .len(name='_SingleCell_count')\
            .pivot(index=y.name, columns=x.name, values='_SingleCell_count')\
            .fill_null(0)
        heatmap_data = count[:, 1:].to_numpy()
        # Normalize, if `normalize_rows=True` or `normalize_columns=True`
        if normalize_rows:
            heatmap_data = \
                heatmap_data / heatmap_data.sum(axis=1, keepdims=True)
        elif normalize_columns:
            heatmap_data = \
                heatmap_data / heatmap_data.sum(axis=0, keepdims=True)
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            num_rows, num_columns = heatmap_data.shape
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                if figure_kwargs is None or 'figsize' not in figure_kwargs:
                    if colorbar:
                        width_ratio = max(4, 0.2 * num_columns)
                        width = 6.4 / (1 + width_ratio) + \
                                6.4 * width_ratio / (1 + width_ratio) * \
                                max(num_columns, 5) / 20
                    else:
                        width = 6.4 * max(num_columns, 5) / 20
                    height = max(4.8, 1 + 3.8 * num_rows / 20)
                    default_figure_kwargs['figsize'] = width, height
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make the heatmap
            xticks = np.arange(0.5, num_columns)
            yticks = np.arange(0.5, num_rows)
            heatmap = \
                ax.pcolormesh(heatmap_data, cmap=colormap, **heatmap_kwargs)
            ax.set_xticks(xticks, count.columns[1:], rotation=90)
            ax.set_yticks(yticks, count[:, 0].to_numpy())
            ax.set_aspect('equal')
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`. If normalizing rows or columns, make the
            # colorbar ticks percentages.
            if colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(heatmap, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
                if normalize_rows or normalize_columns:
                    cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(
                        lambda x, pos: f'{100 * x:.0f}%'))
            # Add labels; this code is edited from `_annotate_heatmap()` at
            # github.com/mwaskom/seaborn/blob/master/seaborn/matrix.py
            if label:
                heatmap.update_scalarmappable()
                xpos, ypos = np.meshgrid(xticks, yticks)
                if label_kwargs is None:
                    label_kwargs = {}
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')))
                if 'c' in label_kwargs or 'color' in label_kwargs:
                    # Use the same color for all labels
                    for x, y, val in zip(xpos.ravel(), ypos.ravel(),
                                         heatmap_data.ravel()):
                        ax.text(x, y, s=label_format.format(val),
                                **label_kwargs)
                else:
                    # Use either dark gray or white for the label, depending on
                    # the cell's luminance
                    rgb_weights = np.array([0.2126, 0.7152, 0.0722])
                    for x, y, color, val in zip(
                            xpos.ravel(), ypos.ravel(),
                            heatmap.get_facecolors(), heatmap_data.ravel()):
                        rgb = plt.matplotlib.colors.colorConverter\
                            .to_rgba_array(color)[:, :3]
                        rgb = np.where(rgb <= 0.03928, rgb / 12.92,
                                       ((rgb + 0.055) / 1.055) ** 2.4)
                        lum = rgb.dot(rgb_weights).item()
                        ax.text(x, y, s=label_format.format(val),
                                c='.15' if lum > .408 else 'w', **label_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                for direction in 'top', 'bottom', 'left', 'right':
                    spines[direction].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    def find_markers(self,
                     cell_type_column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     min_detection_rate: int | float | np.integer |
                                         np.floating = 0.25,
                     min_fold_change: int | float | np.integer |
                                      np.floating = 2,
                     pareto: bool = True,
                     all_genes: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            pl.DataFrame:
        """
        Find "marker genes" that distinguish each cell type from all other cell
        types. This function gives the same result regardless of whether it is
        run before or after normalization.
        
        Marker genes are chosen via an adaptation of the strategy of Fischer
        and Gillis 2021 (ncbi.nlm.nih.gov/pmc/articles/PMC8571500). For a given
        cell type, genes are scored based on a) their "detection rate" in that
        cell type (the fraction of cells of that type that have non-zero count
        for that gene), as well as b) the fold change in detection rate between
        that cell type and every other cell type. Genes must also have a
        detection rate of at least `min_detection_rate` (25% by default) and a
        minimum fold change of at least `min_fold_change` (2-fold by default)
        to be considered as markers.
        
        There is an inherent tradeoff between these two metrics. For instance,
        candidate marker genes with high enough expression to be expressed in
        every cell of a given type (i.e. to have a high detection rate) tend to
        also have at least some expression in other cell types (i.e. a low fold
        change in detection rate).
        
        Thus, marker genes are selected to optimally trade off between these
        two metrics: all genes on the Pareto front of the two metrics (i.e.
        genes for which there is no other gene that does better on both metrics
        simultaneously) are selected as marker genes.
        
        Note that Fischer and Gillis use AUROC versus log2 fold change in
        detection rate, instead of detection rate versus fold change in
        detection rate. However, detection rate is much faster to compute than
        AUROC, and is a very accurate proxy for AUROC: as Figure 1D in their
        paper shows, AUROC is almost perfectly correlated with detection rate
        across marker genes.
        
        Args:
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
            min_detection_rate: the minimum detection rate required to select
                                a gene as a marker gene; must be positive and
                                less than or equal to 1
            min_fold_change: the minimum fold change in detection rate required
                             to select a gene as a marker gene; must be greater
                             than 1
            pareto: if `True`, include only genes on the Pareto front of
                    detection rate and fold change as markers; if `False`,
                    include all genes that pass the `min_detection_rate` and
                    `min_fold_change` thresholds as markers
            all_genes: if `True`, include all genes in the output, not just
                       marker genes. An additional Boolean column will be
                       included to specify which genes are the marker genes.
                       Note that this option does not change which marker genes
                       are selected, only which information is returned.
            num_threads: the number of threads to use for marker-gene finding.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). For count matrices stored in the usual CSR
                         format, the most time-consuming step (calculating
                         detection counts of each gene in each cell type) is
                         parallelized across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.
        
        Returns:
            By default, a DataFrame with one row per marker gene, with columns:
            - `'cell_type'`: a cell-type name from `cell_type_column`
            - `'gene'`: a gene symbol from `var_names`
            - `'detection_rate'`: the gene's detection rate in that cell type
            - `'fold_change'`, the gene's fold change in detection rate
              between that cell type and all other cell types
            If `all_genes=True`, a DataFrame with one row per cell type-gene
            pair, with those four columns plus one other:
            - `'marker'`, a Boolean column listing whether the gene is a marker
              for that cell type
            If `all_genes=False`, marker genes within each cell type will be
            sorted in increasing order of detection rate, and decreasing order
            of fold change.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        X = self._X
        if X is None:
            error_message = 'X is None, so marker gene finding is not possible'
            raise TypeError(error_message)
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        cell_type_column_name = cell_type_column.name
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Check that `min_detection_rate` and `min_fold_change` are numeric and
        # have the correct ranges: 0 < min_detection_rate <= 1,
        # min_fold_change > 1
        check_type(min_detection_rate, 'min_detection_rate',
                   (int, float), 'a positive number less than or equal to 1')
        check_bounds(min_detection_rate, 'min_detection_rate', 0, 1,
                     left_open=True)
        check_type(min_fold_change, 'min_fold_change', (int, float),
                   'a number greater than 1')
        check_bounds(min_fold_change, 'min_fold_change', 1, left_open=True)
        # Check that `all_genes` is Boolean
        check_type(all_genes, 'all_genes', bool, 'Boolean')
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene, i.e. the gene's detection
        # count in that cell type
        num_cell_types = len(groups)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = X.shape[1]
        detection_count = np.zeros((num_cell_types, num_genes),
                                   dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csr(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned group, cell, num_groups = group_ends.shape[0]
                    cdef int row
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse array
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    # Add 1 to the total for this group and gene
                                    result[group, indices[gene]] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    result[group, indices[gene]] += 1
            ''')['groupby_getnnz_csr'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                result=detection_count, num_threads=num_threads)
        else:
            # noinspection PyUnresolvedReferences
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = QC_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            # noinspection PyUnresolvedReferences
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csc(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_map,
                        const bint has_missing,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned column, num_columns = result.shape[1]
                    cdef int group
                    cdef unsigned long row
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene (column of the sparse array)...
                            for column in range(num_columns):
                                # For each cell (row) that's non-zero for this gene...
                                for row in range(<unsigned long> indptr[column], <unsigned long> indptr[column + 1]):
                                    # Get the group index for this row (-1 if it failed
                                    # QC)
                                    group = group_map[indices[row]]
                                    if group == -1: continue
                                    # Add 1 to the total for this group and column
                                    result[group, column] += 1
                        else:
                            for column in range(num_columns):
                                for row in range(<unsigned long> indptr[column], <unsigned long> indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    result[group, column] += 1
                    else:
                        if has_missing:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                for row in range(<unsigned long> indptr[column], <unsigned long> indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    if group == -1: continue
                                    result[group, column] += 1
                        else:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                for row in range(<unsigned long> indptr[column], <unsigned long> indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    result[group, column] += 1
                    ''')['groupby_getnnz_csc'](
                        indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=detection_count, num_threads=num_threads)
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.uint32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes), dtype=float)
        fold_change = np.empty((num_cell_types, num_genes), dtype=float)
        is_pareto = np.empty((num_cell_types, num_genes), dtype=bool)
        cython_inline(r'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const unsigned[:, ::1] detection_count,
                    const unsigned[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    const double min_detection_rate,
                    const double min_fold_change,
                    double[:, ::1] detection_rate,
                    double[:, ::1] fold_change,
                    char[:, ::1] is_pareto,
                    const unsigned num_threads):
                cdef unsigned cell_type, gene, count, background_count, \
                    num_cells, background_num_cells, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef double pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells,
                min_detection_rate=min_detection_rate,
                min_fold_change=min_fold_change, detection_rate=detection_rate,
                fold_change=fold_change, is_pareto=is_pareto,
                num_threads=num_threads)
        # If `pareto=True`, get the set of genes on the Pareto front of the two
        # metrics for each cell type; these are the marker genes.
        # If `pareto=False`, just include the genes we've initialized
        # `is_pareto` to `True` for: all those passing the `min_detection_rate`
        # and `min_fold_change` thresholds.
        if pareto:
            cython_inline(r'''
                from cython.parallel cimport prange
                
                def pareto_front(double[:, ::1] detection_rate,
                                 double[:, ::1] fold_change,
                                 char[:, ::1] is_pareto,
                                 unsigned num_threads):
                    cdef unsigned gene, other_gene, cell_type, \
                        num_cell_types = detection_count.shape[0], \
                        num_genes = detection_count.shape[1]
                    cdef double gene_detection_rate, gene_fold_change
                    
                    if num_threads == 1:
                        for cell_type in range(num_cell_types):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                    else:
                        for cell_type in prange(num_cell_types, nogil=True,
                                                num_threads=num_threads):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                ''')['pareto_front'](
                    detection_rate=detection_rate, fold_change=fold_change,
                    is_pareto=is_pareto, num_threads=num_threads)
        # Return a DataFrame of the selected marker genes, or all genes if
        # `all_genes=True`
        cell_types = groups[cell_type_column_name].rename('cell_type')
        genes = self._var[:, 0].rename('gene')
        if all_genes:
            cell_types = pl.select(pl.lit(cell_types).repeat_by(num_genes))\
                .explode('cell_type')\
                .to_series()
            genes = pl.concat([genes] * num_cell_types)
            return pl.DataFrame((
                cell_types, genes,
                pl.Series('marker', is_pareto.ravel()),
                pl.Series('detection_rate', detection_rate.ravel()),
                pl.Series('fold_change', fold_change.ravel())))
        else:
            cell_type_indices, gene_indices = is_pareto.nonzero()
            return pl.DataFrame((
                cell_types[cell_type_indices], genes[gene_indices],
                pl.Series('detection_rate', detection_rate[
                    cell_type_indices, gene_indices].ravel()),
                pl.Series('fold_change', fold_change[
                    cell_type_indices, gene_indices].ravel())))\
                .select(pl.all().sort_by('detection_rate').over('cell_type'))
    
    # noinspection PyUnresolvedReferences
    def plot_markers(self,
                     genes: str | Iterable[str],
                     cell_type_column: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     cells_to_plot_column: SingleCellColumn |
                                           None = 'passed_QC',
                     figure_kwargs: dict[str, Any] | None = None,
                     colormap: str | 'Colormap' = 'RdBu_r',
                     colorbar: bool = True,
                     colorbar_kwargs: dict[str, Any] | None = None,
                     swap_axes: bool = False,
                     scatter_kwargs: dict[str, Any] | None = None,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = None,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = None,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None,
                     num_threads: int | np.integer | None = None) -> None:
        """
        Make a dot plot of the detection rate and fold change of a set of genes
        across cell types - the same metrics used to select marker genes in
        `find_markers()`. This function gives the same result regardless of
        whether it is run before or after normalization.
        
        The size of a gene's dot for a cell type represents its
        "detection rate" in that cell type (the fraction of cells of that type
        that have non-zero count for that gene), while its color represents the
        gene's fold change in detection rate between that cell type and every
        other cell type (by default: more positive fold changes are redder,
        while more negative fold changes are bluer).
        
        Unlike the other plotting functions, this is a figure-level rather than
        an axis-level function, and does not take an `axis` argument.
        
        Args:
            genes: a list of genes to plot: for instance, marker genes found by
                   `find_markers()`, or marker genes from the literature
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. The default is a
                             complicated formula based on the number of genes
                             and cell types being plotted, unlike Matplotlib's
                             default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in `ax.scatter()` for representing fold
                      changes
            colorbar: whether to add a colorbar
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `colorbar=True`.
            swap_axes: if `True`, plot genes on the y-axis and cell types on
                       the x-axis, instead of the other way around
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since the size and color of each point
                            are set automatically, and `cmap` conflicts with
                            the `colormap` argument.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. The legend will be placed in its
                             own axis in the top right of the plot, and by
                             default, `loc` is set to `'center'`.
                           - `ncols` to set its number of columns
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border. `frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`.
                           - `title` to modify the legend title. Defaults to
                             `'Detection rate'`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`), instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
            num_threads: the number of threads to use when tabulating each
                         gene's detection rate and fold change. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). For count matrices stored in the usual CSR
                         format, the most time-consuming step (calculating
                         detection counts of each gene in each cell type) is
                         parallelized across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        import matplotlib.pyplot as plt
        X = self._X
        if X is None:
            error_message = \
                'X is None, so marker gene plotting is not possible'
            raise TypeError(error_message)
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "plot_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get `genes` as a polars Series of the same dtype as `var_names`; make
        # sure all its entries are unique and present in `var_names`
        genes = to_tuple_checked(genes, 'genes', str, 'strings')
        genes = pl.Series(genes)
        if genes.n_unique() < len(genes):
            error_message = 'genes contains duplicates'
            raise ValueError(error_message)
        var_names = self._var[:, 0]
        if not genes.is_in(var_names).all():
            if not genes.is_in(var_names).any():
                error_message = \
                    'none of the specified genes were found in var_names'
                raise ValueError(error_message)
            else:
                for gene in genes:
                    if gene not in var_names:
                        error_message = (
                            f'one of the specified genes, {gene!r}, was not '
                            f'found in var_names')
                        raise ValueError(error_message)
        if var_names.dtype != pl.String:
            genes = genes.cast(var_names.dtype)
        # Get `cells_to_plot_column`, if not `None`
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        cell_type_column_name = cell_type_column.name
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `colormap` is a string in `plt.colormaps` or a Colormap
        # object
        check_type(colormap, 'colormap', (str, plt.matplotlib.colors.Colormap),
                   'a string or matplotlib Colormap object')
        if isinstance(colormap, str):
            colormap = plt.colormaps[colormap]
        # Check that `colorbar` is Boolean
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        # If `colorbar=False`, check that `colorbar_kwargs` is None
        if not colorbar and colorbar_kwargs is not None:
            error_message = 'colorbar_kwargs must be None when colorbar=False'
            raise ValueError(error_message)
        # Check that `swap_axes` and `despine` are Boolean
        check_type(swap_axes, 'swap_axes', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if cells_to_plot_column is None else
                  pl.LazyFrame((cell_type_column, cells_to_plot_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(cells_to_plot_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        cell_types = groups[cell_type_column_name]
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene in `genes`, i.e. the gene's
        # detection count in that cell type
        num_cell_types = len(cell_types)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = len(genes)
        detection_count = np.zeros((num_cell_types, num_genes),
                                   dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            # Get an array mapping each gene in `var_names` to its position in
            # `genes` (-1 if missing from `genes`)
            gene_map = var_names\
                .to_frame()\
                .join(genes
                      .to_frame(var_names.name)
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.Int32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()
            gene_map = gene_map.fill_null(-1).to_numpy()
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csr(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        const int[::1] gene_map,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned cell, group, num_groups = group_ends.shape[0]
                    cdef int row, column
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse array
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    # Get this gene's column index in `result`
                                    # (-1 if the gene is not in `genes`)
                                    column = gene_map[indices[gene]]
                                    if column == -1: continue
                                    # Add 1 to the total for this group and gene
                                    result[group, column] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row], <unsigned long> indptr[row + 1]):
                                    column = gene_map[indices[gene]]
                                    if column == -1: continue
                                    result[group, column] += 1
            ''')['groupby_getnnz_csr'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                gene_map=gene_map, result=detection_count,
                num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = cells_to_plot_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            # Get an array mapping each gene in `genes` to its position in
            # `var_names`
            gene_map = genes\
                .to_frame(var_names.name)\
                .join(var_names
                      .to_frame()
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.UInt32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()\
                .to_numpy()
            cython_inline('''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csc(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const int[::1] group_map,
                        const unsigned[::1] gene_map,
                        const bint has_missing,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned column, gene, num_columns = result.shape[1]
                    cdef int group
                    cdef unsigned long cell
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene...
                            for column in range(num_columns):
                                # Get the index of this gene in the count matrix
                                gene = gene_map[column]
                                # For each cell (row) that's non-zero for this gene...
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    # Get the group index for this cell (-1 if it
                                    # failed QC)
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    # Add 1 to the total for this group and gene
                                    result[group, column] += 1
                        else:
                            for column in range(num_columns):
                                gene = gene_map[column]
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, column] += 1
                    else:
                        if has_missing:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                gene = gene_map[column]
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    result[group, column] += 1
                        else:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                gene = gene_map[column]
                                for cell in range(<unsigned long> indptr[gene], <unsigned long> indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, column] += 1
                    ''')['groupby_getnnz_csc'](
                        indices=X.indices, indptr=X.indptr,
                        group_map=group_map, gene_map=gene_map,
                        has_missing=has_missing, result=detection_count,
                        num_threads=num_threads)
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.uint32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes), dtype=float)
        fold_change = np.empty((num_cell_types, num_genes), dtype=float)
        cython_inline(r'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const unsigned[:, ::1] detection_count,
                    const unsigned[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    double[:, ::1] detection_rate,
                    double[:, ::1] fold_change,
                    const unsigned num_threads):
                cdef unsigned cell_type, gene, count, background_count, \
                    num_cells, background_num_cells, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef double pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells, detection_rate=detection_rate,
                fold_change=fold_change, num_threads=num_threads)
        
        # If `swap_axes=True`, swap cell types and genes
        if swap_axes:
            cell_types, genes = genes, cell_types
            num_cell_types, num_genes = num_genes, num_cell_types
            detection_rate = detection_rate.T
            fold_change = fold_change.T
        
        # Calculate the range of the legend, and the multiplier to multiply
        # each point's size by
        max_detection_rate = detection_rate.max()
        interval = 0.2 if max_detection_rate > 0.5 else \
            0.1 if max_detection_rate > 0.2 else 0.05
        max_detection_rate = \
            np.ceil(max_detection_rate / interval) * interval
        legend_point_sizes = \
            np.arange(interval, max_detection_rate + interval / 2, interval)
        point_size_multiplier = 180 / max_detection_rate
        
        try:
            # Make the figure, including separate portions on the left for the
            # legend and colorbar (if `colorbar=True`)
            default_figure_kwargs = dict(layout='constrained')
            # noinspection PyTypeChecker
            width_ratio = max(4, 0.2 * num_genes)
            if figure_kwargs is None or 'figsize' not in figure_kwargs:
                if colorbar:
                    width = 6.4 / (1 + width_ratio) + \
                            6.4 * width_ratio / (1 + width_ratio) * \
                            max(num_genes, 5) / 20
                else:
                    width = 6.4 * max(num_genes, 5) / 20
                height = max(4.8, 1 + 3.8 * num_cell_types / 20)
                default_figure_kwargs['figsize'] = width, height
            figure_kwargs = default_figure_kwargs | figure_kwargs \
                if figure_kwargs is not None else default_figure_kwargs
            fig = plt.figure(**figure_kwargs)
            if colorbar:
                gs = fig.add_gridspec(2, 2, width_ratios=[width_ratio, 1],
                                      height_ratios=[1, 1])
            else:
                gs = fig.add_gridspec(2, 1, height_ratios=[1, 1])
            # Plot the circles; override the defaults for certain keys of
            # `scatter_kwargs`
            ax_main = fig.add_subplot(gs[:, 0])  # the main plot spans all rows
            point_size = detection_rate * point_size_multiplier
            x, y = np.meshgrid(range(len(genes)), range(len(cell_types)))
            default_scatter_kwargs = dict(linewidths=0)
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
            scatter = ax_main.scatter(
                x.ravel(), y.ravel(), s=point_size.ravel(),
                c=np.log2(fold_change), cmap=colormap, **scatter_kwargs)
            ax_main.set_aspect('equal')
            ax_main.invert_yaxis()
            
            # Set x and y limits
            padding = 0.6
            ax_main.set_xlim([-padding, len(genes) - 1 + padding])
            ax_main.set_ylim([-padding, len(cell_types) - 1 + padding])
            
            # Add x and y ticks and tick labels
            ax_main.set_xticks(range(len(genes)), genes, rotation=90)
            ax_main.set_yticks(range(len(cell_types)), cell_types)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax_main.set_xlabel(xlabel)
                else:
                    ax_main.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax_main.set_ylabel(ylabel)
                else:
                    ax_main.set_ylabel(ylabel, **ylabel_kwargs)
            
            # Add a legend for detection rate; markers should be at intervals
            # of `X`% (`X`%, `2X`%, `3X`%, ...) up to the maximum detection
            # rate (rounded up to the nearest `X`%). Override the defaults for
            # certain keys of `legend_kwargs`.
            ax_legend = fig.add_subplot(gs[0, 1])
            ax_legend.axis('off')
            legend_elements = [
                plt.Line2D([0], [0], label=f'{100 * size:.0f}%',
                           markersize=np.sqrt(size * point_size_multiplier),
                           marker='o', linestyle='None',
                           markerfacecolor='black', markeredgecolor='None')
                for size in legend_point_sizes]
            default_legend_kwargs = dict(title='Detection rate', loc='center',
                                         frameon=False)
            legend_kwargs = default_legend_kwargs | legend_kwargs \
                if legend_kwargs is not None else default_legend_kwargs
            ax_legend.legend(handles=legend_elements, **legend_kwargs)
            
            # Add a colorbar for fold change, with labels at powers of 2
            if colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else \
                    default_colorbar_kwargs
                ax_colorbar = fig.add_subplot(gs[1, 1])
                cbar = plt.colorbar(scatter, cax=ax_colorbar,
                                    **colorbar_kwargs)
                cbar.outline.set_visible(False)
                cbar.ax.set_box_aspect(12)
                cbar.ax.set_title('Fold change', size='medium')
                cbar.ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))
                cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(
                    lambda x, pos: f'{2 ** x:.4f}'.rstrip('0').rstrip('.')))
                
            # Add the title
            if title is not None:
                if title_kwargs is None:
                    ax_main.set_title(title)
                else:
                    ax_main.set_title(title, **title_kwargs)
        
            # Despine, if specified
            if despine:
                spines = ax_main.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                plt.close()
        except:
            # Since we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            plt.close()
            raise
    
    def embed(self,
              *,
              QC_column: SingleCellColumn | None = 'passed_QC',
              PC_key: str = 'PCs',
              neighbors_key: str = 'neighbors',
              embedding_key: str = 'PaCMAP',
              num_neighbors: int | np.integer = 10,
              num_extra_neighbors: int | np.integer = 10,
              num_mid_near_pairs: int | np.integer = 5,
              num_further_pairs: int | np.integer = 20,
              num_iterations: int | np.integer |
                              tuple[int | np.integer, int | np.integer,
                                    int | np.integer] | None = None,
              learning_rate: int | float | np.integer | np.floating = 1.0,
              seed: int | np.integer | None = None,
              faster_single_threading: bool = False,
              overwrite: bool = False,
              verbose: bool = True,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate a two-dimensional embedding of this SingleCell dataset
        suitable for plotting with `plot_embedding()`.
        
        Uses PaCMAP (Pairwise Controlled Manifold Approximation;
        github.com/YingfanWang/PaCMAP; arxiv.org/pdf/2012.04456), a faster
        alternative to UMAP that also captures global structure better.
        
        This function is intended to be run after `PCA()` and `neighbors()`; by
        default, it uses `obsm['PCs']` and `obsm['neighbors']` as the inputs to
        PaCMAP, and stores the output in `obsm['PaCMAP']` as a `len(obs)` Ã— 2
        NumPy array. It can also be run on Harmony embeddings by running
        `harmonize()` and then specifying `PC_key='Harmony_PCs'`.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their embeddings set to `NaN`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as an input for the
                    embedding calculation. Can also be set to the Harmony
                    embeddings calculated by `harmonize()`, by specifying
                    `PC_key='Harmony_PCs'`.
            neighbors_key: the key of `obsm` containing the nearest-neighbor
                           indices for each cell, to use as an input for the
                           embedding calculation
            embedding_key: the key of `obsm` where the embeddings will be
                           stored
            num_neighbors: the number of nearest neighbors to consider for
                           local structure preservation. `neighbors_key` must
                           contain at least
                           `num_neighbors + num_extra_neighbors` nearest
                           neighbors.
            num_extra_neighbors: the number of extra nearest neighbors (on top
                                 of `num_neighbors`) to search for initially,
                                 before pruning to the `num_neighbors` of these
                                 `num_neighbors + num_extra_neighbors` cells
                                 with the smallest scaled distances. For a pair
                                 of cells `i` and `j`, the scaled distance
                                 between `i` and `j` is its squared Euclidean
                                 distance, divided by `i`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors, divided by `j`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors. Must be a positive integer or 0.
                                 Defaults to 10, instead of PaCMAP's original
                                 default of 50. `neighbors_key` must contain at
                                 least `num_neighbors + num_extra_neighbors`
                                 nearest neighbors.
            num_mid_near_pairs: the number of mid-near pairs to consider for
                                global structure preservation
            num_further_pairs: the number of further pairs to consider for
                               local and global structure preservation
            num_iterations: the number of iterations/epochs to run PaCMAP for.
                            Can be a length-3 tuple of the number of iterations
                            for each of the 3 stages of PaCMAP, or a single
                            integer of the number of iterations for the third
                            stage (in which case the number of iterations for
                            the first two stages will be set to 100).
            learning_rate: the learning rate of the Adam optimizer for PaCMAP
            seed: the random seed to use for PaCMAP, or leave unset to use
                  `single_cell.options()['seed']` as the seed (0 by default)
            faster_single_threading: if `True`, use a different order of
                                     operations for single-threaded PaCMAP,
                                     which gives a modest (~15%) boost in
                                     single-threaded performance at the cost of
                                     no longer matching the embedding produced
                                     by the multithreaded version (due to
                                     differences in floating-point round-off
                                     arising from the different order of
                                     operations). Must be `False` unless
                                     `num_threads=1`.
            overwrite: if `True`, overwrite `embedding_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the PaCMAP construction
            num_threads: the number of threads to run PaCMAP on. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default).
        
        Returns:
            A new SingleCell dataset with the PaCMAP embedding stored in
            `obsm[embedding_key]`.
        
        Note:
            PaCMAP's original implementation assumes generic input data, so it
            initializes the embedding by standardizing the input data, running
            PCA on it, and taking the first two PCs. Because our input data is
            already PCs (or harmonized PCs), we avoid redundancy by omitting
            this step and initializing the embedding with the first two columns
            of our input data, i.e. the first two PCs.
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get PCs, and check that they are float64 and C-contiguous
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = \
                f'PC_key {PC_key!r} is not a key of obsm'
            if neighbors_key == 'PCs':
                error_message += (
                    '; did you forget to run PCA() (and neighbors()) before '
                    'embed()?')
            raise ValueError(error_message)
        PCs = self._obsm[PC_key]
        if PCs.dtype != float:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float64'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
            raise ValueError(error_message)
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += \
                    '; did you forget to run neighbors() before embed()?'
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}] must have integer data type, but '
                f'has data type {str(nearest_neighbor_indices.dtype)!r}')
            raise TypeError(error_message)
        # Subset PCs and nearest-neighbor indices to QCed cells only, if
        # `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `embedding_key` is a string and, unless `overwrite=True`,
        # not already a key in `obsm`
        check_type(embedding_key, 'embedding_key', str, 'a string')
        if not overwrite and embedding_key in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is already a key of obsm; '
                f'did you already run embed()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Check that `num_neighbors`, `num_mid_near_pairs` and
        # `num_further_pairs` are positive integers
        for variable, variable_name in (
                (num_neighbors, 'num_neighbors'),
                (num_mid_near_pairs, 'num_mid_near_pairs'),
                (num_further_pairs, 'num_further_pairs')):
            check_type(variable, variable_name, int, 'a positive integer')
            check_bounds(variable, variable_name, 1)
        # Check that `num_extra_neighbors` is a positive integer or 0
        check_type(num_extra_neighbors, 'num_extra_neighbors', int,
                   'a positive integer or 0')
        check_bounds(num_extra_neighbors, 'num_extra_neighbors', 0)
        # Check that `num_iterations` is an integer or length-3 tuple of
        # integers, or `None`
        if num_iterations is not None:
            check_type(num_iterations, 'num_iterations', (int, tuple),
                       'a positive integer or length-3 tuple of positive '
                       'integers')
            if isinstance(num_iterations, tuple):
                if len(num_iterations) != 3:
                    error_message = (
                        f'num_iterations must be a positive integer or '
                        f'length-3 tuple of positive integers, but has length '
                        f'{len(num_iterations):,}')
                    raise ValueError(error_message)
                for step, step_num_iterations in enumerate(num_iterations):
                    check_type(step_num_iterations,
                               f'num_iterations[{step!r}]', int,
                               'a positive integer')
                    check_bounds(step_num_iterations,
                                 f'num_iterations[{step!r}]', 1)
            else:
                check_bounds(num_iterations, 'num_iterations', 1)
                num_iterations = 100, 100, num_iterations
        else:
            num_iterations = 100, 100, 250
        # Check that `learning_rate` is a positive floating-point number
        check_type(learning_rate, 'learning_rate', (int, float),
                   'a positive number')
        check_bounds(learning_rate, 'learning_rate', 0, left_open=True)
        # Check that `seed` is an integer, if specified; otherwise, use the
        # default seed
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']`, and if -1, set
        # to `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Check that `faster_single_threading` is Boolean, and `False` unless
        # `num_threads` is 1
        check_type(faster_single_threading, 'faster_single_threading', bool,
                   'Boolean')
        if faster_single_threading and num_threads != 1:
            error_message = \
                'faster_single_threading must be False unless num_threads is 1'
            raise ValueError(error_message)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Define Cython functions
        cython_functions = cython_inline(r'''
        from cython.parallel cimport threadid, prange
        from libc.math cimport sqrt
        from libc.string cimport memcpy
        from libcpp.algorithm cimport sort
        from libcpp.vector cimport vector
        
        cdef extern from "limits.h":
            cdef unsigned UINT_MAX
        
        cdef extern from *:
            """
            #define atomic_add(x, y) _Pragma("omp atomic") x += y
            """
            void atomic_add(unsigned &x, unsigned y) nogil
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef extern from *:
            """
            struct Compare {
                const double* data;
                Compare() noexcept {}
                Compare(const double* d) noexcept : data(d) {}
                bool operator()(unsigned a, unsigned b) const noexcept {
                    return data[a] < data[b];
                }
            };
            """
            cdef cppclass Compare:
                Compare(const double*) noexcept nogil
                bint operator()(unsigned int, unsigned int) noexcept nogil
        
        cdef inline void argsort(const double[::1] arr, unsigned[::1] indices,
                                 const unsigned n) noexcept nogil:
            cdef unsigned i
            for i in range(n):
                indices[i] = i
            sort(&indices[0], &indices[0] + n, Compare(&arr[0]))
            
        def get_scaled_distances(const double[:, ::1] X,
                                 const long[:, :] neighbors,
                                 double[:, ::1] scaled_distances,
                                 const unsigned num_threads):
            cdef unsigned i, j, k, num_cells = scaled_distances.shape[0], \
                num_total_neighbors = scaled_distances.shape[1], \
                num_PCs = X.shape[1]
            cdef long neighbor
            cdef unsigned too_small = 0, too_large = 0
            cdef vector[double] sig
            sig.resize(num_cells)
            
            if num_threads == 1:
                for i in range(num_cells):
                    for j in range(num_total_neighbors):
                        scaled_distances[i, j] = 0
                        for k in range(num_PCs):
                            scaled_distances[i, j] = \
                                scaled_distances[i, j] + \
                                (X[i, k] - X[j, k]) ** 2
                    sig[i] = (sqrt(scaled_distances[i, 3]) +
                              sqrt(scaled_distances[i, 4]) +
                              sqrt(scaled_distances[i, 5])) / 3
                    if sig[i] < 1e-10:
                        sig[i] = 1e-10
                
                for i in range(num_cells):
                    for j in range(num_total_neighbors):
                        neighbor = neighbors[i, j]
                        if neighbor < 0:
                            return True, False
                        if neighbor >= <long> num_cells:
                            return False, True
                        scaled_distances[i, j] = scaled_distances[i, j] / \
                            sig[i] / sig[neighbor]
            else:
                with nogil:
                    for i in prange(num_cells, num_threads=num_threads):
                        for j in range(num_total_neighbors):
                            scaled_distances[i, j] = 0
                            for k in range(num_PCs):
                                scaled_distances[i, j] = \
                                    scaled_distances[i, j] + \
                                    (X[i, k] - X[j, k]) ** 2
                        sig[i] = (sqrt(scaled_distances[i, 3]) +
                                  sqrt(scaled_distances[i, 4]) +
                                  sqrt(scaled_distances[i, 5])) / 3
                        if sig[i] < 1e-10:
                            sig[i] = 1e-10
                    
                    for i in prange(num_cells, num_threads=num_threads):
                        for j in range(num_total_neighbors):
                            neighbor = neighbors[i, j]
                            if neighbor < 0:
                                atomic_add(too_small, 1)
                                with gil:
                                    return too_small, too_large
                            if neighbor >= <long> num_cells:
                                atomic_add(too_large, 1)
                                with gil:
                                    return too_small, too_large
                            scaled_distances[i, j] = scaled_distances[i, j] / \
                                sig[i] / sig[neighbor]
            
            return too_small, too_large
    
        def get_neighbor_pairs(const double[:, ::1] X,
                               const double[:, ::1] scaled_distances,
                               const long[:, :] neighbors,
                               unsigned[:, ::1] neighbor_pairs,
                               const unsigned num_threads):
            cdef unsigned i, j, thread_index, num_cells = X.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_total_neighbors = scaled_distances.shape[1]
            cdef vector[vector[unsigned]] thread_indices
            
            if num_threads == 1:
                thread_indices.resize(1)
                thread_indices[0].resize(num_total_neighbors)
                for i in range(num_cells):
                    argsort(scaled_distances[i], thread_indices[0].data(),
                            num_total_neighbors)
                    for j in range(num_neighbors):
                        neighbor_pairs[i, j] = \
                            <unsigned> neighbors[i, thread_indices[0][j]]
            else:
                thread_indices.resize(num_threads)
                for thread_index in range(num_threads):
                    thread_indices[thread_index].resize(num_total_neighbors)
                for i in prange(num_cells, nogil=True,
                                num_threads=num_threads):
                    thread_index = threadid()
                    argsort(scaled_distances[i],
                            thread_indices[thread_index].data(), num_total_neighbors)
                    for j in range(num_neighbors):
                        neighbor_pairs[i, j] = \
                            <unsigned> neighbors[i, thread_indices[thread_index][j]]
        
        def sample_mid_near_pairs(const double[:, ::1] X,
                                  unsigned[:, ::1] mid_near_pairs,
                                  const unsigned long seed,
                                  const unsigned num_threads):
            cdef unsigned i, j, k, l, n = X.shape[0], \
                closest_cell = UINT_MAX, second_closest_cell = UINT_MAX, \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_PCs = X.shape[1]
            cdef double squared_distance, smallest, second_smallest
            cdef unsigned long state
            cdef vector[unsigned] thread_sampled
            cdef unsigned[::1] sampled
            
            if num_threads == 1:
                thread_sampled.resize(1)
                thread_sampled[0].resize(6)
                sampled = <unsigned[:6]> thread_sampled[0].data()
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_mid_near_pairs):
                        # Randomly sample 6 cells (which are not the
                        # current cell) and select the 2nd-closest
                        smallest = UINT_MAX
                        second_smallest = UINT_MAX
                        for k in range(6):
                            while True:
                                # Sample a random cell...
                                sampled[k] = randint(n, &state)
                                # ...that is not this cell...
                                if sampled[k] == i:
                                    continue
                                # ...nor a previously sampled cell
                                for l in range(k):
                                    if sampled[k] == sampled[l]:
                                        break
                                else:
                                    break
                        for k in range(6):
                            squared_distance = 0
                            for l in range(num_PCs):
                                squared_distance = squared_distance + \
                                    (X[i, l] - X[sampled[k], l]) ** 2
                            if squared_distance < smallest:
                                second_smallest = smallest
                                second_closest_cell = closest_cell
                                smallest = squared_distance
                                closest_cell = sampled[k]
                            elif squared_distance < second_smallest:
                                second_smallest = squared_distance
                                second_closest_cell = sampled[k]
                        mid_near_pairs[i, j] = second_closest_cell
            else:
                thread_sampled.resize(num_threads)
                for thread_index in range(num_threads):
                    thread_sampled[thread_index].resize(6)
                for i in prange(n, nogil=True, num_threads=num_threads):
                    sampled = <unsigned[:6]> thread_sampled[threadid()].data()
                    state = srand(seed + i)
                    for j in range(num_mid_near_pairs):
                        smallest = UINT_MAX
                        second_smallest = UINT_MAX
                        for k in range(6):
                            while True:
                                sampled[k] = randint(n, &state)
                                if sampled[k] == i:
                                    continue
                                for l in range(k):
                                    if sampled[k] == sampled[l]:
                                        break
                                else:
                                    break
                        for k in range(6):
                            squared_distance = 0
                            for l in range(num_PCs):
                                squared_distance = squared_distance + \
                                    (X[i, l] - X[sampled[k], l]) ** 2
                            if squared_distance < smallest:
                                second_smallest = smallest
                                second_closest_cell = closest_cell
                                smallest = squared_distance
                                closest_cell = sampled[k]
                            elif squared_distance < second_smallest:
                                second_smallest = squared_distance
                                second_closest_cell = sampled[k]
                        mid_near_pairs[i, j] = second_closest_cell
        
        def sample_further_pairs(const double[:, ::1] X,
                                 const unsigned[:, ::1] neighbor_pairs,
                                 unsigned[:, ::1] further_pairs,
                                 const unsigned long seed,
                                 const unsigned num_threads):
            """Sample Further pairs using the given seed."""
            cdef unsigned i, j, k, further_pair_index, n = X.shape[0], \
                num_further_pairs = further_pairs.shape[1], \
                num_neighbors = neighbor_pairs.shape[1]
            cdef unsigned long state
            
            if num_threads == 1:
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            # Sample a random cell...
                            further_pair_index = randint(n, &state)
                            # ...that is not this cell...
                            if further_pair_index == i:
                                continue
                            # ...nor one of its nearest neighbors...
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                # ...nor a previously sampled cell
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
            else:
                for i in prange(n, nogil=True, num_threads=num_threads):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            further_pair_index = randint(n, &state)
                            if further_pair_index == i:
                                continue
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
        
        def reformat_for_parallel(const unsigned[:, ::1] pairs,
                                  unsigned[::1] pair_indices,
                                  unsigned[::1] pair_indptr):
            cdef unsigned i, j, k, dest_index, num_cells = pairs.shape[0], \
                num_pairs_per_cell = pairs.shape[1]
            cdef vector[unsigned] dest_indices
            
            # Tabulate how often each cell appears in pairs; at a minimum, it
            # will appear `pairs.shape[1]` times (i.e. the number of
            # neighbors), as the `i` in the pair, but it will also appear a
            # variable number of times as the `j` in the pair.
            
            pair_indptr[0] = 0
            pair_indptr[1:] = pairs.shape[1]
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indptr[j + 1] += 1
                    
            # Take the cumulative sum of the values in `pair_indptr`
            
            for i in range(2, pair_indptr.shape[0]):
                pair_indptr[i] += pair_indptr[i - 1]
                
            # Now that we know how many pairs each cell is a part of, do a
            # second pass over `pairs` to populate `pair_indices` with the
            # pairs' indices. Use a temporary buffer, `dest_indices`, to keep
            # track of the index within `pair_indptr` to write each cell's next
            # pair to.
            
            dest_indices.resize(num_cells)
            memcpy(dest_indices.data(), &pair_indptr[0],
                   num_cells * sizeof(unsigned))
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indices[dest_indices[i]] = j
                    pair_indices[dest_indices[j]] = i
                    dest_indices[i] += 1
                    dest_indices[j] += 1

        def get_gradients(const double[:, ::1] embedding,
                          const unsigned[:, ::1] neighbor_pairs,
                          const unsigned[:, ::1] mid_near_pairs,
                          const unsigned[:, ::1] further_pairs,
                          const double w_neighbors,
                          const double w_mid_near,
                          double[:, ::1] gradients):
            cdef unsigned i, j, k, num_cells = neighbor_pairs.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_further_pairs = further_pairs.shape[1]
            cdef double distance_ij, embedding_ij_0, embedding_ij_1, w
            gradients[:] = 0
            
            # Nearest-neighbor pairs
            
            for i in range(num_cells):
                for k in range(num_neighbors):
                    j = neighbor_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                    
            # Mid-near pairs
            
            for i in range(num_cells):
                for k in range(num_mid_near_pairs):
                    j = mid_near_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                    
            # Further pairs
            
            for i in range(num_cells):
                for k in range(num_further_pairs):
                    j = further_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = 2 / (1 + distance_ij) ** 2
                    gradients[i, 0] -= w * embedding_ij_0
                    gradients[j, 0] += w * embedding_ij_0
                    gradients[i, 1] -= w * embedding_ij_1
                    gradients[j, 1] += w * embedding_ij_1
        
        def get_gradients_parallel(const double[:, ::1] embedding,
                                   const unsigned[::1] neighbor_pair_indices,
                                   const unsigned[::1] neighbor_pair_indptr,
                                   const unsigned[::1] mid_near_pair_indices,
                                   const unsigned[::1] mid_near_pair_indptr,
                                   const unsigned[::1] further_pair_indices,
                                   const unsigned[::1] further_pair_indptr,
                                   const double w_neighbors,
                                   const double w_mid_near,
                                   double[:, ::1] gradients,
                                   const unsigned num_threads):
            cdef unsigned i, j, k, num_cells = embedding.shape[0]
            cdef double distance_ij, embedding_ij_0, embedding_ij_1, w
            
            if num_threads == 1:
                for i in range(num_cells):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    
                    # Nearest-neighbor pairs
                    
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                        
                    # Mid-near pairs
                    
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                        
                    # Further pairs
                    
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] - w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] - w * embedding_ij_1
            else:
                for i in prange(num_cells, nogil=True, num_threads=num_threads):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    
                    # Nearest-neighbor pairs
                    
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                        
                    # Mid-near pairs
                    
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                        
                    # Further pairs
                    
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] - w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] - w * embedding_ij_1
        
        def update_embedding_adam(double[:, ::1] embedding,
                                  const double[:, ::1] gradients,
                                  double[:, ::1] momentum,
                                  double[:, ::1] velocity,
                                  const double beta1,
                                  const double beta2,
                                  double learning_rate,
                                  const unsigned iteration,
                                  const unsigned num_threads):
            cdef unsigned i, num_cells = embedding.shape[0]
            
            learning_rate = \
                learning_rate * sqrt(1 - beta2 ** (iteration + 1)) / \
                (1 - beta1 ** (iteration + 1))
            if num_threads == 1:
                for i in range(num_cells):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            else:
                for i in prange(num_cells, nogil=True,
                                num_threads=num_threads):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            ''')
        get_scaled_distances = cython_functions['get_scaled_distances']
        get_neighbor_pairs = cython_functions['get_neighbor_pairs']
        sample_mid_near_pairs = cython_functions['sample_mid_near_pairs']
        sample_further_pairs = cython_functions['sample_further_pairs']
        update_embedding_adam = cython_functions['update_embedding_adam']
        # Get scaled distances between each cell and its nearest neighbors
        scaled_distances = np.empty_like(nearest_neighbor_indices, dtype=float)
        too_small, too_large = get_scaled_distances(
            PCs, nearest_neighbor_indices, scaled_distances, num_threads)
        # If any nearest-neighbor indices were out of range, raise an error
        if too_small:
            error_message = (
                f'some nearest-neighbor indices in obsm[{neighbors_key!r}] '
                f'are negative')
            raise ValueError(error_message)
        if too_large:
            error_message = (
                f'some nearest-neighbor indices in obsm[{neighbors_key!r}] '
                f'are >= the total number of cells, '
                f'{nearest_neighbor_indices.shape[0]:,}. This may happen if '
                f'you subset this SingleCell dataset between neighbors() and '
                f'embed(); if so, make sure to run neighbors() after, not '
                f'before, subsetting.')
            raise ValueError(error_message)
        # Select the `num_neighbors` of the
        # `num_neighbors + num_extra_neighbors` nearest-neighbor pairs with the
        # lowest scaled distances
        num_cells = PCs.shape[0]
        neighbor_pairs = np.empty((num_cells, num_neighbors), dtype=np.uint32)
        get_neighbor_pairs(PCs, scaled_distances, nearest_neighbor_indices,
                           neighbor_pairs, num_threads)
        del scaled_distances, nearest_neighbor_indices
        # Sample mid-near pairs
        mid_near_pairs = np.empty((num_cells, num_mid_near_pairs),
                                  dtype=np.uint32)
        sample_mid_near_pairs(PCs, mid_near_pairs, seed, num_threads)
        # Sample further pairs
        further_pairs = np.empty((num_cells, num_further_pairs),
                                 dtype=np.uint32)
        sample_further_pairs(PCs, neighbor_pairs, further_pairs,
                             seed + mid_near_pairs.size, num_threads)
        # If multithreaded, or single-threaded with
        # `faster_single_threading=False`, reformat the three lists of pairs to
        # allow deterministic parallelism. Specifically, transform pairs of
        # cell indices from the original format of a 2D array `pairs` where
        # `pairs[i]` contains all js for which (i, j) is a pair, to a pair of
        # 1D arrays `pair_indices` and `pair_indptr` forming a sparse array,
        # where `pair_indices[pair_indptr[i]:pair_indptr[i + 1]]` contains all
        # js for which (i, j) is a pair or (j, i) is a pair. `pair_indices`
        # must have length `2 * pairs.size`, since each pair will appear twice,
        # once for (i, j) and once for (j, i). `pair_indptr` must have length
        # equal to the number of cells plus one, just like for scipy sparse
        # matrices.
        if faster_single_threading:
            get_gradients = cython_functions['get_gradients']
        else:
            reformat_for_parallel = cython_functions['reformat_for_parallel']
            
            neighbor_pair_indices = np.empty(2 * neighbor_pairs.size,
                                             dtype=np.uint32)
            neighbor_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(neighbor_pairs, neighbor_pair_indices,
                                  neighbor_pair_indptr)
            del neighbor_pairs
            mid_near_pair_indices = \
                np.empty(2 * mid_near_pairs.size, dtype=np.uint32)
            mid_near_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(mid_near_pairs, mid_near_pair_indices,
                                  mid_near_pair_indptr)
            del mid_near_pairs
            further_pair_indices = \
                np.empty(2 * further_pairs.size, dtype=np.uint32)
            further_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(further_pairs, further_pair_indices,
                                  further_pair_indptr)
            del further_pairs
            get_gradients = cython_functions['get_gradients_parallel']
        # Initialize the embedding, gradients, and other optimizer parameters
        embedding = 0.01 * PCs[:, :2]
        gradients = np.zeros_like(embedding, dtype=float)
        momentum = np.zeros_like(embedding, dtype=float)
        velocity = np.zeros_like(embedding, dtype=float)
        w_mid_near_init = 1000
        beta1 = 0.9
        beta2 = 0.999
        # Optimize the embedding
        for iteration in range(sum(num_iterations)):
            num_phase_1_iterations, num_phase_2_iterations = num_iterations[:2]
            if iteration < num_phase_1_iterations:
                w_mid_near = \
                    (1 - iteration / num_phase_1_iterations) * \
                    w_mid_near_init + iteration / num_phase_1_iterations * 3
                w_neighbors = 2
            elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                w_mid_near = 3
                w_neighbors = 3
            else:
                w_mid_near = 0
                w_neighbors = 1
            # Calculate gradients
            if faster_single_threading:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pairs, mid_near_pairs,
                              further_pairs, w_neighbors, w_mid_near,
                              gradients)
            else:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pair_indices,
                              neighbor_pair_indptr, mid_near_pair_indices,
                              mid_near_pair_indptr, further_pair_indices,
                              further_pair_indptr, w_neighbors, w_mid_near,
                              gradients, num_threads)
            # Update the embedding based on the gradients, via the Adam
            # optimizer
            update_embedding_adam(embedding, gradients, momentum, velocity,
                                  beta1, beta2, learning_rate, iteration,
                                  num_threads)
        # If `QC_column` is not `None`, back-project from QCed cells to all
        # cells, filling with `NaN`
        if QC_column is not None:
            embedding_QCed = embedding
            embedding = np.full((len(self), embedding_QCed.shape[1]), np.nan)
            # noinspection PyUnboundLocalVariable
            embedding[QCed_NumPy] = embedding_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {embedding_key: embedding},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns)
    
    # noinspection PyUnresolvedReferences
    def plot_embedding(
            self,
            color_column: SingleCellColumn | None,
            filename: str | Path | None = None,
            *,
            cells_to_plot_column: SingleCellColumn | None = 'passed_QC',
            embedding_key: str = 'PaCMAP',
            ax: 'Axes' | None = None,
            figure_kwargs: dict[str, Any] | None = None,
            point_size: int | float | np.integer | np.floating | str |
                        None = None,
            sort_by_frequency: bool = False,
            colormap: str | 'Colormap' | dict[Any, Color] = None,
            lightness_range: tuple[float | np.floating,
                                   float | np.floating] = (100 / 3, 200 / 3),
            chroma_range: tuple[float | np.floating,
                                float | np.floating] = (50, 100),
            hue_range: tuple[float | np.floating, float | np.floating] |
                       None = None,
            first_color: Color = '#008cb9',
            stride: int | np.integer = 5,
            default_color: Color = 'lightgray',
            scatter_kwargs: dict[str, Any] | None = None,
            label: bool = False,
            label_kwargs: dict[str, Any] | None = None,
            legend: bool = True,
            legend_kwargs: dict[str, Any] | None = None,
            colorbar: bool = True,
            colorbar_kwargs: dict[str, Any] | None = None,
            title: str | None = None,
            title_kwargs: dict[str, Any] | None = None,
            xlabel: str | None = 'Component 1',
            xlabel_kwargs: dict[str, Any] | None = None,
            ylabel: str | None = 'Component 2',
            ylabel_kwargs: dict[str, Any] | None = None,
            xlim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            ylim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            despine: bool = True,
            savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot an embedding created by `embed()`, using Matplotlib.
        
        Requires the colorspacious package. Install via:
        mamba install -y colorspacious
        
        Args:
            color_column: an optional column of `obs` indicating how to color
                          each cell in the plot. Can be a column name, a polars
                          expression, a polars Series, a 1D NumPy array, or a
                          function that takes in this SingleCell dataset and
                          returns a polars Series or 1D NumPy array. Can be
                          discrete (e.g. cell-type labels), specified as a
                          String/Categorical/Enum column, or quantitative (e.g.
                          the number of UMIs per cell), specified as an
                          integer/floating-point column. Missing (`null`) cells
                          will be plotted with the color `default_color`. Set
                          to `None` to use `default_color` for all cells.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            embedding_key: the key of `obsm` containing the embedding to plot,
                           calculated with `embed()`
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            point_size: the size of the points for each cell; defaults to
                        30,000 divided by the number of cells, one quarter of
                        Scanpy's default. Can be a single number, or the name
                        of a column of `obs` to make each point a different
                        size.
            sort_by_frequency: if `True`, assign colors and sort the legend in
                               order of decreasing frequency; if `False` (the
                               default), use natural sorted order
                               (en.wikipedia.org/wiki/Natural_sort_order).
                               Cannot be `True` unless `colormap` is `None` and
                               `color_column` is discrete; if `colormap` is
                               not `None`, the plot order is determined by the
                               order of the keys in `colormap`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use; or, if `color_column` is discrete, a
                      dictionary mapping values in `color_column` to Matplotlib
                      colors (cells with values of `color_column` that are not
                      in the dictionary will be plotted in the color
                      `default_color`). Defaults to
                      `plt.rcParams['image.cmap']` (`'viridis'` by default) if
                      `color_column` is continous, or the colors from a
                      maximally perceptually distinct colormap if
                      `color_column` is discrete (with colors assigned in
                      decreasing order of frequency). Cannot be specified if
                      `color_column` is `None`.
            lightness_range: a two-element tuple with the lightness range of
                             colors to generate, or `None` to take the full
                             range: `[0, 100]`. Can only be specified when
                             `color_column` is discrete and `colormap` is
                             `None`.
            chroma_range: a two-element tuple with the chroma range of colors
                          to generate, or `None` to take the full range:
                          `[0, 100]`. Grays have low chroma, and vivid colors
                          have high chroma. Can only be specified when
                          `color_column` is discrete and `colormap` is `None`.
            hue_range: a two-element tuple with the hue range of colors to
                       generate, or `None` to take the full range: `[0, 360]`.
                       Red is at 0Â°, green at 120Â°, and blue at 240Â°. Because
                       it wraps around, the first element of the tuple can be
                       greater than the second, unlike for `lightness_range`
                       and `chroma_range`. Can only be specified when
                       `color_column` is discrete and `colormap` is `None`.
            first_color: the first color of the palette. Can be any valid
                         Matplotlib color, like a hex string (e.g.
                         `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            stride: as an optimization, consider only RGB colors where R, G,
                    and B are all multiples of this value. Must be a small
                    divisor of 255: 1, 3, 5, 15, or 17. Set to 1 for the best
                    possible solution, at orders of magnitude more
                    computational cost.
            default_color: the default color to plot cells in when
                           `color_column` is `None`, or when certain cells have
                           missing (`null`) values for `color_column`, or when
                           `colormap` is a dictionary and some cells have
                           values of `color_column` that are not in the
                           dictionary. Can be any valid Matplotlib color, like
                           a hex string (e.g. `'#FF0000'`), a named color (e.g.
                           'red'), a 3- or 4-element RGB/RGBA tuple of integers
                           0-255 or floats 0-1, or a single float 0-1 for
                           grayscale.
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since these arguments conflict with the
                            `point_size`, `color_column`, and `colormap`
                            arguments, respectively.
            label: whether to label cells with each distinct value of
                   `color_column`. Labels will be placed at the median x and y
                   position of the points with that color. Can only be `True`
                   when `color_column` is discrete. When set to `True`, you may
                   also want to set `legend=False` to avoid redundancy.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                           - `path_effects` to set properties for the border
                             around the text. By default, set to
                             `matplotlib.patheffects.withStroke(
                                  linewidth=3, foreground='white', alpha=0.75)`
                             instead of Matplotlib's default of `None`, to put
                             a semi-transparent white border around the labels
                             for better contrast.
                          Can only be specified when `label=True`.
            legend: whether to add a legend for each value in `color_column`.
                    Ignored unless `color_column` is discrete.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. By default, `loc` is set to
                             `'center left'` and `bbox_to_anchor` to `(1, 0.5)`
                             to put the legend to the right of the plot,
                             anchored at the middle.
                           - `ncols` to set its number of columns. By
                             default, set to
                             `obs[color_column].n_unique() // 16 + 1` to have
                             at most 16 items per column.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border. `frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when `color_column` is
                           discrete and `legend=True`.
            colorbar: whether to add a colorbar. Ignored unless `color_column`
                      is quantitative.
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `color_column` is
                             quantitative and `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            xlim: a length-2 tuple of the left and right x-axis limits, or
                 `None` to set the limits based on the data
            ylim: a length-2 tuple of the bottom and top y-axis limits, or
                 `None` to set the limits based on the data
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's 
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Get `cells_to_plot_column`, if not `None`
        original_cells_to_plot_column = cells_to_plot_column
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        # If `color_column` is not `None`, check that it either discrete
        # (Categorical, Enum, or String) or quantitative (integer or
        # floating-point). If discrete, check that `color_column` has at least
        # two distinct values.
        original_color_column = color_column
        if color_column is not None:
            color_column = self._get_column(
                'obs', color_column, 'color_column',
                (pl.Categorical, pl.Enum, pl.String, 'integer',
                 'floating-point'), allow_null=True,
                QC_column=cells_to_plot_column)
            unique_color_column = color_column.unique().drop_nulls()
            dtype = color_column.dtype
            discrete = dtype in (pl.Categorical, pl.Enum, pl.String)
            if discrete and len(unique_color_column) == 1:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} must have at least two '
                    f'distinct values when its data '
                    f'type is {dtype.base_type()!r}')
                raise ValueError(error_message)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `embedding_key` is the name of a key in `obsm`
        check_type(embedding_key, 'embedding_key', str, 'a string')
        if embedding_key not in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is not a key of obsm; '
                f'did you forget to run embed() before plot_embedding()?')
            raise ValueError(error_message)
        # Check that the embedding `embedding_key` references is 2D.
        embedding = self._obsm[embedding_key]
        if embedding.shape[1] != 2:
            error_message = (
                f'the embedding at obsm[{embedding_key!r}] is '
                f'{embedding.shape[1]:,}-dimensional, but must be '
                f'2-dimensional to be plotted')
            raise ValueError(error_message)
        # If `cells_to_plot_column` is not `None`, subset to these cells
        if cells_to_plot_column is not None:
            embedding = embedding[cells_to_plot_column.to_numpy()]
            if color_column is not None:
                color_column = color_column.filter(cells_to_plot_column)
                unique_color_column = color_column.unique().drop_nulls()
        # Check that the embedding does not contain NaNs
        if np.isnan(embedding).any():
            error_message = \
                f'the embedding at obsm[{embedding_key!r}] contains NaNs; '
            if cells_to_plot_column is None:
                error_message += (
                    'did you forget to set QC_column to None in embed(), to '
                    'match the fact that you set cells_to_plot_column to '
                    'None in plot_embedding()?')
            else:
                cells_to_plot_column_description = \
                    SingleCell._describe_column('cells_to_plot_column',
                                                original_cells_to_plot_column)
                error_message += (
                    f'does your {cells_to_plot_column_description} contain '
                    f'cells that were excluded by the QC_column used in '
                    f'embed()?')
            raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # If `figure_kwargs` is not `None`, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        # If `point_size` is `None`, default to 30,000 / num_cells; otherwise,
        # check that it is a positive number or the name of a numeric column of
        # `obs` with all-positive numbers
        num_cells = \
            len(self) if cells_to_plot_column is None else len(embedding)
        if point_size is None:
            # noinspection PyUnboundLocalVariable
            point_size = 30_000 / num_cells
        else:
            check_type(point_size, 'point_size', (int, float, str),
                       'a positive number or string')
            if isinstance(point_size, (int, float)):
                check_bounds(point_size, 'point_size', 0, left_open=True)
            else:
                if point_size not in self._obs:
                    error_message = \
                        f'point_size {point_size!r} is not a column of obs'
                    raise ValueError(error_message)
                point_size = self._obs[point_size]
                if not (point_size.dtype.is_integer() or
                        point_size.dtype.is_float()):
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], must '
                        f'have an integer or floating-point data type, but '
                        f'has data type {point_size.dtype.base_type()!r}')
                    raise TypeError(error_message)
                if point_size.min() <= 0:
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], does '
                        f'not have all-positive elements')
                    raise ValueError(error_message)
        # If `sort_by_frequency=True`, ensure `colormap` is `None` and
        # `color_column` is discrete
        check_type(sort_by_frequency, 'sort_by_frequency', bool, 'Boolean')
        if sort_by_frequency:
            if colormap is not None:
                error_message = (
                    f'sort_by_frequency must be False when colormap is '
                    f'specified')
                raise ValueError(error_message)
            if color_column is None:
                error_message = \
                    'sort_by_frequency must be False when color_column is None'
                raise ValueError(error_message)
            # noinspection PyUnboundLocalVariable
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'sort_by_frequency must be False when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
        # Handle coloring based on the values of `colormap` and `color_column`
        if colormap is not None:
            # If `colormap` is not `None`, check that it is a string in
            # `plt.colormaps`, Colormap object, or dictionary where all keys
            # are in `color_column` and all values are valid Matplotlib colors.
            # Normalize the color(s) to hex codes. Make sure `color_column` is
            # not `None` and `lightness_range`, `chroma_range`, `hue_range`,
            # `first_color`, and `stride` are `None`.
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap, dict),
                       'a string, matplotlib Colormap object, or dictionary')
            if color_column is None:
                error_message = \
                    'colormap must be None when color_column is None'
                raise ValueError(error_message)
            for arg, arg_name in ((lightness_range, 'lightness_range'),
                                  (chroma_range, 'chroma_range'),
                                  (hue_range, 'hue_range'),
                                  (first_color, 'first_color'),
                                  (stride, 'stride')):
                if arg is not None:
                    error_message = \
                        f'{arg_name} must be None when colormap is specified'
                    raise ValueError(error_message)
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
            elif isinstance(colormap, dict):
                # noinspection PyUnboundLocalVariable
                if not discrete:
                    color_column_description = \
                        SingleCell._describe_column('color_column',
                                                    original_color_column)
                    error_message = (
                        f'colormap cannot be a dictionary when '
                        f'{color_column_description} is continuous')
                    raise ValueError(error_message)
                for key, value in colormap.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of colormap must be strings, but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    # noinspection PyUnboundLocalVariable
                    if key not in unique_color_column:
                        error_message = (
                            f'colormap is a dictionary containing the key '
                            f'{key!r}, which is not one of the values in '
                            f'obs[{color_column!r}]')
                        raise ValueError(error_message)
                    if not plt.matplotlib.colors.is_color_like(value):
                        error_message = (
                            f'colormap[{key!r}] is not a valid Matplotlib '
                            f'color')
                        raise ValueError(error_message)
                    colormap[key] = plt.matplotlib.colors.to_hex(value)
        else:
            # noinspection PyUnboundLocalVariable
            if color_column is not None and discrete:
                # `color_column` is discrete and `colormap` was not specified;
                # generate a maximally perceptually distinct colormap. Assign
                # colors in natural sort order, or decreasing order of
                # frequency if `sort_by_frequency=True`.
                # noinspection PyUnboundLocalVariable
                color_order = color_column\
                    .value_counts(sort=True)\
                    .to_series()\
                    .drop_nulls() if sort_by_frequency else \
                        sorted(unique_color_column,
                               key=lambda color_label: [
                                   int(text) if text.isdigit() else
                                   text.lower() for text in
                                   re.split('([0-9]+)', color_label)])
                colormap = generate_palette(num_colors=len(color_order),
                                            lightness_range=lightness_range,
                                            chroma_range=chroma_range,
                                            hue_range=hue_range,
                                            first_color=first_color,
                                            stride=stride)
                colormap = dict(zip(color_order, colormap))
            else:
                # `color_column` is `None` or continuous, so make sure
                # `lightness_range`, `chroma_range`, `hue_range`,
                # `first_color`, and `stride` are `None`
                for arg, arg_name in ((lightness_range, 'lightness_range'),
                                      (chroma_range, 'chroma_range'),
                                      (hue_range, 'hue_range'),
                                      (first_color, 'first_color'),
                                      (stride, 'stride')):
                    if arg is not None:
                        if color_column is None:
                            error_message = (
                                f'{arg_name} must be None when color_column '
                                f'is None')
                            raise ValueError(error_message)
                        else:
                            color_column_description = \
                                SingleCell._describe_column(
                                    'color_column', original_color_column)
                            error_message = (
                                f'{arg_name} must be None when '
                                f'{color_column_description} is continuous')
                            raise ValueError(error_message)
        # Check that `default_color` is a valid Matplotlib color, and convert
        # it to hex
        if not plt.matplotlib.colors.is_color_like(default_color):
            error_message = 'default_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        default_color = plt.matplotlib.colors.to_hex(default_color)
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
            if scatter_kwargs is not None else default_scatter_kwargs
        # Check that `scatter_kwargs` does not contain the `s`, `c`/`color`, or
        # `cmap` keys
        if 's' in scatter_kwargs:
            error_message = (
                "'s' cannot be specified as a key in scatter_kwargs; specify "
                "the point_size argument instead")
            raise ValueError(error_message)
        for key in 'c', 'color', 'cmap':
            if key in scatter_kwargs:
                error_message = (
                    f'{key!r} cannot be specified as a key in scatter_kwargs; '
                    f'specify the color_column, colormap, lightness_range, '
                    f'chroma_range, hue_range, first_color, stride, and/or '
                    f'default_color arguments instead')
                raise ValueError(error_message)
        # If `label=True`, check that `color_column` is discrete.
        # If `label=False`, check that `label_kwargs` is `None`.
        check_type(label, 'label', bool, 'Boolean')
        if label:
            if color_column is None:
                error_message = 'color_column cannot be None when label=True'
                raise ValueError(error_message)
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} cannot be continuous when '
                    f'label=True')
                raise ValueError(error_message)
        elif label_kwargs is not None:
            error_message = 'label_kwargs must be None when label=False'
            raise ValueError(error_message)
        # Only add a legend if `legend=True` and `color_column` is discrete.
        # If not adding a legend, check that `legend_kwargs` is `None`.
        check_type(legend, 'legend', bool, 'Boolean')
        add_legend = legend and color_column is not None and discrete
        if not add_legend and legend_kwargs is not None:
            if color_column is None:
                error_message = \
                    'legend_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = SingleCell._describe_column(
                    'color_column', original_color_column)
                error_message = (
                    f'legend_kwargs must be None when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
        # Only add a colorbar if `colorbar=True` and `color_column` is
        # continuous. If not adding a colorbar, check that `colorbar_kwargs` is
        # `None`.
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        add_colorbar = colorbar and color_column is not None and not discrete
        if not add_colorbar and colorbar_kwargs is not None:
            if color_column is None:
                error_message = \
                    'colorbar_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = SingleCell._describe_column(
                    'color_column', original_color_column)
                error_message = (
                    f'colorbar_kwargs must be None when '
                    f'{color_column_description} is discrete')
                raise ValueError(error_message)
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # Check that `xlim` and `ylim` are be length-2 tuples or `None`, with
        # the first element less than the second
        for arg, arg_name in (xlim, 'xlim'), (ylim, 'ylim'):
            if arg is not None:
                check_type(arg, arg_name, tuple, 'a length-2 tuple')
                if len(arg) != 2:
                    error_message = (
                        f'{arg_name} must be a length-2 tuple, but has length '
                        f'{len(arg):,}')
                    raise ValueError(error_message)
                if arg[0] >= arg[1]:
                    error_message = \
                        f'{arg_name}[0] must be less than {arg_name}[1]'
                    raise ValueError(error_message)
        # If `color_column` is `None`, plot all cells in `default_color`. If
        # `colormap` is a dictionary, generate an explicit list of colors to
        # plot each cell in. If `colormap` is a Colormap, just pass it as the
        # cmap` argument. If `colormap` is missing and `color_column` is
        # continuous, set it to `plt.rcParams['image.cmap']` ('viridis' by
        # default)
        if color_column is None:
            c = default_color
            cmap = None
        elif isinstance(colormap, dict):
            # Note: `replace_strict(..., default=default_color)` fills both
            # missing values and values missing from `colormap` with
            # `default_color`
            c = color_column\
                .replace_strict(colormap, default=default_color,
                                return_dtype=pl.String)\
                .to_numpy()
            cmap = None
        else:
            # Need to `copy()` because `set_bad()` is in-place
            c = color_column.to_numpy()
            if colormap is not None:
                cmap = colormap.copy()
                # noinspection PyUnresolvedReferences
                cmap.set_bad(default_color)
            else:  # `color_column` is continuous
                cmap = plt.rcParams['image.cmap']
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make a scatter plot of the embedding with equal x-y aspect ratios
            scatter = ax.scatter(embedding[:, 0], embedding[:, 1],
                                 s=point_size, c=c, cmap=cmap,
                                 **scatter_kwargs)
            ax.set_aspect('equal')
            # Add the title, axis labels and axis limits
            if title is not None:
                if title_kwargs is None:
                    ax.set_title(title)
                else:
                    ax.set_title(title, **title_kwargs)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax.set_xlabel(xlabel)
                else:
                    ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax.set_ylabel(ylabel)
                else:
                    ax.set_ylabel(ylabel, **ylabel_kwargs)
            if xlim is not None:
                ax.set_xlim(*xlim)
            if ylim is not None:
                ax.set_ylim(*ylim)
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if add_legend:
                default_legend_kwargs = dict(
                    loc='center left', bbox_to_anchor=(1, 0.5), frameon=False,
                    ncols=len(unique_color_column) // 16 + 1)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                if isinstance(colormap, dict):
                    for color_label, color in colormap.items():
                        ax.scatter([], [], c=color, label=color_label,
                                   **scatter_kwargs)
                    plt.legend(**legend_kwargs)
                else:
                    plt.legend(*scatter.legend_elements(), **legend_kwargs)
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`
            if add_colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(scatter, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
            # Label cells; override the defaults for certain keys of
            # `label_kwargs`
            if label:
                from matplotlib.patheffects import withStroke
                if label_kwargs is None:
                    label_kwargs = {}
                # noinspection PyUnresolvedReferences
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')),
                    path_effects=[withStroke(linewidth=3, foreground='white',
                                             alpha=0.75)])
                for color_label in unique_color_column:
                    ax.text(*np.median(embedding[color_column == color_label],
                                       axis=0), color_label, **label_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise


class Pseudobulk:
    """
    A pseudobulked single-cell dataset resulting from calling `pseudobulk()`
    on a SingleCell dataset.
    
    Has slots for:
    - `X`: a dict of NumPy arrays of counts per cell and gene for each cell
      type
    - `obs`: a dict of polars DataFrames of sample metadata for each cell type
    - `var`: a dict of polars DataFrames of gene metadata for each cell type
    as well as `obs_names` and `var_names`, aliases for a dict of `obs[:, 0]`
    and `var[:, 0]` for each cell type.
    
    In many ways, Pseudobulk objects behave like dictionaries:
    - `pb1 | pb2` combines pseudobulks with non-overlapping cell types into one
      big pseudobulk
    - `cell_type in pb` tests whether `cell_type` is a cell type in the
      pseudobulk
    - `for cell_type in pb:` and `for cell_type in pb.keys():` yield the cell
      type names
    - `for X, obs, var in pb.values():` yields each cell type's `X`, `obs`, and
      `var`
    - `for cell_type, (X, obs, var) in pseudobulk.items():` yields both the
      name and the `X`, `obs` and `var` for each cell type
      
    There are also custom iterators if you just want one field per cell type:
    - `for X in pseudobulk.iter_X():` yields just the `X` for each cell type
    - `for X in pseudobulk.iter_obs():` yields just the `obs` for each cell
       type
    - `for X in pseudobulk.iter_var():` yields just the `var` for each cell
       type
    """
    def __init__(self,
                 X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                     np.floating]]] |
                    None = None,
                 *,
                 obs: dict[str, pl.DataFrame] = None,
                 var: dict[str, pl.DataFrame] = None) -> None:
        """
        Load a saved Pseudobulk dataset, or create one from an in-memory count
        matrix + metadata for each cell type. The latter functionality is
        mainly for internal use; most users will create new pseudobulk datasets
        by calling `pseudobulk()` on a SingleCell dataset.
        
        Args:
            X: a {cell type: NumPy array} dictionary of counts or log CPMs, or
               a directory to load a saved Pseudobulk dataset from (see save())
            obs: a {cell type: polars DataFrame} dict of metadata per sample,
                 when `X` is a dictionary. The first column must be String,
                 Enum, or Categorical.
            var: a {cell type: polars DataFrame} dict of metadata per gene,
                 when `X` is a dictionary. The first column must be String,
                 Enum, or Categorical.
        """
        if isinstance(X, dict):
            if obs is None:
                error_message = (
                    'obs is None, but since X is a dictionary, obs must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if var is None:
                error_message = (
                    'var is None, but since X is a dictionary, var must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if not X:
                error_message = 'X is an empty dictionary'
                raise ValueError(error_message)
            if X.keys() != obs.keys():
                error_message = (
                    'X and obs must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            if X.keys() != var.keys():
                error_message = (
                    'X and var must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            for cell_type in X:
                if not isinstance(cell_type, str):
                    error_message = (
                        f'all keys of X (cell types) must be strings, but X '
                        f'contains a key of type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                check_type(X[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                if X[cell_type].ndim != 2:
                    error_message = (
                        f'X[{cell_type!r}] is a {X[cell_type].ndim:,}-'
                        f'dimensional NumPy array, but must be 2-dimensional')
                    raise ValueError(error_message)
                check_type(obs[cell_type], f'obs[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
                check_type(var[cell_type], f'var[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
            self._X = X
            self._obs = obs
            self._var = var
        elif isinstance(X, (str, Path)):
            X = str(X)
            if not os.path.exists(X):
                error_message = f'Pseudobulk directory {X!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{X}/cell_types.txt')]
            self._X = {cell_type: np.load(
                os.path.join(X, f'{cell_type.replace("/", "-")}.X.npy'))
                for cell_type in cell_types}
            self._obs = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.obs.parquet'))
                for cell_type in cell_types}
            self._var = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.var.parquet'))
                for cell_type in cell_types}
        else:
            error_message = (
                f'X must be a dictionary of NumPy arrays or a directory '
                f'containing a saved Pseudobulk dataset, but has type '
                f'{type(X).__name__!r}')
            raise ValueError(error_message)
        for cell_type in self._X:
            dtype = self._X[cell_type].dtype
            if dtype != np.int32 and dtype != np.int64 and \
                    dtype != np.float32 and dtype != np.float64 and \
                    dtype != np.uint32 and dtype != np.uint64:
                error_message = (
                    f'X must be (u)int32/64 or float32/64, but has data type '
                    f'{str(dtype)}')
                raise TypeError(error_message)
            if len(self._obs[cell_type]) == 0:
                error_message = \
                    f'len(obs[{cell_type!r}]) is 0: no samples remain'
                raise ValueError(error_message)
            if len(self._var[cell_type]) == 0:
                error_message = \
                    f'len(var[{cell_type!r}]) is 0: no genes remain'
                raise ValueError(error_message)
            if len(self._obs[cell_type]) != len(self._X[cell_type]):
                error_message = (
                    f'len(obs[{cell_type!r}]) is '
                    f'{len(self._obs[cell_type]):,}, but '
                    f'len(X[{cell_type!r}]) is {len(X[cell_type]):,}')
                raise ValueError(error_message)
            if len(self._var[cell_type]) != self._X[cell_type].shape[1]:
                error_message = (
                    f'len(var[{cell_type!r}]) is '
                    f'{len(self._var[cell_type]):,}, but '
                    f'X[{cell_type!r}].shape[1] is '
                    f'{self._X[cell_type].shape[1]:,}')
                raise ValueError(error_message)
            if self._obs[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of obs[{cell_type!r}] '
                    f'({self._obs[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._obs[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
            if self._var[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of var[{cell_type!r}] '
                    f'({self._var[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._var[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
    
    @staticmethod
    def _setter_check(new: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      old: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      name: str) -> None:
        """
        When setting `X`, `obs` or `var`, raise an error if the new value is
        not a dictionary, the new cell types (keys) differ from the old ones,
        or the new values differ in length (or shape, in the case of `X`) from
        the old ones. For `obs` and `var`, also check that the first column is
        String, Enum, or Categorical.
        
        Args:
            new: the new `X`, `obs` or `var`
            old: the old `X`, `obs` or `var`
            name: the name of the field: `'X'`, `'obs'` or `'var'`
        """
        if not isinstance(new, dict):
            error_message = (
                f'new {name} must be a dictionary, but has type '
                f'{type(new).__name__!r}')
            raise TypeError(error_message)
        if new.keys() != old.keys():
            error_message = (
                f'new {name} has different cell types (keys) from the old '
                f'{name}, or has the same cell types in a different order')
            raise ValueError(error_message)
        if name == 'X':
            for cell_type in new:
                check_type(new[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                new_shape = new[cell_type].shape
                old_shape = old[cell_type].shape
                if new_shape != old_shape:
                    error_message = (
                        f'new X[{cell_type!r}] is {new_shape.shape[0]:,} Ã— '
                        f'{new_shape.shape[1]:,}, but old X is '
                        f'{old_shape.shape[0]:,} Ã— {old_shape.shape[1]:,}')
                    raise ValueError(error_message)
                dtype = new[cell_type].dtype
                if dtype != np.int32 and dtype != np.int64 and \
                        dtype != np.float32 and dtype != np.float64 and \
                        dtype != np.uint32 and dtype != np.uint64:
                    error_message = (
                        f'X must be (u)int32/64 or float32/64, but new '
                        f'X[{cell_type!r}] data type {str(dtype)}')
                    raise TypeError(error_message)
        else:
            for cell_type in new:
                check_type(new[cell_type], f'{name}[{cell_type!r}]',
                           pl.DataFrame, 'a polars DataFrame')
                if new[cell_type][:, 0].dtype not in (
                        pl.String, pl.Categorical, pl.Enum):
                    error_message = (
                        f'the first column of {name}[{cell_type!r}] '
                        f'({new[cell_type].columns[0]!r}) must be String, '
                        f'Enum, or Categorical, but the first column of the '
                        f'new {name} has data type '
                        f'{new[cell_type][:, 0].dtype.base_type()!r}')
                    raise ValueError(error_message)
                if len(new) != len(old):
                    error_message = (
                        f'new {name}[{cell_type!r}] has length {len(new):,}, '
                        f'but old {name} has length {len(old):,}')
                    raise ValueError(error_message)
    
    @property
    def X(self) -> dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]:
        return self._X
    
    @X.setter
    def X(self, X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]) -> None:
        self._setter_check(X, self._X, 'X')
        self._X = X
    
    @property
    def obs(self) -> dict[str, pl.DataFrame]:
        return self._obs
    
    @obs.setter
    def obs(self, obs: dict[str, pl.DataFrame]) -> None:
        self._setter_check(obs, self._obs, 'obs')
        self._obs = obs

    @property
    def var(self) -> dict[str, pl.DataFrame]:
        return self._var
    
    @var.setter
    def var(self, var: dict[str, pl.DataFrame]) -> None:
        self._setter_check(var, self._var, 'var')
        self._var = var

    @property
    def obs_names(self) -> dict[str, pl.Series]:
        return {cell_type: obs[:, 0] for cell_type, obs in self._obs.items()}
    
    @property
    def var_names(self) -> dict[str, pl.Series]:
        return {cell_type: var[:, 0] for cell_type, var in self._var.items()}

    def _process_cell_types(self,
                            cell_types: str | Iterable[str] | None,
                            excluded_cell_types: str | Iterable[str] | None,
                            *,
                            return_description: bool = False) -> \
            tuple[str, ...] | tuple[tuple[str, ...], str]:
        """
        Process the `cell_types` and `excluded_cell_types` arguments of various
        Pseudobulk functions.
        
        Args:
            cell_types: one or more cell types to include in the calling
                        function's operation
            excluded_cell_types: one or more cell types to exclude from the
                                 calling function's operation
            return_description: whether to return a description of the cell
                                types, to use in error messages

        Returns:
            A tuple of cell-type names, or a two-element tuple of
            (cell-type names, cell-type description) if
            `return_description=True`.
        """
        if cell_types is not None:
            if excluded_cell_types is not None:
                error_message = (
                    'cell_types and excluded_cell_types cannot both be '
                    'specified')
                raise ValueError(error_message)
            is_string = isinstance(cell_types, str)
            cell_types = \
                to_tuple_checked(cell_types, 'cell_types', str, 'strings')
            for cell_type in cell_types:
                if cell_type not in self._X:
                    if is_string:
                        error_message = (
                            f'cell_types is {cell_type!r}, which is not a '
                            f'cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            if return_description:
                cell_type_description = 'the cell_types argument'
        elif excluded_cell_types is not None:
            excluded_cell_types = to_tuple_checked(
                excluded_cell_types, 'cell_types', str, 'strings')
            for cell_type in excluded_cell_types:
                if cell_type not in self._X:
                    if excluded_cell_types:
                        error_message = (
                            f'excluded_cell_types is {cell_type!r}, which is '
                            f'not a cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of excluded_cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            cell_types = tuple(cell_type for cell_type in self._X
                               if cell_type not in excluded_cell_types)
            if len(cell_types) == 0:
                error_message = \
                    'all cell types were excluded by excluded_cell_types'
                raise ValueError(error_message)
            if return_description:
                cell_type_description = (
                    'this Pseudobulk dataset (after excluding the cell types '
                    'in excluded_cell_types)')
        else:
            cell_types = tuple(self._X)
            if return_description:
                cell_type_description = 'this Pseudobulk dataset'
        # noinspection PyUnboundLocalVariable
        return (cell_types, cell_type_description) \
            if return_description else cell_types
    
    def set_obs_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of `obs`, i.e. the `obs_names`.
        
        Args:
            column: the column name in `obs`; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's `obs`. If `column` is already the first column for
            every cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._obs[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                if column not in cell_type_obs:
                    error_message = \
                        f'{column!r} is not a column of obs[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_obs, f'obs[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                obs[cell_type] = \
                    cell_type_obs.select(column, pl.exclude(column))
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def set_var_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of `var`, i.e. the `var_names`.
        
        Args:
            column: the column name in `var`; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's `var`. If `column` is already the first column for
            every cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._var[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                if column not in cell_type_var:
                    error_message = \
                        f'{column!r} is not a column of var[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_var, f'var[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                var[cell_type] = \
                    cell_type_var.select(column, pl.exclude(column))
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var)

    def keys(self) -> KeysView[str]:
        """
        Get a KeysView (like you would get from `dict.keys()`) of this
        Pseudobulk dataset's cell types. `for cell_type in pb.keys()` is
        equivalent to `for cell_type in pb`.
        
        Returns:
            A KeysView of the cell types.
        """
        return self._X.keys()
    
    def values(self) -> ValuesView[tuple[np.ndarray[2, np.dtype[np.integer |
                                                                np.floating]],
                                       pl.DataFrame, pl.DataFrame]]:
        """
        Get a `ValuesView` (like you would get from `dict.values()`) of
        `(X, obs, var)` tuples for each cell type in this Pseudobulk dataset.
        
        Returns:
            A `ValuesView` of `(X, obs, var)` tuples for each cell type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.values()
    
    def items(self) -> ItemsView[str, tuple[np.ndarray[2, np.dtype[
            np.integer | np.floating]], pl.DataFrame, pl.DataFrame]]:
        """
        Get an `ItemsView` (like you would get from `dict.items()`) of
        `(cell_type, (X, obs, var))` tuples for each cell type in this
        Pseudobulk dataset.
        
        Yields:
            An `ItemsView` of `(cell_type, (X, obs, var))` tuples for each cell
            type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.items()
    
    def iter_X(self) -> Iterable[np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]]:
        """
        Iterate over each cell type's `X`.
        
        Yields:
            `X` for each cell type.
        """
        for X in self._X.values():
            yield X
    
    def iter_obs(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's `obs`.
        
        Yields:
            `obs` for each cell type.
        """
        for obs in self._obs.values():
            yield obs
    
    def iter_var(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's `var`.
        
        Yields:
            `var` for each cell type.
        """
        for var in self._var.values():
            yield var
    
    def __eq__(self, other: Pseudobulk) -> bool:
        """
        Test for equality with another Pseudobulk dataset.
        
        Args:
            other: the other Pseudobulk dataset to test for equality with

        Returns:
            Whether the two Pseudobulk datasets are identical.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `==` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return tuple(self.keys()) == tuple(other.keys()) and \
            all(obs.equals(other_obs) for obs, other_obs in
                zip(self._obs.values(), other._obs.values())) and \
            all(var.equals(other_var) for var, other_var in
                zip(self._var.values(), other._var.values())) and \
            all(array_equal(X, other_X) for X, other_X in
                zip(self._X.values(), other._X.values()))
    
    def __or__(self, other: Pseudobulk) -> Pseudobulk:
        """
        Combine the cell types of this Pseudobulk dataset with another. The
        two datasets must have non-overlapping cell types.
        
        Args:
            other: the other Pseudobulk dataset to combine with this one

        Returns:
            A Pseudobulk dataset with each of the cell types in the first
            Pseudobulk dataset, followed by each of the cell types in the
            second.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `|` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        if self.keys() & other.keys():
            error_message = (
                'the left- and right-hand operands of `|` are Pseudobulk '
                'datasets that share some cell types')
            raise ValueError(error_message)
        return Pseudobulk(X=self._X | other._X, obs=self._obs | other._obs,
                          var=self._var | other._var)
    
    def __contains__(self, cell_type: str) -> bool:
        """
        Check if this Pseudobulk dataset contains the specified cell type.
        
        Args:
            cell_type: the cell type

        Returns:
            Whether the cell type is present in the Pseudobulk dataset.
        """
        check_type(cell_type, 'cell_type', str, 'a string')
        return cell_type in self._X
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[str, Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'Pseudobulk indices must be a cell-type string, a length-1 tuple '
            f'of (cell_type,), a length-2 tuple of (cell_type, samples), or a '
            f'length-3 tuple of (cell_type, samples, genes). Samples and '
            f'genes must each be a string or integer; a slice of strings or '
            f'integers; or a list, NumPy array, or polars Series of strings, '
            f'integers, or Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (`obs` or `var`)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('__Pseudobulk_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    @staticmethod
    def _getitem_process(item: Indexer | tuple[str, Indexer, Indexer],
                         index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to `__getitem__()`.
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (`obs` or `var`) to process the element with
                respect to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [Pseudobulk._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = Pseudobulk._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if isinstance(stop, str):
                stop = Pseudobulk._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            df_0_dtype = df[:, 0].dtype
            already_series = isinstance(subitem, pl.Series)
            if already_series:
                subitem_dtype = subitem.dtype
            else:
                subitem = pl.Series(subitem, dtype=df_0_dtype)
                subitem_dtype = df_0_dtype
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            if subitem_dtype == pl.String or subitem_dtype == \
                    pl.Categorical or subitem_dtype == pl.Enum:
                if already_series and subitem_dtype != df_0_dtype:
                    subitem = subitem.cast(df_0_dtype)
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_Pseudobulk_index=pl.int_range(
                              pl.len(), dtype=pl.Int32)),
                          on=df.columns[0], how='left')\
                    ['_Pseudobulk_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif subitem.dtype.is_integer() or subitem.dtype == pl.Boolean:
                return subitem
            else:
                Pseudobulk._getitem_error(item)
        else:
            Pseudobulk._getitem_error(item)
    
    def __getitem__(self, item: Indexer | tuple[str, Indexer, Indexer]) -> \
            Pseudobulk:
        """
        Subset to specific cell type(s), sample(s), and/or gene(s).
        
        Index with a tuple of `(cell_types, samples, genes)`. If `samples` and
        `genes` are integers, arrays/lists/slices of integers, or arrays/lists
        of Booleans, the result will be a Pseudobulk dataset subset to
        `X[samples, genes]`, `obs[samples]`, and `var[genes]` for each of the
        cell types in `cell_types`. However, `samples` and/or `genes` can
        instead be strings (or arrays or slices of strings), in which case they
        refer to the first column of `obs` and/or `var`, respectively.
        
        Examples:
        - Subset to one cell type:
          pseudobulk['Astro']
        - Subset to multiple cell types:
          pseudobulk[['Astro', 'Micro']]
        - Subset to one cell type and sample, for all genes:
          pb['Astro', 'H19.30.002']
          pb['Astro', 2]
        - Subset to one gene, for all cell types and samples:
          pb[:, :, 'APOE']
          pb[:, :, 13196]
        - Subset to one cell type, sample and gene:
          pb['Astro', 'H18.30.002', 'APOE']
          pb['Astro', 2, 13196]
        - Subset to one cell type and a range of samples and genes:
          pb['Astro', 'H18.30.002':'H19.33.004', 'APOE':'TREM2']
          pb['Astro', 'H18.30.002':'H19.33.004', 13196:34268]
        - Subset to one a cell type and specific samples and genes:
          pb['Astro', ['H18.30.002', 'H19.33.004']]
          pb['Astro', :, pl.Series(['APOE', 'TREM2'])]
          pb['Astro', ('H18.30.002', 'H19.33.004'),
             np.array(['APOE', 'TREM2'])]
        
        Args:
            item: the item to index with

        Returns:
            A new Pseudobulk dataset subset to the specified cell types,
            samples, and/or genes.
        """
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 3:
                self._getitem_error(item)
            cell_types = to_tuple(item[0])
        elif isinstance(item, list):
            cell_types = to_tuple(item)
        elif isinstance(item, str):
            cell_types = item,
        else:
            self._getitem_error(item)
        # noinspection PyUnboundLocalVariable
        for cell_type in cell_types:
            if cell_type not in self._X:
                if isinstance(cell_type, str):
                    error_message = (
                        f'tried to select {cell_type!r}, which is not a cell '
                        f'type in this Pseudobulk')
                    raise ValueError(error_message)
                else:
                    error_message = (
                        f'tried to select a non-existent cell type of type '
                        f'{type(cell_type).__name__!r}')
                    raise TypeError(error_message)
        if not isinstance(item, tuple) or len(item) == 1:
            return Pseudobulk(X={cell_type: self._X[cell_type]
                                 for cell_type in cell_types},
                              obs={cell_type: self._obs[cell_type]
                                   for cell_type in cell_types},
                              var={cell_type: self._var[cell_type]
                                   for cell_type in cell_types})
        X, obs, var = {}, {}, {}
        for cell_type in cell_types:
            rows = self._getitem_process(item, 1, self._obs[cell_type])
            if isinstance(rows, pl.Series):
                obs[cell_type] = self._obs[cell_type].filter(rows) \
                    if rows.dtype == pl.Boolean else self._obs[cell_type][rows]
                rows = rows.to_numpy()
            else:
                obs[cell_type] = self._obs[cell_type][rows]
            if len(item) == 2:
                X[cell_type] = self._X[cell_type][rows]
                var[cell_type] = self._var[cell_type]
            else:
                columns = self._getitem_process(item, 2, self._var[cell_type])
                if isinstance(columns, pl.Series):
                    var[cell_type] = self._var[cell_type].filter(columns) \
                        if columns.dtype == pl.Boolean \
                        else self._var[cell_type][columns]
                    columns = columns.to_numpy()
                else:
                    var[cell_type] = self._var[cell_type][columns]
                X[cell_type] = self._X[cell_type][rows, columns] \
                    if isinstance(rows, slice) or \
                       isinstance(columns, slice) else \
                    self._X[cell_type][np.ix_(rows, columns)]
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def sample(self, cell_type: str, sample: str) -> np.ndarray[1, Any]:
        """
        Get the row of `X[cell_type]` corresponding to a single sample, based
        on the sample's name in `obs_names`.
        
        Args:
            cell_type: the cell type to retrieve the row of `X` from
            sample: the name of the sample in `obs_names`
        
        Returns:
            The corresponding row of `X[cell_type]`, as a dense 1D NumPy array
            with zeros included.
        """
        row_index = Pseudobulk._getitem_by_string(self._obs[cell_type], sample)
        return self._X[cell_type][[row_index]].toarray().squeeze()
    
    def gene(self, cell_type: str, gene: str) -> np.ndarray[1, Any]:
        """
        Get the column of `X[cell_type]` corresponding to a single gene, based
        on the gene's name in `var_names`.
        
        Args:
            cell_type: the cell type to retrieve the row of `X` from
            gene: the name of the gene in `var_names`
        
        Returns:
            The corresponding column of `X[cell_type]`, as a dense 1D NumPy
            array with zeros included.
        """
        column_index = \
            Pseudobulk._getitem_by_string(self._var[cell_type], gene)
        return self._X[cell_type][:, [column_index]].toarray().squeeze()
    
    def __iter__(self) -> Iterable[str]:
        """
        Iterate over the cell types of this Pseudobulk dataset.
        `for cell_type in pb` is equivalent to `for cell_type in pb.keys()`.
        
        Returns:
            An iterator over the cell types.
        """
        return iter(self._X)
    
    def __len__(self) -> dict[str, int]:
        """
        Get the number of samples in each cell type of this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to its number of samples.
        """
        return {cell_type: len(X_cell_type)
                for cell_type, X_cell_type in self._X.items()}
    
    def __repr__(self) -> str:
        """
        Get a string representation of this Pseudobulk dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        min_num_samples = min(len(obs) for obs in self._obs.values())
        max_num_samples = max(len(obs) for obs in self._obs.values())
        min_num_genes = min(len(var) for var in self._var.values())
        max_num_genes = max(len(var) for var in self._var.values())
        samples_string = \
            f'{min_num_samples:,} {plural("sample", max_num_samples)}' \
            if min_num_samples == max_num_samples else \
            f'{min_num_samples:,}-{max_num_samples:,} samples'
        genes_string = \
            f'{min_num_genes:,} {plural("gene", max_num_genes)}' \
            if min_num_genes == max_num_genes else \
            f'{min_num_genes:,}-{max_num_genes:,} genes'
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        return f'Pseudobulk dataset with {len(self._X):,} cell ' \
               f'{"types, each" if len(self._X) > 1 else "type,"} with ' \
               f'{samples_string} (obs) and {genes_string} (var)\n' + \
            fill(f'    Cell types: {", ".join(self._X)}',
                 width=terminal_width, subsequent_indent=' ' * 17)
    
    @property
    def shape(self) -> dict[str, tuple[int, int]]:
        """
        Get the shape of each cell type in this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to a length-2 tuple where the
            first element is the number of samples, and the second is the
            number of genes.
        """
        return {cell_type: X_cell_type.shape
                for cell_type, X_cell_type in self._X.items()}
    
    def save(self,
             directory: str | Path,
             overwrite: bool = False,
             *,
             cell_types: str | Iterable[str] | None = None,
             excluded_cell_types: str | Iterable[str] | None = None) -> None:
        # noinspection GrazieInspection
        """
        Saves a Pseudobulk dataset to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with three files per cell type:
        the `X` at `f'{cell_type}.X.npy'`, the `obs` at
        `f'{cell_type}.obs.parquet'`, and the `var` at
        `f'{cell_type}.var.parquet'`. Also saves a text file, `cell_types.txt`,
        containing the cell types.
        
        Args:
            directory: the directory to save the Pseudobulk dataset to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
            cell_types: one or more cell types to save; if `None`, save all
                        cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from saving;
                                 mutually exclusive with `cell_types`
        """
        cell_types = self._process_cell_types(cell_types, excluded_cell_types)
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
            # noinspection PyTypeChecker
            print('\n'.join(cell_types), file=f)
        for cell_type in cell_types:
            escaped_cell_type = cell_type.replace('/', '-')
            np.save(os.path.join(directory, f'{escaped_cell_type}.X.npy'),
                    self._X[cell_type])
            self._obs[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.obs.parquet'))
            self._var[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.var.parquet'))
    
    def copy(self, *, deep: bool = False) -> Pseudobulk:
        """
        Make a copy of this Pseudobulk dataset.
        
        Args:
            deep: whether to make a deep copy instead of a shallow one. Since
                  polars DataFrames are immutable, `obs[cell_type]` and
                  `var[cell_type]` will always point to the same underlying
                  data as the original for all cell types. The only difference
                  when `deep=True` is that `X[cell_type]` will point to a fresh
                  copy of the data, rather than the same data. When
                  `deep=False`, any modifications to the underlying count
                  matrix will modify both the original and the copy.

        Returns:
            A copy of the Pseudobulk dataset.
        """
        check_type(deep, 'deep', bool, 'Boolean')
        return Pseudobulk(X={cell_type: cell_type_X.copy()
                             for cell_type, cell_type_X in self._X.items()}
                            if deep else self._X, obs=self._obs, var=self._var)
    
    def to_df(self,
              *,
              obs_columns: str | Iterable[str] | None = None,
              genes: str | Iterable[str] | None = None,
              cell_type_column: str = 'cell_type') -> pl.DataFrame:
        """
        Convert this Pseudobulk object to a polars DataFrame, with one row per
        (sample, cell type) pair and one column per gene.
        
        The first columns of the DataFrame will contain metadata: a `cell_type`
        column, a sample ID column (the `obs_names`), a `num_cells` column, and
        whichever additional columns are specified in `obs_columns`.
        
        Genes or columns of obs not present in every cell type will contain
        `null` values for cell types where they are missing.
        
        Args:
            obs_columns: one or more names of columns of `obs` to include in
                         the DataFrame, in addition to the cell type, the
                         sample ID, and the number of cells
            genes: one or more genes to include as columns
            cell_type_column: the name of the cell-type column to be added as
                              the first column of the DataFrame

        Returns:
            A polars DataFrame containing the gene counts and metadata for each
            (sample, cell type) pair.
        """
        # Check that `cell_type_column` is a string, and not `num_cells`
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        if cell_type_column == 'num_cells':
            error_message = "the cell_type_column cannot be named 'num_cells'"
            raise ValueError(error_message)
        # Get `obs_columns` and `genes` as tuples, if they are single values,
        # and check that all their elements are strings and that
        # `cell_type_column` name is not in them
        if obs_columns is not None:
            obs_columns = to_tuple_checked(obs_columns, 'obs_columns', str,
                                           'strings')
            if cell_type_column in obs_columns:
                error_message = (
                    f'cell_type_column {cell_type_column!r} is in '
                    f'obs_columns; specify a different name for the '
                    f'cell_type_column, or remove {cell_type_column!r} from '
                    f'obs_columns')
                raise ValueError(error_message)
        if genes is not None:
            genes = to_tuple_checked(genes, 'genes', str, 'strings')
            if cell_type_column in genes:
                error_message = (
                    f'cell_type_column {cell_type_column!r} is in genes; '
                    f'specify a different name for the cell_type_column, or '
                    f'remove {cell_type_column!r} from genes')
                raise ValueError(error_message)
        # Get the DataFrame for each cell type
        dfs = []
        for cell_type, (X, obs, var) in self.items():
            columns = [pl.lit(cell_type).alias(cell_type_column),
                       obs[:, 0].name]
            if 'num_cells' in obs:
                columns.append('num_cells')
            if obs_columns is not None:
                columns += [column for column in obs_columns if column in obs]
            df = obs.select(columns)
            if genes is None:
                gene_df = pl.from_numpy(X, schema=var[:, 0].to_list())
            else:
                gene_df = pl.from_numpy(X[:, genes], schema=genes)
            df = pl.concat((df, gene_df), how='horizontal')
            dfs.append(df)
        # Concatenate across cell types
        df = pl.concat(dfs, how='diagonal_relaxed')
        # Check that all `obs_columns` and/or `genes` appear in the DataFrame
        # (i.e. were present in at least one cell type), if either argument was
        # specified
        for column_set, column_set_name, obs_or_var in \
                (obs_columns, 'obs_columns', 'obs'), (genes, 'genes', 'var'):
            if column_set is not None:
                for column in column_set:
                    if column not in df:
                        error_message = (
                            f"column {column!r} was specified in "
                            f"{column_set_name}, but did not appear in any "
                            f"cell type's {obs_or_var}")
                        raise ValueError(error_message)
        return df
    
    def concat_obs(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate one or more other Pseudobulk datasets with this one,
        sample-wise. All datasets must have the same cell types.
        
        By default, all datasets must have the same `var`. They must also have
        the same columns in `obs`, with the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of `var`, i.e. the `var_names`) before
        concatenating. Subset to columns of `var` that are identical in all
        datasets after this subsetting. Also, subset to columns of `obs` that
        are present in all datasets, and have the same data types. All
        datasets' `obs_names` must have the same name and dtype, and similarly
        for their `var_names`.
        
        The one exception to the `obs` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `obs`.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes and columns of `obs` and `var`
                      common to all datasets before concatenating, rather than
                      raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._obs:
            if flexible:
                # Check that `obs_names` and `var_names` have the same name and
                # data type for each cell type across all datasets
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0] == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0].name == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                # Subset to genes in common across all datasets
                genes_in_common = self._var[cell_type][:, 0]\
                    .filter(self._var[cell_type][:, 0]
                            .is_in(pl.concat([dataset._var[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(genes_in_common) == 0:
                    error_message = (
                        f'no genes are shared across all Pseudobulk datasets '
                        f'for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_var = []
                for dataset in datasets:
                    gene_indices = dataset._getitem_process(
                        genes_in_common, 1, dataset._var[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, gene_indices.to_numpy()])
                    cell_type_var.append(dataset._var[cell_type][gene_indices])
                # Subset to columns of `var` that are identical in all datasets
                # after this subsetting
                var_columns_in_common = [
                    column.name for column in cell_type_var[0][:, 1:]
                    if all(column.name in dataset_cell_type_var and
                           dataset_cell_type_var[column.name].equals(column)
                           for dataset_cell_type_var in cell_type_var[1:])]
                cell_type_var = cell_type_var[0]
                cell_type_var = cell_type_var.select(cell_type_var.columns[0],
                                                     var_columns_in_common)
                # Subset to columns of `obs` that are present in all datasets,
                # and have the same data types. Also include columns of `obs`
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                obs_mismatched_categoricals = {
                    column for column, dtype in self._obs[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._obs[cell_type] and
                           dataset._obs[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._obs[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                obs_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._obs[cell_type].schema.items(), 1, None)
                    if column in obs_mismatched_categoricals or
                       all(column in dataset[cell_type]._obs and
                           dataset._obs[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._obs[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in obs_mismatched_categoricals}
                cell_type_obs = [
                    dataset._obs[cell_type]
                    .cast(cast_dict)
                    .select(obs_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all `var` are identical
                cell_type_var = self._var[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._var[cell_type].equals(cell_type_var):
                        error_message = (
                            f'all Pseudobulk datasets must have the same var '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                # Check that all `obs` have the same columns and data types
                schema = self._obs[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._obs[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in obs for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_obs = [dataset._obs[cell_type]
                                 for dataset in datasets]
            # Concatenate
            X[cell_type] = np.vstack(cell_type_X)
            obs[cell_type] = pl.concat(cell_type_obs)
            var[cell_type] = cell_type_var
        return Pseudobulk(X=X, obs=obs, var=var)

    def concat_var(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate one or more other Pseudobulk datasets with this one,
        gene-wise. This is much less common than the sample-wise concatenation
        provided by `concat_obs()`. All datasets must have the same cell types.
        
        By default, all datasets must have the same `obs`. They must also have
        the same columns in `var`, with the same data types.
        
        Conversely, if `flexible=True`, subset to samples present in all
        datasets (according to the first column of `obs`, i.e. the `obs_names`)
        before concatenating. Subset to columns of `obs` that are identical in
        all datasets after this subsetting. Also, subset to columns of `var`
        that are present in all datasets, and have the same data types. All
        datasets' `obs_names` must have the same name and dtype, and similarly
        for their `var_names`.
        
        The one exception to the `var` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `var`.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to samples and columns of `obs` and
                      `var` common to all datasets before concatenating, rather
                      than raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._var:
            if flexible:
                # Check that `var_names` and `obs_names` have the same name and
                # data type for each cell type across all datasets
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0] == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0].name == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                # Subset to samples in common across all datasets
                samples_in_common = self._obs[cell_type][:, 0]\
                    .filter(self._obs[cell_type][:, 0]
                            .is_in(pl.concat([dataset._obs[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(samples_in_common) == 0:
                    error_message = (
                        f'no samples are shared across all Pseudobulk '
                        f'datasets for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_obs = []
                for dataset in datasets:
                    sample_indices = dataset._getitem_process(
                        samples_in_common, 1, dataset._obs[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, sample_indices.to_numpy()])
                    cell_type_obs.append(
                        dataset._obs[cell_type][sample_indices])
                # Subset to columns of `obs` that are identical in all datasets
                # after this subsetting
                obs_columns_in_common = [
                    column.name for column in cell_type_obs[0][:, 1:]
                    if all(column.name in dataset_cell_type_obs and
                           dataset_cell_type_obs[column.name].equals(column)
                           for dataset_cell_type_obs in cell_type_obs[1:])]
                cell_type_obs = cell_type_obs[0]
                cell_type_obs = cell_type_obs.select(cell_type_obs.columns[0],
                                                     obs_columns_in_common)
                # Subset to columns of `var` that are present in all datasets,
                # and have the same data types. Also include columns of `var`
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                var_mismatched_categoricals = {
                    column for column, dtype in self._var[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._var[cell_type] and
                           dataset._var[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._var[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                var_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._var[cell_type].schema.items(), 1, None)
                    if column in var_mismatched_categoricals or
                       all(column in dataset[cell_type]._var and
                           dataset._var[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._var[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in var_mismatched_categoricals}
                cell_type_var = [
                    dataset._var[cell_type]
                    .cast(cast_dict)
                    .select(var_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all `obs` are identical
                cell_type_obs = self._obs[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._obs[cell_type].equals(cell_type_obs):
                        error_message = (
                            f'all Pseudobulk datasets must have the same obs '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                # Check that all `var` have the same columns and data types
                schema = self._var[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._var[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in var for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_var = [dataset._var[cell_type]
                                 for dataset in datasets]
            # Concatenate
            X[cell_type] = np.hstack(cell_type_X)
            var[cell_type] = pl.concat(cell_type_var)
            obs[cell_type] = cell_type_obs
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],       
                    column: PseudobulkColumn | None |
                            dict[str, PseudobulkColumn | None],
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                                  ...],
                    custom_error: str | None = None,
                    allow_None: bool = True,
                    allow_null: bool = False,
                    cell_types: Sequence[str] | None = None) -> \
            dict[str, pl.Series | None]:
        """
        Get a column of the same length as `obs` or `var` for each cell type.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of each cell type's `obs`/`var`, a
                    polars expression that evaluates to a single column when 
                    applied to each cell type's `obs`/`var`, a polars Series or
                    NumPy array of the same length as each cell type's 
                    `obs`/`var`, or a function that takes in two arguments,
                    `self` and a cell type, and returns a polars Series or
                    NumPy array of the same length as `obs`/`var`. Or, a
                    dictionary mapping cell-type names to any of the above;
                    each cell type in this Pseudobulk dataset must be present.
                    May also be `None` (or a dictionary containing `None`
                    values) if `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the column
            custom_error: a custom error message for when (an element of)
                          `columns` is a string and is not found in
                          `obs`/`var`; use `{}` as a placeholder for the name
                          of the column
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_null: whether to allow `columns` to contain `null` values
            cell_types: a list of cell types; if `None`, use all cell types. If
                        specified and `column` is a Sequence, `column` and
                        `cell_types` should have the same length.
        
        Returns:
            A dictionary mapping each cell type to a polars Series of the same
            length as the cell type's `obs`/`var`. Or, if `columns` is `None`
            (or if some elements are `None`), a dict where all (or some) values
            are `None`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if cell_types is None:
            cell_types = self._X
        if column is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return {cell_type: None for cell_type in cell_types}
        columns = {}
        if isinstance(column, str):
            for cell_type in cell_types:
                if column not in obs_or_var[cell_type]:
                    error_message = (
                        f'{variable_name} {column!r} is not a column of '
                        f'{obs_or_var_name}[{cell_type!r}]'
                        if custom_error is None else
                        custom_error.format(f'{column!r}'))
                    raise ValueError(error_message)
                columns[cell_type] = obs_or_var[cell_type][column]
        elif isinstance(column, pl.Expr):
            for cell_type in cell_types:
                columns[cell_type] = obs_or_var[cell_type].select(column)
                if columns[cell_type].width > 1:
                    error_message = (
                        f'{variable_name} is a polars expression that expands '
                        f'to {columns[cell_type].width:,} columns rather '
                        f'than 1 for cell type {cell_type!r}')
                    raise ValueError(error_message)
                columns[cell_type] = columns[cell_type].to_series()
        elif isinstance(column, pl.Series):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a polars Series of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = column
        elif isinstance(column, np.ndarray):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a NumPy array of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = pl.Series(variable_name, column)
        elif callable(column):
            function = column
            for cell_type in cell_types:
                columns = function(self, cell_type)
                if isinstance(columns, np.ndarray):
                    if columns.ndim != 1:
                        error_message = (
                            f'{variable_name} is a function that returns a '
                            f'{columns.ndim:,}D NumPy array, but must return '
                            f'a polars Series or 1D NumPy array')
                        raise ValueError(error_message)
                    columns = pl.Series(variable_name, columns)
                elif not isinstance(columns, pl.Series):
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'variable of type {type(columns).__name__}, but must '
                        f'return a polars Series or 1D NumPy array')
                    raise TypeError(error_message)
                if len(columns) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a function that returns a column '
                        f'of length {len(columns):,} for cell type '
                        f'{cell_type!r}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = columns
        elif isinstance(column, dict):
            if len(column) != len(cell_types):
                error_message = (
                    f'{variable_name} is a dictionary of length '
                    f'{len(column):,}, which differs from the number of cell '
                    f'types ({len(cell_types):,})')
                raise ValueError(error_message)
            column_set = set(column)
            cell_type_set = set(cell_types)
            if column_set != cell_type_set:
                overlap = len(column_set & cell_type_set)
                if overlap:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but only {overlap:,} '
                        f'of its {len(cell_types):,} keys '
                        f'{plural("correspond", overlap)} to cell types')
                    raise ValueError(error_message)
                else:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but None of its '
                        f'{len(cell_types):,} keys correspond to cell types')
                    raise ValueError(error_message)
            for cell_type, col in column.items():
                if col is None:
                    if allow_None:
                        columns[cell_type] = None
                    else:
                        error_message = \
                            f'{variable_name}[{cell_type!r}] is None'
                        raise TypeError(error_message)
                elif isinstance(col, str):
                    columns[cell_type] = col
                elif isinstance(col, pl.Expr):
                    col = obs_or_var[cell_type].select(col)
                    if col.width > 1:
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'expression that expands to {col.width:,} '
                            f'columns rather than 1')
                        raise ValueError(error_message)
                    columns[cell_type] = col.to_series()
                elif isinstance(col, pl.Series):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'Series of length {len(col):,}, which differs '
                            f'from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                elif isinstance(col, np.ndarray):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a NumPy array '
                            f'of length {len(col):,}, which differs from the '
                            f'length of {obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = pl.Series(variable_name, col)
                elif callable(col):
                    # noinspection PyCallingNonCallable
                    col = col(self, cell_type)
                    if isinstance(col, np.ndarray):
                        if col.ndim != 1:
                            error_message = (
                                f'{variable_name}[{cell_type!r}] is a '
                                f'function that returns a {col.ndim:,}D NumPy '
                                f'array, but must return a polars Series or '
                                f'1D NumPy array')
                            raise ValueError(error_message)
                        col = pl.Series(variable_name, col)
                    elif not isinstance(col, pl.Series):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a variable of type '
                            f'{type(col).__name__}, but must return a '
                            f'polars Series or 1D NumPy array')
                        raise TypeError(error_message)
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a column of length {len(col):,}, '
                            f'which differs from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                else:
                    error_message = (
                        f'{variable_name}[{cell_type!r}] must be a string '
                        f'column name, a polars expression or Series, a 1D '
                        f'NumPy array, or a function that returns any of '
                        f'these when applied to this Pseudobulk dataset and a '
                        f'given cell type, but has type '
                        f'{type(col).__name__!r}')
                    raise TypeError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression or Series, a 1D NumPy array, or a function that '
                f'returns any of these when applied to this Pseudobulk '
                f'dataset and a given cell type, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        # Check dtypes
        if not isinstance(dtypes, tuple):
            dtypes = dtypes,
        for cell_type, col in columns.items():
            base_type = col.dtype.base_type()
            for expected_type in dtypes:
                if base_type == expected_type or expected_type == 'integer' \
                        and base_type in pl.INTEGER_DTYPES or \
                        expected_type == 'floating-point' and \
                        base_type in pl.FLOAT_DTYPES:
                    break
            else:
                if len(dtypes) == 1:
                    dtypes = str(dtypes[0])
                elif len(dtypes) == 2:
                    dtypes = ' or '.join(map(str, dtypes))
                else:
                    dtypes = \
                        ', '.join(map(str, dtypes[:-1])) + f', or {dtypes[-1]}'
                if isinstance(columns, str):
                    error_message = (
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]'
                        f'[{columns!r}] must be {dtypes}, but has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
                else:
                    error_message = (
                        f'{variable_name} must be {dtypes}, but has data type '
                        f'{base_type!r} for cell type {cell_type!r}')
                    raise TypeError(error_message)
        # Check `null` values, if `allow_null=False`
        if not allow_null:
            for cell_type, col in columns.items():
                null_count = col.null_count()
                if null_count > 0:
                    full_variable_name = \
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]' \
                        f'[{columns!r}]' if isinstance(columns, str) else \
                            variable_name
                    error_message = (
                        f'{full_variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cell type '
                        f'{cell_type!r}, but must not contain any')
                    raise ValueError(error_message)
        return columns
    
    def _describe_column(self,
                         column_name: str,
                         column: SingleCellColumn,
                         cell_type: str):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
            cell_type: the cell type where the error was triggered
    
        Returns:
            The column's description: just the argument's name unless the value
            (for this cell type) is a string (i.e. the column's name in `obs`
            or `var`), in which case also include the value.
        """
        if isinstance(column, Sequence):
            cell_type_index = next(i for i, cell_type_i in enumerate(self._obs)
                                   if cell_type_i == cell_type)
            column = column[cell_type_index]
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
        
    def filter_obs(self,
                   *predicates: str | pl.Expr | pl.Series |
                                Iterable[str | pl.Expr | pl.Series] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both `obs` and
        `X` for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to samples
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to samples passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        obs = {}
        for cell_type in self._obs:
            if cell_type in cell_types:
                obs[cell_type] = self._obs[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.Int32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    obs[cell_type]['__Pseudobulk_index'].to_numpy()]
                obs[cell_type] = obs[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                obs[cell_type] = self._obs[cell_type]
        return Pseudobulk(X=X, obs=obs, var=self._var)
    
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both `var` and
        `X` for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        var = {}
        for cell_type in self._var:
            if cell_type in cell_types:
                var[cell_type] = self._var[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.Int32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    :, var[cell_type]['__Pseudobulk_index'].to_numpy()]
                var[cell_type] = var[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                var[cell_type] = self._var[cell_type]
        return Pseudobulk(X=X, obs=self._obs, var=var)
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's `obs`. `obs_names` will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `obs[cell_type]=obs[cell_type].select(*exprs, **named_exprs)` for
            all cell types in `obs`, and `obs_names` as the first column unless
            already included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                new_cell_type_obs = cell_type_obs.select(*exprs, **named_exprs)
                if cell_type_obs.columns[0] not in new_cell_type_obs:
                    new_cell_type_obs = \
                        new_cell_type_obs.select(cell_type_obs[:, 0], pl.all())
                obs[cell_type] = new_cell_type_obs
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's `var`. `var_names` will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `var[cell_type]=var[cell_type].select(*exprs, **named_exprs)` for
            all cell types in `var`, and `var_names` as the first column unless
            already included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                new_cell_type_var = cell_type_var.select(*exprs, **named_exprs)
                if cell_type_var.columns[0] not in new_cell_type_var:
                    new_cell_type_var = \
                        new_cell_type_var.select(cell_type_var[:, 0], pl.all())
                var[cell_type] = new_cell_type_var
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var)
    
    def select_cell_types(self,
                          cell_types: str | Iterable[str],
                          *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset subset to the cell type(s) in
        `cell_types` and `more_cell_types`.
        
        Args:
            cell_types: cell type(s) to select
            *more_cell_types: additional cell types to select, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset subset to the specified cell type(s).
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types += more_cell_types
        for cell_type in cell_types:
            if cell_type not in self._X:
                error_message = (
                    f'tried to select {cell_type!r}, which is not a cell type '
                    f'in this Pseudobulk')
                raise ValueError(error_message)
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in cell_types})
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's `obs`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `obs[cell_type]=obs[cell_type].with_columns(*exprs, **named_exprs)`
            for all cell types.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs={
            cell_type: obs.with_columns(*exprs, **named_exprs)
                       if cell_type in cell_types else obs
            for cell_type, obs in self._obs.items()}, var=self._var)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's `var`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `var[cell_type]=var[cell_type].with_columns(*exprs, **named_exprs)`
            for all cell types.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs, var={
            cell_type: var.with_columns(*exprs, **named_exprs)
                       if cell_type in cell_types else var
            for cell_type, var in self._var.items()})

    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from `obs`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`, 
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the 
                                 operation; mutually exclusive with 
                                 `cell_types`               
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X, 
                          obs={cell_type: obs.drop(columns)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from `var`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`, 
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the 
                                 operation; mutually exclusive with 
                                 `cell_types`           
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: var.drop(columns) 
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()})
    
    def drop_cell_types(self,
                        cell_types: str | Iterable[str],
                        *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset with `cell_types` and `more_cell_types`
        removed. Raises an error if all cell types would be dropped.
        
        Args:
            cell_types: cell type(s) to drop
            *more_cell_types: additional cell types to drop, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) removed.
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types = set(cell_types) | set(more_cell_types)
        # noinspection PyTypeChecker
        original_cell_types = set(self)
        if not cell_types < original_cell_types:
            if cell_types == original_cell_types:
                error_message = 'all cell types would be dropped'
                raise ValueError(error_message)
            for cell_type in cell_types:
                if cell_type not in original_cell_types:
                    error_message = (
                        f'tried to drop {cell_type!r}, which is not a cell '
                        f'type in this Pseudobulk')
                    raise ValueError(error_message)
        new_cell_types = \
            [cell_type for cell_type in self if cell_type not in cell_types]
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in new_cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in new_cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in new_cell_types})
    
    def rename_obs(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of `obs` renamed for
        each cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of `obs` renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs={
            cell_type: obs.rename(mapping) if cell_type in cell_types else obs
            for cell_type, obs in self._obs.items()}, var=self._var)
    
    def rename_var(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of `var` renamed for
        each cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of `var` renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs, var={
            cell_type: var.rename(mapping) if cell_type in cell_types else var
            for cell_type, var in self._var.items()})
    
    def rename_cell_types(self,
                          mapping: dict[str, str] | Callable[[str], str]) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with cell type(s) renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with old
                     cell type names as keys and new names as values, or a
                     function that takes an old name and returns a new name.
                     If `mapping` is a dictionary, cell types missing from its
                     keys will retain their original names.
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) renamed.
        """
        if isinstance(mapping, dict):
            new_cell_types = [mapping.get(cell_type, cell_type)
                              for cell_type in self._X]
        elif callable(mapping):
            new_cell_types = [mapping(cell_type) for cell_type in self._X]
        else:
            raise TypeError(f'mapping must be a dictionary or function, but '
                            f'has type {type(mapping).__name__!r}')
        return Pseudobulk(X={new_cell_type: X
                             for new_cell_type, X in
                             zip(new_cell_types, self._X.values())},
                          obs={new_cell_type: obs
                               for new_cell_type, obs in
                               zip(new_cell_types, self._obs.values())},
                          var={new_cell_type: var
                               for new_cell_type, var in
                               zip(new_cell_types, self._var.values())})
    
    def cast_X(self,
               dtype: np._typing.DTypeLike,
               *,
               cell_types: str | Iterable[str] | None = None,
               excluded_cell_types: str | Iterable[str] |
                                    None = None) -> Pseudobulk:
        """
        Cast each cell type's `X` to the specified data type.
        
        Args:
            dtype: a NumPy data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with each cell type's `X` cast to the
            specified data type.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X={cell_type: X.astype(dtype)
                                        if X in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's `obs` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's `obs`
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs={cell_type: obs.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's `var` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's `var`
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs=self._obs,
                          var={cell_type: var.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()},)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Left join each cell type's `obs` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's `obs` with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in `obs`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `obs` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `obs`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's `obs`.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    f"either 'on' or both of 'left_on' and 'right_on' must be "
                    f"specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type not in cell_types:
                obs[cell_type] = cell_type_obs
                continue
            left = cell_type_obs
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'obs[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            obs[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(obs[cell_type]) > len(self._obs[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Join each cell type's `var` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's `var` with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in `var`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `var` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `var`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's `var`.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type not in cell_types:
                var[cell_type] = cell_type_var
                continue
            left = cell_type_var
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'var[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            var[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(var[cell_type]) > len(self._var[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=self._obs, var=var)
    
    def peek_obs(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of `obs` (the first row, by default) for a cell type (the
        first cell type, by default) with each column on its own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._obs))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def peek_var(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of `var` (the first row, by default) for a cell type (the
        first cell type, by default) with each column on its own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._var))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer | None = None,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of samples.
        
        Args:
            n: the number of samples to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of samples to return; mutually exclusive
                      with `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `obs` to subsample by. Can be `None`, a column name,
                       a polars expression, a polars Series, a 1D NumPy array,
                       or a function that takes in this Pseudobulk dataset and
                       a cell type and returns a polars Series or 1D NumPy
                       array. Or, a dictionary mapping cell-type names to any
                       of the above; each cell type in this Pseudobulk dataset
                       must be present. Specifying `by_column` ensures that the
                       same fraction of cells with each value of `by_column`
                       are subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups can be undersampled by one
                       element. Can contain `null` entries: the corresponding
                       samples will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled samples; if `None`,
                              subset to these samples instead
            seed: the random seed to use when subsampling, or leave unset to
                  use `single_cell.options()['seed']` as the seed (0 by
                  default)
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `obs`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new Pseudobulk dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `obs`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'obs', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._obs[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of obs[{cell_type!r}]')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = (
                'overwrite must be False when subsample_column is None; did '
                'you already run subsample_obs()? Set overwrite=True to '
                'overwrite.')
            raise ValueError(error_message)
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign `n` to be a vector of sample sizes per group, broadcast
            # to the length of `obs`. The total sample size should exactly
            # match the original `n`; if necessary, oversample the smallest
            # groups or undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            obs = {}
            for cell_type, cell_type_obs in self._obs.items():
                if cell_type in cell_types:
                    cell_type_obs = cell_type_obs\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.Int32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_obs['__Pseudobulk_index'].to_numpy()]
                    obs[cell_type] = cell_type_obs.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    obs[cell_type] = cell_type_obs
            return Pseudobulk(X=X, obs=obs, var=self._var)
        else:
            return Pseudobulk(X=self._X, obs={
                cell_type: obs.with_columns(expressions[cell_type]
                                            .alias(subsample_column)) 
                           if cell_type in cell_types else obs
                for cell_type, obs in self._obs.items()}, var=self._var)
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer | None = None,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `var` to subsample by. Can be `None`, a column name,
                       a polars expression, a polars Series, a 1D NumPy array,
                       or a function that takes in this Pseudobulk dataset and
                       a cell type and returns a polars Series or 1D NumPy
                       array. Or, a dictionary mapping cell-type names to any
                       of the above; each cell type in this Pseudobulk dataset
                       must be present. Specifying `by_column` ensures that the
                       same fraction of cells with each value of `by_column`
                       are subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups may be undersampled by one
                       element. Can contain `null` entries: the corresponding
                       genes will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling, or leave unset to
                  use `single_cell.options()['seed']` as the seed (0 by
                  default)
            overwrite: if `True`, overwrite `subsample_column` if already 
                       present in `var`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.

        Returns:
            A new Pseudobulk dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `var`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'var', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._var[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of var[{cell_type!r}]')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = (
                'overwrite must be False when subsample_column is None; did '
                'you already run subsample_var()? Set overwrite=True to '
                'overwrite.')
            raise ValueError(error_message)
        if seed is None:
            seed = _seed
        else:
            check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign `n` to be a vector of sample sizes per group, broadcast
            # to the length of `var`. The total sample size should exactly
            # match the original `n`; if necessary, oversample the smallest
            # groups or undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            var = {}
            for cell_type, cell_type_var in self._var.items():
                if cell_type in cell_types:
                    cell_type_var = cell_type_var\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.Int32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_var['__Pseudobulk_index'].to_numpy()]
                    var[cell_type] = cell_type_var.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    var[cell_type] = cell_type_var
            return Pseudobulk(X=X, obs=self._obs, var=var)
        else:
            return Pseudobulk(X=self._X, obs=self._obs, var={
                cell_type: var.with_columns(expressions[cell_type]
                                            .alias(subsample_column))
                           if cell_type in cell_types else var
                for cell_type, var in self._var.items()})
    
    def pipe(self,
             function: Callable[[Pseudobulk, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a Pseudobulk dataset.
        
        Args:
            function: the function to apply
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this Pseudobulk dataset.
        """
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]], ...],
                                  dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]]],
               *args: Any,
               **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `X`. To apply a function to
        each cell type's `X`, rather than to `X` as a whole, use `map_X()`.
        
        Args:
            function: the function to apply to `X`. It must take the old `X` as
                      its first argument and return the new `X`. The function
                      may also take other arguments after `X`, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            `X`.
        """
        return Pseudobulk(X=function(self._X, *args, **kwargs),
                          obs=self._obs, var=self._var)
    
    def pipe_obs(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `obs`. To apply a function
        to each cell type's `obs`, rather than to `obs` as a whole, use
        `map_obs()`.
        
        Args:
            function: the function to apply to `obs`. It must take the old
                      `obs` as its first argument and return the new `obs`. The
                      function may also take other arguments after `obs`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to 
            obs.
        """
        return Pseudobulk(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var)
    
    def pipe_var(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `var`. To apply a function
        to each cell type's `var`, rather than to `var` as a whole, use
        `map_var()`.
        
        Args:
            function: the function to apply to `var`. It must take the old
                      `var` as its first argument and return the new `var`. The
                      function may also take other arguments after `var`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to 
            var.
        """
        return Pseudobulk(X=self._X, obs=self._obs, 
                          var=function(self._var, *args, **kwargs))

    def map_X(self,
              function: Callable[[np.ndarray[2, np.dtype[np.integer |
                                                         np.floating]], ...],
                                 np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]],
              *args: Any,
              cell_types: str | Iterable[str] | None = None,
              excluded_cell_types: str | Iterable[str] | None = None,
              **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `X`. To apply a function to `X` as
        a whole, rather than each cell type's `X`, use `pipe_X()`.
        
        Args:
            function: the function to apply to each cell type's `X`. It must
                      take the old `X` for a cell type and return the new `X`.
                      The function may also take other arguments after `X`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `X`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X={cell_type: function(X, *args, **kwargs)
                                        if cell_type in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var)
    
    def map_obs(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `obs`. To apply a function to
        `obs` as a whole, rather than each cell type's `obs`, use `pipe_obs()`.
        
        Args:
            function: the function to apply to each cell type's `obs`. It must
                      take the old `obs` for a cell type and return the new
                      `obs`. The function may also take other arguments after
                      `obs`, which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `obs`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs={cell_type: function(obs, *args, **kwargs) 
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)
    
    def map_var(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `var`. To apply a function to
        `var` as a whole, rather than each cell type's `var`, use `pipe_var()`.
        
        Args:
            function: the function to apply to each cell type's `var`. It must
                      take the old `var` for a cell type and return the new
                      `var`. The function may also take other arguments after
                      `var`, which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `var`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: function(var, *args, **kwargs) 
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()})
    
    @staticmethod
    def _too_few_samples(obs: pl.DataFrame,
                         group_column: pl.Series | None,
                         min_samples: int | np.integer,
                         cell_type: str,
                         verbose: bool,
                         after_filtering: bool = False) -> bool:
        """
        Skip cell types with fewer than `min_samples` samples, or with fewer
        than `min_samples` samples in any group if `group_column` is not None.
        
        Args:
            obs: the cell type's `obs`, after applying one or more QC filters
            group_column: the column with sample group information (e.g. which
                          samples are disease cases and which are controls),
                          after applying one or more QC filters. The samples
                          in this column must be the same as those in `obs`.
            min_samples: filter to cell types with at least this many samples
                         in each group, or with at least this many total
                         samples if `group_column` is `None`
            cell_type: the name of the cell type
            verbose: whether to explain why the cell type is being skipped, if
                     it is
            after_filtering: whether this function is being run after sample
                             filtering

        Returns:
            Whether this cell type has too few samples and should be skipped.
        """
        num_samples = len(obs)
        if num_samples == 0:
            if verbose:
                print(f'[{cell_type}] Skipping this cell type because '
                      f'it has 0 samples'
                      f'{" after filtering" if after_filtering else ""}.\n')
            return True
        elif group_column is None:
            if num_samples < min_samples:
                if verbose:
                    print(f'[{cell_type}] Skipping this cell type because '
                          f'it has only {num_samples:,} '
                          f'{plural("sample", num_samples)}'
                          f'{" after filtering" if after_filtering else ""}, '
                          f'which is fewer than min_samples '
                          f'({min_samples:,})\n')
                return True
        else:
            too_small_groups = group_column\
                .value_counts()\
                .filter(pl.col.count < min_samples)\
                .to_series()\
                .drop_nulls()
            if len(too_small_groups) > 0:
                if verbose:
                    group_description = (
                        f'{too_small_groups["count"][-1]:,} samples where '
                        f'group_column = {too_small_groups["group"][-1]}')
                    if len(too_small_groups) > 1:
                        group_description = (
                            ', '.join(f'{count:,} samples where '
                                      f'group_column = {group}'
                                      for group, count in
                                      too_small_groups[:-1].iter_rows()) +
                            f' and {group_description}')
                    print(f'[{cell_type}] Skipping this cell type because it '
                          f'has only {group_description} after filtering, '
                          f'which '
                          f'{"is" if len(too_small_groups) == 1 else "are"} '
                          f'fewer than min_samples ({min_samples:,})\n')
                return True
        return False
    
    def qc(self,
           group_column: PseudobulkColumn | None |
                         dict[str, PseudobulkColumn | None],
           *,
           custom_filter: PseudobulkColumn | None |
                          dict[str, PseudobulkColumn | None] = None,
           min_samples: int | np.integer = 2,
           min_cells: int | np.integer | None = 10,
           max_standard_deviations: int | float | np.integer | np.floating |
                                    None = 3,
           min_nonzero_fraction: int | float | np.integer | np.floating |
                                 None = 0.8,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           error_if_negative_counts: bool = True,
           allow_float: bool = False,
           verbose: bool = True) -> Pseudobulk:
        """
        Subsets each cell type to samples passing quality control (QC). If
        samples fall into discrete groups (e.g. disease cases versus controls),
        these should be specified via the `group_column` argument.
        
        Filters, in order, to:
        - samples that pass the `custom_filter` (if specified), have
          non-missing values for `group_column` (if specified), and have at
          least `min_cells` cells of that type (default: 10)
        - samples where the number of genes with 0 counts is at most
          `max_standard_deviations` standard deviations above the mean
          (default: 3)
        - genes with at least 1 count in `100 * min_nonzero_fraction`%
          (default: 80%) of samples (in every group, if `group_column` is
          specified)
        
        If at any point during this filtering process, there are fewer than
        `min_samples` samples (in any group, if `group_column` is specified),
        the cell type is filtered out entirely.
        
        Args:
            group_column: an optional String, Categorical, Enum, Boolean, or
                          integer column of `obs` with sample group
                          information, e.g. which samples are disease cases and
                          which are controls. If specified, the
                          `min_nonzero_fraction` and `min_samples` filters must
                          pass for every group, rather than merely passing for
                          the dataset as a whole. Set to `None` if samples do
                          not fall into discrete groups. Can be `None`, a
                          column name, a polars expression, a polars Series, a
                          1D NumPy array, or a function that takes in this
                          Pseudobulk dataset and a cell type and returns a
                          polars Series or 1D NumPy array. Or, a dictionary
                          mapping cell-type names to any of the above; each
                          cell type in this Pseudobulk dataset must be present.
                          Can contain `null` entries: the corresponding samples
                          will be deemed to fail QC.
            custom_filter: an optional Boolean column of `obs` containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be `None`, a
                           column name, a polars expression, a polars Series, a
                           1D NumPy array, or a function that takes in this
                           Pseudobulk dataset and a cell type and returns a
                           polars Series or 1D NumPy array. Or, a dictionary
                           mapping cell-type names to any of the above; each
                           cell type in this Pseudobulk dataset must be
                           present.
            min_samples: filter to cell types with at least this many samples
                         in every group, or with at least this many total
                         samples if `group_column` is `None`
            min_cells: if not `None`, filter to samples with â‰¥ this many cells
                       of each cell type
            max_standard_deviations: if not `None`, filter to samples where the
                                     number of genes with 0 counts is at most
                                     this many standard deviations above the
                                     mean
            min_nonzero_fraction: if not `None`, filter to genes with at least
                                  one count in this fraction of samples in each
                                  group, or if `group_column` is `None`, at
                                  least one count in this fraction of samples
                                  overall. Note: `min_nonzero_fraction=0`
                                  filters out only genes with all-zero counts,
                                  while `min_nonzero_fraction=None` does not
                                  filter out any genes.
            cell_types: one or more cell types to QC; if `None`, QC all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from QC;
                                 mutually exclusive with `cell_types`
            error_if_negative_counts: if `True`, raise an error if any counts
                                      are negative
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            verbose: whether to print how many samples and genes were filtered
                     out at each step of the QC process
        
        Returns:
            A new Pseudobulk dataset with each cell type's `X`, `obs` and `var`
            subset to samples and genes passing QC.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        # Check inputs
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        group_column = self._get_column(
            'obs', group_column, 'group_column',
            (pl.String, pl.Categorical, pl.Enum, pl.Boolean, 'integer'),
            allow_null=True)
        custom_filter = self._get_column(
            'obs', custom_filter, 'custom_filter', pl.Boolean)
        check_type(min_samples, 'min_samples', int,
                   'an integer greater than or equal to 2')
        check_bounds(min_samples, 'min_samples', 2)
        if min_cells is not None:
            check_type(min_cells, 'min_cells', int, 'a positive integer')
            check_bounds(min_cells, 'min_cells', 1)
        if max_standard_deviations is not None:
            check_type(max_standard_deviations, 'max_standard_deviations',
                       (int, float), 'a positive number')
            check_bounds(max_standard_deviations, 'max_standard_deviations', 0,
                         left_open=True)
        if min_nonzero_fraction is not None:
            check_type(min_nonzero_fraction, 'min_nonzero_fraction',
                       (int, float), 'a number between 0 and 1, inclusive')
            check_bounds(min_nonzero_fraction, 'min_nonzero_fraction', 0, 1)
        check_type(error_if_negative_counts, 'error_if_negative_counts', bool,
                   'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `group_column` is None when neither the
        # `min_nonzero_fraction` nor the `min_samples` filters will be applied
        if group_column is not None and min_nonzero_fraction is None and \
                min_samples is None:
            error_message = (
                'group_column must be None when min_nonzero_fraction and '
                'min_samples are both None')
            raise ValueError(error_message)
        # If `error_if_negative_counts=True`, raise an error if `X` has any
        # negative values
        if error_if_negative_counts:
            for cell_type in cell_types:
                if self._X[cell_type].ravel().min() < 0:
                    error_message = f'X[{cell_type!r}] has negative counts'
                    raise ValueError(error_message)
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float:
            for cell_type in cell_types:
                dtype = self._X[cell_type].dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"qc() requires raw counts but X[{cell_type!r}] "
                        f"has data type {str(dtype)!r}, a floating-point data "
                        f"type; if you are sure that all values are raw "
                        f"integer counts, i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        if verbose:
            print()
        X_qced, obs_qced, var_qced = {}, {}, {}
        for cell_type in self._X:
            X = self._X[cell_type]
            obs = self._obs[cell_type]
            var = self._var[cell_type]
            if cell_type not in cell_types:
                X_qced[cell_type] = X
                obs_qced[cell_type] = obs
                var_qced[cell_type] = var
                if verbose:
                    if cell_types is not None:
                        print(f'[{cell_type}] Skipping this cell type due to '
                              f'being absent from cell_types')
                    else:
                        print(f'[{cell_type}] Skipping this cell type due to '
                              f'being present in excluded_cell_types')
                continue
            if verbose:
                print(f'[{cell_type}] Starting with {len(obs):,} samples and '
                      f'{len(var):,} genes.')
            # Get the group column for this cell type.
            groups = \
                group_column[cell_type] if group_column is not None else None
            # Check if we have enough samples for this cell type
            if Pseudobulk._too_few_samples(obs, groups, min_samples, cell_type,
                                           verbose):
                continue
            # Get a mask of samples passing the custom filter, if specified
            if custom_filter is not None:
                if verbose:
                    print(f'[{cell_type}] Applying the custom filter...')
                sample_mask = custom_filter[cell_type]
                if sample_mask is not None:
                    if verbose:
                        print(f'[{cell_type}] {sample_mask.sum():,} samples '
                              f'remain after applying the custom filter.')
            else:
                sample_mask = None
            # If `groups` is not `None` and some samples have missing groups,
            # get a mask of samples with non-missing groups
            if groups is not None:
                null_count = groups.null_count()
                if null_count:
                    if verbose:
                        print(f'[{cell_type}] Filtering to samples with '
                              f'non-missing values for group_column...')
                    if sample_mask is None:
                        sample_mask = groups.is_not_null()
                    else:
                        sample_mask &= groups.is_not_null()
                    if verbose:
                        print(f'[{cell_type}] {sample_mask.sum():,} samples '
                              f'remain after filtering to samples with '
                              f'non-missing values for group_column.')
            # Get a mask of samples with at least `min_cells` cells of this
            # cell type, if `min_cells` was specified. Combine this with the
            # sample mask from `custom_filter` above, if both were specified.
            if min_cells is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples with at least '
                          f'{min_cells} {cell_type} cells...')
                if sample_mask is None:
                    sample_mask = obs['num_cells'] >= min_cells
                else:
                    sample_mask &= obs['num_cells'] >= min_cells
                if verbose:
                    print(f'[{cell_type}] {sample_mask.sum():,} samples '
                          f'remain after filtering to samples with at least '
                          f'{min_cells} {cell_type} cells.')
            # Now apply the sample mask, which contains the samples passing the
            # custom filter and/or `min_cells` filter
            if sample_mask is not None:
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                # Check if we still have enough samples for this cell type,
                # after applying these three filters
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
                # noinspection PyUnresolvedReferences
                X = X[sample_mask.to_numpy()]
            # Filter to samples where the number of genes with 0 counts is less
            # than `max_standard_deviations` standard deviations above the mean
            if max_standard_deviations is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples where the '
                          f'number of genes with 0 counts is '
                          f'<{max_standard_deviations} standard deviations '
                          f'above the mean...')
                # noinspection PyUnresolvedReferences
                num_zero_counts = (X == 0).sum(axis=1, dtype=np.uint64)
                sample_mask_NumPy = \
                    num_zero_counts < num_zero_counts.mean() + \
                    max_standard_deviations * num_zero_counts.std()
                sample_mask = pl.Series(sample_mask_NumPy)
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                if verbose:
                    print(f'[{cell_type}] {len(obs):,} samples remain after '
                          f'filtering to samples where the number of genes '
                          f'with 0 counts is <{max_standard_deviations} '
                          f'standard deviations above the mean.')
                # Check if we have still enough samples for this cell type,
                # after applying this filter
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
                X = X[sample_mask_NumPy]
            # Filter to genes with at least 1 count in
            # `100 * min_nonzero_fraction`% of samples (or samples in each
            # group, if `group_column` is not None for this cell type)
            if min_nonzero_fraction is not None:
                if groups is not None:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples in '
                              f'each group...')
                    gene_mask = np.logical_and.reduce([
                        np.quantile(X[mask.to_numpy()],
                                    1 - min_nonzero_fraction, axis=0) > 0
                        for mask in groups.to_dummies().cast(pl.Boolean)])
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        print(f'[{cell_type}] {len(var):,} genes remain '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples in each group.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples...')
                    gene_mask = np.quantile(X, 1 - min_nonzero_fraction,
                                            axis=0) > 0
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        print(f'[{cell_type}] {len(var):,} genes remain '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples.')
            X_qced[cell_type] = np.ascontiguousarray(X)
            obs_qced[cell_type] = obs
            var_qced[cell_type] = var
            if verbose:
                print()
        return Pseudobulk(X=X_qced, obs=obs_qced, var=var_qced)
    
    @staticmethod
    def _library_size(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                      cell_type: str,
                      *,
                      logratio_trim: int | float | np.integer |
                                     np.floating = 0.3,
                      sum_trim: int | float | np.integer | np.floating = 0.05,
                      A_cutoff: int | float | np.integer |
                                np.floating = -1e10) -> \
            np.ndarray[1, np.dtype[np.float64]]:
        """
        Calculate normalization factor-adjusted library sizes according to the
        method of edgeR's `calcNormFactors()` with the default `method='TMM'`.
        Used by `DE()`, `regress_out()`, and `library_size()`.

        Results differ from edgeR due to the presence of a floating-point bug
        in the original `calcNormFactors()` implementation. When calculating
        `logR`, the log2 ratio of `count / library_size` for a gene between a
        particular sample and a "reference" sample, the numerator and
        denominator of the ratio both involve a division by their sample's
        library size. In principle, these divisions by library size are
        equivalent to multiplying by the same constant across genes, namely the
        ratio of the two samples' library sizes. But in practice, even if two
        genes have the same count ratio between the two samples, they may still
        have slightly different `count / library_size` ratios due to
        floating-point roundoff, leading to these genes erroneously being
        assigned different `logR` ranks instead of being treated as tied. Our
        implementation fixes this bug by changing the order of operations so
        that the library size ratio is calculated first, then multiplied by the
        count ratio.

        Because this bug affects which genes are included in the trimmed mean,
        its impact can be relatively large, sometimes leading to a >1% error in
        edgeR's estimated library size relative to our correct implementation.

        Does not support the `lib.size` and `refColumn` arguments to
        `calcNormFactors()`; these are both assumed to be `NULL` (the default)
        and will always be calculated internally. The `doWeighting` argument is
        also not supported and is assumed to be `TRUE` (the default), so
        asymptotic binomial precision weights will always be used.

        Args:
            X: a matrix of raw (read) counts. `X` is assumed to have the
               opposite orientation from the original `calcNormFactors()`:
               samples are rows and genes are columns.
            cell_type: the cell type `X` is from, used in error messages
            logratio_trim: the amount of trim to use on log-ratios ("M"
                           values); must be greater than 0 and less than 1
            sum_trim: the amount of trim to use on the combined absolute levels
                      ("A" values); must be greater than 0 and less than 1
            A_cutoff: the cutoff on "A" values to use before trimming

        Returns:
            The norm factor-corrected library sizes: raw library sizes (column
            sums) times norm factors.
        """
        # Degenerate cases
        num_samples, num_genes = X.shape
        if num_samples == 1 or num_genes == 0:
            return np.ones(num_samples)

        # Raise an error if there are any all-zero columns (genes)
        has_all_zero_columns = cython_inline('''
            ctypedef fused numeric:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            def has_all_zero_columns(const numeric[:, :] X):
                cdef unsigned i, j
                for j in range(X.shape[1]):
                    for i in range(X.shape[0]):
                        if X[i, j] != 0:
                            break
                    else:
                        return True
                return False
            ''')['has_all_zero_columns'](X=X)
        if has_all_zero_columns:
            error_message = (
                f'[{cell_type}] some genes have all-zero counts; did you '
                f'forget to run Pseudobulk.qc()?')
            raise ValueError(error_message)

        # Calculate raw library sizes
        library_size = X.sum(axis=1)

        # Raise an error if any raw library sizes are 0
        if library_size.min() == 0:
            error_message = (
                'some samples have all-zero counts; did you forget to run '
                'Pseudobulk.qc()?')
            raise ValueError(error_message)

        # Determine which sample is the reference sample
        f75 = np.quantile(X, 0.75, axis=1) / library_size
        if np.median(f75) < 1e-20:
            ref_sample = np.argmax(np.sqrt(X).sum(axis=1))
        else:
            ref_sample = np.argmin(np.abs(f75 - f75.mean()))

        # Preallocate arrays for norm factor calculation
        norm_factors = np.empty(num_samples)
        inv_relative_library_size = np.empty(num_samples)
        log_normalized_X_ref = np.empty(num_genes)
        logR = np.empty(num_genes)
        absE = np.empty(num_genes)
        counts = np.empty(num_genes, dtype=X.dtype)
        ref_counts = np.empty(num_genes, dtype=X.dtype)
        indices = np.empty(num_genes, dtype=np.uint32)
        logR_rank = np.empty(num_genes)
        absE_rank = np.empty(num_genes)
        # Calculate norm factors
        cython_inline(r'''
            from libcpp.algorithm cimport sort
            from libcpp.cmath cimport abs, exp, log, log2
            
            ctypedef fused numeric:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            ctypedef fused numeric_2:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            cdef extern from *:
                """
                struct Compare {
                    const double* data;
                    Compare() noexcept {}
                    Compare(const double* d) noexcept : data(d) {}
                    bool operator()(unsigned a, unsigned b) const noexcept {
                        return data[a] < data[b];
                    }
                };
                """
                cdef cppclass Compare:
                    Compare(const double*) noexcept nogil
                    bint operator()(unsigned int, unsigned int) noexcept nogil
            
            cdef inline void argsort(const double[::1] arr, unsigned[::1] indices,
                                     const unsigned n) noexcept nogil:
                cdef unsigned i
                for i in range(n):
                    indices[i] = i
                sort(&indices[0], &indices[0] + n, Compare(&arr[0]))

            cdef inline void rankdata(const double[::1] data,
                                      unsigned[::1] indices,
                                      double[::1] ranks,
                                      const unsigned n) noexcept nogil:
                cdef unsigned i = 0, start_pos = 0
                cdef double current_val, rank
                cdef bint end = False

                argsort(data, indices, n)

                while True:
                    current_val = data[indices[i]]

                    # Count elements equal to current value
                    
                    i += 1
                    if i == n:
                        end = True
                    else:
                        while data[indices[i]] == current_val:
                            i += 1
                            if i == n:
                                end = True
                                break

                    # Assign average rank to all tied elements
                    
                    rank = 0.5 * (start_pos + i) + 0.5
                    while start_pos < i:
                        ranks[indices[start_pos]] = rank
                        start_pos += 1

                    if end:
                        break

            def calc_norm_factors(
                    const numeric[:, :] X,
                    const double logratio_trim,
                    const double sum_trim,
                    const double A_cutoff,
                    const unsigned ref_sample,
                    double[::1] norm_factors,
                    numeric_2[::1] library_size,
                    double[::1] inv_relative_library_size,
                    double[::1] log_normalized_X_ref,
                    double[::1] logR,
                    double[::1] absE,
                    numeric[::1] counts,
                    numeric[::1] ref_counts,
                    unsigned[::1] indices,
                    double[::1] logR_rank,
                    double[::1] absE_rank):

                cdef numeric ref_count, count, ref_library_size
                cdef unsigned i, j, n, loL, hiL, loS, hiS, \
                    num_samples = X.shape[0], num_genes = X.shape[1]
                cdef double inverse_ref_library_size, logR_, absE_, \
                    inverse_library_size, total_inverse_library_size, \
                    norm_factor, total_weight, variance, weight, scale
                cdef bint large_enough_logR

                # Calculate each sample's library size relative to the
                # reference sample's (to use in the `logR` calculation)
                
                ref_library_size = library_size[ref_sample]
                for i in range(num_samples):
                    inv_relative_library_size[i] = \
                        <double> ref_library_size / library_size[i]

                # Calculate each gene's log normalized expression (to use in
                # the `absE` calculation)
                
                inverse_ref_library_size = 1. / ref_library_size
                for j in range(num_genes):
                    count = X[ref_sample, j]
                    log_normalized_X_ref[j] = \
                        log2(count * inverse_ref_library_size)

                # Calculate the normalization factor for each sample
                
                for i in range(num_samples):
                    inverse_library_size = 1. / library_size[i]
                    large_enough_logR = False
                    n = 0
                    for j in range(num_genes):
                        # Get the count and reference count for this gene; skip
                        # the gene if either are 0
                        
                        ref_count = X[ref_sample, j]
                        if ref_count == 0:
                            continue

                        count = X[i, j]
                        if count == 0:
                            continue

                        # Calculate the log ratio of expression accounting for
                        # library size
                        
                        logR_ = log2(inv_relative_library_size[i] * (
                            <double> count / ref_count))

                        # Calculate "absolute expression": the average log2
                        # expression of this gene between this sample and the
                        # reference sample
                        
                        absE_ = 0.5 * (log2(count * inverse_library_size) +
                                       log_normalized_X_ref[j])

                        # Cutoff based on `A_cutoff`
                        
                        if absE_ <= A_cutoff:
                            continue

                        # Store `logR`, `absE`, and the count for genes passing
                        # the infinite value and `A_cutoff` filters above
                        
                        logR[n] = logR_
                        absE[n] = absE_
                        counts[n] = count
                        ref_counts[n] = ref_count
                        n += 1

                        # Keep track of whether any gene's `logR` is above 1e-6
                        # in magnitude for this sample
                        
                        large_enough_logR |= abs(logR_) >= 1e-6

                    # If every gene's `logR` is below 1e-6 in magnitude for
                    # this sample (i.e. expression is extremely low across the
                    # board), set the sample's norm factor to 1
                    
                    if not large_enough_logR:
                        norm_factors[i] = 1
                        continue

                    # Rank genes by `logR` and `absE`
                    
                    loL = <unsigned> (n * logratio_trim)
                    hiL = n - loL
                    loS = <unsigned> (n * sum_trim)
                    hiS = n - loS
                    rankdata(logR, indices, logR_rank, n)
                    rankdata(absE, indices, absE_rank, n)

                    # Calculate the norm factors themselves. Find genes with
                    # intermediate ranks of both `logR` and `absE` (this is the
                    # "trimmed" part, the "T" in "TMM"). The norm factors are 2
                    # to the power of the weighted average of the logRs for
                    # these intermediate-ranked genes, where the weights are
                    # the inverse asymptotic variances.
                    
                    total_inverse_library_size = \
                        inverse_library_size + inverse_ref_library_size
                    norm_factor = 0
                    total_weight = 0
                    for j in range(n):
                        if loL + 1 <= logR_rank[j] <= hiL and \
                                loS + 1 <= absE_rank[j] <= hiS:
                            variance = 1. / counts[j] + 1. / ref_counts[j] + \
                                total_inverse_library_size
                            weight = 1 / variance
                            norm_factor += weight * logR[j]
                            total_weight += weight
                    norm_factor = 2 ** (norm_factor / total_weight)

                    # Results will be missing if the two libraries share no
                    # features with positive counts; in this case, set to 1
                    
                    if norm_factor != norm_factor:  # i.e. NaN
                        norm_factor = 1

                    norm_factors[i] = norm_factor

                # Normalize factors across samples so that they multiply to 1
                
                scale = 0
                for i in range(num_samples):
                    scale += log(norm_factors[i])
                scale = exp(-scale / num_samples)
                for i in range(num_samples):
                    norm_factors[i] *= scale

                # Multiply norm factors by library sizes
                
                for i in range(num_samples):
                    norm_factors[i] *= library_size[i]

            ''')['calc_norm_factors'](
                X=X, logratio_trim=logratio_trim, sum_trim=sum_trim,
                A_cutoff=A_cutoff, ref_sample=ref_sample,
                norm_factors=norm_factors, library_size=library_size,
                inv_relative_library_size=inv_relative_library_size,
                log_normalized_X_ref=log_normalized_X_ref,
                logR=logR, absE=absE, counts=counts, ref_counts=ref_counts,
                indices=indices, logR_rank=logR_rank, absE_rank=absE_rank)

        return norm_factors  # this is actually `library_size * norm_factors`
    
    def library_size(self,
                     *,
                     library_size_column: str = 'library_size',
                     cell_types: str | Iterable[str] | None = None,
                     excluded_cell_types: str | Iterable[str] | None = None,
                     logratio_trim: int | float | np.integer |
                                    np.floating = 0.3,
                     sum_trim: int | float | np.integer | np.floating = 0.05,
                     A_cutoff: int | float | np.integer |
                               np.floating = -1e10,
                     overwrite: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            Pseudobulk:
        """
        Calculate normalization factor-adjusted library sizes according to the
        method of edgeR's `calcNormFactors()` with the default `method='TMM'`.
        
        Results differ from edgeR due to the presence of a floating-point bug
        in the original `calcNormFactors()` implementation. When calculating
        `logR`, the log2 ratio of `count / library_size` for a gene between a
        particular sample and a "reference" sample, the numerator and
        denominator of the ratio both involve a division by their sample's
        library size. In principle, these divisions by library size are
        equivalent to multiplying by the same constant across genes, namely the
        ratio of the two samples' library sizes. But in practice, even if two
        genes have the same count ratio between the two samples, they may still
        have slightly different `count / library_size` ratios due to
        floating-point roundoff, leading to these genes erroneously being
        assigned different `logR` ranks instead of being treated as tied. Our
        implementation fixes this bug by changing the order of operations so
        that the library size ratio is calculated first, then multiplied by the
        count ratio.
        
        Because this bug affects which genes are included in the trimmed mean,
        its impact can be relatively large, sometimes leading to a >1% error in
        edgeR's estimated library size relative to our correct implementation.

        Does not support the `lib.size` and `refColumn` arguments to
        `calcNormFactors()`; these are both assumed to be `NULL` (the default)
        and will always be calculated internally. The `doWeighting` argument is
        also not supported and is assumed to be `TRUE` (the default), so
        asymptotic binomial precision weights will always be used.

        Args:
            library_size_column: the name of a floating-point column to add to
                                 obs containing each sample's library size
            cell_types: one or more cell types to calculate library sizes for;
                        if `None`, calculate library sizes for all cell types.
                        Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when
                                 calculating library sizes; mutually exclusive
                                 with `cell_types`
            logratio_trim: the amount of trim to use on log-ratios ("M"
                           values); must be greater than 0 and less than 1
            sum_trim: the amount of trim to use on the combined absolute levels
                      ("A" values); must be greater than 0 and less than 1
            A_cutoff: the cutoff on "A" values to use before trimming
            overwrite: if `True`, overwrite `library_size_column` if already
                       present in `obs`, instead of raising an error.
            num_threads: the number of threads to use when calculating library
                         sizes. Multithreading is only supported for
                         "free-threaded" builds of Python 3.13 and later with
                         the global interpreter lock (GIL) disabled. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). Parallelization takes place across cell
                         types, so specifying more cores than the number of
                         cell types may not improve performance.
        
        Returns:
            A new Pseudobulk dataset where `obs[library_size_column]` contains
            the norm factor-corrected library sizes for each cell type: raw
            library sizes (column sums) times norm factors.
        """
        # Get the list of cell types to calculate library sizes for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `library_size_column` is a string and, unless
        # `overwrite=True`, not already a column of `obs` for any cell type
        check_type(library_size_column, 'library_size_column', str, 'a string')
        if not overwrite:
            for cell_type, obs in self._obs.items():
                if library_size_column in obs:
                    error_message = (
                        f'library_size_column {library_size_column!r} is '
                        f'already a column of obs for cell type '
                        f'{cell_type!r}; did you already run library_size()? '
                        f'Set overwrite=True to overwrite.')
                    raise ValueError(error_message)
        # Check that `logratio_trim`, `sum_trim`, and `A_cutoff` are
        # floating-point numbers with the correct ranges
        check_type(logratio_trim, 'logratio_trim', float,
                   'a floating-point number')
        check_bounds(logratio_trim, 'logratio_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(sum_trim, 'sum_trim', float, 'a floating-point number')
        check_bounds(sum_trim, 'sum_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(A_cutoff, 'A_cutoff', float, 'a floating-point number')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']` if
        # free-threaded and 1 otherwise, and if -1, set to `os.cpu_count()`.
        # Raise an error if the user specified multiple threads but lacks
        # free-threaded Python.
        num_threads = Pseudobulk._process_num_threads(num_threads)
        # Compute library sizes for each cell type
        if num_threads == 1:
            library_sizes = {
                cell_type: Pseudobulk._library_size(
                    X=self._X[cell_type], cell_type=cell_type,
                    logratio_trim=logratio_trim, sum_trim=sum_trim,
                    A_cutoff=A_cutoff)
                for cell_type in cell_types}
        else:
            from concurrent.futures import ThreadPoolExecutor
            kwargs_list = [{'X': self._X[cell_type], 'cell_type': cell_type,
                            'logratio_trim': logratio_trim,
                            'sum_trim': sum_trim, 'A_cutoff': A_cutoff}
                           for cell_type in cell_types]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                library_sizes = executor.map(
                    lambda kwargs: self._library_size(**kwargs), kwargs_list)
        # Add library sizes to each cell type's `obs`
        obs = self._obs.copy()
        for cell_type, library_size in zip(cell_types, library_sizes):
            obs[cell_type] = obs[cell_type]\
                .with_columns(pl.lit(library_size).alias(library_size_column))
        # Return a new Pseudobulk dataset with the residuals
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def CPM(self) -> Pseudobulk:
        """
        Calculate counts per million for each cell type.

        Returns:
            A new Pseudobulk dataset containing the CPMs.
        """
        CPMs = {}
        for cell_type, X in self._X.items():
            library_size = Pseudobulk._library_size(X, cell_type)
            CPMs[cell_type] = X / library_size[:, None] * 1e6
        return Pseudobulk(X=CPMs, obs=self._obs, var=self._var)
    
    def log_CPM(self,
                *,
                prior_count: int | float | np.integer |
                             np.floating = 2) -> Pseudobulk:
        """
        Calculate log counts per million for each cell type.
        
        Do NOT run this before DE(), since DE() already runs it internally.
        
        Based on the R translation of edgeR's C++ cpm() code at
        bioinformatics.stackexchange.com/a/4990.
        
        Results were verified to match edgeR to within floating-point error.
        
        Args:
            prior_count: the pseudocount to add before log-transforming. In the
                         current version of edgeR, prior.count is now 2 instead
                         of the old value of 0.5: code.bioconductor.org/browse/
                         edgeR/blob/RELEASE_3_18/R/cpm.R
        
        Returns:
            A new Pseudobulk dataset containing the log(CPMs).
        """
        check_type(prior_count, 'prior_count', (int, float),
                   'a positive number')
        check_bounds(prior_count, 'prior_count', 0, left_open=True)
        log_CPMs = {}
        for cell_type, X in self._X.items():
            library_size = Pseudobulk._library_size(X, cell_type)
            pseudocount = prior_count * library_size / library_size.mean()
            library_size += 2 * pseudocount
            log_CPMs[cell_type] = np.log2(X + pseudocount[:, None]) - \
                np.log2(library_size[:, None]) + np.log2(1e6)
        return Pseudobulk(X=log_CPMs, obs=self._obs, var=self._var)
    
    @staticmethod
    def _get_unique_variables(formulas: str | Iterable[str],
                              composite: bool = False) -> list[str]:
        """
        Get a list of the unique variables referenced in one or more R
        formulas. Include backtick-quoted variable names that contain spaces
        or other characters that would otherwise be invalid in R variables.
        Do not include R functions (e.g. `exp(x1)` adds `'x1'` to the list, but
        not `exp`) or numbers.

        Args:
            formulas: one or more R formulas, represented as Python strings
            composite: if `True`, avoid splitting "composite" variables like
                       `x1:x2`, so that the unique variables are columns of the
                       design matrix. If `False`, split these into their
                       components, so that the unique variables are columns of
                       obs.

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        if isinstance(formulas, str):
            formulas = formulas,
        pattern = f'[+\\-*/^{":" if composite else ""}()]|`[^`]+`|[\\w.]+'
        seen = set()
        # noinspection PyUnresolvedReferences
        unique_variables = [
            token[1:-1] if token[0] == '`' else token
            for formula in formulas
            for token, next_token in pairwise(re.findall(pattern, formula) +
                                              [''])
            if (token not in seen and not seen.add(token) and
                not token.isdigit() and
                (token[0] == '`' or re.fullmatch(r'[\w.]*', token)) and
                next_token != '(')]
        return unique_variables
    
    @staticmethod
    def _process_formula_variables(formula: str,
                                   cell_type: str,
                                   obs: pl.DataFrame) -> list[str]:
        """
        Check that all variables referenced in `formula` are Categorical, Enum, 
        Boolean, integer, or floating-point columns of `obs`. Make a set of 
        these variables. Used by `DE()` and `regress_out()`.
        
        Args:
            formula: the formula to process
            cell_type: the cell type the formula is for, used in error messages
            obs: the `obs` for this cell type

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        unique_formula_variables = \
            Pseudobulk._get_unique_variables(formula)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        # noinspection PyUnboundLocalVariable
        for variable in unique_formula_variables:
            if variable not in obs:
                error_message = (
                    f'formula contains the variable {variable!r}, which is '
                    f'not the name of a column of obs[{cell_type!r}]')
                raise ValueError(error_message)
            base_type = obs[variable].dtype.base_type()
            if base_type not in valid_dtypes:
                error_message = (
                    f'all columns of obs referenced in formula must be '
                    f'Categorical, Enum, Boolean, integer, or floating-point, '
                    f'but it contains the variable {variable!r} and '
                    f'obs[{cell_type!r}][{variable!r}] has data type '
                    f'{base_type!r}')
                raise TypeError(error_message)
        return unique_formula_variables
    
    @staticmethod
    def _create_design_matrix(formula: str,
                              cell_type: str,
                              obs: pl.DataFrame,
                              obs_columns: list[str],
                              categorical_columns: str | Iterable[str] | None,
                              prefix: str) -> None:
        """
        Create a design matrix from a formula. Used by `DE()` and
        `regress_out()`.
        
        Adds variables called `{prefix}.formula`, `{prefix}.obs`, and
        `{prefix}.design.matrix` to the ryp R workspace, which need to be
        deleted by the calling function.
        
        Args:
            formula: the formula to construct the design matrix from
            cell_type: the cell type the formula is for
            obs: the `obs` for this cell type
            obs_columns: the columns of `obs` that need to be converted to R
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal
            prefix: a prefix to use for the three variables to be added to the
                    ryp R workspace
        """
        from ryp import r, to_py, to_r, \
            _bytestring_to_character_vector, _rlib, _RMemory
        # Convert the formula to R
        to_r(formula, f'{prefix}.formula')
        r(f'{prefix}.formula = as.formula({prefix}.formula)')
        # Subset `obs` to just the columns we need
        obs_names = obs[:, 0]
        obs = obs.select(*obs_columns)
        # Check that these columns do not contain any `null` values
        for column in obs:
            null_count = column.null_count()
            if null_count > 0:
                error_message = (
                    f'{column.name} contains {null_count:,} '
                    f'{plural("null value", null_count)}, but must not '
                    f'contain any')
                raise ValueError(error_message)
        # Convert the integer columns in `categorical_columns` to Enums, after
        # checking that all elements of `categorical_columns` are integer or
        # Enum columns of `obs`
        if categorical_columns is not None:
            for column in categorical_columns:
                if column not in obs:
                    error_message = (
                        f'one of the columns in categorical_columns, '
                        f'{column!r}, is not a column of obs[{cell_type!r}]')
                    raise ValueError(error_message)
                base_type = obs[column].dtype.base_type()
                if base_type != pl.Enum and base_type not in pl.INTEGER_DTYPES:
                    error_message = (
                        f'all columns in categorical_columns must be integer '
                        f'or Enum, but one of the columns is {column!r} and '
                        f'obs[{cell_type!r}][{column!r}] has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
            obs = obs\
                .cast({row[0]: pl.Enum(row[1]) for row in
                       obs.select(
                           (pl.selectors.integer() &
                            pl.selectors.by_name(categorical_columns))
                           .unique(maintain_order=True)
                           .implode()
                           .list.drop_nulls())
                      .unpivot()
                      .cast({'value': pl.List(pl.String)})
                      .rows()})
        # Convert the selected columns of `obs` to R
        obs_name = f'{prefix}.obs'
        to_r(obs, obs_name, rownames=obs_names)
        # If specified, convert the columns listed in `categorical_columns`
        # from ordered to unordered factors by removing the `'ordered'`
        # class and leaving only the `'factor'` class. This has to be done
        # through the R C API to be in-place.
        if categorical_columns is not None:
            R_obs = _rlib.Rf_findVar(_rlib.Rf_install(obs_name.encode()),
                                     _rlib.R_GlobalEnv)
            with _RMemory(_rlib) as rmemory:
                new_class = \
                    _bytestring_to_character_vector(b'factor', rmemory)
                for column in categorical_columns:
                    column_index = obs.columns.index(column)
                    R_column = _rlib.VECTOR_ELT(R_obs, column_index)
                    _rlib.Rf_setAttrib(R_column, _rlib.R_ClassSymbol,
                                       new_class)
        # Create the design matrix
        r(f'{prefix}.design.matrix = '
          f'model.matrix({prefix}.formula, {prefix}.obs)')
        # Check that the design matrix has more rows than columns
        height = to_py(f'nrow({prefix}.design.matrix)')
        width = to_py(f'ncol({prefix}.design.matrix)')
        if width >= height:
            error_message = (
                f'the design matrix must have more rows (samples) than '
                f'columns (one plus the number of covariates), but has '
                f'{height:,} rows and {width:,} columns for cell type '
                f'{cell_type!r}. Either reduce the number of covariates, or '
                f'exclude this cell type with e.g. '
                f'excluded_cell_types={cell_type!r}.')
            raise ValueError(error_message)
    
    @staticmethod
    def _process_num_threads(num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        Pseudobulk function.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use. If `num_threads` is a positive
            integer, raise an error if the user lacks free-threaded Python,
            otherwise return it unchanged. If `num_threads` is `None`, return
            `single_cell.options()['num_threads']` if free-threaded and 1
            otherwise. If `num_threads` is -1, return `os.cpu_count()`.
            Otherwise, raise an error.
        """
        try:
            # noinspection PyUnresolvedReferences
            has_GIL = sys._is_gil_enabled()
        except AttributeError:
            has_GIL = False
        if num_threads is None and not has_GIL:
            num_threads = 1
        else:
            num_threads = SingleCell._process_num_threads(num_threads)
            # If `num_threads` is greater than 1, check that the user is using
            # free-threaded Python 3.13+
            if num_threads > 1 and not has_GIL:
                error_message = (
                    f'num_threads is {num_threads}, but multithreading in '
                    f'Pseudobulk methods is only supported for '
                    f'"free-threaded" builds of Python 3.13 and later with '
                    f'the global interpreter lock (GIL) disabled')
                raise ValueError(error_message)
        return num_threads
    
    @staticmethod
    def _regress_out(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                     obs: pl.DataFrame,
                     cell_type: str,
                     cell_type_index: int,
                     formula: str,
                     categorical_columns: str | Iterable[str] | None,
                     library_size_as_covariate: bool,
                     num_cells_as_covariate: bool,
                     error_if_int: bool,
                     verbose: bool) -> np.ndarray[2, np.dtype[np.integer |
                                                              np.floating]]:
        """
        Regress out covariates from `obs` for a single cell type. Used by
        `regress_out()`.
        
        Args:
            X: the `X` for this cell type
            obs: the `obs` for this cell type
            cell_type: the cell type covariates will be regressed out for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of `obs`,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of `obs`, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process

        Returns:
            `X` with covariates regressed out.
        """
        from ryp import r, to_py
        # If `error_if_int=True`, raise an error if `X` has an integer dtype
        if error_if_int and np.issubdtype(X.dtype, np.integer):
            error_message = (
                f'X[{cell_type!r}] has data type {str(X.dtype)!r}, an integer '
                f'data type; did you forget to run log_CPM() before '
                f'regress_out()?')
            raise ValueError(error_message)
        # Check that all variables referenced by `formula` are Categorical,
        # Enum, Boolean, integer, or floating-point columns of `obs`. Make a
        # set of these variables.
        if verbose:
            print(f'[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
        # If `library_size_as_covariate=True`, check that `formula` does not
        # already contain a `'log_library_size'` variable, then add
        # `log2(library_size)` as a covariate
        if library_size_as_covariate:
            if 'log_library_size' in obs_columns:
                error_message = (
                    'formula cannot contain a log_library_size column when '
                    'library_size_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            if verbose:
                print(f'[{cell_type}] Estimating library sizes...')
            library_size = Pseudobulk._library_size(X, cell_type)
            if verbose:
                print(f'[{cell_type}] Adding library size as a covariate...')
            obs = obs.with_columns(
                log_library_size=pl.Series(library_size).log(2))
            obs_columns.append('log_library_size')
            formula += ' + log_library_size'
        # If `num_cells_as_covariate=True`, check that `formula` does not
        # already contain a `'num_cells'` or `'log_num_cells'` variable, then
        # add `log2(num_cells)` as a covariate
        if num_cells_as_covariate:
            if verbose:
                print(f'[{cell_type}] Adding num_cells as a covariate...')
            if 'num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain num_cells explicitly when '
                    'num_cells_as_covariate=True, to avoid collinearity')
                raise ValueError(error_message)
            if 'log_num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain a log_num_cells column when '
                    'num_cells_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            obs = obs.with_columns(log_num_cells=obs['num_cells'].log(2))
            obs_columns.append('log_num_cells')
            formula += ' + log_num_cells'
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        # Create the design matrix
        try:
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_columns,
                                             categorical_columns, prefix)
            design_matrix = to_py(f'{prefix}.design.matrix', format='numpy')
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix")))')
        # Regress out the design matrix; silence warnings with `rcond=None`
        if verbose:
            print(f'[{cell_type}] Regressing out...')
        beta, _, rank, _ = np.linalg.lstsq(design_matrix, X, rcond=None)
        # Check that the design matrix is full-rank
        if rank < design_matrix.shape[1]:
            error_message = (
                f'the design matrix is not full-rank for cell type '
                f'{cell_type!r} (rank {rank} with {design_matrix.shape[1]} '
                f'columns); some of your covariates are linear '
                f'combinations of other covariates')
            raise ValueError(error_message)
        # Calculate the residuals
        residuals = X - design_matrix @ beta
        return residuals
    
    def regress_out(self,
                    formula: str | dict[str, str],
                    *,
                    categorical_columns: str | Iterable[str] | None |
                                         dict[str, str | Iterable[str] |
                                                   None] = None,
                    cell_types: str | Iterable[str] | None = None,
                    excluded_cell_types: str | Iterable[str] | None = None,
                    library_size_as_covariate: bool = True,
                    num_cells_as_covariate: bool = True,
                    error_if_int: bool = True,
                    verbose: bool = True,
                    num_threads: int | np.integer | None = None) -> Pseudobulk:
        """
        Regress out covariates from `obs`. Must be run after log_CPM().
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of `obs` referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of `obs` represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```r
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of `obs`,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to regress the covariates out
                        of; if `None`, regress covariates out of all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when
                                 regressing out covariates; mutually exclusive
                                 with `cell_types`
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of `obs`, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process
            num_threads: the number of threads to use when regressing out.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). Parallelization takes place across cell
                         types, so specifying more cores than the number of
                         cell types may not improve performance.

        Returns:
            A new Pseudobulk dataset with covariates regressed out.
        """
        from ryp import r
        # Get the list of cell types to regress out covariates for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings, and that each string is a valid R formula
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
                if not value.lstrip().startswith('~'):
                    error_message = \
                        f'formula[{key!r}] must start with a tilde (~)'
                    raise ValueError(error_message)
                try:
                    r(f'as.formula({value!r})')
                except RuntimeError as e:
                    error_message = \
                        f'formula[{key!r}] is not a valid R formula'
                    raise ValueError(error_message) from e
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        else:
            if not formula.lstrip().startswith('~'):
                error_message = 'formula must start with a tilde (~)'
                raise ValueError(error_message)
            try:
                r(f'as.formula({formula!r})')
            except RuntimeError as e:
                error_message = 'formula is not a valid R formula'
                raise ValueError(error_message) from e
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        # Check that Boolean arguments are Boolean
        check_type(library_size_as_covariate, 'library_size_as_covariate',
                   bool, 'Boolean')
        check_type(num_cells_as_covariate, 'num_cells_as_covariate', bool,
                   'Boolean')
        check_type(error_if_int, 'error_if_int', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']` if
        # free-threaded and 1 otherwise, and if -1, set to `os.cpu_count()`.
        # Raise an error if the user specified multiple threads but lacks
        # free-threaded Python.
        num_threads = Pseudobulk._process_num_threads(num_threads)
        # Compute residuals for each cell type
        if num_threads == 1:
            # noinspection PyUnboundLocalVariable
            residuals = {
                cell_type: Pseudobulk._regress_out(
                    X=X, obs=obs, cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type] if formula_is_dict else
                            formula,
                    library_size_as_covariate=library_size_as_covariate,
                    num_cells_as_covariate=num_cells_as_covariate,
                    categorical_columns=categorical_columns,
                    error_if_int=error_if_int, verbose=verbose)
                    for cell_type_index, (cell_type, (X, obs, var))
                    in enumerate(self.items())}
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            kwargs_list = [{
                'X': X,
                'obs': obs,
                'cell_type': 'cell_type',
                'cell_type_index': 'cell_type_index',
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'library_size_as_covariate': library_size_as_covariate,
                'num_cells_as_covariate': num_cells_as_covariate,
                'error_if_int': error_if_int,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                residuals = dict(zip(cell_types, executor.map(
                    lambda kwargs: self._DE(**kwargs), kwargs_list)))
        # Return a new Pseudobulk dataset with the residuals
        return Pseudobulk(X=residuals, obs=self._obs, var=self._var)
    
    # A slightly reformatted version of the voomByGroup source code from
    # github.com/YOU-k/voomByGroup/blob/main/voomByGroup.R, which is available
    # under the MIT license. Copyright (c) 2023 Yue You. Also added 
    # `drop=FALSE` to the `countsi` subsetting to avoid an error with length-1
    # groups.
    _voomByGroup_source_code = r'''
    voomByGroup <- function (counts, group = NULL, design = NULL,
                             lib.size = NULL, dynamic = NULL,
                             normalize.method = "none", span = 0.5,
                             save.plot = FALSE, print = TRUE, plot = c("none",
                             "all", "separate", "combine"),
                             col.lines = NULL, pos.legend = c("inside",
                             "outside", "none"), fix.y.axis = FALSE, ...) {
      out <- list()
      if (is(counts, "DGEList")) {
        out$genes <- counts$genes
        out$targets <- counts$samples
        if(is.null(group))
          group <- counts$samples$group
        if (is.null(lib.size))
          lib.size <- with(counts$samples, lib.size * norm.factors)
        counts <- counts$counts
      }
      else {
        isExpressionSet <-
          suppressPackageStartupMessages(is(counts, "ExpressionSet"))
        if (isExpressionSet) {
          if (length(Biobase::fData(counts)))
            out$genes <- Biobase::fData(counts)
          if (length(Biobase::pData(counts)))
            out$targets <- Biobase::pData(counts)
          counts <- Biobase::exprs(counts)
        }
        else {
          counts <- as.matrix(counts)
        }
      }
      if (nrow(counts) < 2L)
        stop("Need at least two genes to fit a mean-variance trend")
      # Library size
      if(is.null(lib.size))
        lib.size <- colSums(counts)
      # Group
      if(is.null(group))
        group <- rep("Group1", ncol(counts))
      group <- as.factor(group)
      intgroup <- as.integer(group)
      levgroup <- levels(group)
      ngroups <- length(levgroup)
      # Design matrix
      if (is.null(design)) {
        design <- matrix(1L, ncol(counts), 1)
        rownames(design) <- colnames(counts)
        colnames(design) <- "GrandMean"
      }
      # Dynamic
      if (is.null(dynamic)) {
        dynamic <- rep(FALSE, ngroups)
      }
      # voom by group
      if(print)
        cat("Group:\n")
      E <- w <- counts
      xy <- line <- as.list(rep(NA, ngroups))
      names(xy) <- names(line) <- levgroup
      for (lev in 1L:ngroups) {
        if(print)
          cat(lev, levgroup[lev], "\n")
        i <- intgroup == lev
        countsi <- counts[, i, drop = FALSE]
        libsizei <- lib.size[i]
        designi <- design[i, , drop = FALSE]
        QR <- qr(designi)
        if(QR$rank<ncol(designi))
          designi <- designi[,QR$pivot[1L:QR$rank], drop = FALSE]
        if(ncol(designi)==ncol(countsi))
          designi <- matrix(1L, ncol(countsi), 1)
        voomi <- voom(counts = countsi, design = designi, lib.size = libsizei,
                      normalize.method = normalize.method, span = span,
                      plot = FALSE, save.plot = TRUE, ...)
        E[, i] <- voomi$E
        w[, i] <- voomi$weights
        xy[[lev]] <- voomi$voom.xy
        line[[lev]] <- voomi$voom.line
      }
      #voom overall
      if (TRUE %in% dynamic){
        voom_all <- voom(counts = counts, design = design, lib.size = lib.size,
                         normalize.method = normalize.method, span = span,
                         plot = FALSE, save.plot = TRUE, ...)
        E_all <- voom_all$E
        w_all <- voom_all$weights
        xy_all <- voom_all$voom.xy
        line_all <- voom_all$voom.line
        dge <- DGEList(counts)
        disp <- estimateCommonDisp(dge)
        disp_all <- disp$common
      }
      # Plot, can be "both", "none", "separate", or "combine"
      plot <- plot[1]
      if(plot!="none"){
        disp.group <- c()
        for (lev in levgroup) {
          dge.sub <- DGEList(counts[,group == lev])
          disp <- estimateCommonDisp(dge.sub)
          disp.group[lev] <- disp$common
        }
        if(plot %in% c("all", "separate")){
          if (fix.y.axis == TRUE) {
            yrange <- sapply(levgroup, function(lev){
              c(min(xy[[lev]]$y), max(xy[[lev]]$y))
            }, simplify = TRUE)
            yrange <- c(min(yrange[1,]) - 0.1, max(yrange[2,]) + 0.1)
          }
          for (lev in 1L:ngroups) {
            if (fix.y.axis == TRUE){
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25,
                   ylim = yrange)
            } else {
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25)
            }
            title(paste("voom: Mean-variance trend,", levgroup[lev]))
            lines(line[[lev]], col = "red")
            legend("topleft", bty="n", paste("BCV:",
              round(sqrt(disp.group[lev]), 3)), text.col="red")
          }
        }
        
        if(plot %in% c("all", "combine")){
          if(is.null(col.lines))
            col.lines <- 1L:ngroups
          if(length(col.lines)<ngroups)
            col.lines <- rep(col.lines, ngroups)
          xrange <- unlist(lapply(line, `[[`, "x"))
          xrange <- c(min(xrange)-0.3, max(xrange)+0.3)
          yrange <- unlist(lapply(line, `[[`, "y"))
          yrange <- c(min(yrange)-0.1, max(yrange)+0.3)
          plot(1L,1L, type="n", ylim=yrange, xlim=xrange,
               xlab = "log2( count size + 0.5 )",
               ylab = "Sqrt( standard deviation )")
          title("voom: Mean-variance trend")
          if (TRUE %in% dynamic){
            for (dy in which(dynamic)){
              line[[dy]] <- line_all
              disp.group[dy] <- disp_all
              levgroup[dy] <- paste0(levgroup[dy]," (all)")
            }
          }
          for (lev in 1L:ngroups)
            lines(line[[lev]], col=col.lines[lev], lwd=2)
          pos.legend <- pos.legend[1]
          disp.order <- order(disp.group, decreasing = TRUE)
          text.legend <-
            paste(levgroup, ", BCV: ", round(sqrt(disp.group), 3), sep="")
          if(pos.legend %in% c("inside", "outside")){
            if(pos.legend=="outside"){
              plot(1,1, type="n", yaxt="n", xaxt="n", ylab="", xlab="",
                   frame.plot=FALSE)
              legend("topleft", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            } else {
              legend("topright", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            }
          }
        }
      }
      # Output
      if (TRUE %in% dynamic){
        E[,intgroup %in% which(dynamic)] <-
          E_all[,intgroup %in% which(dynamic)]
        w[,intgroup %in% which(dynamic)] <-
          w_all[,intgroup %in% which(dynamic)]
      }
      out$E <- E
      out$weights <- w
      out$design <- design
      if(save.plot){
        out$voom.line <- line
        out$voom.xy <- xy
      }
      new("EList", out)
    }
    '''
    
    def _DE(self,
            cell_type: str,
            cell_type_index: int,
            formula: str,
            coefficient: str | int | np.integer |
                         Iterable[str | int | np.integer],
            contrasts: dict[str, str] | None,
            group: Literal[False] | PseudobulkColumn | None,
            categorical_columns: str | Iterable[str] | None,
            library_size_as_covariate: bool,
            num_cells_as_covariate: bool,
            robust: bool,
            return_voom_info: bool,
            allow_float: bool,
            verbose: bool) -> pl.DataFrame | \
                              tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """
        Compute differential expression for a single cell type. Used by `DE()`.
        
        Args:
            cell_type: the cell type DE will be calculated for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of `obs`, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Negative indices work in the
                         usual Python way. Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. If
                       specified, DE will be performed with respect to each
                       contrast by running limma's `makeContrasts()` and
                       `contrasts.fit()` functions after `lmFit()`. Mutually
                       exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of `obs`
                   referenced in `coefficient`, or the columns of `obs`
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   column (the name of a String, Categorical, Enum, Boolean, or
                   integer column of `obs`, a polars expression, a polars
                   Series, a 1D NumPy array, or a function that takes in this
                   Pseudobulk dataset and a cell type and returns a polars
                   Series or 1D NumPy array), force the use of voomByGroup and
                   group on the unique values of that column. `group` can also
                   be a dictionary mapping cell-type names to `False`, `None`,
                   or a column for each cell type. When using voomByGroup, the
                   same groups are also used as the `group` argument to
                   `calcNormFactors()` when normalizing by library size.
            categorical_columns: one or more names of integer or Enum columns
                                 of `obs` to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of `obs`, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         log_CPM() already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation

        Returns:
            A DataFrame of the DE results for this cell type. Or, if
            `return_voom_info=True`, a tuple of three DataFrames: the DE
            results, the voom weights, and the voom plot info for this cell
            type.
        """
        from ryp import r, to_py, to_r
        # Get the data for this cell type
        X = self._X[cell_type]
        obs = self._obs[cell_type]
        var = self._var[cell_type]
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f"DE() requires raw counts but X[{cell_type!r}] has data type "
                f"{str(X.dtype)!r}, a floating-point data type. If you "
                f"are sure that all values are integers, i.e. that "
                f"X[{cell_type!r}].data == X[{cell_type!r}].data"
                f".astype(int)).all(), then set allow_float=True (or just "
                f"cast X to an integer data type). Alternatively, did you "
                f"accidentally run log_CPM() before DE()?")
            raise TypeError(error_message)
        # Check that all variables referenced by `formula` are Categorical,
        # Enum, Boolean, integer, or floating-point columns of `obs`. Make a
        # set of these variables so we can subset to them before converting
        # `obs` to R.
        if verbose:
            print(f'[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
        # Estimate library sizes
        if verbose:
            print(f'[{cell_type}] Estimating library sizes...')
        library_size = Pseudobulk._library_size(X, cell_type)
        # If `library_size_as_covariate=True`, check that `formula` does not
        # already contain a `'log_library_size'` variable, then add
        # `log2(library_size)` as a covariate
        if library_size_as_covariate:
            if verbose:
                print(f'[{cell_type}] Adding library size as a covariate...')
            if 'log_library_size' in obs_columns:
                error_message = (
                    'formula cannot contain a log_library_size column when '
                    'library_size_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            obs = obs.with_columns(
                log_library_size=pl.Series(library_size).log(2))
            obs_columns.append('log_library_size')
            formula += ' + log_library_size'
        # If `num_cells_as_covariate=True`, check that `formula` does not
        # already contain a `'num_cells'` or `'log_num_cells'` variable, then
        # add `log2(num_cells)` as a covariate
        if num_cells_as_covariate:
            if verbose:
                print(f'[{cell_type}] Adding num_cells as a covariate...')
            if 'num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain num_cells explicitly when '
                    'num_cells_as_covariate=True, to avoid collinearity')
                raise ValueError(error_message)
            if 'log_num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain a log_num_cells column when '
                    'num_cells_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            obs = obs.with_columns(log_num_cells=obs['num_cells'].log(2))
            obs_columns.append('log_num_cells')
            formula += ' + log_num_cells'
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        try:
            # Create the design matrix
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            obs_names = obs[:, 0]
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_columns, categorical_columns,
                                             prefix)
            design_matrix_columns = to_py(f'colnames({prefix}.design.matrix)')
            # Check that the design matrix is full-rank
            rank = to_py(f'qr({prefix}.design.matrix)$rank')
            width = len(design_matrix_columns)
            if rank < width:
                error_message = (
                    f'the design matrix is not full-rank for cell type '
                    f'{cell_type!r} (rank {rank:,} with {width:,} columns); '
                    f'some of your covariates are linear combinations of '
                    f'other covariates')
                raise ValueError(error_message)
            # If `contrasts` was specified, validate `contrasts`; otherwise,
            # validate `coefficient`
            if contrasts is None:
                if verbose:
                    print(f'[{cell_type}] Validating coefficient...')
                # Check that all string entries of `coefficient` are names of
                # columns of the design matrix
                for coef in coefficient:
                    if isinstance(coef, str) and \
                            coef not in design_matrix_columns:
                        error_message = (
                            f'coefficient {coef!r} is not a column of the '
                            f'design matrix for cell type {cell_type!r}. The '
                            f'design matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns: '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
                # Extract the name of the design matrix column corresponding to
                # each integer in `coefficient`; make sure none of the integers
                # are >= the design matrix width
                for coef in coefficient:
                    if isinstance(coef, (int, np.integer)) and coef >= width:
                        error_message = (
                            f'coefficient '
                            f'{"is" if len(coefficient) == 1 else "contains"} '
                            f'the integer {coef}, which is more than the '
                            f'number of columns of the design matrix '
                            f'({width:,}) minus 1 for cell type {cell_type!r}')
                        raise ValueError(error_message)
                coefficient = [design_matrix_columns[coef]
                               if isinstance(coef, (int, np.integer)) else coef
                               for coef in coefficient]
                # Convert `coefficient` to R
                to_r(pl.Series(coefficient), f'{prefix}.coef')
            else:
                if verbose:
                    print(f'[{cell_type}] Validating contrasts...')
                # Check that all variables referenced in `contrasts` are in the
                # design matrix. Use `composite=True` to avoid splitting e.g.
                # `x1:x2` into `x1` and `x2`.
                contrasts = {contrast_name: contrast.replace(' ', '')
                             for contrast_name, contrast in contrasts.items()}
                valid_dtypes = pl.INTEGER_DTYPES | \
                               {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
                unique_contrast_variables = \
                    Pseudobulk._get_unique_variables(contrasts.values(),
                                                     composite=True)
                for variable in unique_contrast_variables:
                    if variable not in design_matrix_columns:
                        error_message = (
                            f'a contrast contains the variable {variable!r}, '
                            f'which is not the name of a column of the design '
                            f'matrix for cell type {cell_type!r}. The design '
                            f'matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
            if group is None or contrasts is not None:
                # Get the list of columns of `obs` referenced in `coefficient`
                # or `contrasts`. This can be done by:
                # 1. Getting the columns of the design matrix referenced in
                #    `coefficient` or `contrasts`. For `coefficient`, this is
                #    just the entries of `coefficient` themselves. For
                #    `contrasts`, this is the `unique_contrast_variables`
                #    variable we defined above.
                # noinspection PyUnboundLocalVariable
                referenced_design_matrix_columns = coefficient \
                    if contrasts is None else unique_contrast_variables
                # 2. Mapping each column of the design matrix listed in
                #    `coefficient` or `contrasts` back to the integer index of
                #    the term in `formula` it was derived from, using the
                #    design matrix's `assign` attribute. Subtract 1 to convert
                #    from 1-based to 0-based indexing.
                assign = to_py(f'attr({prefix}.design.matrix, "assign")')
                referenced_term_indices = assign\
                    .filter(design_matrix_columns
                            .is_in(referenced_design_matrix_columns)) - 1
                # 3. Using R's `terms()` function to expand the formula into
                #    the list of terms corresponding to these integer indices.
                term_labels = \
                    to_py(f'attr(terms({prefix}.formula), "term.labels")')
                # 4. Pulling out the terms corresponding to the indices from
                #    step 2, to get the terms of `formula` referenced in
                #    `coefficient` or `contrasts`.
                referenced_terms = term_labels[referenced_term_indices]
                # 5. Getting the unique variables in these terms. These are the
                #    columns of `obs` referenced in `coefficient` or
                #    `contrasts`.
                referenced_columns = \
                    Pseudobulk._get_unique_variables(referenced_terms)
            # If `contrasts` is not `None`, check that all columns of `obs`
            # referenced in `contrasts` are categorical, now that we know which
            # columns are referenced
            if contrasts is not None:
                # noinspection PyUnboundLocalVariable
                for column in referenced_columns:
                    base_type = obs[column].dtype.base_type()
                    # noinspection PyUnboundLocalVariable
                    if base_type not in valid_dtypes:
                        error_message = (
                            f'all columns of obs referenced in contrasts must '
                            f'be String, Categorical, Enum, Boolean, or '
                            f'integer, but a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}], which has data '
                            f'type {base_type!r}')
                        raise TypeError(error_message)
                    if (base_type == pl.Enum or
                        base_type in pl.INTEGER_DTYPES) and (
                            categorical_columns is None or
                            column not in categorical_columns):
                        error_message = (
                            f'a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}] with data type '
                            f'{base_type!r}, but all columns referenced in '
                            f'contrasts must be categorical and integer/Enum '
                            f'columns are not treated as categorical unless '
                            f'specified in categorical_columns; did you '
                            f'forget to add {column!r} to '
                            f'categorical_columns?')
                        raise TypeError(error_message)
            # If `group=None`, group on the unique combinations of values of
            # the categorical columns of `obs` referenced in `coefficient` or
            # `contrasts`
            if group is None:
                if verbose:
                    print(f'[{cell_type}] Defining groups...')
                if contrasts is not None:
                    # If using `contrasts`, always use voomByGroup, and group
                    # on the unique combinations of the columns referenced in
                    # the contrasts (recall that we already checked that they
                    # are categorical)
                    # noinspection PyUnboundLocalVariable
                    group_columns = referenced_columns
                else:
                    # If using `coefficient`, group on the unique combinations
                    # of the categorical columns referenced in the
                    # coefficients. If no columns are categorical, use voom
                    # instead of voomByGroup.
                    categorical_selector = \
                        pl.selectors.by_dtype(pl.String, pl.Categorical,
                                              pl.Boolean)
                    if categorical_columns is not None:
                        categorical_selector |= \
                            pl.selectors.by_name(categorical_columns)
                    group_columns = pl.selectors.expand_selector(
                        obs,
                        pl.selectors.by_name(referenced_columns) &
                        categorical_selector)
                if len(group_columns) > 0:
                    # Create a descriptive name for each combination of values
                    # in the `group_columns`
                    group = obs\
                        .select(pl.format(', '.join(f'{column} = {{}}'
                                                    for column in
                                                    group_columns),
                                          *group_columns))\
                        .to_series()
                    group = group\
                        .cast(pl.Enum(group.unique().sort().to_list()))
                    # If the group columns have the same values for every 
                    # sample, disable grouping
                    single_group = len(group.cat.get_categories())
                    if single_group:
                        group = None
                    # If `verbose=True`, print whether and how we're grouping
                    if verbose:
                        if len(group_columns) == 1:
                            group_column_description = f'{group_columns[0]!r}'
                        elif len(group_columns) == 2:
                            group_column_description = \
                                f'{group_columns[0]!r} and ' \
                                f'{group_columns[1]!r}'
                        else:
                            group_column_description = \
                                ', '.join(map(repr, group_columns[:-1])) + \
                                f', and {group_columns[-1]!r}'
                        if single_group:
                            if len(group_columns) == 1:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group column '
                                      f'{group_column_description} has the '
                                      f'same value for every sample')
                            else:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group columns '
                                      f'{group_column_description} have the '
                                      f'same values for every sample')
                        else:
                            print(f'[{cell_type}] Grouping on the '
                                  f'{group_column_description} '
                                  f'{plural("column", len(group_columns))} of '
                                  f'obs.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Not grouping since coefficient '
                              f'does not reference any categorical variables.')
                    group = None
            # If grouping, check that all groups have at least two samples
            if group is not None:
                group_counts = group.value_counts().sort('count')
                if group_counts['count'][0] == 1:
                    error_message = (
                        f'all groups must have at least two samples, but '
                        f'group {group_counts[0, 0]!r} has only one '
                        f'sample for cell type {cell_type}')
                    raise ValueError(error_message)
            # Convert the expression matrix and library sizes to R
            if verbose:
                if group is not None:
                    print(f'[{cell_type}] Converting the expression matrix, '
                          f'library sizes and groups to R...')
                else:
                    print(f'[{cell_type}] Converting the expression matrix '
                          f'and library sizes to R...')
            to_r(X.T, f'{prefix}.X.T', rownames=var[:, 0],
                 colnames=obs_names)
            to_r(library_size, f'{prefix}.library.size', rownames=obs_names)
            to_r(group, f'{prefix}.group')
            # Run voom
            to_r(return_voom_info, 'save.plot')
            if group is not None:
                if verbose:
                    print(f'[{cell_type}] Running voomByGroup...')
                r(f'{prefix}.voom.result = voomByGroup('
                  f'{prefix}.X.T, {prefix}.group, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot, print=FALSE)')
            else:
                if verbose:
                    print(f'[{cell_type}] Running voom...')
                r(f'{prefix}.voom.result = voom('
                  f'{prefix}.X.T, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot)')
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                voom_weights = \
                    to_py(f'{prefix}.voom.result$weights', index='gene')
                if group is not None:
                    frames = [
                        pl.DataFrame({
                            'gene': to_py(
                                f'names({prefix}.voom.result$voom.xy$'
                                f'`{group_name}`$x)')} | {
                            f'{prop}_{dim}_{group_name}': to_py(
                                f'{prefix}.voom.result$voom.{prop}$'
                                f'`{group_name}`${dim}', index=False)
                            for prop in ('xy', 'line')
                            for dim in ('x', 'y')})
                        for group_name in group.unique(maintain_order=True)]
                    frames = pl.align_frames(*frames, on='gene', how='outer')
                    voom_plot_data = pl.concat(
                        [frames[0]] + [frame[:, 1:] for frame in frames[1:]],
                        how='horizontal')
                else:
                    voom_plot_data = pl.DataFrame({
                        'gene': to_py(
                            f'names({prefix}.voom.result$voom.xy$x)')} | {
                        f'{prop}_{dim}': to_py(
                            f'{prefix}.voom.result$voom.{prop}${dim}',
                            index=False)
                        for prop in ('xy', 'line') for dim in ('x', 'y')})
            # Run lmFit
            if verbose:
                print(f'[{cell_type}] Running lmFit...')
            r(f'{prefix}.lmFit.result = lmFit('
              f'{prefix}.voom.result, {prefix}.design.matrix)')
            if contrasts is not None:
                # Convert contrasts to R
                to_r(pl.Series(contrasts.values()), f'{prefix}.contrasts')
                # Make the contrast terms and design matrix colnames into
                # valid R variable names by temporarily:
                # - escaping interaction terms (e.g. renaming `A:B` to `A.B`)
                #   in both the contrasts and the design matrix colnames
                # - renaming the design matrix column `(Intercept)` to
                #   `.Intercept` (not `Intercept` in case the user already has
                #   a column called `Intercept` for some reason)
                r(f'{prefix}.contrasts = gsub(":", ".", {prefix}.contrasts)')
                r(f'{prefix}.columns = colnames({prefix}.design.matrix)')
                r(rf'{prefix}.levels = gsub("\\(Intercept\\)", ".Intercept", '
                  rf'gsub(":", ".", {prefix}.columns))')
                r(f'colnames({prefix}.design.matrix) = {prefix}.levels')
                # Make contrasts
                r(f'{prefix}.contrasts = makeContrasts('
                  f'contrasts={prefix}.contrasts, levels={prefix}.levels)')
                # Rename the design matrix colnames (and the contrast rownames)
                # to the original design matrix colnames
                r(f'rownames({prefix}.contrasts) = {prefix}.columns')
                r(f'colnames({prefix}.design.matrix) = {prefix}.columns')
                # Set the colnames of the contrasts to `contrasts.keys()`, so
                # they display as those names in the output DE table
                to_r(pl.Series(contrasts.keys()), f'{prefix}.coef')
                r(f'colnames({prefix}.contrasts) = {prefix}.coef')
                # Fit contrasts
                r(f'{prefix}.lmFit.result = contrasts.fit('
                  f'{prefix}.lmFit.result, {prefix}.contrasts)')
            # Run eBayes
            if verbose:
                print(f'[{cell_type}] Running eBayes...')
            to_r(robust, f'{prefix}.robust')
            r(f'{prefix}.eBayes.result = eBayes('
              f'{prefix}.lmFit.result, trend=FALSE, robust={prefix}.robust)')
            # Make a table of the DE results
            if verbose:
                print(f'[{cell_type}] Collating results...')
            gene = to_py(f'rownames({prefix}.eBayes.result)')
            logFC = to_py(f'{prefix}.eBayes.result$coefficients['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            SE = to_py(f'{prefix}.eBayes.result$s2.post').sqrt() * \
                 to_py(f'{prefix}.eBayes.result$stdev.unscaled['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            margin_error = \
                SE * stdtrit(to_py(f'{prefix}.eBayes.result$df.total'), 0.975)
            LCI = logFC - margin_error
            UCI = logFC + margin_error
            AveExpr = to_py(f'{prefix}.eBayes.result$Amean', index=False)
            p = to_py(f'{prefix}.eBayes.result$p.value['
                      f',{prefix}.coef, drop=FALSE]', index=False)
            DE_results = pl.concat([
                pl.DataFrame({
                    'coefficient': coef, 'gene': gene, 'logFC': logFC[coef],
                    'SE': SE[coef], 'LCI': LCI[coef], 'UCI': UCI[coef],
                    'AveExpr': AveExpr, 'p': p[coef]})
                .with_columns(Bonferroni=bonferroni(pl.col.p),
                              FDR=fdr(pl.col.p))
                for coef in (
                    contrasts if contrasts is not None else coefficient)])
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix", '
              f'"{prefix}.library.size", "{prefix}.X.T", "{prefix}.group", '
              f'"{prefix}.voom.result", "{prefix}.lmFit.result", '
              f'"{prefix}.contrasts", "{prefix}.columns", "{prefix}.levels", '
              f'"{prefix}.robust", "{prefix}.eBayes.result", '
              f'"{prefix}.coef")))')
        # noinspection PyUnboundLocalVariable
        return DE_results, voom_weights, voom_plot_data \
            if return_voom_info else DE_results
    
    def DE(self,
           formula: str | dict[str, str],
           coefficient: str | int | np.integer |
                        Iterable[str | int | np.integer] |
                        dict[str, str | int | np.integer |
                                  Iterable[str | int | np.integer]] = 1,
           *,
           contrasts: dict[str, str] | None |
                      dict[str, dict[str, str] | None] = None,
           group: Literal[False] | PseudobulkColumn | None |
                  dict[str, Literal[False] | str | pl.Expr | pl.Series |
                            np.ndarray | None] = None,
           categorical_columns: str | Iterable[str] | None |
                                dict[str, str | Iterable[str] | None] = None,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           library_size_as_covariate: bool = True,
           num_cells_as_covariate: bool = True,
           robust: bool = False,
           return_voom_info: bool = True,
           allow_float: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> DE:
        """
        Perform differential expression (DE) on a Pseudobulk dataset with
        limma-voom. The DE design is specified via an R formula string that
        references columns from `obs`.
        
        By default, DE is reported with respect to the first column of the
        design matrix after the intercept, which is usually just the first term
        in the formula. This can be changed via the `coefficient` and
        `contrasts` arguments.
        
        By default, voomByGroup is used instead of voom when reporting DE with
        respect to a categorical variable, e.g. when comparing disease cases to
        healthy controls. This can be changed via the `group` argument.
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of `obs` referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of `obs` represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of `obs`, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Or, a dictionary mapping
                         cell-type names to any of the above; each cell type in
                         this Pseudobulk dataset must be present. Negative
                         indices work in the usual Python way. Defaults to
                         `coefficient=1`, the first variable in `formula`
                         (`coefficient=0` would refer to the intercept).
                         Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. Or, a
                       dictionary mapping cell-type names to these
                       dictionaries; each cell type in this Pseudobulk dataset
                       must be present. If specified, DE will be performed with
                       respect to each contrast by running limma's
                       `makeContrasts()` and `contrasts.fit()` functions after
                       `lmFit()`. Mutually exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of `obs`
                   referenced in `coefficient`, or the columns of `obs`
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   column (the name of a String, Categorical, Enum, Boolean, or
                   integer column of `obs`, a polars expression, a polars
                   Series, a 1D NumPy array, or a function that takes in this
                   Pseudobulk dataset and a cell type and returns a polars
                   Series or 1D NumPy array), force the use of voomByGroup and
                   group on the unique values of that column. `group` can also
                   be a dictionary mapping cell-type names to `False`, `None`,
                   or a column for each cell type. When using voomByGroup, the
                   same groups are also used as the `group` argument to
                   `calcNormFactors()` when normalizing by library size. All
                   groups must have at least two samples.
            categorical_columns: one or more names of integer or Enum columns
                                 of `obs` to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to test for DE; if `None`, test
                        all cell types. If specified and using dictionaries for
                        `formula`, `coefficient`, `contrasts`, or `group`, the
                        keys of these dictionaries must match the cell types
                        specified here. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when testing
                                 for DE. If specified and using dictionaries
                                 for `formula`, `coefficient`, `contrasts`, or
                                 `group`, the keys of these dictionaries must
                                 also exclude these cell types. Mutually
                                 exclusive with `cell_types`.
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of `obs`, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         log_CPM() already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation
            num_threads: the number of threads to use for DE estimation.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.options()['num_threads']` cores (1 by
                         default). Parallelization takes place across cell
                         types, so specifying more cores than the number of
                         cell types may not improve performance.

        Returns:
            A DE object with a `table` attribute containing a polars DataFrame
            of the DE results, with columns:
            - cell_type: the cell type in which DE was tested
            - coefficient: the coefficient (or contrast) for which DE was 
                           tested
            - gene: the gene for which DE was tested
            - logFC: the log2 fold change of the gene, i.e. its effect size
            - SE: the standard error of the effect size
            - LCI: the lower 95% confidence interval of the effect size
            - UCI: the upper 95% confidence interval of the effect size
            - AveExpr: the gene's average expression in this cell type, in log
                       CPM
            - p: the DE p-value
            - Bonferroni: the Bonferroni-corrected DE p-value
            - FDR: the FDR q-value for the DE
            If `return_voom_info=True`, the DE object also includes a
            `voom_weights` attribute containing a {cell_type: DataFrame}
            dictionary of voom weights, and a `voom_plot_data` attribute
            containing a {cell_type: DataFrame} dictionary of info necessary to
            construct a voom plot with `DE.plot_voom()`.
        """
        # Import required Python and R packages, and source voomByGroup code
        from ryp import r
        r('suppressPackageStartupMessages(library(limma))')
        r(self._voomByGroup_source_code)
        # Get the list of cell types to compute DE for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings, and that each formula is a valid R formula
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
                if not value.lstrip().startswith('~'):
                    error_message = \
                        f'formula[{key!r}] must start with a tilde (~)'
                    raise ValueError(error_message)
                try:
                    r(f'as.formula({value!r})')
                except RuntimeError as e:
                    error_message = \
                        f'formula[{key!r}] is not a valid R formula'
                    raise ValueError(error_message) from e
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        else:
            if not formula.lstrip().startswith('~'):
                error_message = 'formula must start with a tilde (~)'
                raise ValueError(error_message)
            try:
                r(f'as.formula({formula!r})')
            except RuntimeError as e:
                error_message = 'formula is not a valid R formula'
                raise ValueError(error_message) from e
        # Check that `coefficient` is one or more strings or integers, or a
        # dictionary mapping cell types to one or more strings or integers.
        # Convert it (or its values, if a dictionary) to tuples, storing the
        # result as a new variable, `coefficients`.
        coefficient_is_dict = isinstance(coefficient, dict)
        if coefficient_is_dict:
            coefficients = {}
            for cell_type, value in coefficient.items():
                if not isinstance(cell_type, str):
                    error_message = (
                        f'when coefficient is a dictionary, all its keys must '
                        f'be strings (cell types), but it contains a key of '
                        f'type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                coefficients[cell_type] = to_tuple_checked(
                    value, f'coefficient[{cell_type!r}]', (str, int),
                    'strings or integers')
            if tuple(coefficients) != cell_types:
                error_message = (
                    f'coefficient is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
        else:
            coefficients = to_tuple_checked(coefficient, 'coefficient',
                                            (str, int), 'strings or integers')
        # Check that `contrasts` is `None`, a dictionary mapping strings to
        # strings, or a dictionary mapping cell types to `None` or dictionaries
        # mapping strings to strings. Check that each contrast is a valid
        # R formula.
        contrasts_is_nested_dict = False
        if contrasts is not None:
            check_type(contrasts, 'contrasts', dict, 'a dictionary')
            if not isinstance(coefficient, (int, np.integer)) or \
                    coefficient != 1:
                error_message = \
                    'coefficient and contrasts cannot both be specified'
                raise ValueError(error_message)
            for key in contrasts:
                if not isinstance(key, str):
                    error_message = (
                        f'all keys of contrasts must be strings, but it '
                        f'contains a key of type {type(key).__name__!r}')
                    raise TypeError(error_message)
            if all(isinstance(value, str) for value in contrasts.values()):
                # `contrasts` is a dictionary mapping strings to strings
                for key, value in contrasts.items():
                    try:
                        r(f'as.formula({f"~{value}"!r})')
                    except RuntimeError as e:
                        error_message = \
                            f'contrasts[{key!r}] is not a valid R formula'
                        raise ValueError(error_message) from e
            elif all(isinstance(value, dict) or value is None
                     for value in contrasts.values()):
                # `contrasts` is a dictionary mapping cell types to `None` or
                # dictionaries mapping strings to strings
                contrasts_is_nested_dict = True
                if tuple(contrasts) != cell_types:
                    error_message = (
                        f'contrasts is a dictionary of dictionaries, but does '
                        f'not have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
                for key, value in contrasts.values():
                    if value is not None:
                        for inner_key, inner_value in value.items():
                            if not isinstance(inner_key, str):
                                error_message = (
                                    f'all keys of contrasts[{key!r}] must be '
                                    f'strings, but it contains a key of type '
                                    f'{type(inner_key).__name__!r}')
                                raise TypeError(error_message)
                            if not isinstance(inner_value, str):
                                error_message = (
                                    f'all values of contrasts[{key!r}] must '
                                    f'be strings, but it contains a value of '
                                    f'type {type(inner_value).__name__!r}')
                                raise TypeError(error_message)
                            try:
                                r(f'as.formula({f"~{inner_value}"!r})')
                            except RuntimeError as e:
                                error_message = (
                                    f'contrasts[{key!r}][{inner_key!r}] is '
                                    f'not a valid R formula')
                                raise ValueError(error_message) from e
                all_contrasts = contrasts
            else:
                error_message = (
                    'contrasts.values() must either be all strings or all '
                    'dictionaries/None')
                raise TypeError(error_message)
        # Check that `group` is `False`, `None`, a categorical column, or a
        # dictionary mapping cell types to `False`, `None`, or categorical
        # columns.
        if group is not None and group is not False:
            if group is True:
                error_message = \
                    'group must be None, False, or a column of obs, not True'
                raise TypeError(error_message)
            if isinstance(group, dict):
                for key, value in group.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when group is a dictionary, all its keys must '
                            f'be strings (cell types), but it contains a key '
                            f'of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None and value is not False:
                        if value is True:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, not True')
                            raise TypeError(error_message)
                        else:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, but has type '
                                f'{type(value).__name__!r}')
                            raise TypeError(error_message)
                # noinspection PyTypeChecker
                if tuple(group) != cell_types:
                    error_message = (
                        f'group is a dictionary, but does not have the same '
                        f'cell types (keys) as {cell_type_description}, or '
                        f'has the same cell types in a different order')
                    raise ValueError(error_message)
                groups = {}
                for cell_type, column in group.items():
                    obs = self._obs[cell_type]
                    if column is None:
                        groups[cell_type] = None
                    if isinstance(column, str):
                        if column not in obs:
                            error_message = (
                                f'group[{cell_type!r}] is {column!r}, which '
                                f'is not a column of obs[{cell_type!r}]')
                            raise ValueError(error_message)
                        groups[cell_type] = obs[column]
                    elif isinstance(column, pl.Expr):
                        groups[cell_type] = obs.select(column)
                        if groups[cell_type].width > 1:
                            # noinspection PyUnresolvedReferences
                            error_message = (
                                f'group[{cell_type!r}] is a polars expression '
                                f'that expands to {group[cell_type].width:,} '
                                f'columns rather than 1 for cell type ')
                            raise ValueError(error_message)
                        # noinspection PyUnresolvedReferences
                        groups[cell_type] = groups[cell_type].to_series()
                    elif isinstance(column, pl.Series):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a polars Series of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = column
                    elif isinstance(column, np.ndarray):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a NumPy array of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = pl.Series('group', column)
                    else:
                        error_message = (
                            f'group[{cell_type!r}] must be None, False, a '
                            f'string column name, a polars expression or '
                            f'Series, or a 1D NumPy array, but has type '
                            f'{type(column).__name__!r}')
                        raise TypeError(error_message)
                    # Check dtype
                    base_type = column.dtype.base_type()
                    if base_type not in (pl.String, pl.Categorical, pl.Enum,
                                         pl.Boolean) and \
                            base_type not in pl.INTEGER_DTYPES:
                        error_message = (
                            f'group[{cell_type!r}] must be String, '
                            f'Categorical, Enum, Boolean, or integer, but has '
                            f'data type {base_type!r}')
                        raise TypeError(error_message)
                    # Check `null` values
                    null_count = column.null_count()
                    if null_count > 0:
                        error_message = (
                            f'group[{cell_type!r}] contains {null_count:,} '
                            f'{plural("null value", null_count)}, but must '
                            f'not contain any')
                        raise ValueError(error_message)
            else:
                # noinspection PyTypeChecker
                groups = self._get_column(
                    'obs', group, 'group',
                    (pl.String, pl.Categorical, pl.Enum, pl.Boolean,
                     'integer'))
                for cell_type, group in groups.items():
                    if group is not None and group is not False:
                        null_count = group.null_count()
                        if null_count > 0:
                            error_message = (
                                f'group contains {null_count:,} '
                                f'{plural("null value", null_count)} for cell '
                                f'type {cell_type!r}, but must not contain '
                                f'any. Specify the same group column via the '
                                f'group_column argument to Pseudobulk.qc(), '
                                f'explicitly remove nulls from your group '
                                f'column with fill_null(), or specify a '
                                f'different group column (or none at all, to '
                                f'group automatically).')
                            raise ValueError(error_message)
        else:
            groups = None
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        # Check that Boolean arguments are Boolean
        check_type(library_size_as_covariate, 'library_size_as_covariate',
                   bool, 'Boolean')
        check_type(num_cells_as_covariate, 'num_cells_as_covariate', bool,
                   'Boolean')
        check_type(robust, 'robust', bool, 'Boolean')
        check_type(return_voom_info, 'return_voom_info', bool, 'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.options()['num_threads']` if
        # free-threaded and 1 otherwise, and if -1, set to `os.cpu_count()`.
        # Raise an error if the user specified multiple threads but lacks
        # free-threaded Python.
        num_threads = Pseudobulk._process_num_threads(num_threads)
        # If `num_cells_as_covariate=True`, check that `'num_cells'` is a
        # column of `obs` for every cell type
        if num_cells_as_covariate:
            for cell_type in cell_types:
                if 'num_cells' not in self._obs[cell_type]:
                    error_message = (
                        f"num_cells_as_covariate is True, but 'num_cells' is "
                        f"not a column of obs[{cell_type!r}]")
                    raise ValueError(error_message)
        # Compute DE for each cell type
        if num_threads == 1:
            DE_results = {}
            if return_voom_info:
                voom_weights = {}
                voom_plot_data = {}
            for cell_type_index, cell_type in enumerate(cell_types):
                # noinspection PyUnboundLocalVariable
                results = self._DE(
                    cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type]
                            if formula_is_dict else formula,
                    coefficient=coefficients[cell_type]
                                if coefficient_is_dict else coefficients,
                    contrasts=all_contrasts[cell_type]
                              if contrasts_is_nested_dict else contrasts,
                    group=groups[cell_type] if groups is not None else None,
                    categorical_columns=all_categorical_columns[cell_type]
                                        if categorical_columns_is_dict else
                                        categorical_columns,
                    library_size_as_covariate=library_size_as_covariate,
                    num_cells_as_covariate=num_cells_as_covariate,
                    robust=robust,
                    return_voom_info=return_voom_info,
                    allow_float=allow_float,
                    verbose=verbose)
                if return_voom_info:
                    # noinspection PyUnboundLocalVariable
                    DE_results[cell_type], voom_weights[cell_type], \
                        voom_plot_data[cell_type] = results
                else:
                    DE_results[cell_type] = results
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            kwargs_list = [{
                'cell_type': cell_type,
                'cell_type_index': cell_type_index,
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'coefficient': coefficients[cell_type]
                               if coefficient_is_dict else coefficients,
                'contrasts': all_contrasts[cell_type]
                             if contrasts_is_nested_dict else contrasts,
                'group': groups[cell_type] if groups is not None else None,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'library_size_as_covariate': library_size_as_covariate,
                'num_cells_as_covariate': num_cells_as_covariate,
                'robust': robust,
                'return_voom_info': return_voom_info,
                'allow_float': allow_float,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                results = tuple(executor.map(
                    lambda kwargs: self._DE(**kwargs), kwargs_list))
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                DE_results = {cell_type: result[0] for cell_type, result in
                              zip(cell_types, results)}
                voom_weights = {cell_type: result[1] for cell_type, result in
                                zip(cell_types, results)}
                voom_plot_data = {cell_type: result[2] for cell_type, result in
                                  zip(cell_types, results)}
            else:
                DE_results = dict(zip(cell_types, results))
        # Concatenate across cell types
        table = pl.concat([
            cell_type_DE_results
            .select(pl.lit(cell_type).alias('cell_type'), pl.all())
            for cell_type, cell_type_DE_results in DE_results.items()])
        if return_voom_info:
            return DE(table, voom_weights, voom_plot_data)
        else:
            return DE(table)


class DE:
    """
    Differential expression results returned by Pseudobulk.DE().
    """
    
    def __init__(self,
                 table: pl.DataFrame,
                 voom_weights: dict[str, pl.DataFrame] | None = None,
                 voom_plot_data: dict[str, pl.DataFrame] | None = None) -> \
            None:
        """
        Initialize the DE object.
        
        Args:
            table: a polars DataFrame containing the DE results, with columns:
                   - cell_type: the cell type in which DE was tested
                   - coefficient: the coefficient (or contrast) for which DE 
                                  was tested
                   - gene: the gene for which DE was tested
                   - logFC: the log2 fold change of the gene, i.e. its effect
                            size
                   - SE: the standard error of the effect size
                   - LCI: the lower 95% confidence interval of the effect size
                   - UCI: the upper 95% confidence interval of the effect size
                   - AveExpr: the gene's average expression in this cell type,
                              in log CPM
                   - p: the DE p-value
                   - Bonferroni: the Bonferroni-corrected DE p-value
                   - FDR: the FDR q-value for the DE
                   Or, a directory containing a DE object saved with `save()`.
            voom_weights: an optional {cell_type: DataFrame} dictionary of voom
                         weights, where rows are genes and columns are samples.
                         The first column of each cell type's DataFrame,
                         'gene', contains the gene names.
            voom_plot_data: an optional {cell_type: DataFrame} dictionary of
                            info necessary to construct a voom plot with
                            `DE.plot_voom()`
        """
        if isinstance(table, pl.DataFrame):
            if voom_weights is not None:
                if voom_plot_data is None:
                    error_message = (
                        'voom_plot_data must be specified when voom_weights '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_weights, 'voom_weights', dict, 'a dictionary')
                if voom_weights.keys() != voom_plot_data.keys():
                    error_message = (
                        'voom_weights and voom_plot_data must have matching '
                        'cell types (keys)')
                    raise ValueError(error_message)
                for key in voom_weights:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of voom_weights and voom_plot_data '
                            f'must be strings (cell types), but they contain '
                            f'a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
            if voom_plot_data is not None:
                if voom_weights is None:
                    error_message = (
                        'voom_weights must be specified when voom_plot_data '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_plot_data, 'voom_plot_data', dict,
                           'a dictionary')
        elif isinstance(table, (str, Path)):
            table = str(table)
            if not os.path.exists(table):
                error_message = f'DE object directory {table!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{table}/cell_types.txt')]
            voom_weights = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_weights.parquet'))
                for cell_type in cell_types}
            voom_plot_data = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_plot_data.parquet'))
                for cell_type in cell_types}
            table = pl.read_parquet(os.path.join(table, 'table.parquet'))
        else:
            error_message = (
                f'table must be a polars DataFrame or a directory (string or '
                f'pathlib.Path) containing a saved DE object, but has type '
                f'{type(table).__name__!r}')
            raise TypeError(error_message)
        self.table = table
        self.voom_weights = voom_weights
        self.voom_plot_data = voom_plot_data
    
    def __repr__(self) -> str:
        """
        Get a string representation of this DE object.
        
        Returns:
            A string summarizing the object.
        """
        num_cell_types = self.table['cell_type'].n_unique()
        descr = (
            f'DE object with {len(self.table):,} '
            f'{"entries" if len(self.table) != 1 else "entry"} across '
            f'{num_cell_types:,} {plural("cell type", num_cell_types)}:\n'
            f'{self.table}')
        return descr
    
    def __eq__(self, other: DE) -> bool:
        """
        Test for equality with another DE object.
        
        Args:
            other: the other DE object to test for equality with

        Returns:
            Whether the two DE objects are identical.
        """
        if not isinstance(other, DE):
            error_message = (
                f'the left-hand operand of `==` is a DE object, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        return self.table.equals(other.table) and \
            (other.voom_weights is None if self.voom_weights is None else
             self.voom_weights.keys() == other.voom_weights.keys() and
             all(self.voom_weights[cell_type].equals(
                     other.voom_weights[cell_type]) and
                 self.voom_plot_data[cell_type].equals(
                     other.voom_plot_data[cell_type])
                 for cell_type in self.voom_weights))
    
    @property
    def groups(self) -> dict[str, tuple[str, ...] | None] | None:
        """
        Get the groups used by voomByGroup for each cell type.
        
        Returns:
            A dictionary mapping cell type names to group names used by
            voomByGroup for that cell type, or None if voomByGroup was not used
            for that cell type. If `Pseudobulk.DE()` was called with
            `return_voom_info=False`, `groups` will be None instead of a
            dictionary.
        """
        return {cell_type: groups if groups else None
                for cell_type, groups in (
                    (cell_type, tuple(column[5:] for column in data.columns
                                      if column[:4] == 'xy_x'))
                    for cell_type, data in self.voom_plot_data.items())} \
            if self.voom_plot_data is not None else None
    
    def save(self, directory: str | Path, overwrite: bool = False) -> None:
        # noinspection GrazieInspection
        """
        Save a DE object to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with the table at
        `table.parquet`.
        
        If the DE object contains voom info (i.e. was created with
        `return_voom_info=True` in `Pseudobulk.DE()`, the default), also saves
        each cell type's voom weights and voom plot data to
        f'{cell_type}_voom_weights.parquet' and
        f'{cell_type}_voom_plot_data.parquet', as well as a text file,
        cell_types.txt, containing the cell types.
        
        Args:
            directory: the directory to save the DE object to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
        """
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        self.table.write_parquet(os.path.join(directory, 'table.parquet'))
        if self.voom_weights is not None:
            with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
                # noinspection PyTypeChecker
                print('\n'.join(self.voom_weights), file=f)
            for cell_type in self.voom_weights:
                escaped_cell_type = cell_type.replace('/', '-')
                self.voom_weights[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_weights.parquet'))
                self.voom_plot_data[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_plot_data.parquet'))
    
    def get_hits(self,
                 significance_column: str = 'FDR',
                 threshold: int | float | np.integer | np.floating = 0.05,
                 num_top_hits: int | np.integer | None = None) -> pl.DataFrame:
        """
        Get all (or the top) differentially expressed genes.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            num_top_hits: the number of top hits to report for each cell type;
                          if `None`, report all hits

        Returns:
            The `table` attribute of this DE object, subset to (top) DE hits.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and â‰¤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        if num_top_hits is not None:
            check_type(num_top_hits, 'num_top_hits', int, 'a positive integer')
            check_bounds(num_top_hits, 'num_top_hits', 1)
        return self.table\
            .filter(pl.col(significance_column) < threshold)\
            .pipe(lambda df: df.group_by('cell_type', maintain_order=True)
                  .head(num_top_hits) if num_top_hits is not None else df)
    
    def get_num_hits(self,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer |
                                np.floating = 0.05) -> pl.DataFrame:
        """
        Get the number of differentially expressed genes in each cell type.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`

        Returns:
            A DataFrame with one row per cell type and two columns:
            'cell_type' and 'num_hits'.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and â‰¤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        return self.table\
            .lazy()\
            .filter(pl.col(significance_column) < threshold)\
            .group_by('cell_type')\
            .agg(num_hits=pl.len())\
            .sort('cell_type')\
            .collect()
    
    # noinspection PyUnresolvedReferences
    def plot_voom(self,
                  cell_type: str,
                  filename: str | Path | None = None,
                  *,
                  ax: 'Axes' | None = None,
                  figure_kwargs: dict[str, Any] | None = None,
                  point_color: Color | dict[str, Color] | None = None,
                  point_size: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1,
                  line_color: Color | dict[str, Color] | None = None,
                  line_width: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1.5,
                  scatter_kwargs: dict[str, Any] | None |
                                  dict[str, dict[str, Any] | None] = None,
                  plot_kwargs: dict[str, Any] | None |
                               dict[str, dict[str, Any] | None] = None,
                  legend: bool = True,
                  legend_kwargs: dict[str, Any] | None = None,
                  title: str | None = None,
                  title_kwargs: dict[str, Any] | None = None,
                  xlabel: str | None = 'Average log2(count + 0.5)',
                  xlabel_kwargs: dict[str, Any] | None = None,
                  ylabel: str | None = 'sqrt(standard deviation)',
                  ylabel_kwargs: dict[str, Any] | None = None,
                  despine: bool = True,
                  savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a voom plot for a cell type that differential expression was
        calculated for.
        
        Voom plots consist of a scatter plot with one point per gene. They
        visualize how the mean expression of each gene across samples (x)
        relates to the gene's variation in expression across samples (y). The
        plot also includes a LOESS (also called LOWESS) curve, a type of
        non-linear curve fit, of the mean-variance (x-y) trend.
        
        Specifically, the x position of a gene's point is the average, across
        samples, of the base-2 logarithm of the gene's count in each sample,
        plus a pseudocount of 0.5: in other words, mean(log2(count + 0.5)).
        The y position is the square root of the standard deviation, across
        samples, of the gene's log counts per million after regressing out,
        across samples, the differential expression design matrix.
        
        When running differential expression with voomByGroup, voom is run
        separately within each group, so the voom plot will show a separate
        LOESS trendline for each group, with the points and trendlines for each
        group shown in distinct colors.
        
        Many arguments to this function can be either a single value or a
        dictionary mapping group names to values. The group names can be viewed
        with `self.groups[cell_type]`.
        
        Args:
            cell_type: the cell type to generate the voom plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            point_color: the color of the points in the voom plot. Can be a
                         single color or a dictionary mapping each of the group
                         names in `self.groups[cell_type]` to colors. When not
                         using voomByGroup, defaults to `'#666666'` (gray).
                         When using voomByGroup with two groups, defaults to
                         `'#666666'` for the first group in
                         `self.groups[cell_type]` and `'#FF6666'` (red) for the
                         second. When using voomByGroup with more than two
                         groups, must be specified manually. Can be any valid
                         Matplotlib color, like a hex string (e.g.
                         `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            point_size: the size of the points in the voom plot. Can be a
                        single number or a dictionary mapping each of the group
                        names in `self.groups[cell_type]` to numbers.
            line_color: the color of the LOESS trendline. Can be a single color
                        or a dictionary mapping each of the group names in
                        `self.groups[cell_type]` to colors. When not using
                        voomByGroup, defaults to `'#000000'` (black). When
                        using voomByGroup with two groups, defaults to
                        `'#000000'` for the first group and `'#FF0000'` (red).
                        for the second. When using voomByGroup with more than
                        two groups, must be specified manually. Can be any
                        valid Matplotlib color, like a hex string (e.g.
                        `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            line_width: the width of the LOESS trendline. Can be a single
                        number or a dictionary mapping each of the group names
                        in `self.groups[cell_type]` to numbers.
            scatter_kwargs: a dictionary (or dictionary mapping each of the
                            group names in `self.groups[cell_type]` to
                            dictionaries) of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              numbers in `color_column` are converted to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s` or `c`/`color`/`norm`/`vmin`/`vmax`
                            will raise an error, since these arguments conflict
                            with the `point_size` and `point_color` arguments,
                            respectively.
            plot_kwargs: a dictionary (or dictionary mapping each of the group
                         names in `self.groups[cell_type]` to dictionaries) of
                         keyword arguments to be passed to `ax.plot()` when
                         plotting the trendlines, such as `linestyle='--'` for
                         dashed trendlines. Specifying `color`/`c` or
                         `linewidth` will raise an error, since these arguments
                         conflict with the `line_color` and `line_width`
                         arguments, respectively.
            legend: whether to add a legend with the colors for each group when
                    using voomByGroup. Only `legend=False` has an effect, and
                    it can only be specified when using voomByGroup. Without 
                    groups, there will never be a legend, so specifying 
                    `legend=False` would be redundant.                     
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border. `frameon` defaults to `False`, instead
                             of Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when using voomByGroup with
                           `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the voom plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Check that this DE object contains `voom_plot_data`, the data
        # necessary to generate the voom plot from (scatter-plot points and
        # LOESS trendlines)
        if self.voom_plot_data is None:
            error_message = (
                'this DE object does not contain the voom_plot_data '
                'attribute, which is necessary to generate voom plots; re-run '
                'Pseudobulk.DE() with return_voom_info=True to include this '
                'attribute')
            raise AttributeError(error_message)
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.voom_plot_data:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        # Get the voom plot data for this cell type
        voom_plot_data = self.voom_plot_data[cell_type]
        # Get the voomByGroup groups for this cell type (`None` if voomByGroup
        # was not used)
        groups = [column[5:] for column in voom_plot_data.columns
                  if column[:4] == 'xy_x']
        if len(groups) == 0:
            groups = None
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # If `figure_kwargs` is not `None`, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        # Check that `point_color` and `line_color` are valid Matplotlib colors
        # or dictionaries thereof, and convert them to hex. Or, if `None`, set
        # to their default value if there are no groups or exactly two groups.
        point_color_is_dict = isinstance(point_color, dict)
        if point_color is None:
            if groups is None:
                point_color = '#666666'
            elif len(groups) == 2:
                point_color = {groups[0]: '#666666', groups[1]: '#FF6666'}
                point_color_is_dict = True
            else:
                error_message = (
                    f'point_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif point_color_is_dict:
            for group, group_point_color in point_color.items():
                if not plt.matplotlib.colors.is_color_like(group_point_color):
                    error_message = (
                        f'point_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            point_color = {
                group: plt.matplotlib.colors.to_hex(group_point_color)
                for group, group_point_color in point_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(point_color):
                error_message = (
                    f'point_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            point_color = plt.matplotlib.colors.to_hex(point_color)
        line_color_is_dict = isinstance(line_color, dict)
        if line_color is None:
            if groups is None:
                line_color = '#000000'
            elif len(groups) == 2:
                line_color = {groups[0]: '#000000', groups[1]: '#FF0000'}
                line_color_is_dict = True
            else:
                error_message = (
                    f'line_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif line_color_is_dict:
            for group, group_line_color in line_color.items():
                if not plt.matplotlib.colors.is_color_like(group_line_color):
                    error_message = (
                        f'line_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            line_color = {
                group: plt.matplotlib.colors.to_hex(group_line_color)
                for group, group_line_color in line_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(line_color):
                error_message = (
                    f'line_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            line_color = plt.matplotlib.colors.to_hex(line_color)
        # Check that `point_size` and `line_width` are positive numbers or
        # dicts thereof
        point_size_is_dict = isinstance(point_size, dict)
        line_width_is_dict = isinstance(line_width, dict)
        for number, number_name, is_dict in (
                (point_size, 'point_size', point_size_is_dict),
                (line_width, 'line_width', line_width_is_dict)):
            if is_dict:
                for group, group_number in number.items():
                    check_type(group_number, f'{number_name}[{group!r}]',
                               (int, float), 'a positive number')
                    check_bounds(group_number, f'{number_name}[{group!r}]', 0,
                                 left_open=True)
            else:
                check_type(number, number_name, (int, float),
                           'a positive number')
                check_bounds(number, number_name, 0, left_open=True)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (plot_kwargs, 'plot_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (title_kwargs, 'title_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # If using voomByGroup, for each of `scatter_kwargs` and `plot_kwargs`,
        # if the kwarg is not `None`, check that either all keys are group
        # names and in the correct order, or that no keys are group names. If
        # all keys are group names, check that all values are either `None` or
        # dictionaries with all-string keys, and make note that the kwargs is a
        # nested dict.
        scatter_kwargs_is_nested_dict = False
        plot_kwargs_is_nested_dict = False
        if groups is not None:
            for kwargs, kwargs_name in ((scatter_kwargs, 'scatter_kwargs'),
                                        (plot_kwargs, 'plot_kwargs')):
                if kwargs is not None:
                    if tuple(kwargs) == groups:
                        # The kwargs's keys exactly match the group names
                        for key, value in kwargs.items():
                            if value is not None:
                                check_type(value, f'{kwargs_name}[{key!r}]',
                                           dict, 'a dictionary')
                                for inner_key in value:
                                    if not isinstance(inner_key, str):
                                        error_message = (
                                            f'all keys of '
                                            f'{kwargs_name}[{key!r}] must be '
                                            f'strings, but it contains a key '
                                            f'of type '
                                            f'{type(inner_key).__name__!r}')
                                        raise TypeError(error_message)
                        if kwargs is scatter_kwargs:
                            scatter_kwargs_is_nested_dict = True
                        else:
                            plot_kwargs_is_nested_dict = True
                    else:
                        # Check that none of the kwargs's keys are group names
                        for group in groups:
                            if group in kwargs:
                                if set(groups) == set(kwargs):
                                    error_message = (
                                        f'{kwargs_name}.keys() does have the '
                                        f'same groups as '
                                        f'self.groups[{cell_type!r}], but '
                                        f'they are in a different order')
                                    raise ValueError(error_message)
                                else:
                                    error_message = (
                                        f'some keys of {kwargs_name}.keys() '
                                        f'are groups in '
                                        f'self.groups[{cell_type!r}], but '
                                        f'others are not')
                                    raise ValueError(error_message)
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        if scatter_kwargs_is_nested_dict:
            for key, value in scatter_kwargs.items():
                scatter_kwargs[key] = default_scatter_kwargs | value \
                    if value is not None else default_scatter_kwargs
        else:
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
        # Set `plot_kwargs` to `{}` if it is `None`, or set the `None` values
        # of `plot_kwargs` to `{}` if `plot_kwargs` is a nested dict
        if plot_kwargs is None:
            plot_kwargs = {}
        elif plot_kwargs_is_nested_dict:
            for key, value in plot_kwargs.items():
                if value is None:
                    plot_kwargs[key] = {}
        # Check that `scatter_kwargs` does not contain the `s` or
        # `c`/`color`/`norm`/`vmin`/`vmax` keys and that `plot_kwargs` does
        # not contain the `c`/`color`/`norm`/`vmin`/`vmax` or `linewidth` keys,
        # or that their non-`None` values do not contain these keys if a nested
        # dict
        for kwargs, kwargs_name, alternate_color, is_nested_dict in (
                (scatter_kwargs, 'scatter_kwargs', 'line_color',
                 scatter_kwargs_is_nested_dict),
                (plot_kwargs, 'plot_kwargs', 'point_color',
                 plot_kwargs_is_nested_dict)):
            bad_keys = (('linewidth', 'line_width')
                        if kwargs is plot_kwargs else ('s', 'point_size'),
                        ('c', alternate_color),
                        ('color', alternate_color),
                        ('norm', alternate_color),
                        ('vmin', alternate_color),
                        ('vmax', alternate_color))
            if is_nested_dict:
                for key, value in kwargs.items():
                    if value is not None:
                        for bad_key, alternate_argument in bad_keys:
                            if bad_key in value:
                                error_message = (
                                    f'{bad_key!r} cannot be specified as a '
                                    f'key in {kwargs_name}[{key}!r]; specify '
                                    f'the {alternate_argument} argument '
                                    f'instead')
                                raise ValueError(error_message)
            elif kwargs is not None:
                for bad_key, alternate_argument in bad_keys:
                    if bad_key in kwargs:
                        error_message = (
                            f'{bad_key!r} cannot be specified as a key in '
                            f'{kwargs_name}; specify the {alternate_argument} '
                            f'argument instead')
                        raise ValueError(error_message)
        # Check that `legend` is Boolean. If not using voomByGroup, check that 
        # the user did not specify `legend=False`.
        check_type(legend, 'legend', bool, 'Boolean')
        if groups is None:
            if not legend:
                error_message = (
                    'legend=False cannot be specified when there are no '
                    'groups, since it would be redundant: without groups, '
                    'there will never be a legend')
                raise ValueError(error_message)
        # Override the defaults for certain values of `legend_kwargs`; check
        # that it is `None` when not using a legend
        default_legend_kwargs = dict(frameon=False)
        if legend_kwargs is not None:
            if groups is None:
                error_message = (
                    'legend_kwargs cannot be specified when there are no '
                    'groups, since there will not be a legend')
                raise ValueError(error_message)
            if not legend:
                error_message = \
                    'legend_kwargs cannot be specified when legend=False'
                raise ValueError(error_message)
            legend_kwargs = default_legend_kwargs | legend_kwargs
        else:
            legend_kwargs = default_legend_kwargs
        # If `title` is not `None`, check that it is a string
        if title is not None:
            check_type(title, 'title', str, 'a string')
        # Check that `title_kwargs` is `None` when `title` is `None`
        if title is None and title_kwargs is not None:
            error_message = 'title_kwargs cannot be specified when title=None'
            raise ValueError(error_message)
        # Check that `xlabel` is a string or `None`; if `None`, check that
        # `xlabel_kwargs` is `None` as well. Ditto for `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
        # Override the defaults for certain values of `savefig_kwargs`
        default_savefig_kwargs = \
            dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                 transparent=filename is not None and
                             filename.endswith('.pdf'))
        savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
            if savefig_kwargs is not None else default_savefig_kwargs
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            if groups is not None:
                if legend:
                    legend_patches = []
                for group in groups:
                    # Get this group's point size, point color, line color,
                    # line width, plot kwargs, and scatter kwargs
                    group_point_size = point_size[group] \
                        if point_size_is_dict else point_size
                    group_point_color = point_color[group] \
                        if point_color_is_dict else point_color
                    group_line_color = line_color[group] \
                        if line_color_is_dict else line_color
                    group_line_width = line_width[group] \
                        if line_width_is_dict else line_width
                    group_scatter_kwargs = scatter_kwargs[group] \
                        if scatter_kwargs_is_nested_dict else scatter_kwargs
                    group_plot_kwargs = plot_kwargs[group] \
                        if plot_kwargs_is_nested_dict else plot_kwargs
                    # Plot the scatter plot for this group
                    ax.scatter(voom_plot_data[f'xy_x_{group}'].drop_nulls(),
                               voom_plot_data[f'xy_y_{group}'].drop_nulls(),
                               s=group_point_size, c=group_point_color,
                               **group_scatter_kwargs)
                    # Plot the LOESS trendline for this group
                    ax.plot(voom_plot_data[f'line_x_{group}'].drop_nulls(),
                            voom_plot_data[f'line_y_{group}'].drop_nulls(),
                            c=group_line_color, linewidth=group_line_width,
                            **group_plot_kwargs)
                    # Create a rectangle for the legend for this group, where
                    # the border matches the color of the trendline and the
                    # fill matches the color of the scatter plot points
                    if legend:
                        # noinspection PyUnboundLocalVariable
                        # noinspection PyUnresolvedReferences
                        legend_patches.append(plt.matplotlib.patches.Patch(
                            facecolor=group_point_color,
                            edgecolor=group_line_color,
                            linewidth=group_line_width, label=group))
                # Add the legend
                if legend:
                    ax.legend(handles=legend_patches, **legend_kwargs)
            else:
                # Plot the scatter plot
                ax.scatter(voom_plot_data['xy_x'], voom_plot_data['xy_y'],
                           s=point_size, c=point_color, **scatter_kwargs)
                # Plot the LOESS trendline
                ax.plot(voom_plot_data['line_x'], voom_plot_data['line_y'],
                         c=line_color, linewidth=line_width, **plot_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not False:
                if title_kwargs is None:
                    title_kwargs = {}
                # noinspection PyCallingNonCallable
                ax.set_title(title[cell_type] if isinstance(title, dict)
                             else title if isinstance(title, str) else
                             title(cell_type) if isinstance(title, Callable)
                             else cell_type, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    # noinspection PyUnresolvedReferences
    def plot_volcano(self,
                     cell_type: str,
                     filename: str | Path | None = None,
                     *,
                     ax: 'Axes' | None = None,
                     figure_kwargs: dict[str, Any] | None = None,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer | np.floating = 0.05,
                     genes_to_label: int | np.integer | str |
                                     Iterable[str] = 10,
                     label_kwargs: dict[str, Any] | None = None,
                     upregulated_size: int | float | np.integer |
                                       np.floating = 6,
                     downregulated_size: int | float | np.integer |
                                         np.floating = 6,
                     non_significant_size: int | float | np.integer |
                                           np.floating = 4,
                     upregulated_color: Color = '#FC4E07',
                     downregulated_color: Color = '#00AFBB',
                     non_significant_color: Color = 'lightgray',
                     upregulated_scatter_kwargs: dict[str, Any] | None = None,
                     downregulated_scatter_kwargs: dict[str, Any] |
                                                   None = None,
                     non_significant_scatter_kwargs: dict[str, Any] |
                                                     None = None,
                     legend: bool = True,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = '$log_2(FC)$',
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = '$-log_{10}(FDR)$',
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a volcano plot of the DE hits, with negative log FDRs (or
        another `significance_column`) on the y-axis plotted against log fold
        changes on the x-axis. Upregulated, downregulated and non-significant
        genes are plotted in three different colors.
        
        When labeling genes (`genes_to_label != 0`), requires the textalloc
        package (github.com/ckjellson/textalloc) to make sure the labels don't
        overlap. Install it with:
        
        pip install --no-deps --no-build-isolation textalloc
        
        Args:
            cell_type: the cell type to generate the volcano plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             Matplotlib's default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            genes_to_label: an integer number of top DE genes to label, a name
                            or sequence of names of genes to label, or `None`
                            to not add labels
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `textalloc.allocate()` when adding gene labels to
                          control the text properties, such as:
                           - `textcolor`/`textsize`: the text color and size
                           - `x_scatter`/`y_scatter`: the x/y coordinates of
                             points in the scatter plot, to repel labels away
                             from. Defaults to all points in the plot.
                           - `min_distance`/`max_distance`: the minimum and
                             maximum distances from each point to its label, as
                             a proportion of the width of the x-axis. Defaults
                             to 0 and 0.02, instead of textalloc's defaults of
                             0.015 and 0.2
                           - `draw_lines`: whether to draw lines between each
                              label and its corresponding point. Defaults to
                              `False`, instead of textalloc's default of
                              `True`.
                          See github.com/ckjellson/textalloc#parameters for the
                          full list of possible arguments. Can only be
                          specified when `genes_to_label` is non-zero.
            upregulated_size: the size of each upregulated gene's point
            downregulated_size: the size of each downregulated gene's point
            non_significant_size: the size of each non-significant gene's point
            upregulated_color: the color of each upregulated gene's point. Can
                               be any valid Matplotlib color, like a hex string
                               (e.g. `'#FF0000'`), a named color (e.g. 'red'),
                               a 3- or 4-element RGB/RGBA tuple of integers
                               0-255 or floats 0-1, or a single float 0-1 for
                               grayscale.
            downregulated_color: the color of each downregulated gene's point.
                                 Can be any valid Matplotlib color, like a hex
                                 string (e.g. `'#FF0000'`), a named color (e.g.
                                 'red'), a 3- or 4-element RGB/RGBA tuple of
                                 integers 0-255 or floats 0-1, or a single
                                 float 0-1 for grayscale.
            non_significant_color: the color of each non-significant gene's
                                   point. Can be any valid Matplotlib color,
                                   like a hex string (e.g. `'#FF0000'`), a
                                   named color (e.g. 'red'), a 3- or 4-element
                                   RGB/RGBA tuple of integers 0-255 or floats
                                   0-1, or a single float 0-1 for grayscale.
            upregulated_scatter_kwargs: a dictionary of keyword arguments to be
                                        passed to `ax.scatter()` for
                                        upregulated genes, such as:
                                        - `rasterized`: whether to convert the
                                          scatter plot points to a raster
                                          (bitmap) image when saving to a
                                          vector format like PDF. Defaults to
                                          `True`, instead of Matplotlib's
                                          default of `False`.
                                        - `marker`: the shape to use for
                                          plotting each gene
                                        - `alpha`: the transparency of each
                                          point
                                        - `linewidths` and `edgecolors`: the
                                          width and color of the borders around
                                          each marker. These are absent by
                                          default (`linewidths=0`), unlike
                                          Matplotlib's default. Both arguments
                                          can be either single values or
                                          sequences.
                                        - `zorder`: the order in which the
                                          genes are plotted, with higher values
                                          appearing on top of lower ones.
                                        Specifying `s` or `c`/`color`/`norm`/
                                        `vmin`/`vmax` will raise an error,
                                        since these arguments conflict with the
                                        `upregulated_size` and
                                        `upregulated_color` arguments,
                                        respectively.
            downregulated_scatter_kwargs: a dictionary of keyword arguments to
                                          be passed to `ax.scatter()` for
                                          downregulated genes; see the
                                          documentation of the
                                          `upregulated_scatter_kwargs` argument
                                          for details
            non_significant_scatter_kwargs: a dictionary of keyword arguments
                                            to be passed to `ax.scatter()` for
                                            non-significant genes; see the
                                            documentation of the
                                            `upregulated_scatter_kwargs`
                                            argument for details
            legend: whether to add a legend showing the marker style for
                    upregulated, downregulated, and non-significant points
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border. `frameon` defaults to `False`, instead
                             of Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the volcano plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.table['cell_type']:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # If `figure_kwargs` is not `None`, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        # Check that `significance_column` is the name of a floating-point
        # column in `self.table`
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        # Check that `threshold` is greater than 0 and less than or equal to 1
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and â‰¤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        # Subset `self.table` to the selected cell type, and log-transform the 
        # significance column and the threshold
        table = self.table.filter(cell_type=cell_type)\
            .with_columns(-pl.col(significance_column).log10())
        threshold = -np.log10(threshold)
        # Check that `genes_to_label` is an integer, a sequence of strings, or
        # `None`. If an integer, take that many gene names.
        if isinstance(genes_to_label, (int, np.integer)):
            label = genes_to_label != 0
            if label:
                x_to_label = table['logFC'][:genes_to_label]
                y_to_label = table[significance_column][:genes_to_label]
                genes_to_label = table['gene'][:genes_to_label]
            elif label_kwargs is not None:
                error_message = (
                    'label_kwargs cannot be specified when genes_to_label=0, '
                    'since no genes are being labeled')
                raise ValueError(error_message)
        else:
            label = True
            if genes_to_label is not None:
                genes_to_label = \
                    to_tuple_checked(genes_to_label, 'genes_to_label', str,
                                     'strings')
                genes_to_label = pl.DataFrame({'gene': genes_to_label})\
                    .join(table.select('gene', 'logFC', significance_column),
                          how='left', on='gene')
                num_missing = genes_to_label['logFC'].null_count()
                if num_missing == len(genes_to_label):
                    error_message = (
                        "none of the specified genes were found in "
                        "table['gene']")
                    raise ValueError(error_message)
                elif num_missing > 0:
                    gene = genes_to_label\
                        .filter(pl.col.logFC.is_null())['gene'][0]
                    error_message = (
                        f"one of the specified genes, {gene!r}, was not found "
                        f"in table['gene']")
                    raise ValueError(error_message)
                x_to_label = genes_to_label['logFC']
                y_to_label = genes_to_label[significance_column]
                genes_to_label = genes_to_label['gene']
        if label:
            # noinspection PyUnresolvedReferences
            import textalloc
        # Check that `upregulated_size`, `downregulated_size`, and
        # `non_significant_size` are positive numbers
        for size, size_name in (upregulated_size, 'upregulated_size'), \
                (downregulated_size, 'downregulated_size'), \
                (non_significant_size, 'non_significant_size'):
            check_type(size, size_name, (int, float), 'a positive number')
            check_bounds(size, size_name, 0, left_open=True)
        # Check that `upregulated_color`, `downregulated_color`, and
        # `non_significant_color` are valid Matplotlib colors, and convert them
        # to hex
        if not plt.matplotlib.colors.is_color_like(upregulated_color):
            error_message = 'upregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        upregulated_color = plt.matplotlib.colors.to_hex(upregulated_color)
        if not plt.matplotlib.colors.is_color_like(downregulated_color):
            error_message = \
                'downregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        downregulated_color = plt.matplotlib.colors.to_hex(downregulated_color)
        if not plt.matplotlib.colors.is_color_like(non_significant_color):
            error_message = \
                'non_significant_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        non_significant_color = \
            plt.matplotlib.colors.to_hex(non_significant_color)
        # Check that the three `scatter_kwargs` arguments do not contain
        # the `s` or `c`/`color`/`cmap`/`norm`/`vmin`/`vmax` keys
        for kwargs, kwargs_prefix in (
                (upregulated_scatter_kwargs, 'upregulated'),
                (downregulated_scatter_kwargs, 'downregulated'),
                (non_significant_scatter_kwargs, 'non_significant')):
            if kwargs is None:
                continue
            if 's' in kwargs:
                error_message = (
                    f"'s' cannot be specified as a key in "
                    f"{kwargs_prefix}_scatter_kwargs; specify the "
                    f"{kwargs_prefix}_size argument instead")
                raise ValueError(error_message)
            for key in 'c', 'color', 'cmap', 'norm', 'vmin', 'vmax':
                if key in kwargs:
                    error_message = (
                        f'{key!r} cannot be specified as a key in '
                        f'scatter_kwargs; specify the {kwargs_prefix}_color '
                        f'argument instead')
                    raise ValueError(error_message)
        # Override the defaults for certain values of the three
        # `scatter_kwargs` arguments
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        upregulated_scatter_kwargs = \
            default_scatter_kwargs | upregulated_scatter_kwargs \
            if upregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        downregulated_scatter_kwargs = \
            default_scatter_kwargs | downregulated_scatter_kwargs \
            if downregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        non_significant_scatter_kwargs = \
            default_scatter_kwargs | non_significant_scatter_kwargs \
            if non_significant_scatter_kwargs is not None else \
                default_scatter_kwargs
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (upregulated_scatter_kwargs,
                                     'upregulated_scatter_kwargs'),
                                    (downregulated_scatter_kwargs,
                                     'downregulated_scatter_kwargs'),
                                    (non_significant_scatter_kwargs,
                                     'non_significant_scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Check that `legend` and `despine` are Boolean
        check_type(legend, 'legend', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make the volcano plot
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC > 0)
                       .to_numpy()
                       .T, s=upregulated_size, c=upregulated_color,
                       label='Upregulated', **upregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC < 0)
                       .to_numpy()
                       .T, s=downregulated_size, c=downregulated_color,
                       label='Downregulated', **downregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) < threshold)
                       .to_numpy()
                       .T, s=non_significant_size, c=non_significant_color,
                       label='Non-significant',
                       **non_significant_scatter_kwargs)
            ax.set_ylim(bottom=0)
            # Add labels, using textalloc to avoid overlap
            if label:
                # noinspection PyUnboundLocalVariable
                default_label_kwargs = dict(
                    ax=ax, x=x_to_label, y=y_to_label,
                    text_list=genes_to_label,
                    x_scatter=table['logFC'].to_numpy(), 
                    y_scatter=table[significance_column].to_numpy(),
                    min_distance=0, max_distance=0.02, draw_lines=False)
                label_kwargs = default_label_kwargs | label_kwargs \
                    if label_kwargs is not None else default_label_kwargs
                textalloc.allocate(**label_kwargs)
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if legend:
                default_legend_kwargs = dict(frameon=False)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                ax.legend(**legend_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise


def concat_obs(datasets: SingleCell | Iterable[SingleCell] |
                         Pseudobulk | Iterable[Pseudobulk],
               *more_datasets: SingleCell | Pseudobulk,
               flexible: bool = False) -> SingleCell | Pseudobulk:
    """
    Concatenate multiple SingleCell datasets cell-wise, or multiple Pseudobulk
    datasets sample-wise.
    
    Delegates to `SingleCell.concat_obs()` or `Pseudobulk.concat_obs()`,
    depending on whether the datasets are SingleCell or Pseudobulk. For
    Pseudobulk datasets, all datasets must have the same cell types.
    
    Args:
        datasets: one or more SingleCell or Pseudobulk datasets to concatenate
        *more_datasets: additional Pseudobulk datasets to concatenate with this
                        one, specified as positional arguments
        flexible: whether to subset to genes, columns of `obs` and `var`, and
                  (for SingleCell datasets) keys of `obsm`, `varm` and `uns`
                  common to all datasets before concatenating, rather than
                  raising an error on any mismatches

    Returns:
        The concatenated SingleCell or Pseudobulk dataset.
    """
    datasets = to_tuple(datasets) + more_datasets
    check_type(datasets[0], 'the first dataset', (SingleCell, Pseudobulk),
               'a SingleCell or Pseudobulk dataset')
    return datasets[0].concat_obs(datasets[1:], flexible=flexible)


def concat_var(datasets: SingleCell | Iterable[SingleCell] |
                         Pseudobulk | Iterable[Pseudobulk],
               *more_datasets: SingleCell | Pseudobulk,
               flexible: bool = False) -> SingleCell | Pseudobulk:
    """
    Concatenate multiple SingleCell datasets or multiple Pseudobulk datasets,
    gene-wise. This is much less common than the cell- or sample-wise
    concatenation provided by `concat_obs()`.
    
    Delegates to `SingleCell.concat_var()` or `Pseudobulk.concat_var()`,
    depending on whether the datasets are SingleCell or Pseudobulk. For
    Pseudobulk datasets, all datasets must have the same cell types.
    
    Args:
        datasets: one or more SingleCell or Pseudobulk datasets to concatenate
        *more_datasets: additional Pseudobulk datasets to concatenate with this
                        one, specified as positional arguments
        flexible: whether to subset to cells/samples, columns of `obs` and
                  `var`, and (for SingleCell datasets) keys of `obsm`, `varm`
                  and `uns` common to all datasets before concatenating, rather
                  than raising an error on any mismatches

    Returns:
        The concatenated SingleCell or Pseudobulk dataset.
    """
    datasets = to_tuple(datasets) + more_datasets
    check_type(datasets[0], 'the first dataset', (SingleCell, Pseudobulk),
               'a SingleCell or Pseudobulk dataset')
    return datasets[0].concat_var(datasets[1:], flexible=flexible)
