from __future__ import annotations
import os
import re
import signal
from decimal import Decimal
from datetime import date, datetime, time, timedelta
from itertools import chain, islice, pairwise
from packaging import version
from pathlib import Path
from textwrap import fill
from typing import Any, Callable, Dict, ItemsView, Iterable, KeysView, \
    Literal, Mapping, Sequence, ValuesView, Union


class ignore_sigint:
    """
    Ignore Ctrl + C when importing certain modules, to avoid errors due to
    incomplete imports.
    """
    def __enter__(self):
        signal.signal(signal.SIGINT, signal.SIG_IGN)
    
    def __exit__(self, *_):
        signal.signal(signal.SIGINT, signal.default_int_handler)


with ignore_sigint():
    import h5py
    import numpy as np
    import polars as pl
    from scipy.sparse import csr_array, csc_array, csr_matrix, csc_matrix, \
        hstack, vstack
    from scipy.special import stdtrit

import sys
sys.path.append(os.path.expanduser('~wainberg'))
import sparse
from utils import array_equal, bonferroni, bincount, check_bounds, \
    check_dtype, check_R_variable_name, check_type, check_types, \
    cython_inline, cython_type, fdr, filter_columns, generate_palette, \
    getnnz, plural, sparse_matrix_vector_op, Timer, to_tuple, to_tuple_checked

Color = Union[str, float, np.floating,
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer]],
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer], Union[int, np.integer]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating], Union[float, np.floating]]]
Indexer = Union[int, np.integer, str, slice,
                np.ndarray[1, Union[np.dtype[np.integer], np.dtype[np.bool_]]],
                pl.Series, list[Union[int, np.integer, str, bool, np.bool_]]]
Scalar = Union[str, int, float, Decimal, date, time, datetime, timedelta, bool,
               bytes]
NestedScalarOrArrayDict = \
    Dict[str, Union[str, int, np.integer, float, np.floating, bool, np.bool_,
         np.ndarray[Any, Any], 'NestedScalarOrArrayDict']]
SingleCellColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['SingleCell'], Union[pl.Series, np.ndarray]]]
PseudobulkColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['Pseudobulk', str], Union[pl.Series, np.ndarray]]]


_num_threads = 1


def get_num_threads() -> int:
    """
    Get the default number of threads used for SingleCell operations.
    
    Returns:
        The default number of threads (a positive integer).
    """
    return _num_threads


def set_num_threads(num_threads: int | np.integer) -> None:
    """
    Set the default number of threads used for SingleCell operations.
    
    Args:
        num_threads: the new default number of threads. Set `num_threads=-1` to
                     use all available cores (as determined by
                     `os.cpu_count()`).
    """
    global _num_threads
    check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
    if num_threads == -1:
        _num_threads = os.cpu_count()
    elif num_threads >= 1:
        _num_threads = int(num_threads)
    else:
        error_message = (
            f'num_threads is {num_threads}, but must be a positive integer or '
            f'-1')
        raise ValueError(error_message)


class SingleCell:
    """
    A lightweight alternative to AnnData for representing single-cell data.
    
    Has slots for:
    - `X`: a scipy sparse array of counts per cell and gene
    - `obs`: a polars DataFrame of cell metadata
    - `var`: a polars DataFrame of gene metadata
    - `obsm`: a dictionary of NumPy arrays and polars DataFrames of cell
      metadata
    - `varm`: a dictionary of NumPy arrays and polars DataFrames of gene
      metadata
    - `uns`: a dictionary of scalars (strings, numbers or Booleans) or NumPy
      arrays, or nested dictionaries thereof
    as well as `obs_names` and `var_names`, aliases for obs[:, 0] and
    var[:, 0].
    
    Why is X a sparse array rather than matrix? Aside from being more modern
    and consistent with np.array, it's also faster, as explained at
    github.com/scipy/scipy/blob/2aee5efcbe3720f41fe55f336f492ae0acbecdee/scipy/
    sparse/_base.py#L1333-L1337.
    """
    # noinspection PyUnresolvedReferences
    def __init__(self,
                 X: csr_array | csc_array | csr_matrix | csc_matrix |
                    'AnnData' | str | Path,
                 obs: pl.DataFrame | None = None,
                 var: pl.DataFrame | None = None,
                 obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       None = None,
                 varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       None = None,
                 uns: NestedScalarOrArrayDict | None = None,
                 *,
                 X_key: str | None = None,
                 assay: str | None = None,
                 obs_columns: str | Iterable[str] = None,
                 var_columns: str | Iterable[str] = None,
                 num_threads: int | np.integer | None = None) -> None:
        """
        Load a SingleCell dataset from a file, or create one from an in-memory
        AnnData object or count matrix + metadata.
        
        The supported file types are:
        - AnnData (.h5ad)
        - 10x (.h5 or .mtx)
        - Seurat (.rds) - requires the ryp Python-R bridge
        - SingleCellExperiment (.rds) - requires the ryp Python-R bridge
        
        By default, when an AnnData file, AnnData object, Seurat file, or
        SingleCellExperiment file contains both raw and normalized counts, only
        the raw counts will be loaded. To load normalized counts instead, use
        the `X_key` argument.
        
        To create a SingleCell dataset from an in-memory Seurat or
        SingleCellExperiment object, use `SingleCell.from_seurat()` or
        `SingleCell.from_sce()`.
        
        Args:
            X: the data as a sparse array or matrix (with rows = cells,
               columns = genes), AnnData object, AnnData .h5ad file, 10x .h5
               or .mtx.gz file, or Seurat or SingleCellExperiment .rds file. If
               `X` is a 10x .mtx.gz file, barcodes.tsv.gz and features.tsv.gz
               are assumed to be in the same directory, unless custom paths to
               these files are specified via the `obs` and/or `var` arguments.
            obs: a polars DataFrame of metadata for each cell (row of X), or
                 if X is a 10x .mtx.gz file, an optional filename for
                 cell-level metadata (which is otherwise assumed to be at
                 barcodes.tsv.gz in the same directory as the .mtx.gz file)
            var: a polars DataFrame of metadata for each gene (column of X), or
                 if X is a 10x .mtx.gz file, an optional filename for
                 gene-level metadata (which is otherwise assumed to be at
                 features.tsv.gz in the same directory as the .mtx.gz file)
            obsm: a dictionary of NumPy arrays and polars DataFrames of
                  metadata for each cell. Keys must be strings.
            varm: a dictionary of NumPy arrays and polars DataFrames of
                  metadata for each gene. Keys must be strings.
            uns: a dictionary of unstructured metadata. Keys must be strings;
                 values can be scalars (strings, numbers or Booleans), NumPy
                 arrays, or nested dictionaries thereof.
            X_key: when X is an AnnData object or .h5ad or .rds filename, which
                   location within the object or file to use as X.
                   - If X is an AnnData object, the count matrix to use as X.
                     If `None`, defaults to `self.layers['UMIs']` or
                     `self.raw.X` if present, otherwise `self.X`.
                   - If X is an .h5ad filename, the name of the key in the
                     .h5ad file to use as X. If `None`, defaults to
                     `'layers/UMIs'` or `'raw/X'` if present, otherwise `'X'`.
                     Tip: use `SingleCell.ls(h5ad_file)` to see the structure
                     of an .h5ad file without loading it, to figure out which
                     key to use as `X_key`.
                   - If X is a Seurat .rds filename, the slot within the active
                     assay (or the assay specified by the `assay` argument, if
                     not `None`) to use as X. Set to `'data'` to load the
                     normalized counts, or `'scale.data'` to load the
                     normalized and scaled counts, if available. If `None`,
                     defaults to `'counts'`.
                   - If X is a SingleCellExperiment .rds filename, the element
                     within `sce_object@assays@data` to use as `X`. Set to
                     `'logcounts'` to load the normalized counts, if available.
                     If `None`, defaults to `'counts'`.
            assay: if X is a Seurat .rds filename, the name of the assay within
                   the Seurat object to load data from. Defaults to
                   `seurat_object@active_assay` (usually `'RNA'`).
            obs_columns: if X is an .h5ad filename, the columns of obs to load.
                         If not specified, load all columns. Specifying only a
                         subset of columns can speed up reading. Not supported
                         for .h5 files, since they only have a single obs
                         column (`'barcodes'`), nor for Seurat and
                         SingleCellExperiment .rds files, since .rds files do
                         not support partial loading.
            var_columns: if X is an .h5ad or .h5 filename, the columns of var
                         to load. If not specified, load all columns.
                         Specifying only a subset of columns can speed up
                         reading. Not supported for Seurat and
                         SingleCellExperiment .rds files, since .rds files do
                         not support partial loading.
            num_threads: the number of threads to use when reading .h5ad and
                         .h5 files. Set `num_threads=-1` to use all available
                         cores (as determined by `os.cpu_count()`), or leave
                         unset to use `single_cell.get_num_threads()` cores.
        
        Note:
            Both ordered and unordered categorical columns of obs and var will
            be loaded as polars Enums rather than polars Categoricals. This is
            because polars Categoricals use a shared numerical encoding across
            columns, so their codes are not [0, 1, 2, ...] like pandas
            categoricals and polars Enums are. Using Categoricals leads to a
            large overhead (~25%) when loading obs from an .h5ad file, for
            example.
        
        Note:
            SingleCell does not support dense matrices, which are highly
            memory-inefficient for single-cell data. Passing a NumPy array as
            the `X` argument will give an error; if for some reason your data
            has been improperly stored as a dense matrix, convert it to a
            sparse matrix first with `csr_matrix(numpy_array)`). However, when
            loading from disk or converting from other formats, dense matrices
            will be automatically converted to sparse matrices, to avoid giving
            an error when loading or converting.
        """
        type_string = str(type(X))
        is_anndata = type_string.startswith("<class 'anndata")
        if is_anndata:
            with ignore_sigint():
                from anndata import AnnData
            if not isinstance(X, AnnData):
                is_anndata = False
        is_filename = isinstance(X, (str, Path))
        if is_filename:
            X = str(X)
            is_h5ad = X.endswith('.h5ad')
            is_h5 = X.endswith('.h5')
            is_hdf5 = is_h5ad or is_h5
        else:
            is_h5ad = False
            is_hdf5 = False
        if is_h5ad:
            if obs_columns is not None:
                obs_columns = to_tuple_checked(obs_columns, 'obs_columns', str,
                                               'strings')
        else:
            if obs_columns is not None:
                error_message = (
                    'obs_columns can only be specified when loading an .h5ad '
                    'file')
                raise ValueError(error_message)
        if is_hdf5:
            if var_columns is not None:
                var_columns = to_tuple_checked(var_columns, 'var_columns', str,
                                               'strings')
            num_threads = SingleCell._process_num_threads(num_threads)
        else:
            if var_columns is not None:
                error_message = (
                    'var_columns can only be specified when loading an .h5ad '
                    'or .h5 file')
                raise ValueError(error_message)
            if num_threads is not None:
                error_message = (
                    'num_threads can only be specified when loading an .h5ad '
                    'or .h5 file')
                raise ValueError(error_message)
        if isinstance(X, (csr_array, csc_array, csr_matrix, csc_matrix,
                          sparse.csr_array, sparse.csc_array)):
            for prop, prop_name in (X_key, 'X_key'), (assay, 'assay'):
                if prop is not None:
                    error_message = (
                        f'when X is a sparse array or matrix, {prop_name} '
                        f'must be None')
                    raise ValueError(error_message)
            check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
            check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
            obsm = {} if obsm is None else obsm.copy()
            varm = {} if varm is None else varm.copy()
            uns = {} if uns is None else uns.copy()
            for field, field_name in (obsm, 'obsm'), (varm, 'varm'):
                for key, value in field.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {field_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
                    if isinstance(value, np.ndarray):
                        if value.ndim != 2:
                            error_message = (
                                f'all values of {field_name} must be 2D NumPy '
                                f'arrays or polars DataFrames, but '
                                f'{field_name}[{key!r}] is a {value.ndim:,}D '
                                f'NumPy array')
                            raise ValueError(error_message)
                    elif not isinstance(value, pl.DataFrame):
                        error_message = (
                            f'all values of {field_name} must be NumPy '
                            f'arrays or polars DataFrames, but {field_name}'
                            f'[{key!r}] has type {type(value).__name__!r}')
                        raise TypeError(error_message)
                valid_uns_types = str, int, np.integer, float, np.floating, \
                    bool, np.bool_, np.ndarray
                for description, value in SingleCell._iter_uns(uns):
                    if not isinstance(value, valid_uns_types):
                        error_message = (
                            f'all values of uns must be scalars (strings, '
                            f'numbers or Booleans) or NumPy arrays, or nested '
                            f'dictionaries thereof, but {description} has '
                            f'type {type(value).__name__!r}')
                        raise TypeError(error_message)
            if isinstance(X, (sparse.csr_array, sparse.csc_array)):
                pass
            elif isinstance(X, (csr_array, csr_matrix)):
                X = sparse.csr_array(X)
            elif isinstance(X, (csc_array, csc_matrix)):
                X = sparse.csc_array(X)
            self._X = X
            self._obs = obs
            self._var = var
            self._obsm = obsm
            self._varm = varm
            self._uns = uns
        elif is_filename:
            filename = os.path.expanduser(X)
            # noinspection PyUnboundLocalVariable
            if is_h5ad:
                if not os.path.exists(filename):
                    error_message = f'.h5ad file {X!r} does not exist'
                    raise FileNotFoundError(error_message)
                for prop, prop_name in (obs, 'obs'), (var, 'var'), \
                        (obsm, 'obsm'), (varm, 'varm'), (uns, 'uns'), \
                        (assay, 'assay'):
                    if prop is not None:
                        error_message = (
                            f'when loading an .h5ad file, {prop_name} must be '
                            f'None')
                        raise ValueError(error_message)
                # See anndata.readthedocs.io/en/latest/fileformat-prose.html
                # for the AnnData on-disk format specification
                with h5py.File(filename) as h5ad_file:
                    # Load obs and var
                    self._obs = SingleCell._read_h5ad_dataframe(
                        h5ad_file, 'obs', columns=obs_columns,
                        num_threads=num_threads)
                    self._var = SingleCell._read_h5ad_dataframe(
                        h5ad_file, 'var', columns=var_columns,
                        num_threads=num_threads)
                    # Load obsm
                    if 'obsm' in h5ad_file:
                        obsm = h5ad_file['obsm']
                        self._obsm = {
                            key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     h5ad_file, f'obsm/{key}',
                                     num_threads=num_threads)
                            for key, value in obsm.items()}
                    else:
                        self._obsm = {}
                    # Load varm
                    if 'varm' in h5ad_file:
                        varm = h5ad_file['varm']
                        self._varm = {
                            key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     h5ad_file, f'varm/{key}',
                                     num_threads=num_threads)
                            for key, value in varm.items()}
                    else:
                        self._varm = {}
                    # Load uns
                    if 'uns' in h5ad_file:
                        self._uns = SingleCell._read_uns(h5ad_file['uns'])
                    else:
                        self._uns = {}
                    # Load X
                    if X_key is None:
                        has_layers_UMIs = 'layers/UMIs' in h5ad_file
                        has_raw_X = 'raw/X' in h5ad_file
                        if has_layers_UMIs and has_raw_X:
                            error_message = (
                                "both layers['UMIs'] and raw.X are present; "
                                "this should never happen in well-formed "
                                "AnnData files")
                            raise ValueError(error_message)
                        X_key = 'layers/UMIs' if has_layers_UMIs else \
                            'raw/X' if has_raw_X else 'X'
                    else:
                        check_type(X_key, 'X_key', str, 'a string')
                        if X_key not in h5ad_file:
                            error_message = (
                                f'X_key {X_key!r} is not present in the .h5ad '
                                f'file')
                            raise ValueError(error_message)
                    X = h5ad_file[X_key]
                    matrix_class = X.attrs['encoding-type'] \
                        if 'encoding-type' in X.attrs else \
                        X.attrs['h5sparse_format'] + '_matrix'
                    if matrix_class == 'csr_matrix':
                        array_class = sparse.csr_array
                    elif matrix_class == 'csc_matrix':
                        array_class = sparse.csc_array
                    else:
                        error_message = (
                            f"X has unsupported encoding-type "
                            f"{matrix_class!r}, but should be 'csr_matrix' or "
                            f"'csc_matrix' (csr is preferable for speed)")
                        raise ValueError(error_message)
                    self._X = array_class((
                        self._read_dataset(X['data'], num_threads),
                        self._read_dataset(X['indices'], num_threads),
                        self._read_dataset(X['indptr'], num_threads)),
                        shape=X.attrs['shape'] if 'shape' in X.attrs else
                              X.attrs['h5sparse_shape'])
            else:
                # noinspection PyUnboundLocalVariable
                if is_h5:
                    if not os.path.exists(filename):
                        error_message = f'.h5 file {X!r} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (obs, 'obs'), (var, 'var'), \
                            (obsm, 'obsm'), (varm, 'varm'), (uns, 'uns'), \
                            (X_key, 'X_key'), (assay, 'assay'):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5 file, {prop_name} must '
                                f'be None')
                            raise ValueError(error_message)
                    with h5py.File(filename) as h5_file:
                        matrix = h5_file['matrix']
                        features = matrix['features']
                        self._obs = pl.Series('barcodes',
                                              matrix['barcodes'][:])\
                            .cast(pl.String)\
                            .to_frame()
                        var_columns = \
                            ['name', 'id', 'feature_type', 'genome'] + \
                            [column for column in
                             ('pattern', 'read', 'sequence')
                             if column in features]
                        self._var = pl.DataFrame([
                            pl.Series(column, features[column][:])
                            .cast(pl.String) for column in var_columns])
                        self._X = sparse.csr_array((
                            self._read_dataset(matrix['data'], num_threads),
                            self._read_dataset(matrix['indices'], num_threads),
                            self._read_dataset(matrix['indptr'], num_threads)),
                            shape=matrix['shape'][:][::-1])
                        self._obsm = {}
                        self._varm = {}
                        self._uns = {}
                elif X.endswith('.mtx.gz'):
                    if not os.path.exists(filename):
                        error_message = f'10x file {X} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name, prop_description in (
                            (obs, 'obs',
                             'a barcodes.tsv.gz file of cell-level metadata'),
                            (var, 'var',
                             'a features.tsv.gz file of gene-level metadata')):
                        if prop is not None and not \
                                isinstance(prop, (str, Path)):
                            error_message = (
                                f'when loading a 10x .mtx.gz file, '
                                f'{prop_name} must be None or the path to '
                                f'{prop_description}')
                            raise TypeError(error_message)
                    for prop, prop_name in \
                            (obsm, 'obsm'), (varm, 'varm'), (uns, 'uns'), \
                            (X_key, 'X_key'), (assay, 'assay'):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5ad file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    from scipy.io import mmread
                    self._X = sparse.csr_array(mmread(X).T.tocsr())
                    self._obs = pl.read_csv(
                        f'{os.path.dirname(X)}/barcodes.tsv.gz'
                        if obs is None else obs,
                        has_header=False, new_columns=['cell'])
                    self._var = pl.read_csv(
                        f'{os.path.dirname(X)}/features.tsv.gz'
                        if var is None else var,
                        has_header=False, new_columns=['gene'])
                    self._obsm = {}
                    self._varm = {}
                    self._uns = {}
                elif X.endswith('.rds'):
                    if not os.path.exists(filename):
                        error_message = f'.rds file {X} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (obs, 'obs'), (var, 'var'), \
                            (obsm, 'obsm'), (varm, 'varm'), (uns, 'uns'):
                        if prop is not None:
                            error_message = (
                                f'when loading an .rds file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    from ryp import r, to_py, to_r
                    r(f'.SingleCell.object = readRDS({X!r})')
                    try:
                        if X_key is None:
                            X_key = 'counts'
                        classes = to_py('class(.SingleCell.object)',
                                        squeeze=False)
                        if len(classes) == 1:
                            if classes[0] == 'Seurat':
                                r('suppressPackageStartupMessages('
                                  'library(SeuratObject))')
                                self._X, self._obs, self._var, self._obsm, \
                                    self._uns = SingleCell._from_seurat(
                                        '.SingleCell.object', assay=assay,
                                        slot=X_key, constructor=True)
                                self._varm = {}
                            elif classes[0] == 'SingleCellExperiment':
                                if assay is not None:
                                    error_message = (
                                        f'when loading a SingleCellExperiment '
                                        f'.rds file, assay must be None')
                                    raise ValueError(error_message)
                                r('suppressPackageStartupMessages('
                                  'library(SingleCellExperiment))')
                                self._X, self._obs, self._var, self._obsm, \
                                    self._uns = SingleCell._from_sce(
                                        '.SingleCell.object', slot=X_key,
                                        constructor=True)
                                self._varm = {}
                            else:
                                error_message = (
                                    f'the R object loaded from {X} must be a '
                                    f'Seurat or SingleCellExperiment object, '
                                    f'but has class {classes[0]!r}')
                                raise TypeError(error_message)
                        elif len(classes) == 0:
                            error_message = (
                                f'the R object loaded from {X} must be a '
                                f'Seurat or SingleCellExperiment object, but '
                                f'has no class')
                            raise TypeError(error_message)
                        else:
                            classes_string = \
                                ', '.join(f'{c!r}' for c in classes[:-1])
                            error_message = (
                                f'the R object loaded from {X} must be a '
                                f'Seurat object, but has classes '
                                f'{classes_string} and {classes[-1]!r}')
                            raise TypeError(error_message)
                    finally:
                        r('rm(.SingleCell.object)')
                else:
                    error_message = (
                        f'X is a filename with unknown extension '
                        f'{".".join(X.split(".")[1:])}; it must be .h5ad '
                        f'(AnnData), .rds (Seurat) or .mtx.gz (10x)')
                    raise ValueError(error_message)
                if obs_columns is not None:
                    self._obs = self._obs.select(obs_columns)
                if var_columns is not None:
                    self._var = self._var.select(var_columns)
        elif is_anndata:
            for prop, prop_name in (obs, 'obs'), (var, 'var'), \
                    (obsm, 'obsm'), (varm, 'varm'), (uns, 'uns'), \
                    (assay, 'assay'):
                if prop is not None:
                    error_message = (
                        f'when initializing a SingleCell dataset from an '
                        f'AnnData object, {prop_name} must be None')
                    raise ValueError(error_message)
            if X_key is None:
                has_layers_UMIs = 'UMIs' in X._layers
                has_raw_X = hasattr(X._raw, '_X')
                if has_layers_UMIs and has_raw_X:
                    error_message = (
                        "both layers['UMIs'] and raw.X are present; this "
                        "should never happen in well-formed AnnData objects")
                    raise ValueError(error_message)
                counts = X._layers['UMIs'] if has_layers_UMIs else \
                    X._raw._X if has_raw_X else X._X
                if not isinstance(counts, (
                        csr_array, csc_array, csr_matrix, csc_matrix)):
                    error_message = (
                        f'to initialize a SingleCell dataset from an AnnData '
                        f'object, its X must be a csr_array, csc_array, '
                        f'csr_matrix, or csc_matrix, but it has type '
                        f'{type(counts).__name__!r}. Either convert X to a '
                        f'csr_array or csc_array, or specify a custom X via '
                        f'the X_key argument')
                    raise TypeError(error_message)
            else:
                check_type(X_key, 'X_key',
                           (csr_array, csc_array, csr_matrix, csc_matrix),
                           'a csr_array, csc_array, csr_matrix, or csc_matrix')
                counts = X_key
            if isinstance(counts, (sparse.csr_array, sparse.csc_array)):
                pass
            elif isinstance(counts, (csr_array, csr_matrix)):
                counts = sparse.csr_array(counts)
            elif isinstance(counts, (csc_array, csc_matrix)):
                counts = sparse.csc_array(counts)
            self._X = counts
            for attr in '_obs', '_var':
                df = getattr(X, attr)
                if df.index.name is None:
                    df = df.rename_axis('_index')  # for consistency with .h5ad
                # Convert Categoricals with string categories to polars Enums,
                # and Categoricals with other types of categories (e.g.
                # integers) to non-categorical columns of the corresponding
                # polars dtype (e.g. pl.Int64) since polars only supports
                # string categories
                schema_overrides = {}
                cast_dict = {}
                for column, dtype in \
                        df.dtypes[df.dtypes == 'category'].items():
                    categories_dtype = dtype.categories.dtype
                    if categories_dtype == object:
                        schema_overrides[column] = pl.Enum(dtype.categories)
                    else:
                        cast_dict[column] = categories_dtype
                setattr(self, attr, pl.from_pandas(
                    df.astype(cast_dict), schema_overrides=schema_overrides,
                    include_index=True))
            self._obsm: dict[str, np.ndarray] = dict(X._obsm)
            self._varm: dict[str, np.ndarray] = dict(X._varm)
            self._uns = dict(X._uns)
        else:
            error_message = (
                f'X must be a csc_array, csr_array, csc_matrix, or '
                f'csr_matrix, an AnnData object, or an .h5ad (AnnData), .rds '
                f'(Seurat) or .mtx.gz (10x) filename, but has type '
                f'{type(X).__name__!r}')
            raise TypeError(error_message)
        # Check dtype of X
        dtype = self._X.dtype
        if dtype != np.int64 and dtype != np.int32 and \
                dtype != np.float64 and dtype != np.float32:
            error_message = (
                f'X must be int32/int64 or float32/float64, but has data type '
                f'{str(dtype)}')
            raise TypeError(error_message)
        # Check that shapes match
        if len(self._X.shape) == 1:
            error_message = 'X is 1D, but must be 2D'
            raise ValueError(error_message)
        num_cells, num_genes = self._X.shape
        if num_cells > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) cells, which is '
                'not currently supported')
            raise ValueError(error_message)
        if num_genes > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) genes, which is '
                'not currently supported')
            raise ValueError(error_message)
        if len(self._obs) == 0:
            error_message = 'len(obs) is 0: no cells remain'
            raise ValueError(error_message)
        if len(self._var) == 0:
            error_message = 'len(var) is 0: no genes remain'
            raise ValueError(error_message)
        if len(self._obs) != num_cells:
            error_message = (
                f'len(obs) is {len(self._obs):,}, but X.shape[0] is '
                f'{num_cells:,}')
            raise ValueError(error_message)
        if len(self._var) != num_genes:
            error_message = (
                f'len(var) is {len(self._var):,}, but X.shape[1] is '
                f'{num_genes:,}')
            raise ValueError(error_message)
        for key, value in self._obsm.items():
            if len(value) != num_cells:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_cells:,}')
                raise ValueError(error_message)
        for key, value in self._varm.items():
            if len(value) != num_genes:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_genes:,}')
                raise ValueError(error_message)
        # Set `uns['normalized']` and `uns['QCed']` to `False` if not set yet;
        # if set and not a Boolean, move to `uns['_normalized']`/`uns['_QCed']`
        for key in 'normalized', 'QCed':
            if key in self._uns:
                if not isinstance(self._uns[key], bool):
                    import warnings
                    new_key = f'_{key}'
                    while new_key in self._uns:
                        new_key = f'_{new_key}'
                    warning_message = (
                        f'uns[{key!r}] already exists and is not Boolean; '
                        f'moving it to uns[{new_key!r}]')
                    warnings.warn(warning_message)
                    self._uns[new_key] = self._uns[key]
                    self._uns[key] = False
            else:
                self._uns[key] = False
    
    @property
    def X(self) -> csr_array | csc_array:
        return self._X
    
    @X.setter
    def X(self, X: csr_array | csc_array | csr_matrix | csc_matrix) -> None:
        if isinstance(X, (sparse.csr_array, sparse.csc_array)):
            pass
        elif isinstance(X, (csr_array, csr_matrix)):
            X = sparse.csr_array(X)
        elif isinstance(X, (csc_array, csc_matrix)):
            X = sparse.csc_array(X)
        else:
            error_message = (
                f'new X must be a csr_array, csc_array, csr_matrix, or '
                f'csc_matrix, but has type {type(X).__name__!r}')
            raise TypeError(error_message)
        if X.shape != self._X.shape:
            error_message = (
                f'new X is {X.shape[0]:,} × {X.shape[1]:,}, but old X is '
                f'{self._X.shape[0]:,} × {self._X.shape[1]:,}')
            raise ValueError(error_message)
        dtype = self._X.dtype
        if dtype != np.int64 and dtype != np.int32 and \
                dtype != np.float64 and dtype != np.float32:
            error_message = (
                f'new X must be int32/int64 or float32/float64, but has data '
                f'type {str(dtype)}')
            raise TypeError(error_message)
        self._X = X
    
    @property
    def obs(self) -> pl.DataFrame:
        return self._obs
    
    @obs.setter
    def obs(self, obs: pl.DataFrame) -> None:
        check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
        if len(obs) != len(self._obs):
            error_message = (
                f'new obs has length {len(obs):,}, but old obs has length '
                f'{len(self._obs):,}')
            raise ValueError(error_message)
        self._obs = obs

    @property
    def var(self) -> pl.DataFrame:
        return self._var
    
    @var.setter
    def var(self, var: pl.DataFrame) -> None:
        check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
        if len(var) != len(self._var):
            error_message = (
                f'new var has length {len(var):,}, but old var has length '
                f'{len(self._var):,}')
            raise ValueError(error_message)
        self._var = var
    
    @property
    def obsm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._obsm
    
    @obsm.setter
    def obsm(self, obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_cells = len(self)
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but new obsm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(obsm) != num_cells:
                error_message = (
                    f'the length of new obsm[{key!r}] is {len(value):,}, but '
                    f'X.shape[0] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._obsm = obsm.copy()
    
    @property
    def varm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._varm
    
    @varm.setter
    def varm(self, varm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_cells = len(self)
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but new varm '
                    f'contains a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(varm) != num_cells:
                error_message = (
                    f'the length of new varm[{key!r}] is {len(value):,}, but '
                    f'X.shape[0] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._varm = varm.copy()
    
    @property
    def uns(self) -> NestedScalarOrArrayDict:
        return self._uns
    
    @uns.setter
    def uns(self, uns: NestedScalarOrArrayDict) -> None:
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        self._uns = uns.copy()
    
    @staticmethod
    def _iter_uns(uns: NestedScalarOrArrayDict, *, prefix: str = 'uns') -> \
            Iterable[tuple[str, str | int | float | np.integer | np.floating |
                                bool | np.bool_ | np.ndarray[Any, Any]]]:
        """
        Recurse through uns, yielding tuples of a string describing each key
        (e.g. "uns['a']['b']") and the corresponding value.
        
        Args:
            uns: an uns dictionary

        Yields:
            Length-2 tuples where the first element is a string describing each
            key, and the second element is the corresponding value.
        """
        for key, value in uns.items():
            key = f'{prefix}[{key!r}]'
            if isinstance(value, dict):
                SingleCell._iter_uns(value, prefix=key)
            else:
                yield key, value
    
    @staticmethod
    def _read_uns(uns_group: h5py.Group) -> NestedScalarOrArrayDict:
        """
        Recursively load uns from an .h5ad file.
        
        Args:
            uns_group: uns as an h5py.Group

        Returns:
            The loaded uns.
        """
        return {key: SingleCell._read_uns(value)
                     if isinstance(value, h5py.Group) else
                     (pl.Series(value[:]).cast(pl.String).to_numpy()
                      if value.shape else value[()].decode('utf-8'))
                     if value.dtype == object else
                     (value[:] if value.shape else value[()].item())
                for key, value in uns_group.items()}
    
    @staticmethod
    def _save_uns(uns: NestedScalarOrArrayDict,
                  uns_group: h5py.Group,
                  h5ad_file: h5py.File) -> None:
        """
        Recursively save uns to an .h5ad file.
        
        Args:
            uns: an uns dictionary
            uns_group: uns as an h5py.Group
            h5ad_file: an `h5py.File` open in write mode
        """
        uns_group.attrs['encoding-type'] = 'dict'
        uns_group.attrs['encoding-version'] = '0.1.0'
        for key, value in uns.items():
            if isinstance(value, dict):
                SingleCell._save_uns(value, uns_group.create_group(key),
                                     h5ad_file)
            else:
                dataset = uns_group.create_dataset(key, data=value)
                dataset.attrs['encoding-type'] = \
                    ('string-array' if value.dtype == object else 'array') \
                    if isinstance(value, np.ndarray) else \
                    'string' if isinstance(value, str) else 'numeric-scalar'
                dataset.attrs['encoding-version'] = '0.2.0'
    
    @property
    def obs_names(self) -> pl.Series:
        return self._obs[:, 0]
    
    @property
    def var_names(self) -> pl.Series:
        return self._var[:, 0]
    
    def set_obs_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of obs, i.e. the obs_names.
        
        Args:
            column: the column name in obs; must be String, Enum, or Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of obs.
            If `column` is already the first column, return this dataset
            unchanged.
        """
        obs = self._obs
        check_type(column, 'column', str, 'a string')
        if column == obs.columns[0]:
            return self
        if column not in obs:
            error_message = f'{column!r} is not a column of obs'
            raise ValueError(error_message)
        check_dtype(obs[column], f'obs[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=obs.select(column, pl.exclude(column)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def set_var_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of var, i.e. the var_names.
        
        Args:
            column: the column name in var; must be String, Enum, or Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of var.
            If `column` is already the first column, return this dataset
            unchanged.
        """
        var = self._var
        check_type(column, 'column', str, 'a string')
        if column == var.columns[0]:
            return self
        if column not in var:
            error_message = f'{column!r} is not a column of var'
            raise ValueError(error_message)
        check_dtype(self._var[column], f'var[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=var.select(column, pl.exclude(column)),
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    @staticmethod
    def _read_datasets(datasets: Sequence[h5py.Dataset],
                       num_threads: int | np.integer) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Read a sequence of HDF5 datasets into a dictionary of 1D NumPy arrays.
        Assume all are from the same file (this is not checked).
        
        Args:
            datasets: a sequence of `h5py.Dataset` objects to read
            num_threads: the number of threads to use when reading; if >1,
                         spawn multiple processes and read into shared memory

        Returns:
            A dictionary of NumPy arrays with the contents of the datasets; the
            keys are taken from each dataset's `name` attribute.
        """
        if len(datasets) == 0:
            return {}
        import multiprocessing
        # noinspection PyUnresolvedReferences
        from multiprocessing.sharedctypes import _new_value
        import resource
        
        # Increase the maximum number of possible file descriptors this process
        # can use, since dataframes with hundreds of columns can easily exhaust
        # the common default limit of 1024 file descriptors (`ulimit -n`)
        
        soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft_limit < hard_limit:
            resource.setrlimit(resource.RLIMIT_NOFILE,
                               (hard_limit, hard_limit))
        
        # Allocate shared memory for each dataset. Use _new_value() instead of
        # multiprocessing.Array() to avoid the memset at github.com/python/
        # cpython/blob/main/Lib/multiprocessing/sharedctypes.py#L62.
        
        # noinspection PyTypeChecker
        buffers = {dataset.name: _new_value(
            len(dataset) * np.ctypeslib.as_ctypes_type(dataset.dtype))
            for dataset in datasets}
        
        # Spawn num_threads processes: the first loads the first len(dataset) /
        # num_threads elements of each dataset, the second loads the next
        # len(dataset) / num_threads, etc. Because the chunks loaded by each
        # process are non-overlapping, there's no need to lock.
        
        filename = datasets[0].file.filename
        
        def read_dataset_chunks(thread_index: int) -> None:
            try:
                with h5py.File(filename) as h5ad_file:
                    for dataset_name, buffer in buffers.items():
                        chunk_size = len(buffer) // num_threads
                        start = thread_index * chunk_size
                        end = len(buffer) \
                            if thread_index == num_threads - 1 else \
                            start + chunk_size
                        chunk = np.s_[start:end]
                        dataset = h5ad_file[dataset_name]
                        dataset.read_direct(np.frombuffer(
                            buffer, dtype=dataset.dtype),
                            source_sel=chunk, dest_sel=chunk)
            except KeyboardInterrupt:
                pass  # do not print KeyboardInterrupt tracebacks, just return
        
        processes = []
        for thread_index in range(num_threads):
            process = multiprocessing.Process(
                target=read_dataset_chunks, args=(thread_index,))
            processes.append(process)
            process.start()
        for process in processes:
            process.join()
        # Wrap the shared memory in NumPy arrays
        arrays = {
            dataset_name: np.frombuffer(buffer, dtype=dataset.dtype)
            for (dataset_name, buffer), dataset in
            zip(buffers.items(), datasets)}
        return arrays
    
    @staticmethod
    def _read_dataset(dataset: h5py.Dataset,
                      num_threads: int | np.integer,
                      preloaded_datasets: dict[str, np.ndarray[1, Any]] |
                                          None = None) -> np.ndarray[1, Any]:
        """
        Read an HDF5 dataset into a 1D NumPy array.
        
        Args:
            dataset: the `h5py.Dataset` to read
            num_threads: the number of threads to use for reading; if >1, spawn
                         multiple processes and read into shared memory, unless
                         - the array is small enough (heuristically, under 10k
                           elements) that it's probably faster to read serially
                         - the array is so small that there's less than one
                           element to read per thread
                         - the array has `dtype=object` (not compatible with
                           shared memory)
            preloaded_datasets: a dictionary of preloaded datasets, or `None`
                                to always load from scratch. If specified and
                                `dataset.name` is in `preloaded_datasets`,
                                just return `preloaded_datasets[dataset.name]`.
        
        Returns:
            A 1D NumPy array with the contents of the dataset.
        """
        if preloaded_datasets is not None and \
                dataset.name in preloaded_datasets:
            return preloaded_datasets[dataset.name]
        dtype = dataset.dtype
        min_size = max(10_000, num_threads)
        if num_threads == 1 or dtype == object or dataset.size < min_size:
            return dataset[:]
        else:
            return SingleCell._read_datasets([dataset], num_threads)\
                [dataset.name]
    
    @staticmethod
    def _preload_datasets(group: h5py.Group,
                          num_threads: int | np.integer = 1) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Given a group from an .h5ad file, preload all datasets inside it,
        except for those where:
        - the array is small enough (heuristically, under 10k elements) that
          it's probably faster to read serially
        - the array is so small that there's less than one element to read per
          thread the array has `dtype=object` (not compatible with shared
          memory)
        
        Args:
            group: an `h5py.Group` to preload
            num_threads: the number of threads to use when preloading; if
                         `num_threads == 1`, do not preload

        Returns:
            A (possibly empty) dictionary of preloaded datasets.
        """
        if num_threads == 1:
            return {}
        datasets = []
        min_size = max(10_000, num_threads)
        group.visititems(
            lambda name, node: datasets.append(node)
            if isinstance(node, h5py.Dataset) and node.dtype != object
            and node.size >= min_size else None)
        return SingleCell._read_datasets(datasets, num_threads)
    
    @staticmethod
    def _read_h5ad_dataframe(h5ad_file: h5py.File,
                             key: str,
                             columns: str | Sequence[str] | None = None,
                             num_threads: int | np.integer = 1) -> \
            pl.DataFrame:
        """
        Load obs or var from an .h5ad file as a polars DataFrame.
        
        Args:
            h5ad_file: an `h5py.File` open in read mode
            key: the key to load as a DataFrame, e.g. `'obs'` or `'var'`
            columns: the column(s) of the DataFrame to load; the index column
                     is always loaded as the first column, regardless of
                     whether it is specified here, and then the remaining
                     columns are loaded in the order specified
            num_threads: the number of threads to use when reading
        
        Returns:
            A polars DataFrame of the data in h5ad_file[key].
        """
        group = h5ad_file[key]
        # Special case: the entire obs or var may rarely be a single NumPy
        # structured array (dtype=void)
        if isinstance(group, h5py.Dataset) and \
                np.issubdtype(group.dtype, np.void):
            data = pl.from_numpy(group[:])
            data = data.with_columns(pl.col(pl.Binary).cast(pl.String))
            return data
        preloaded_datasets = SingleCell._preload_datasets(group, num_threads)
        data = {}
        if columns is None:
            columns = group.attrs['column-order']
        else:
            columns = [column for column in to_tuple(columns)
                       if column != group.attrs['_index']]
            for column in columns:
                if column not in group.attrs['column-order']:
                    error_message = f'{column!r} is not a column of {key}'
                    raise ValueError(error_message)
        for column in chain((group.attrs['_index'],), columns):
            value = group[column]
            encoding_type = value.attrs.get('encoding-type')
            if encoding_type == 'categorical' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'categories' or key == 'codes'
                    for key in value.keys())) or 'categories' in value.attrs:
                # Sometimes, the categories are stored in a different place
                # which is pointed to by value.attrs['categories']
                if 'categories' in value.attrs:
                    category_object = h5ad_file[value.attrs['categories']]
                    category_encoding_type = None
                    # noinspection PyTypeChecker
                    codes = SingleCell._read_dataset(
                        value, num_threads, preloaded_datasets)
                else:
                    category_object = value['categories']
                    category_encoding_type = \
                        category_object.attrs.get('encoding-type')
                    codes = SingleCell._read_dataset(
                        value['codes'], num_threads, preloaded_datasets)
                # Sometimes, the categories are themselves nullable
                # integer or Boolean arrays
                if category_encoding_type == 'nullable-integer' or \
                        category_encoding_type == 'nullable-boolean' or (
                        isinstance(category_object, h5py.Group) and all(
                        key == 'values' or key == 'mask'
                        for key in category_object.keys())):
                    data[column] = pl.Series(SingleCell._read_dataset(
                        category_object['values'], num_threads,
                        preloaded_datasets)[codes])
                    mask = pl.Series(SingleCell._read_dataset(
                        category_object['mask'], num_threads,
                        preloaded_datasets)[codes] | (codes == -1))
                    has_missing = mask.any()
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                    continue
                # noinspection PyTypeChecker
                categories = SingleCell._read_dataset(
                    category_object, num_threads, preloaded_datasets)
                mask = pl.Series(codes == -1)
                has_missing = mask.any()
                # polars does not (as of version 0.20.2) support Categoricals
                # or Enums with non-string categories, so if the categories are
                # not strings, just map the codes to the categories.
                if category_encoding_type == 'array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype != object):
                    data[column] = pl.Series(categories[codes],
                                             nan_to_null=True)
                    if has_missing:
                        # noinspection PyUnresolvedReferences
                        data[column] = data[column].set(mask, None)
                elif category_encoding_type == 'string-array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype == object):
                    if has_missing:
                        codes[mask] = 0
                    data[column] = pl.Series(codes, dtype=pl.UInt32)
                    if has_missing:
                        # noinspection PyUnresolvedReferences
                        data[column] = data[column].set(mask, None)
                    # noinspection PyUnresolvedReferences
                    data[column] = data[column].cast(
                        pl.Enum(pl.Series(categories).cast(pl.String)))
                else:
                    encoding = \
                        f'encoding-type {category_encoding_type!r}' \
                        if category_encoding_type is not None else \
                            'encoding'
                    error_message = (
                        f'{column!r} column of {key!r} is a categorical '
                        f'with unsupported {encoding}')
                    raise ValueError(error_message)
            elif encoding_type == 'nullable-integer' or \
                    encoding_type == 'nullable-boolean' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'values' or key == 'mask' for key in value.keys())):
                values = SingleCell._read_dataset(
                    value['values'], num_threads, preloaded_datasets)
                mask = SingleCell._read_dataset(
                    value['mask'], num_threads, preloaded_datasets)
                data[column] = pl.Series(values).set(pl.Series(mask), None)
            elif encoding_type == 'array' or (
                    isinstance(value, h5py.Dataset) and value.dtype != object):
                data[column] = pl.Series(SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets), nan_to_null=True)
            elif encoding_type == 'string-array' or (
                    isinstance(value, h5py.Dataset) and value.dtype == object):
                data[column] = SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets)
            else:
                encoding = f'encoding-type {encoding_type!r}' \
                    if encoding_type is not None else 'encoding'
                error_message = \
                    f'{column!r} column of {key!r} has unsupported {encoding}'
                raise ValueError(error_message)
        # NumPy doesn't support encoding object-dtyped string arrays as UTF-8,
        # so do the conversion in polars instead
        data = pl.DataFrame(data)\
            .with_columns(pl.col(pl.Binary).cast(pl.String))
        return data
    
    @staticmethod
    def read_obs(h5ad_file: h5py.File | str | Path,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer | None = None) -> pl.DataFrame:
        """
        Load just obs from an .h5ad file as a polars DataFrame.
        
        Args:
            h5ad_file: an .h5ad filename
            columns: the column(s) of obs to load; if `None`, load all columns
            num_threads: the number of threads to use when reading. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
    
        Returns:
            A polars DataFrame of the data in obs.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'obs', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_var(h5ad_file: str | Path,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer | None = None) -> pl.DataFrame:
        """
        Load just var from an .h5ad file as a polars DataFrame.
        
        Args:
            h5ad_file: an .h5ad filename
            columns: the column(s) of var to load; if `None`, load all columns
            num_threads: the number of threads to use when reading. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
    
        Returns:
            A polars DataFrame of the data in var.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'var', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_obsm(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None,
                  num_threads: int | np.integer | None = None) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just obsm from an .h5ad file as a polars DataFrame.
        
        Args:
            h5ad_file: an .h5ad filename
            keys: the keys(s) of obsm to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of obsm; set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use `single_cell.get_num_threads()`
                         cores.
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            obsm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'obsm' in f:
                obsm = f['obsm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'obsm/{key}', num_threads=num_threads)
                            for key, value in obsm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in obsm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of obsm')
                            raise ValueError(error_message)
                    return {key: obsm[key][:]
                                 if isinstance(obsm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'obsm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but obsm is empty'
                    raise ValueError(error_message)
                return {}
         
    @staticmethod
    def read_varm(h5ad_file: str | Path,
                  keys: str | Iterable[str] | None = None,
                  num_threads: int | np.integer | None = None) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just varm from an .h5ad file as a polars DataFrame.
        
        Args:
            h5ad_file: an .h5ad filename
            keys: the keys(s) of varm to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of varm. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use `single_cell.get_num_threads()`
                         cores.
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            varm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        with h5py.File(filename) as f:
            if 'varm' in f:
                varm = f['varm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'varm/{key}', num_threads=num_threads)
                            for key, value in varm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in varm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of varm')
                            raise ValueError(error_message)
                    return {key: varm[key][:]
                                 if isinstance(varm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'varm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but varm is empty'
                    raise ValueError(error_message)
                return {}
    
    @staticmethod
    def read_uns(h5ad_file: str | Path) -> NestedScalarOrArrayDict:
        """
        Load just uns from an .h5ad file as a dictionary.
        
        Args:
            h5ad_file: an .h5ad filename
        
        Returns:
            A dictionary of the data in uns.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        with h5py.File(filename) as f:
            if 'uns' in f:
                return SingleCell._read_uns(f['uns'])
            else:
                return {}
    
    @staticmethod
    def _print_matrix_info(X: h5py.Group | h5py.Dataset, X_name: str) -> None:
        """
        Given a key of an .h5ad file representing a sparse or dense matrix,
        print its shape, data type and (if sparse) numbr of non-zero elements.
        
        Args:
            X: the key in the .h5ad file representing the matrix, as a Group or
               Dataset object
            X_name: the name of the key
        """
        is_sparse = isinstance(X, h5py.Group)
        if is_sparse:
            data = X['data']
            shape = X.attrs['shape'] if 'shape' in X.attrs else \
                X.attrs['h5sparse_shape']
            dtype = str(data.dtype)
            nnz = data.shape[0]
            print(f'{X_name}: {shape[0]:,} × {shape[1]:,} sparse matrix with '
                  f'{nnz:,} non-zero elements, data type {dtype!r}, and '
                  f'first non-zero element = {data[0]:.6g}')
        else:
            shape = X.shape
            dtype = str(X.dtype)
            print(f'{X_name}: {shape[0]:,} × {shape[1]:,} dense matrix with '
                  f'data type {dtype!r} and first element = {X[0, 0]:.6g}')
    
    @staticmethod
    def ls(h5ad_file: str | Path) -> None:
        """
        Print the fields in an .h5ad file. This can be useful e.g. when
        deciding which count matrix to load via the `X_key` argument to
        `SingleCell()`.
        
        Args:
            h5ad_file: an .h5ad filename
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file!r} does not exist'
            raise FileNotFoundError(error_message)
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        attrs = 'obs', 'var', 'obsm', 'varm', 'obsp', 'varp', 'uns'
        with h5py.File(filename) as f:
            # X
            SingleCell._print_matrix_info(f['X'], 'X')
            # layers
            if 'layers' in f:
                layers = f['layers']
                if len(layers) > 0:
                    for layer_name, layer in layers.items():
                        SingleCell._print_matrix_info(
                            layer, f'layers[{layer_name!r}]')
            # obs, var, obsm, varm, obsp, varp, uns
            for attr in attrs:
                if attr in f:
                    entries = f[attr]
                    if (attr == 'obs' or attr == 'var') and \
                            isinstance(entries, h5py.Dataset) and \
                            np.issubdtype(entries.dtype, np.void):
                        entries = entries.dtype.fields
                    if len(entries) > 0:
                        print(fill(f'{attr}: {", ".join(entries)}',
                                   width=terminal_width,
                                   subsequent_indent=' ' * (len(attr) + 2)))
            # raw
            if 'raw' in f:
                raw = f['raw']
                if len(raw) > 0:
                    print('raw:')
                    if 'X' in raw:
                        SingleCell._print_matrix_info(raw['X'], '    X')
                    if 'layers' in raw:
                        layers = raw['layers']
                        if len(layers) > 0:
                            for layer_name, layer in layers.items():
                                SingleCell._print_matrix_info(
                                    layer, f'    layers[{layer_name!r}]')
                    for attr in attrs:
                        if attr in raw:
                            entries = raw[attr]
                            if (attr == 'obs' or attr == 'var') and \
                                    isinstance(entries, h5py.Dataset) and \
                                    np.issubdtype(entries.dtype, np.void):
                                entries = entries.dtype.fields
                            if len(entries) > 0:
                                print(fill(f'    {attr}: {", ".join(entries)}',
                                           width=terminal_width,
                                           subsequent_indent=' ' * (
                                                   len(attr) + 6)))
    
    def __eq__(self, other: SingleCell) -> bool:
        """
        Test for equality with another SingleCell dataset.
        
        Args:
            other: the other SingleCell dataset to test for equality with

        Returns:
            Whether the two SingleCell datasets are identical.
        """
        if not isinstance(other, SingleCell):
            error_message = (
                f'the left-hand operand of `==` is a SingleCell dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return self._obs.equals(other._obs) and \
               self._var.equals(other._var) and \
               self._obsm.keys() == other._obsm.keys() and \
               self._varm.keys() == other._varm.keys() and \
            all(type(other._obsm[key]) is type(value) and
                (array_equal(other._obsm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._obsm[key].equals(value))
                for key, value in self._obsm.items()) and \
            all(type(other._varm[key]) is type(value) and
                (array_equal(other._varm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._varm[key].equals(value))
                for key, value in self._varm.items()) and \
            SingleCell._eq_uns(self._uns, other._uns) and \
            self._X.nnz == other._X.nnz and not (self._X != other._X).nnz
    
    @staticmethod
    def _eq_uns(uns: NestedScalarOrArrayDict,
                other_uns: NestedScalarOrArrayDict,
                different_order_ok: bool = False) -> bool:
        """
        Test whether two uns are equal.
        
        Args:
            uns: an uns
            other_uns: another uns
            different_order_ok: whether to consider `uns` and `other_uns` equal
                                when they have the same keys and values, but in
                                a different order

        Returns:
            Whether `uns` and `other_uns` are equal.
        """
        return set(uns.keys()) == set(other_uns.keys()) \
            if different_order_ok else uns.keys() == other_uns.keys() and all(
            isinstance(value, dict) and isinstance(other_value, dict) and
            SingleCell._eq_uns(value, other_value, different_order_ok) or
            isinstance(value, np.ndarray) and
            isinstance(other_value, np.ndarray) and
            array_equal(value, other_value) or
            not isinstance(other_value, (dict, np.ndarray)) and
            value == other_value
            for key, value, other_value in
            ((key, value, other_uns[key]) for key, value in uns.items()))
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'SingleCell indices must be cells, a length-1 tuple of (cells,), '
            f'or a length-2 tuple of (cells, genes). Cells and genes must '
            f'each be a string or integer; a slice of strings or integers; or '
            f'a list, NumPy array, or polars Series of strings, integers, or '
            f'Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (obs or var)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('_SingleCell_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    @staticmethod
    def _getitem_process(item: Indexer | tuple[Indexer, Indexer], index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to __getitem__().
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (obs or var) to process the element with respect
                to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [SingleCell._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = SingleCell._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                SingleCell._getitem_error(item)
            if isinstance(stop, str):
                stop = SingleCell._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                SingleCell._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                SingleCell._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            if not isinstance(subitem, pl.Series):
                subitem = pl.Series(subitem)
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            if subitem.dtype == pl.String or subitem.dtype == \
                    pl.Categorical or subitem.dtype == pl.Enum:
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_SingleCell_index=pl.int_range(
                              pl.len(), dtype=pl.Int32)),
                          on=df.columns[0], how='left')\
                    ['_SingleCell_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif subitem.dtype.is_integer() or subitem.dtype == pl.Boolean:
                return subitem
            else:
                SingleCell._getitem_error(item)
        else:
            SingleCell._getitem_error(item)
            
    def __getitem__(self, item: Indexer | tuple[Indexer, Indexer]) -> \
            SingleCell:
        """
        Subset to specific cell(s) and/or gene(s).
        
        Index with a tuple of `(cells, genes)`. If `cells` and `genes` are
        integers, arrays/lists/slices of integers, or arrays/lists of Booleans,
        the result will be a SingleCell dataset subset to `X[cells, genes]`,
        `obs[cells]`, `var[genes]`, `obsm[cells]`, and `varm[genes]`. However,
        `cells` and/or `genes` can instead be strings (or arrays or slices of
        strings), in which case they refer to the first column of obs
        (obs_names) and/or var (var_names), respectively.
        
        Examples:
        - Subset to one cell, for all genes:
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416']
          sc[2]
        - Subset to one gene, for all cells:
          sc[:, 'APOE']
          sc[:, 13196]
        - Subset to one cell and one gene:
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416', 'APOE']
          sc[2, 13196]
        - Subset to a range of cells and genes:
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416':
             'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522',
             'APOE':'TREM2']
          sc[2:6, 13196:34268]
        - Subset to specific cells and genes:
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522']]
          sc[:, pl.Series(['APOE', 'TREM2'])]
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522'],
              np.array(['APOE', 'TREM2'])]
        
        Args:
            item: the item to index with
        
        Returns:
            A new SingleCell dataset subset to the specified cells and/or
            genes.
        """
        if not isinstance(item, (int, str, slice, tuple, list,
                                 np.ndarray, pl.Series)):
            error_message = (
                f'SingleCell datasets must be indexed with an integer, '
                f'string, slice, tuple, list, NumPy array, or polars Series, '
                f'but you tried to index with an object of type '
                f'{type(item).__name__!r}')
            raise TypeError(error_message)
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 2:
                self._getitem_error(item)
        else:
            item = item,
        rows = self._getitem_process(item, 0, self._obs)
        rows_are_Series = isinstance(rows, pl.Series)
        if rows_are_Series:
            boolean_Series = rows.dtype == pl.Boolean
            obs = self._obs.filter(rows) if boolean_Series else self._obs[rows]
        else:
            boolean_Series = False
            obs = self._obs[rows]
        if self._obsm:
            rows_NumPy = rows.to_numpy() if rows_are_Series else rows
            obsm = {key: (value.filter(rows) if boolean_Series else
                          value[rows]) if isinstance(value, pl.DataFrame) else
                         value[rows_NumPy]
                    for key, value in self._obsm.items()}
        else:
            obsm = {}
        if len(item) == 1:
            return SingleCell(X=self._X[rows], obs=obs, var=self._var,
                              obsm=obsm, varm=self._varm, uns=self._uns)
        cols = self._getitem_process(item, 1, self._var)
        cols_are_Series = isinstance(cols, pl.Series)
        if cols_are_Series:
            boolean_Series = cols.dtype == pl.Boolean
            var = self._var.filter(cols) if boolean_Series else self._var[cols]
        else:
            boolean_Series = False
            var = self._var[cols]
        if self._varm:
            cols_NumPy = cols.to_numpy() if cols_are_Series else cols
            varm = {key: (value.filter(cols) if boolean_Series else
                          value[cols]) if isinstance(value, pl.DataFrame) else
                         value[cols_NumPy]
                    for key, value in self._varm.items()}
        else:
            varm = {}
        X = self._X[rows, cols] \
            if isinstance(rows, slice) or isinstance(cols, slice) else \
            self._X[np.ix_(rows, cols)]
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, varm=varm,
                          uns=self._uns)
    
    def cell(self,
             cell: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the row of X corresponding to a single cell, based on the cell's
        name in obs_names.
        
        Args:
            cell: the name of the cell in obs_names
            num_threads: the number of threads to use when retrieving the
                         row of X. Must be 1 when X is a CSR array, since there
                         is no benefit to parallelism in that case. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        
        Returns:
            The corresponding row of X, as a dense 1D NumPy array with zeros
            included.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        if num_threads > 1 and isinstance(self._X, csr_array):
            error_message = (
                'num_threads must be 1 when X is a CSR array, since '
                'parallelism does not give any speedup')
            raise ValueError(error_message)
        row_index = SingleCell._getitem_by_string(self._obs, cell)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[[row_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    def gene(self,
             gene: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the column of X corresponding to a single gene, based on the gene's
        name in var_names.
        
        Args:
            gene: the name of the gene in var_names
            num_threads: the number of threads to use when retrieving the
                         row of X. Must be 1 when X is a CSC array, since there
                         is no benefit to parallelism in that case. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        
        Returns:
            The corresponding column of X, as a dense 1D NumPy array with zeros
            included.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        if num_threads > 1 and isinstance(self._X, csc_array):
            error_message = (
                'num_threads must be 1 when X is a CSC array, since '
                'parallelism does not give any speedup')
            raise ValueError(error_message)
        column_index = SingleCell._getitem_by_string(self._var, gene)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[:, [column_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    def __len__(self) -> int:
        """
        Get the number of cells in this SingleCell dataset.
        
        Returns:
            The number of cells.
        """
        return self._X.shape[0]
       
    def __repr__(self) -> str:
        """
        Get a string representation of this SingleCell dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        descr = (
            f'SingleCell dataset with {len(self._obs):,} '
            f'{plural("cell", len(self._obs))} (obs), {len(self._var):,} '
            f'{plural("gene", len(self._var))} (var), and {self._X.nnz:,} '
            f'non-zero {"entries" if self._X.nnz != 1 else "entry"} (X)')
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        for attr in 'obs', 'var', 'obsm', 'varm', 'uns':
            entries = getattr(self, attr).columns \
                if attr == 'obs' or attr == 'var' else getattr(self, attr)
            if len(entries) > 0:
                descr += '\n' + fill(
                    f'    {attr}: {", ".join(entries)}',
                    width=terminal_width,
                    subsequent_indent=' ' * (len(attr) + 6))
        return descr
    
    @property
    def shape(self) -> tuple[int, int]:
        """
        Get the shape of this SingleCell dataset.
        
        Returns:
            A length-2 tuple where the first element is the number of cells,
            and the second is the number of genes.
        """
        return self._X.shape
    
    @staticmethod
    def _process_num_threads(num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        SingleCell function.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use. If `num_threads` is a positive
            integer, return it unchanged. If `num_threads` is `None`, return
            `single_cell.get_num_threads()`. If `num_threads` is -1, return
            `os.cpu_count()`. Otherwise, raise an error.
        """
        if num_threads is None:
            return get_num_threads()
        check_type(num_threads, 'num_threads', int,
                   'a positive integer, -1 or None')
        if num_threads == -1:
            return os.cpu_count()
        elif num_threads <= 0:
            error_message = (
                f'num_threads is {num_threads:,}, but must be a positive '
                f'integer, -1 or None')
            raise ValueError(error_message)
        else:
            return int(num_threads)
    
    @staticmethod
    def _write_h5ad_dataframe(h5ad_file: h5py.File,
                              df: pl.DataFrame,
                              key: str,
                              preserve_strings: bool) -> None:
        """
        Write obs or var to an .h5ad file.
        
        Args:
            h5ad_file: an `h5py.File` open in write mode
            df: the DataFrame to write, e.g. obs or var
            key: the name of the DataFrame, e.g. `'obs'` or `'var'`
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space; if `True`,
                              preserve these columns as string columns
        """
        # Create a group for the data frame and add top-level metadata
        group = h5ad_file.create_group(key)
        group.attrs['_index'] = df.columns[0]
        group.attrs['column-order'] = df.columns[1:]
        group.attrs['encoding-type'] = 'dataframe'
        group.attrs['encoding-version'] = '0.2.0'
        for column in df:
            dtype = column.dtype
            if dtype == pl.String:
                if column.null_count() or \
                        not preserve_strings and column.is_duplicated().any():
                    column = column\
                        .cast(pl.Enum(column.unique(maintain_order=True)
                                      .drop_nulls()))
                    dtype = column.dtype
                else:
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'string-array'
                    dataset.attrs['encoding-version'] = '0.2.0'
                    continue
            if dtype == pl.Enum or dtype == pl.Categorical:
                is_Enum = dtype == pl.Enum
                subgroup = group.create_group(column.name)
                subgroup.attrs['encoding-type'] = 'categorical'
                subgroup.attrs['encoding-version'] = '0.2.0'
                subgroup.attrs['ordered'] = is_Enum
                categories = column.cat.get_categories()
                if not is_Enum:
                    column = column.cast(pl.Enum(categories))
                codes = column.to_physical().fill_null(-1)
                subgroup.create_dataset('codes', data=codes.to_numpy())
                if len(categories) == 0:
                    subgroup.create_dataset('categories', shape=(0,),
                                            dtype=h5py.special_dtype(vlen=str))
                else:
                    subgroup.create_dataset('categories',
                                            data=categories.to_numpy())
            elif dtype.is_float():
                # Nullable floats are not supported, so convert null to NaN
                dataset = group.create_dataset(
                    column.name, data=column.fill_null(np.nan).to_numpy())
                dataset.attrs['encoding-type'] = 'array'
                dataset.attrs['encoding-version'] = '0.2.0'
            elif dtype == pl.Boolean or dtype.is_integer():
                is_boolean = dtype == pl.Boolean
                if column.null_count():
                    # Store as nullable integer/Boolean
                    subgroup = group.create_group(column.name)
                    subgroup.attrs['encoding-type'] = \
                        f'nullable-{"boolean" if is_boolean else "integer"}'
                    subgroup.attrs['encoding-version'] = '0.1.0'
                    subgroup.create_dataset(
                        'values',
                        data=column.fill_null(False if is_boolean else 1)
                        .to_numpy())
                    subgroup.create_dataset(
                        'mask', data=column.is_null().to_numpy())
                else:
                    # Store as regular integer/Boolean
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'array'
                    dataset.attrs['encoding-version'] = '0.2.0'
            else:
                error_message = \
                    f'internal error: unsupported data type {dtype!r}'
                raise TypeError(error_message)
    
    def save(self,
             filename: str | Path,
             *,
             overwrite: bool = False,
             preserve_strings: bool = False,
             sce: bool = False) -> None:
        """
        Save this SingleCell dataset to an AnnData .h5ad file, 10x .h5 or
        .mtx.gz file, or Seurat or SingleCellExperiment .rds file.
        
        Args:
            filename: an AnnData .h5ad file, 10x .h5 or .mtx.gz file, or Seurat
                      or SingleCellExperiment .rds file to save to. File format
                      will be inferred from the extension. If the extension is
                      .rds, the `sce` argument will determine whether to save
                      to a Seurat or SingleCellExperiment object.
                      - When saving to a 10x .h5 file, `obs['barcodes']`,
                        `var['feature_type']`, `var['genome']`, `var['id']`,
                        and `var['name']` must all exist. Only X and these
                        columns will be saved, along with whichever of
                        `var['pattern']`, `var['read']`, and `var['sequence']`
                        exist. All of these columns (if they exist) must be
                        String, Enum, or Categorical.
                      - When saving to a 10x .mtx.gz file, barcodes.tsv.gz and
                        features.tsv.gz will be created in the same directory.
                        Only X, obs and var will be saved.
                      - When saving to a Seurat .rds file, to match the
                        requirements of Seurat objects, the `'X_'` prefix
                        (often used by Scanpy) will be removed from each key of
                        obsm where it is present (e.g. `'X_umap'` will become
                        `'umap'`).
                      - When swaving to a Seurat .rds file, Seurat will add
                       `'orig.ident'`, `'nCount_RNA'` and `'nFeature_RNA'` as
                        gene-level metadata by default; you can disable the
                        calculation of the latter two columns with:
                        
                        ```python
                        from ryp import r
                        r('options(Seurat.object.assay.calcn = FALSE)')
                        ```
                        
                      - When saving to a Seurat .rds file, non-string keys of
                        uns will not be saved.
                      - When saving to a Seurat or SingleCellExperiment .rds
                        file, varm will not be saved.
            overwrite: if `False`, raises an error if (any of) the file(s)
                       exist; if `True`, overwrites them
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space, when saving to
                              AnnData .h5ad or Seurat or SingleCellExperiment
                              .rds; if `True`, preserve these columns as string
                              columns. (Regardless of the value of
                              `preserve_strings`, String columns with `null`
                              values will be encoded as Enums when saving to
                              .h5ad, since the .h5ad format cannot represent
                              them otherwise.)
            sce: if `True` and the extension of filename is .rds, save to a
                 SingleCellExperiment object instead of a Seurat object
        """
        # Check inputs
        check_type(filename, 'filename', (str, Path),
                   'a string or pathlib.Path')
        filename = str(filename)
        is_anndata = filename.endswith('.h5ad')
        is_h5 = filename.endswith('.h5')
        is_mtx = filename.endswith('.mtx.gz')
        is_rds = filename.endswith('.rds')
        if not (is_anndata or is_h5 or is_mtx or is_rds):
            error_message = (
                f"filename {filename!r} does not end with '.h5ad', '.h5', "
                f"'.mtx.gz', or '.rds'")
            raise ValueError(error_message)
        filename_expanduser = os.path.expanduser(filename)
        if not overwrite and os.path.exists(filename_expanduser):
            error_message = (
                f'filename {filename!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        check_type(sce, 'sce', bool, 'Boolean')
        if sce and not is_rds:
            error_message = 'sce can only be True when saving to an .rds file'
            raise ValueError(error_message)
        # Raise an error if obs, var, or DataFrame keys of obsm or varm contain
        # columns with unsupported data types (anything but float, int, String,
        # Categorical, Enum, Boolean)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'saving')
                    raise TypeError(error_message)
        for field, field_name in (self._obsm, 'obsm'), (self._varm, 'varm'):
            for key, value in field.items():
                if not isinstance(value, pl.DataFrame):
                    continue
                for column, dtype in value.schema.items():
                    if dtype.base_type() not in valid_dtypes:
                        error_message = (
                            f'{field}[{key!r}][{column!r}] has the data type '
                            f'{dtype.base_type()!r}, which is not supported '
                            f'when saving')
                        raise TypeError(error_message)
        # Raise an error if obsm, varm or uns contain NumPy arrays with
        # unsupported data types (datetime64, timedelta64, unstructured void).
        # Do not specifically check `dtype=object` to avoid extra overhead.
        for field, field_name in \
                (self._obsm, 'obsm'), (self._varm, 'varm'), (self._uns, 'uns'):
            for key, value in field.items():
                if not isinstance(value, np.ndarray):
                    continue
                if value.dtype.type == np.void and value.dtype.names is None:
                    error_message = (
                        f'{field_name}[{key!r}] is an unstructured void '
                        f'array, which is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.datetime64:
                    error_message = (
                        f'{field_name}[{key!r}] is a datetime64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.timedelta64:
                    error_message = (
                        f'{field_name}[{key!r}] is a timedelta64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
        # Save, depending on the file extension
        if is_anndata:
            try:
                with h5py.File(filename_expanduser, 'w') as h5ad_file:
                    # Add top-level metadata
                    h5ad_file.attrs['encoding-type'] = 'anndata'
                    h5ad_file.attrs['encoding-version'] = '0.1.0'
                    # Save obs and var
                    SingleCell._write_h5ad_dataframe(
                        h5ad_file, self._obs, 'obs', preserve_strings)
                    SingleCell._write_h5ad_dataframe(
                        h5ad_file, self._var, 'var', preserve_strings)
                    # Save obsm
                    if self._obsm:
                        obsm = h5ad_file.create_group('obsm')
                        obsm.attrs['encoding-type'] = 'dict'
                        obsm.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._obsm.items():
                            if isinstance(value, pl.DataFrame):
                                SingleCell._write_h5ad_dataframe(
                                    h5ad_file, value, f'obsm/{key}',
                                    preserve_strings)
                            else:
                                obsm.create_dataset(key, data=value)
                    # Save varm
                    if self._varm:
                        varm = h5ad_file.create_group('varm')
                        varm.attrs['encoding-type'] = 'dict'
                        varm.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._varm.items():
                            if isinstance(value, pl.DataFrame):
                                SingleCell._write_h5ad_dataframe(
                                    h5ad_file, value, f'varm/{key}',
                                    preserve_strings)
                            else:
                                varm.create_dataset(key, data=value)
                    # Save uns
                    if self._uns:
                        SingleCell._save_uns(self._uns,
                                             h5ad_file.create_group('uns'),
                                             h5ad_file)
                    # Save X
                    X = h5ad_file.create_group('X')
                    X.attrs['encoding-type'] = 'csr_matrix' \
                        if isinstance(self._X, csr_array) else 'csc_matrix'
                    X.attrs['encoding-version'] = '0.1.0'
                    X.attrs['shape'] = self._X.shape
                    X.create_dataset('data', data=self._X.data)
                    X.create_dataset('indices', data=self._X.indices)
                    X.create_dataset('indptr', data=self._X.indptr)
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
        elif is_h5:
            obs_columns = ['barcodes']
            var_columns = ['feature_type', 'genome', 'id', 'name']
            for columns, df, df_name in (obs_columns, self._obs, 'obs'), \
                    (var_columns, self._var, 'var'):
                for column in columns:
                    if column not in df:
                        error_message = (
                            f'{column!r} was not found in {df_name}, but is a '
                            f'required column when saving to a 10x .h5 file')
                        raise ValueError(error_message)
                    check_dtype(df[column], f'{df_name}[{column!r}]',
                                (pl.String, pl.Categorical, pl.Enum))
            all_tag_keys = ['genome']
            for column in 'pattern', 'read', 'sequence':
                if column in self._var:
                    check_dtype(self._var[column], f'var[{column!r}]',
                                (pl.String, pl.Categorical, pl.Enum))
                    var_columns.append(column)
                    all_tag_keys.append(column)
            try:
                with h5py.File(filename_expanduser, 'w') as h5_file:
                    matrix = h5_file.create_group('matrix')
                    matrix.create_dataset('barcodes',
                                          data=self._obs[:, 0].to_numpy())
                    matrix.create_dataset('data', data=self._X.data)
                    features = matrix.create_group('features')
                    matrix.create_dataset('indices', data=self._X.indices)
                    matrix.create_dataset('indptr', data=self._X.indptr)
                    matrix.create_dataset('shape', data=self._X.shape[::-1])
                    features.create_dataset('_all_tag_keys', data=all_tag_keys)
                    for column in var_columns:
                        features.create_dataset(
                            column, data=self._var[column].to_numpy())
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
        elif is_mtx:
            from scipy.io import mmwrite
            barcode_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'barcodes.tsv.gz')
            feature_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'features.tsv.gz')
            if not overwrite:
                for ancillary_filename in barcode_filename, feature_filename:
                    if os.path.exists(ancillary_filename):
                        error_message = (
                            f'{ancillary_filename!r} already exists; set '
                            f'overwrite=True to overwrite')
                        raise FileExistsError(error_message)
            try:
                mmwrite(filename_expanduser, self._X.T)
                self._obs.write_csv(barcode_filename, include_header=False)
                self._var.write_csv(feature_filename, include_header=False)
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                if os.path.exists(barcode_filename):
                    os.unlink(barcode_filename)
                if os.path.exists(feature_filename):
                    os.unlink(feature_filename)
                raise
        else:
            from ryp import r
            if preserve_strings:
                sc = self
            else:
                # Convert string columns with duplicate values to Enum
                enumify = lambda df: df.cast({
                    row[0]: pl.Enum(row[1]) for row in df
                    .select(pl.selectors.string()
                            .unique(maintain_order=True)
                            .implode()
                            .list.drop_nulls())
                    .unpivot()
                    .filter(pl.col.value.list.len() == len(df))
                    .rows()})
                sc = SingleCell(X=self._X, obs=enumify(self._obs),
                                var=enumify(self._var), obsm=self._obsm,
                                uns=self._uns)
            if sce:
                sc.to_sce('.SingleCell.object')
            else:
                sc.to_seurat('.SingleCell.object')
            try:
                r(f'saveRDS(.SingleCell.object, {filename_expanduser!r})')
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
            finally:
                r('rm(.SingleCell.object)')
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],
                    column: SingleCellColumn,
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                            ...],
                    *,
                    QC_column: pl.Series | None = None,
                    allow_missing: bool = False,
                    allow_null: bool = False,
                    custom_error: str | None = None) -> pl.Series | None:
        """
        Get a column of the same length as obs/var, or `None` if the column is
        missing from obs/var and `allow_missing=True`.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of obs/var, a polars expression
                    that evaluates to a single column when applied to obs/var,
                    a polars Series or 1D NumPy array of the same length as
                    obs/var, or a function that takes in `self` and returns a
                    polars Series or 1D NumPy array of the same length as
                    obs/var
            variable_name: the name of the variable corresponding to `column`
            dtypes: the required dtype(s) of the column
            QC_column: an optional column of cells passing QC. If specified,
                       the presence of null values will only raise an error for
                       cells passing QC. Has no effect when `allow_null=True`.
            allow_missing: whether to allow `column` to be a string missing
                           from obs/var, returning `None` in this case
            allow_null: whether to allow `column` to contain null values
            custom_error: a custom error message for when `column` is a string
                          and is not found in obs/var, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A polars Series of the same length as obs/var, or `None` if the
            column is missing from obs/var and `allow_missing=True`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if isinstance(column, str):
            if column in obs_or_var:
                column = obs_or_var[column]
            elif allow_missing:
                return None
            else:
                error_message = (
                    f'{variable_name} {column!r} is not a column of '
                    f'{obs_or_var_name}' if custom_error is None else
                    custom_error.format(f'{column!r}'))
                raise ValueError(error_message)
            variable_name = f'{variable_name} {column!r}'
        elif isinstance(column, pl.Expr):
            column = obs_or_var.select(column)
            if column.width > 1:
                error_message = (
                    f'{variable_name} is a polars expression that expands to '
                    f'{column.width:,} columns rather than 1')
                raise ValueError(error_message)
            column = column.to_series()
        elif isinstance(column, pl.Series):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a polars Series of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        elif isinstance(column, np.ndarray):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a NumPy array of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
            column = pl.Series(variable_name, column)
        elif callable(column):
            column = column(self)
            if isinstance(column, np.ndarray):
                if column.ndim != 1:
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'{column.ndim:,}D NumPy array, but must return a '
                        f'polars Series or 1D NumPy array')
                    raise ValueError(error_message)
                column = pl.Series(variable_name, column)
            elif not isinstance(column, pl.Series):
                error_message = (
                    f'{variable_name} is a function that returns a variable '
                    f'of type {type(column).__name__}, but must return a '
                    f'polars Series or 1D NumPy array')
                raise TypeError(error_message)
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a function that returns a column of '
                    f'length {len(column):,}, which differs from the length '
                    f'of {obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression, a polars Series, a 1D NumPy array, or a '
                f'function that returns a polars Series or 1D NumPy array '
                f'when applied to this SingleCell dataset, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        check_dtype(column, variable_name, dtypes)
        if not allow_null:
            if QC_column is None:
                null_count = column.null_count()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)}, but must not '
                        f'contain any')
                    raise ValueError(error_message)
            else:
                null_count = (column.is_null() & QC_column).sum()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cells '
                        f'passing QC, but must not contain any')
                    raise ValueError(error_message)
        return column
    
    @staticmethod
    def _get_columns(obs_or_var_name: Literal['obs', 'var'],
                     datasets: Sequence[SingleCell],
                     columns: SingleCellColumn | None |
                              Sequence[SingleCellColumn | None],
                     variable_name: str,
                     dtypes: pl.datatypes.classes.DataTypeClass | str |
                             tuple[pl.datatypes.classes.DataTypeClass | str,
                                   ...],
                     *,
                     QC_columns: list[pl.Series | None] = None,
                     allow_None: bool = True,
                     allow_missing: bool = False,
                     allow_null: bool = False,
                     custom_error: str | None = None) -> \
            list[pl.Series | None]:
        """
        Get a column of the same length as obs/var from each dataset.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            datasets: a sequence of SingleCell datasets
            columns: a string naming a column of obs/var, a polars expression
                     that evaluates to a single column when applied to obs/var,
                     a polars Series or 1D NumPy array of the same length as
                     obs/var, or a function that takes in `self` and returns a
                     polars Series or 1D NumPy array of the same length as
                     obs/var. Or, a Sequence of these, one per dataset in
                     `datasets`. May also be `None` (or a Sequence containing
                     `None`) if `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the columns
            QC_columns: an optional column of cells passing QC for each
                        dataset. If not `None` for a given dataset, the
                        presence of null values for that dataset will only
                        raise an error for cells passing QC. Has no effect when
                        `allow_null=True`.
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_missing: whether to allow `columns` to be a string (or
                           contain strings) missing from certain datasets'
                           obs/var, returning `None` for these datasets
            allow_null: whether to allow `columns` to contain null values
            custom_error: a custom error message for when `column` is a string
                          and is not found in obs/var, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A list of polars Series of the same length as `datasets`, where
            each Series has the same length as the corresponding dataset's
            obs/var. Or, if `columns` is `None` (or if some elements are
            `None`) or missing from obs/var (when `allow_missing=True`), a list
            of `None` (or where the corresponding elements are `None`).
        """
        if columns is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return [None] * len(datasets)
        if isinstance(columns, Sequence) and not isinstance(columns, str):
            if len(columns) != len(datasets):
                error_message = (
                    f'{variable_name} has length {len(columns):,}, but you '
                    f'specified {len(datasets):,} datasets')
                raise ValueError(error_message)
            if not allow_None and any(column is None for column in columns):
                error_message = \
                    f'{variable_name} contains an element that is None'
                raise TypeError(error_message)
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error)
                    if column is not None else None
                    for dataset, column in zip(datasets, columns)]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    if column is not None else None
                    for dataset, column, QC_column in
                    zip(datasets, columns, QC_columns)]
        else:
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error) for dataset in datasets]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    for dataset, QC_column in zip(datasets, QC_columns)]
    
    @staticmethod
    def _describe_column(column_name: str, column: SingleCellColumn):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
    
        Returns:
            The column's description: just the argument's name unless the value
            is a string (i.e. the column's name in obs or var), in which case
            also include the value.
        """
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
    
    # noinspection PyUnresolvedReferences
    def to_anndata(self, *, QC_column: str | None = 'passed_QC') -> 'AnnData':
        """
        Converts this SingleCell dataset to an AnnData object.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the AnnData object, set `QC_column` to `None`.
        
        Note that there is no `from_anndata()`; simply do
        `SingleCell(anndata_object)` to initialize a SingleCell dataset from an
        in-memory AnnData object.
        
        Args:
            QC_column: if not `None`, give an error if this column is present
                       in obs and not all cells pass QC
        
        Returns:
            An AnnData object. For AnnData versions older than 0.11.0, which
            do not support csr_array/csc_array, counts will be converted to
            csr_matrix/csc_matrix.
        """
        with ignore_sigint():
            import anndata
            import pandas as pd
            import pyarrow as pa
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported by '
                        f'AnnData')
                    raise TypeError(error_message)
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the AnnData object')
                    raise ValueError(error_message)
        type_mapping = {
            pa.int8(): pd.Int8Dtype(), pa.int16(): pd.Int16Dtype(),
            pa.int32(): pd.Int32Dtype(), pa.int64(): pd.Int64Dtype(),
            pa.uint8(): pd.UInt8Dtype(), pa.uint16(): pd.UInt16Dtype(),
            pa.uint32(): pd.UInt32Dtype(), pa.uint64(): pd.UInt64Dtype(),
            pa.string(): pd.StringDtype(storage='pyarrow'),
            pa.bool_(): pd.BooleanDtype()}
        to_pandas = lambda df: df\
            .to_pandas(split_blocks=True, types_mapper=type_mapping.get)\
            .set_index(df.columns[0])
        return anndata.AnnData(
            X=self._X if version.parse(anndata.__version__) >=
                         version.parse('0.11.0') else
              csr_matrix(self._X) if isinstance(self._X, csr_array)
              else csc_matrix(self._X),
            obs=to_pandas(self._obs), var=to_pandas(self._var),
            obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    @staticmethod
    def _from_seurat(seurat_object_name: str,
                     *,
                     assay: str | None,
                     slot: str,
                     constructor: bool) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], NestedScalarOrArrayDict]:
        """
        Create a SingleCell dataset from an in-memory Seurat object loaded with
        the ryp Python-R bridge. Used by `__init__()` and `from_seurat()`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to `seurat_object@active.assay`
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as X. Set to
                  `'data'` to load the normalized counts, or `'scale.data'` to
                  load the normalized and scaled counts, if available. If
                  dense, will be automatically converted to a sparse array.
            constructor: whether this method is being called from the
                         constructor or from `from_seurat()`
        
        Returns:
            A length-5 tuple of (X, obs, var, obsm, uns).
        """
        from ryp import r, to_py
        if assay is None:
            assay = to_py(f'{seurat_object_name}@active.assay')
        elif to_py(f'{seurat_object_name}@{assay}') is None:
            error_message = (
                f'assay {assay!r} does not exist in the Seurat object '
                f'{seurat_object_name!r}')
            raise ValueError(error_message)
        # If Seurat v5, merge layers if necessary, and use $slot
        # instead of @slot for X and meta.data instead of
        # meta.features for var
        if to_py(f'inherits({seurat_object_name}@assays${assay}, "Assay5")'):
            if not to_py(f'"{slot}" %in% names({seurat_object_name}@assays$'
                         f'{assay}@layers)'):
                error_message = (
                    f'slot {slot!r} does not exist in '
                    f'{seurat_object_name}@assays${assay}@layers')
                raise ValueError(error_message)
            if to_py(f'length({seurat_object_name}@assays${assay}@'
                     f'layers)') > 1:
                r(f'{seurat_object_name}@assays${assay} = '
                  f'JoinLayers({seurat_object_name}@assays${assay}, "{slot}")')
            X_slot = f'{seurat_object_name}@assays${assay}${slot}'
            var = to_py(f'{seurat_object_name}@assays${assay}@meta.data')
        else:
            # unlike v5 objects, v3 objects indicate the absence of a slot with
            # a 0 x 0 matrix
            if not (to_py(f'"{slot}" %in% slotNames({seurat_object_name}@'
                         f'assays${assay})') and
                    to_py(f'prod(dim({seurat_object_name}@assays${assay}@'
                          f'{slot}))') > 0):
                error_message = (
                    f'slot {slot!r} does not exist in '
                    f'{seurat_object_name}@assays${assay}')
                raise ValueError(error_message)
            X_slot = f'{seurat_object_name}@assays${assay}@{slot}'
            var = to_py(f'{seurat_object_name}@assays${assay}@meta.features')
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = sparse.csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{X_slot} must be a dgCMatrix (column-oriented sparse '
                f'matrix) or matrix, but has ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; consider setting {"X_key" if constructor else "slot"} to '
                f'a different value than {slot!r}')
            raise TypeError(error_message)
        obs_key = f'{seurat_object_name}@meta.data'
        obs = to_py(obs_key, index='_index' if to_py(
            f'"cell" %in% {obs_key}') else 'cell')
        if var is None:
            var = to_py(f'rownames({seurat_object_name}@assays${assay}@'
                        f'{slot}').to_frame('gene')
        reductions = to_py(f'names({seurat_object_name}@reductions)')
        obsm = {key: to_py(
            f'{seurat_object_name}@reductions${key}@cell.embeddings',
            format='numpy') for key in reductions
            if not to_py(f'is.null({seurat_object_name}@reductions${key})')} \
            if reductions is not None else {}
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        uns = to_py(f'{seurat_object_name}@misc')
        return X, obs, var, obsm, uns
    
    @staticmethod
    def from_seurat(seurat_object_name: str, *,
                    assay: str | None = None,
                    slot: str = 'counts') -> SingleCell:
        """
        Create a SingleCell dataset from a Seurat object that has already been
        loaded into memory via the ryp Python-R bridge. To load a Seurat object
        from disk, use e.g. `SingleCell('filename.rds')`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to seurat_object@active.assay
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as X. Defaults to
                  `'counts'`. Set to `'data'` to load the normalized counts,
                  or `'scale.data'` to load the normalized and scaled counts,
                  if available. If dense, will be automatically converted to a
                  sparse array.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import to_py
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if assay is not None:
            check_type(assay, 'assay', str, 'a string')
        check_type(slot, 'slot', str, 'a string')
        if not to_py(f'inherits({seurat_object_name}, "Seurat")'):
            classes = to_py(f'class({seurat_object_name})', squeeze=False)
            error_message = (
                f'the R object named by seurat_object_name, '
                f'{seurat_object_name}, must be a Seurat object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, uns = \
            SingleCell._from_seurat(seurat_object_name, assay=assay, slot=slot,
                                    constructor=False)
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, uns=uns)
    
    def to_seurat(self,
                  seurat_object_name: str,
                  *,
                  QC_column: str | None = 'passed_QC') -> None:
        """
        Convert this SingleCell dataset to a Seurat object (version 3, not
        version 5) in the R workspace of the ryp Python-R bridge. varm,
        DataFrame keys of obsm, and non-string keys of uns will not be
        converted.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the Seurat object, set `QC_column` to `None`.
        
        When converting to Seurat, to match the requirements of Seurat objects,
        the `'X_'` prefix (often used by Scanpy) will be removed from each key
        of obsm where it is present (e.g. `'X_umap'` will become `'umap'`).
        Seurat will also add `'orig.ident'`, `'nCount_RNA'` and
        `'nFeature_RNA'` as gene-level metadata by default; you can disable
        the calculation of the latter two columns with:
        
        ```python
        from ryp import r
        r('options(Seurat.object.assay.calcn = FALSE)')
        ```
        
        Args:
            seurat_object_name: the name of the R variable to assign the Seurat
                                object to
            QC_column: if not `None`, give an error if this column is present
                       in obs and not all cells pass QC
        """
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SeuratObject))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the Seurat object')
                    raise ValueError(error_message)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a Seurat object')
                    raise TypeError(error_message)
        is_string = self.var_names.dtype == pl.String
        num_with_underscores = self.var_names.str.contains('_').sum() \
            if is_string else \
            self.var_names.cat.get_categories().str.contains('_').sum()
        if num_with_underscores:
            var_names_expression = f'pl.col.{self.var_names.name}' \
                if self.var_names.name.isidentifier() else \
                f'pl.col({self.var_names.name!r})'
            error_message = (
                f"var_names contains {num_with_underscores:,}"
                f"{'' if is_string else ' unique'} gene "
                f"{plural('name', num_with_underscores)} with "
                f"underscores, which are not supported by Seurat; Seurat "
                f"recommends changing the underscores to dashes, which you "
                f"can do with .with_columns_var({var_names_expression}"
                f"{'' if is_string else '.cast(pl.String)'}"
                f".str.replace_all('_', '-'))")
            raise ValueError(error_message)
        try:
            to_r(self._X.T, '.SingleCell.X.T', rownames=self.var_names,
                 colnames=self.obs_names)
            try:
                to_r(self._obs.drop(self.obs_names.name), '.SingleCell.obs',
                     rownames=self.obs_names)
                try:
                    r(f'{seurat_object_name} = CreateSeuratObject('
                      'CreateAssayObject(.SingleCell.X.T), '
                      'meta.data=.SingleCell.obs)')
                    # Reverse the column name-mangling introduced by Seurat
                    r('.SingleCell.cols_to_ignore = '
                      'c("orig.ident", "nCount_RNA", "nFeature_RNA")')
                    try:
                        r(f'names({seurat_object_name}@meta.data)['
                          f'!names({seurat_object_name}@meta.data) %in% '
                          f'.SingleCell.cols_to_ignore] = '
                          f'names(.SingleCell.obs)[!names(.SingleCell.obs) '
                          f'%in% .SingleCell.cols_to_ignore]')
                    finally:
                        r('rm(.SingleCell.cols_to_ignore)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            to_r(self._var.drop(self.var_names.name), '.SingleCell.var',
                 rownames=self.var_names)
            try:
                r(f'{seurat_object_name}@assays$RNA@meta.features = '
                  f'.SingleCell.var')
            finally:
                r('rm(.SingleCell.var)')
            if self._obsm:
                for key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    # Remove the initial X_ from the reduction name and suffix
                    # with `'_'` when creating the key, like
                    # mojaveazure.github.io/seurat-disk/reference/Convert.html
                    key = key.removeprefix('X_')
                    to_r(value, '.SingleCell.value', rownames=self.obs_names,
                         colnames=[f'{key}_{i}'
                                   for i in range(1, value.shape[1] + 1)])
                    try:
                        r(f'{seurat_object_name}@reductions${key} = '
                          f'CreateDimReducObject(.SingleCell.value, '
                          f'key="{key}_", assay="RNA")')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._uns:
                to_r({key: value for key, value in self._uns.items()
                      if isinstance(value, str)}, '.SingleCell.uns')
                try:
                    r(f'{seurat_object_name}@misc = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({seurat_object_name!r})'):
                r(f'rm({seurat_object_name})')
            raise
    
    @staticmethod
    def _get_DFrame(dframe: str, *, index: str) -> pl.DataFrame:
        """
        Convert a DFrame in the ryp R workspace to a Python DataFrame, raising
        an error if the DFrame contains any nested data stuctures.
        
        Args:
            dframe: the name of the DFrame object in the ryp R workspace
            index: the name of the column containing the rownames in the output
                   DataFrame; if this column name is already present in the
                   DFrame, fall back to calling the rownames column `'_index'`

        Returns:
            A polars DataFrame containing the data in `dframe`.
        """
        from ryp import to_py
        df = to_py(f'{dframe}@listData', index=False)
        if not all(isinstance(value, pl.Series) for value in df.values()):
            error_message = (
                f'{dframe} contains nested data; unnest before converting to '
                f'a SingleCell dataset')
            raise ValueError(error_message)
        if index in df.keys():
            index = '_index'
        df = pl.DataFrame({index: to_py(f'{dframe}@rownames')} | df)
        return df
    
    @staticmethod
    def _from_sce(sce_object_name: str,
                  *,
                  slot: str,
                  constructor: bool) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], NestedScalarOrArrayDict]:
        """
        Create a SingleCell dataset from an in-memory SingleCellExperiment
        object loaded with the ryp Python-R bridge. Used by `__init__()` and
        `from_sce()`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `sce_object@assays@data` to use as `X`.
                  Set to `'counts'` to load raw counts, or `'logcounts'` to
                  load the normalized counts if available. If dense, will be
                  automatically converted to a sparse array.
            constructor: whether this method is being called from the
                         constructor or from `from_sce()`
        
        Returns:
            A length-5 tuple of (X, obs, var, obsm, uns).
        """
        from ryp import to_py
        X_slot = f'{sce_object_name}@assays@data${slot}'
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = sparse.csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{X_slot} must be a dgCMatrix (column-oriented sparse '
                f'matrix) or matrix, but has ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; consider setting {"X_key" if constructor else "slot"} to '
                f'a different value than {slot!r}')
            raise TypeError(error_message)
        obs = SingleCell._get_DFrame(f'colData({sce_object_name})',
                                     index='cell')
        var = SingleCell._get_DFrame(f'rowData({sce_object_name})',
                                     index='gene')
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        obsm = to_py(f'reducedDims({sce_object_name})@listData',
                     format='numpy')
        uns = to_py(f'{sce_object_name}@metadata', format='numpy')
        return X, obs, var, obsm, uns
    
    @staticmethod
    def from_sce(sce_object_name: str,
                 *,
                 slot: str = 'counts') -> SingleCell:
        """
        Create a SingleCell dataset from a SingleCellExperiment object that has
        already been loaded into memory via the ryp Python-R bridge. To load a
        SingleCellExperiment object from disk, use e.g.
        `SingleCell('filename.rds')`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `{sce_object_name}@assays@data` to use as
                  `X`. Defaults to `'counts'`. If available, set to
                  `'logcounts'` to load the normalized counts. If dense, will
                  be automatically converted to a sparse array.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import to_py
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        check_type(slot, 'slot', str, 'a string')
        if not to_py(f'inherits({sce_object_name}, "SingleCellExperiment")'):
            classes = to_py(f'class({sce_object_name})', squeeze=False)
            error_message = (
                f'the R object named by sce_object_name, {sce_object_name}, '
                f'must be a SingleCellExperiment object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, uns = \
            SingleCell._from_sce(sce_object_name, slot=slot, constructor=False)
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, uns=uns)
        
    def to_sce(self,
               sce_object_name: str,
               *,
               QC_column: str | None = 'passed_QC') -> None:
        """
        Convert this SingleCell dataset to a SingleCellExperiment object in the
        R workspace of the ryp Python-R bridge. varm and DataFrame keys of obsm
        will not be converted.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the SingleCellExperiment object, set `QC_column` to
        `None`.
        
        Args:
            sce_object_name: the name of the R variable to assign the
                             SingleCellExperiment object to
            QC_column: if not `None`, give an error if this column is present
                       in obs and not all cells pass QC
        """
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SingleCellExperiment))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the SingleCellExperiment object')
                    raise ValueError(error_message)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a SingleCellExperiment object')
                    raise TypeError(error_message)
        try:
            to_r(self._X.T, '.SingleCell.X.T', rownames=self.var_names,
                 colnames=self.obs_names)
            try:
                to_r(self._obs.drop(self.obs_names.name), '.SingleCell.obs',
                     rownames=self.obs_names)
                try:
                    to_r(self._var.drop(self.var_names.name),
                         '.SingleCell.var', rownames=self.var_names)
                    try:
                        r(f'{sce_object_name} = SingleCellExperiment('
                          f'assays = list(counts = .SingleCell.X.T), '
                          f'colData = S4Vectors::DataFrame('
                          f'    .SingleCell.obs, check.names=FALSE), '
                          f'rowData = S4Vectors::DataFrame('
                          f'    .SingleCell.var, check.names=FALSE))')
                    finally:
                        r('rm(.SingleCell.var)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            if self._obsm:
                for key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    to_r(value, '.SingleCell.value', rownames=self.obs_names,
                             colnames=[f'{key}_{i}'
                                       for i in range(1, value.shape[1] + 1)])
                    try:
                        r(f'reducedDim({sce_object_name}, {key!r}) = '
                          f'.SingleCell.value')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._uns:
                to_r(self._uns, '.SingleCell.uns')
                try:
                    r(f'{sce_object_name}@metadata = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({sce_object_name!r})'):
                r(f'rm({sce_object_name})')
            raise
        
    def copy(self, deep: bool = False) -> SingleCell:
        """
        Make a deep (if `deep=True`) or shallow copy of this SingleCell
        dataset.
        
        Returns:
            A copy of the SingleCell dataset. Since polars DataFrames are
            immutable, obs and var will always point to the same underlying
            data as the original. The only difference when deep=True is that X
            will point to a fresh copy of the data, rather than the same data.
            Watch out: when deep=False, any modifications to X will modify both
            copies!
        """
        check_type(deep, 'deep', bool, 'Boolean')
        # noinspection PyTypeChecker
        return SingleCell(X=self._X.copy() if deep else self._X, obs=self._obs,
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def concat_obs(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False) -> SingleCell:
        """
        Concatenate the cells of multiple SingleCell datasets.
        
        By default, all datasets must have the same var, varm and uns. They
        must also have the same columns in obs and the same keys in obsm, with
        the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of var, i.e. the var_names) before
        concatenating. Subset to columns of var and keys of varm and uns that
        are identical in all datasets after this subsetting. Also, subset to
        columns of obs and keys of obsm that are present in all datasets, and
        have the same data types. All datasets' obs_names must have the same
        name and dtype, and similarly for their var_names.
        
        The one exception to the obs "same data type" rule: if a column is Enum
        in some datasets and Categorical in others, or Enum in all datasets but
        with different categories in each dataset, that column will be retained
        as an Enum column (with the union of the categories) in the
        concatenated obs.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes, columns of obs and var, and
                      keys of obsm, varm and uns common to all datasets before
                      concatenating, rather than raising an error on any
                      mismatches
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        if isinstance(datasets, SingleCell):
            datasets = datasets,
        datasets = (self,) + datasets + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that obs_names and var_names have the same name and data
            # type across all datasets
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            # Subset to genes in common across all datasets
            genes_in_common = self.var_names\
                .filter(self.var_names
                        .is_in(pl.concat([dataset.var_names
                                          for dataset in datasets[1:]])))
            if len(genes_in_common) == 0:
                error_message = \
                    'no genes are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[:, genes_in_common] for dataset in datasets]
            # Subset to columns of var and keys of varm and uns that are
            # identical in all datasets after this subsetting
            var_columns_in_common = [
                column.name for column in datasets[0]._var[:, 1:]
                if all(column.name in dataset._var and
                       dataset._var[column.name].equals(column)
                       for dataset in datasets[1:])]
            varm = self._varm
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in varm.items()
                if all(key in dataset._varm and
                       type(value) is type(dataset._varm[key]) and
                       (dataset._varm[key].dtype == value.dtype and
                        array_equal(dataset._varm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].equals(value))
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and dataset._uns[key] == value
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._var = dataset._var.select(dataset.var_names,
                                                   *var_columns_in_common)
                dataset._varm = {key: dataset._varm[key]
                                  for key in varm_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            # Subset to columns of obs and keys of obsm that are present in all
            # datasets, and have the same data types. Also include columns of
            # obs that are Enum in some datasets and Categorical in others, or
            # Enum in all datasets but with different categories in each
            # dataset; cast these to Categorical.
            obs_mismatched_categoricals = {
                column for column, dtype in self._obs[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._obs and
                       dataset._obs[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._obs[column].dtype == dtype
                           for dataset in datasets[1:])}
            obs_columns_in_common = [
                column
                for column, dtype in islice(self._obs.schema.items(), 1, None)
                if column in obs_mismatched_categoricals or
                   all(column in dataset._obs and
                       dataset._obs[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._obs[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in obs_mismatched_categoricals}
            for dataset in datasets:
                dataset._obs = dataset._obs\
                    .select(dataset.obs_names, *obs_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in self._obsm.items()
                if all(key in dataset._obsm and
                       type(dataset._obsm[key]) is type(value) and
                       (dataset._obsm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obsm = {key: dataset._obsm[key]
                                 for key in obsm_keys_in_common}
        else:  # non-flexible
            # Check that all var, varm and uns are identical
            var = self._var
            for dataset in datasets[1:]:
                if not dataset._var.equals(var):
                    error_message = (
                        'all SingleCell datasets must have the same var, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            varm = self._varm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._varm.keys() != varm.keys() or \
                        any(type(dataset._varm[key]) is not type(value) or
                            (dataset._varm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._varm[key].schema != value.schema)
                            for key, value in varm.items()) or \
                        any((dataset._varm[key] != value).any()
                            for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same varm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            # Check that all obs have the same columns and data types
            schema = self._obs.schema
            for dataset in datasets[1:]:
                if dataset._obs.schema != schema:
                    error_message = (
                        'all SingleCell datasets must have the same columns '
                        'in obs, with the same data types, unless '
                        'flexible=True')
                    raise ValueError(error_message)
            # Check that all obsm have the same keys and data types
            obsm = self._obsm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._obsm.keys() != obsm.keys() or any(
                        type(dataset._obsm[key]) is not type(value) or
                        (dataset._obsm[key].dtype != value.dtype
                         if isinstance(value, np.ndarray) else
                         dataset._obsm[key].schema != value.schema)
                        for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'obsm, with the same data types, unless flexible=True')
                    raise ValueError(error_message)
        # Concatenate; output should be CSR when there's a mix of inputs
        X = vstack([dataset._X for dataset in datasets],
                   format='csr' if any(isinstance(dataset._X, csr_array)
                                       for dataset in datasets) else 'csc')
        obs = pl.concat([dataset._obs for dataset in datasets])
        # noinspection PyTypeChecker
        obsm = {key: np.concatenate([dataset._obsm[key]
                                     for dataset in datasets])
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._obsm[key] for dataset in datasets])
                for key, value in self._obsm.items()}
        return SingleCell(X=X, obs=obs, var=datasets[0]._var, obsm=obsm,
                          varm=datasets[0]._varm, uns=datasets[0]._uns)

    def concat_var(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False) -> SingleCell:
        """
        Concatenate the genes of multiple SingleCell datasets.
        
        By default, all datasets must have the same obs, obsm and uns. They
        must also have the same columns in var and the same keys in varm, with
        the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of obs, i.e. the obs_names) before
        concatenating. Subset to columns of obs and keys of obsm and uns that
        are identical in all datasets after this subsetting. Also, subset to
        columns of var and keys of varm that are present in all datasets, and
        have the same data types. All datasets' var_names must have the same
        name and dtype, and similarly for their obs_names.
        
        The one exception to the var "same data type" rule: if a column is Enum
        in some datasets and Categorical in others, or Enum in all datasets but
        with different categories in each dataset, that column will be retained
        as an Enum column (with the union of the categories) in the
        concatenated var.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes, columns of var and obs, and
                      keys of varm, obsm and uns common to all datasets before
                      concatenating, rather than raising an error on any
                      mismatches
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        if isinstance(datasets, SingleCell):
            datasets = datasets,
        datasets = (self,) + datasets + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that var_names and obs_names have the same name and data
            # type across all datasets
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            # Subset to genes in common across all datasets
            genes_in_common = self.obs_names\
                .filter(self.obs_names
                        .is_in(pl.concat([dataset.obs_names
                                          for dataset in datasets[1:]])))
            if len(genes_in_common) == 0:
                error_message = \
                    'no genes are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[:, genes_in_common] for dataset in datasets]
            # Subset to columns of obs and keys of obsm and uns that are
            # identical in all datasets after this subsetting
            obs_columns_in_common = [
                column.name for column in datasets[0]._obs[:, 1:]
                if all(column.name in dataset._obs and
                       dataset._obs[column.name].equals(column)
                       for dataset in datasets[1:])]
            obsm = self._obsm
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in obsm.items()
                if all(key in dataset._obsm and
                       type(value) is type(dataset._obsm[key]) and
                       (dataset._obsm[key].dtype == value.dtype and
                        array_equal(dataset._obsm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].equals(value))
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and value == dataset._uns[key]
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obs = dataset._obs.select(dataset.obs_names,
                                                   *obs_columns_in_common)
                dataset._obsm = {key: dataset._obsm[key]
                                  for key in obsm_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            # Subset to columns of var and keys of varm that are present in all
            # datasets, and have the same data types. Also include columns of
            # var that are Enum in some datasets and Categorical in others, or
            # Enum in all datasets but with different categories in each
            # dataset; cast these to Categorical.
            var_mismatched_categoricals = {
                column for column, dtype in self._var[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._var and
                       dataset._var[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._var[column].dtype == dtype
                           for dataset in datasets[1:])}
            var_columns_in_common = [
                column
                for column, dtype in islice(self._var.schema.items(), 1, None)
                if column in var_mismatched_categoricals or
                   all(column in dataset._var and
                       dataset._var[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._var[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in var_mismatched_categoricals}
            for dataset in datasets:
                dataset._var = dataset._var\
                    .select(dataset.var_names, *var_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in self._varm.items()
                if all(key in dataset._varm and
                       type(dataset._varm[key]) is type(value) and
                       (dataset._varm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._varm = {key: dataset._varm[key]
                                  for key in varm_keys_in_common}
        else:  # non-flexible
            # Check that all obs, obsm and uns are identical
            obs = self._obs
            for dataset in datasets[1:]:
                if not dataset._obs.equals(obs):
                    error_message = (
                        'all SingleCell datasets must have the same obs, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            obsm = self._obsm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._obsm.keys() != obsm.keys() or \
                        any(type(dataset._obsm[key]) is not type(value) or
                            (dataset._obsm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._obsm[key].schema != value.schema)
                            for key, value in obsm.items()) or \
                        any((dataset._obsm[key] != value).any()
                            for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same obsm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            # Check that all var have the same columns and data types
            schema = self._var.schema
            for dataset in datasets[1:]:
                if dataset._var.schema != schema:
                    error_message = (
                        'all SingleCell datasets must have the same columns '
                        'in var, with the same data types, unless '
                        'flexible=True')
                    raise ValueError(error_message)
            # Check that all varm have the same keys and data types
            varm = self._varm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._varm.keys() != varm.keys() or any(
                        type(dataset._varm[key]) is not type(value) or
                       (dataset._varm[key].dtype != value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema != value.schema)
                        for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'varm, with the same data types, unless flexible=True')
                    raise ValueError(error_message)
        # Concatenate; output should be CSR when there's a mix of inputs
        X = hstack([dataset._X for dataset in datasets],
                   format='csr' if any(isinstance(dataset._X, csr_array)
                                       for dataset in datasets) else 'csc')
        var = pl.concat([dataset._var for dataset in datasets])
        # noinspection PyTypeChecker
        varm = {key: np.concatenate([dataset._varm[key]
                                     for dataset in datasets])
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._varm[key] for dataset in datasets])
                for key, value in self._varm.items()}
        return SingleCell(X=X, obs=datasets[0]._obs, var=var,
                          obsm=datasets[0]._obsm, varm=varm,
                          uns=datasets[0]._uns)

    def split_by_obs(self,
                     column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     sort: bool = False) -> tuple[SingleCell, ...]:
        """
        The opposite of concat_obs(): splits a SingleCell dataset into a tuple
        of SingleCell datasets, one per unique value of a column of obs.

        Args:
            column: a String, Enum, or Categorical column of obs to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain null entries: the corresponding cells
                    will not be included in the result.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will not be selected when
                       subsampling.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
        
        Returns:
            A tuple of SingleCell datasets, one per unique value of `column`.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        column = self._get_column('obs', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  QC_column=QC_column, allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        values = (column.value_counts(sort=True).to_series().drop_nulls()
                  if sort else column.unique(maintain_order=True))
        if QC_column is None:
            return tuple(self.filter_obs(column == value) for value in values)
        else:
            return tuple(self.filter_obs(column.eq(value) & QC_column)
                         for value in values)
    
    def split_by_var(self,
                     column: SingleCellColumn,
                     *,
                     sort: bool = False) -> tuple[SingleCell, ...]:
        """
        The opposite of concat_var(): splits a SingleCell dataset into a tuple
        of SingleCell datasets, one per unique value of a column of var.

        Args:
            column: a String, Enum, or Categorical column of var to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain null entries: the corresponding genes
                    will not be included in the result.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
        
        Returns:
            A tuple of SingleCell datasets, one per unique value of `column`.
        """
        column = self._get_column('var', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        return tuple(self.filter_var(column == value) for value in
                     (column.value_counts(sort=True).to_series().drop_nulls()
                      if sort else column.unique(maintain_order=True)))
    
    def tocsr(self) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting X to a csr_array.
        Raise an error if X is already a csr_array.
        
        Returns:
            A copy of this SingleCell dataset, with X as a csr_array.
        """
        if isinstance(self._X, csr_array):
            error_message = 'X is already a csr_array'
            raise TypeError(error_message)
        return SingleCell(X=self._X.tocsr(), obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def tocsc(self) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting X to a csc_array.
        Raise an error if X is already a csc_array.
        
        This function is provided for completeness, but csc_array is a far
        better format for cell-wise operations like pseudobulking.
        
        Returns:
            A copy of this SingleCell dataset, with X as a csc_array.
        """
        if isinstance(self._X, csc_array):
            error_message = 'X is already a csc_array'
            raise TypeError(error_message)
        return SingleCell(X=self._X.tocsc(), obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def filter_obs(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both obs/obsm
        and X.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
            **constraints: column filters: `name=value` filters to cells
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to cells passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        obs = self._obs\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        mask = obs['_SingleCell_index'].to_numpy()
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X[mask],
                              obs=obs.drop('_SingleCell_index'), var=self._var,
                              obsm={key: value[mask]
                                    for key, value in self._obsm.items()},
                              varm=self._varm, uns=self._uns)
        finally:
            self._X._num_threads = original_num_threads
    
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both var/varm
        and X.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        var = self._var\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X[:, var['_SingleCell_index']
                                           .to_numpy()],
                              obs=self._obs, var=var.drop('_SingleCell_index'),
                              obsm=self._obsm, varm=self._varm, uns=self._uns)
        finally:
            self._X._num_threads = original_num_threads
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to obs. obs_names
        will be automatically included as the first column, if not included
        explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.select(*exprs, **named_exprs)`, and obs_names as the first
            column unless already included explicitly.
        """
        obs = self._obs.select(*exprs, **named_exprs)
        if self.obs_names.name not in obs:
            obs = obs.select(self.obs_names, pl.all())
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to var. var_names
        will be automatically included as the first column, if not included
        explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.select(*exprs, **named_exprs)`, and var_names as the first
            column unless already included explicitly.
        """
        var = self._var.select(*exprs, **named_exprs)
        if self.var_names.name not in var:
            var = var.select(self.var_names, pl.all())
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)

    def select_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets obsm to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with obsm subset to the specified key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key in keys},
                          varm=self._varm, uns=self._uns)
    
    def select_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets varm to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with varm subset to the specified key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key in keys},
                          uns=self._uns)
    
    def select_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets uns to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with uns subset to the specified key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to select {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          uns={key: value for key, value in self._uns.items()
                                if key in keys})
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to obs.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(*exprs, **named_exprs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to var.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.with_columns(*exprs, **named_exprs),
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def with_obsm(self,
                  obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_obsm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to obsm, overwriting existing keys with the same
        names if present.
        
        Args:
            obsm: a dictionary of keys to add to (or overwrite in) obsm
            **more_obsm: additional keys to add to (or overwrite in) obsm,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in obsm.
        """
        check_type(obsm, 'obsm', dict, 'a dictionary')
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but it contains a key '
                    f'of type {type(key).__name__!r}')
                raise TypeError(error_message)
        obsm |= more_obsm
        if len(obsm) == 0:
            error_message = \
                'obsm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in obsm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | obsm, varm=self._varm,
                          uns=self._uns)
    
    def with_varm(self,
                  varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_varm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to varm, overwriting existing keys with the same
        names if present.
        
        Args:
            varm: a dictionary of keys to add to (or overwrite in) varm
            **more_varm: additional keys to add to (or overwrite in) varm,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in varm.
        """
        check_type(varm, 'varm', dict, 'a dictionary')
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but it contains a key '
                    f'of type {type(key).__name__!r}')
                raise TypeError(error_message)
        varm |= more_varm
        if len(varm) == 0:
            error_message = \
                'varm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in varm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm | varm,
                          uns=self._uns)
    
    def with_uns(self,
                 uns: dict[str, NestedScalarOrArrayDict] = {},
                  **more_uns: NestedScalarOrArrayDict) -> SingleCell:
        """
        Adds one or more keys to uns, overwriting existing keys with the same
        names if present.
        
        Args:
            uns: a dictionary of keys to add to (or overwrite in) uns
            **more_uns: additional keys to add to (or overwrite in) uns,
                        specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in uns.
        """
        check_type(uns, 'uns', dict, 'a dictionary')
        for key, value in uns.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of uns must be strings, but it contains a key '
                    f'of type {type(key).__name__!r}')
                raise TypeError(error_message)
        uns |= more_uns
        if len(uns) == 0:
            error_message = \
                'uns is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          uns=self._uns | uns)
    
    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from obs.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                              positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs.drop(columns),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from var.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.drop(columns), obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def drop_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from obsm.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            obsm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key not in keys},
                          varm=self._varm, uns=self._uns)
    
    def drop_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from varm.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            varm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key not in keys},
                          uns=self._uns)
    
    def drop_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from uns.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            uns.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          uns={key: value for key, value in self._uns.items()
                                if key not in keys})
    
    def rename_obs(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of obs renamed.
        
        Rename column(s) of obs.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of obs renamed.
        """
        return SingleCell(X=self._X, obs=self._obs.rename(mapping),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def rename_var(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of var renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of var renamed.
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.rename(mapping), obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def rename_obsm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of obsm renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of obsm renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._obsm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of obsm'
                    raise ValueError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            obsm = {mapping.get(key, key): value
                    for key, value in self._obsm.items()}
        elif isinstance(mapping, Callable):
            obsm = {}
            for key, value in self._obsm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename obsm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                obsm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var, obsm=obsm,
                          varm=self._varm, uns=self._uns)
    
    def rename_varm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of varm renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of varm renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._varm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of varm'
                    raise ValueError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            varm = {mapping.get(key, key): value
                    for key, value in self._varm.items()}
        elif isinstance(mapping, Callable):
            varm = {}
            for key, value in self._varm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename varm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                varm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=varm, uns=self._uns)
    
    def rename_uns(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of uns renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of uns renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._uns:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of uns'
                    raise ValueError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
            uns = {mapping.get(key, key): value
                   for key, value in self._uns.items()}
        elif isinstance(mapping, Callable):
            uns = {}
            for key, value in self._uns.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename uns[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
                uns[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, uns=uns)
    
    def cast_X(self, dtype: np._typing.DTypeLike) -> SingleCell:
        """
        Cast X to the specified data type.
        
        Args:
            dtype: a NumPy data type

        Returns:
            A new SingleCell dataset with X cast to the specified data type.
        """
        return SingleCell(X=self._X.astype(dtype),
                          obs=self._obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of obs to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of obs cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs.cast(dtypes, strict=strict),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of var to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of var cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.cast(dtypes, strict=strict),
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Left join obs with another DataFrame.
        
        Args:
            other: a polars DataFrame to join obs with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in obs
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in obs or more
                        than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in obs.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include null as a valid value to join on.
                        By default, null values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from obs
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            obs.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in obs and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._obs
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'obs[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        obs = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(obs) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Join var with another DataFrame.
        
        Args:
            other: a polars DataFrame to join var with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in var
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in var or more
                        than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in var.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include null as a valid value to join on.
                        By default, null values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from obs
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            var.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in obs and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._var
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'var[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        # noinspection PyTypeChecker
        var = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(var) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    def peek_obs(self, row: int = 0) -> None:
        """
        Print a row of obs (the first row, by default) with each column on its
        own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[row].unpivot(variable_name='column'))
    
    def peek_var(self, row: int = 0) -> None:
        """
        Print a row of var (the first row, by default) with each column on its
        own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[row].unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> SingleCell:
        """
        Subsample a specific number or fraction of cells.
        
        Args:
            n: the number of cells to return; mutually exclusive with
               `fraction`
            fraction: the fraction of cells to return; mutually exclusive with
                      `n`
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will not be selected when
                       subsampling, and will not count towards the denominator
                       of `fraction`; QC_column will not appear in the returned
                       SingleCell object, since it would be redundant.
            by_column: an optional String, Enum, Categorical, or integer column
                       of obs to subsample by. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of cells with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       null entries: the corresponding cells will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in obs, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to obs.
        """
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._obs:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of obs; did you already run subsample_obs()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        check_type(seed, 'seed', int, 'an integer')
        if by_column is not None:
            by_column = self._get_column(
                'obs', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column, allow_null=True)
            if QC_column is not None:
                by_column = by_column.filter(QC_column)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))\
                    .to_series()
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))\
                    .to_series()
        elif QC_column is not None:
            selected = pl.int_range(QC_column.sum(), dtype=pl.Int32,
                                    eager=True)\
                .shuffle(seed=seed)\
                .lt(n if fraction is None else (fraction * pl.len()).round())
        else:
            selected = self._obs\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))\
                .to_series()
        if QC_column is not None:
            # Back-project from QCed cells to all cells, filling with nulls
            selected = pl.when(QC_column)\
                .then(selected.gather(QC_column.cum_sum().cast(pl.Int32) - 1))
        sc = self.filter_obs(selected) if subsample_column is None else \
            self.with_columns_obs(selected.alias(subsample_column))
        if QC_column is not None:
            # noinspection PyTypeChecker
            sc._obs = sc._obs.drop(QC_column.name)
        return sc
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> SingleCell:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of var to subsample by. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of genes with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       null entries: the corresponding genes will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in var, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to var.
        """
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._var:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of var; did you already run subsample_var()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        check_type(seed, 'seed', int, 'an integer')
        if by_column is not None:
            by_column = self._get_column(
                'var', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                allow_null=True)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))
        else:
            selected = self._var\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))
        selected = selected.to_series()
        return self.filter_var(selected) if subsample_column is None else \
            self.with_columns_var(selected.alias(subsample_column))
    
    def pipe(self,
             function: Callable[[SingleCell, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a SingleCell dataset.
        
        Args:
            function: the function to apply to the SingleCell dataset. It must
                      take a SingleCell dataset as its first argument, and can
                      return any value. The function may also allow other
                      arguments after the count matrix, which can be specified
                      via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this SingleCell dataset.
        """
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[csr_array | csc_array, ...],
                                  csr_array | csc_array],
               *args: Any,
               **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's X.
        
        Args:
            function: the function to apply to X. It must take the old X as its
                      first argument and return the new X. The function may
                      also take other arguments after X, which can be specified
                      via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to X.
        """
        return SingleCell(X=function(self._X, *args, **kwargs), obs=self._obs,
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def pipe_obs(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's obs.
        
        Args:
            function: the function to apply to obs. It must take the old obs as
                      its first argument and return the new obs. The function
                      may also take other arguments after obs, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obs.
        """
        return SingleCell(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def pipe_var(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's var.
        
        Args:
            function: the function to apply to var. It must take the old var as
                      its first argument and return the new var. The function
                      may also take other arguments after var, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            var.
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=function(self._var, *args, **kwargs),
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def pipe_obsm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's obsm.
        
        Args:
            function: the function to apply to obsm. It must take the old obsm
                      as its first argument and return the new obsm. The
                      function may also take other arguments after obsm, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obsm.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=function(self._obsm, *args, **kwargs),
                          varm=self._varm, uns=self._uns)
    
    def pipe_varm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's varm.
        
        Args:
            function: the function to apply to varm. It must take the old varm
                      as its first argument and return the new varm. The
                      function may also take other arguments after varm, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            varm.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm=function(self._varm, *args, **kwargs),
                          uns=self._uns)
    
    def pipe_uns(self,
                 function: Callable[[NestedScalarOrArrayDict, ...],
                                    NestedScalarOrArrayDict],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's uns.
        
        Args:
            function: the function to apply to uns. It must take the old uns as
                      its first argument and return the new uns. The function
                      may also take other arguments after uns, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            uns.
        """
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          uns=function(self._uns, *args, **kwargs))
    
    def qc(self,
           custom_filter: SingleCellColumn | None = None,
           *,
           subset: bool = True,
           QC_column: str = 'passed_QC',
           max_mito_fraction: int | float | np.integer | np.floating |
                              None = 0.1,
           min_genes: int | np.integer | None = 100,
           MALAT1_filter: bool = True,
           remove_doublets: bool = False,
           batch_column: SingleCellColumn | None = None,
           num_doublet_genes: int | np.integer = 500,
           allow_float: bool = False,
           overwrite: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Adds a Boolean column to obs indicating which cells passed quality
        control (QC), or subsets to these cells if `subset=True`.
        
        By default, filters to cells with a cell-type confidence of ≥90%, ≤10%
        mitochondrial reads, ≥100 genes detected, and nonzero MALAT1 or Malat1
        expression. Can also filter out doublets when `remove_doublets=True`.
        
        Raises an error if any gene names appear more than once in var_names
        (they can be deduplicated with `make_var_names_unique()`).
        
        Args:
            custom_filter: an optional Boolean column of obs containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be a column name,
                           a polars expression, a polars Series, a 1D NumPy
                           array, or a function that takes in this SingleCell
                           dataset and returns a polars Series or 1D NumPy
                           array.
            subset: whether to subset to cells passing QC, instead of merely
                    adding a `QC_column` to obs. This will roughly double
                    memory usage, but speed up subsequent operations.
            QC_column: the name of a Boolean column to add to obs indicating
                       which cells passed QC, if `subset=False`. Gives an error
                       if obs already has a column with this name, unless
                       `overwrite=True`.
            max_mito_fraction: if not `None`, filter to cells with <= this
                               fraction of mitochondrial counts. The default
                               value of 10% is in between Seurat's recommended
                               value of 5%, and Scanpy's recommendation to not
                               filter on mitochondrial counts at all, at least
                               initially.
            min_genes: if not `None`, filter to cells with >= this many genes
                       detected (with non-zero count). The default of 100
                       matches Scanpy's recommended value, while Seurat
                       recommends a minimum of 200.
            MALAT1_filter: if `True`, filter out cells with 0 expression of the
                           nuclear-expressed lncRNA MALAT1, which likely
                           represent empty droplets or poor-quality cells
                           (biorxiv.org/content/10.1101/2024.07.14.603469v1).
                           There must be exactly one gene in obs_names with the
                           name MALAT1 or Malat1 to use this filter.
            remove_doublets: if `True`, remove predicted doublets (see
                             `find_doublets()`). Doublet detection uses the
                             cxds algorithm to score each cell, then thresholds
                             this continuous score to a binary one (doublet
                             versus non-doublet) using a threshold derived from
                             simulated doublets.
            batch_column: an optional String, Enum, Categorical, or integer
                          column of obs indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Only used during doublet
                          detection; doublet detection will be performed
                          separately for each batch. Set to `None` if all cells
                          belong to the same sequencing batch. Must be `None`
                          when `remove_doublets=False`.
            num_doublet_genes: the number of highly variable genes, i.e. genes
                               expressed in as close to 50% of cells as
                               possible, to use during doublet detection. This
                               parameter usually has a minimal influence on
                               accuracy as long as it is sufficiently large (in
                               the hundreds), so increasing it further will
                               mainly just increase runtime. If
                               `num_doublet_genes` is greater than the number
                               of genes in the dataset, all genes will be used.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            overwrite: if `True`, overwrite `QC_column` if already present in
                       obs, instead of raising an error. Must be `False` when
                       `subset=True`.
            verbose: whether to print how many cells were filtered out at each
                     step of the QC process
            num_threads: the number of threads to use when filtering based on
                         mitochondrial counts and MALAT1 expression, and for
                         doublet detection. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use `single_cell.get_num_threads()`
                         cores.
                
        Returns:
            A new SingleCell dataset with `QC_column` added to obs, or subset
            to QCed cells if `subset=True`, and `uns['QCed']` set to `True`.
        """
        X = self._X
        # Check that `self` is not already QCed
        if self._uns['QCed']:
            error_message = (
                "uns['QCed'] is True; did you already run qc()? Set "
                "uns['QCed'] = False or run with_uns(QCed=False) to bypass "
                "this check.")
            raise ValueError(error_message)
        # Check inputs
        if self.var_names.n_unique() < len(self.var):
            error_message = (
                'var_names contains duplicates; deduplicate with '
                'make_var_names_unique()')
            raise ValueError(error_message)
        if custom_filter is not None:
            custom_filter = self._get_column(
                'obs', custom_filter, 'custom_filter', pl.Boolean)
        check_type(subset, 'subset', bool, 'Boolean')
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if not subset:
            check_type(QC_column, 'QC_column', str, 'a string')
            if not overwrite and QC_column in self._obs:
                error_message = (
                    f'QC_column {QC_column!r} is already a column of obs; did '
                    f'you already run qc()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = 'overwrite must be False when subset is True'
            raise ValueError(error_message)
        if max_mito_fraction is not None:
            check_type(max_mito_fraction, 'max_mito_fraction', (int, float),
                       'a number between 0 and 1, inclusive')
            check_bounds(max_mito_fraction, 'max_mito_fraction', 0, 1)
        if min_genes is not None:
            check_type(min_genes, 'min_genes', int, 'a non-negative integer')
            check_bounds(min_genes, 'min_genes', 0)
        check_type(MALAT1_filter, 'MALAT1_filter', bool, 'Boolean')
        check_type(remove_doublets, 'remove_doublets', bool, 'Boolean')
        if batch_column is not None:
            if not remove_doublets:
                error_message = \
                    'batch_column must be None when remove_doublets is False'
                raise ValueError(error_message)
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'))
        check_type(num_doublet_genes, 'num_doublet_genes', int,
                   'a positive integer')
        check_bounds(num_doublet_genes, 'num_doublet_genes', 1)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'qc() requires raw counts but X.dtype is {str(X.dtype)!r}, '
                f'a floating-point data type. If you are sure that all values '
                f'are raw integer counts, i.e. that (X.data == '
                f'X.data.astype(int)).all(), then set allow_float=True.')
            raise TypeError(error_message)
        # Apply the custom filter, if specified
        if verbose:
            print(f'Starting with {len(self):,} cells.')
        mask = None
        if custom_filter is not None:
            if verbose:
                print('Applying the custom filter...')
            mask = custom_filter
            if verbose:
                print(f'{mask.sum():,} cells remain after applying the custom '
                      f'filter.')
        # Filter to cells with ≤ `100 * max_mito_fraction`% mitochondrial
        # counts, if `max_mito_fraction` was specified
        if max_mito_fraction is not None:
            if verbose:
                print(f'Filtering to cells with ≤{100 * max_mito_fraction}% '
                      f'mitochondrial counts...')
            var_names = self.var_names
            if var_names.dtype != pl.String:
                var_names = var_names.cast(pl.String)
            mt_genes = var_names.str.to_uppercase().str.starts_with('MT-')
            if not mt_genes.any():
                error_message = (
                    'no genes are mitochondrial (start with "MT-" or "mt-"); '
                    'this may happen if your var_names are Ensembl IDs (ENSG) '
                    'rather than gene symbols (in which case you should set '
                    'the gene symbols as the var_names with set_var_names()), '
                    'or if mitochondrial genes have already been filtered out '
                    '(in which case you can set max_mito_fraction to None)')
                raise ValueError(error_message)
            mito_mask = np.empty(X.shape[0], dtype=bool)
            if isinstance(X, csr_array):
                cython_inline(rf'''
                    from cython.parallel cimport prange
                    
                    def mito_mask(
                            const {cython_type(X.dtype)}[::1] data,
                            const {cython_type(X.indices.dtype)}[::1] indices,
                            const {cython_type(X.indptr.dtype)}[::1] indptr,
                            char[::1] mt_genes,
                            const double max_mito_fraction,
                            char[::1] mito_mask,
                            const unsigned num_threads):
                        
                        cdef int row, row_sum, mt_sum, \
                            num_genes = indptr.shape[0] - 1
                        cdef long col
                        
                        if num_threads == 1:
                            for row in range(num_genes):
                                row_sum = mt_sum = 0
                                for col in range(indptr[row], indptr[row + 1]):
                                    row_sum = row_sum + <int> data[col]
                                    if mt_genes[indices[col]]:
                                        mt_sum = mt_sum + <int> data[col]
                                mito_mask[row] = \
                                    (<double> mt_sum / row_sum) <= \
                                    max_mito_fraction
                        else:
                            for row in prange(num_genes, nogil=True,
                                              num_threads=num_threads):
                                row_sum = mt_sum = 0
                                for col in range(indptr[row], indptr[row + 1]):
                                    row_sum = row_sum + <int> data[col]
                                    if mt_genes[indices[col]]:
                                        mt_sum = mt_sum + <int> data[col]
                                mito_mask[row] = \
                                    (<double> mt_sum / row_sum) <= \
                                    max_mito_fraction
                        ''')['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            mito_mask=mito_mask, num_threads=num_threads)
            else:
                row_sums = np.empty(X.shape[0], dtype=np.int32)
                mt_sums = np.empty(X.shape[0], dtype=np.int32)
                cython_inline(rf'''
                    from cython.parallel cimport prange
                    from libc.stdlib cimport free, malloc
                    
                    def mito_mask(
                            const {cython_type(X.dtype)}[::1] data,
                            const {cython_type(X.indices.dtype)}[::1] indices,
                            const {cython_type(X.indptr.dtype)}[::1] indptr,
                            char[::1] mt_genes,
                            const double max_mito_fraction,
                            int[::1] row_sums,
                            int[::1] mt_sums,
                            char[::1] mito_mask,
                            const unsigned num_threads):
                        
                        cdef int gene, chunk_size, start, end, dest_start, \
                            dest_end, num_genes = indptr.shape[0] - 1, \
                            num_cells = mito_mask.shape[0]
                        cdef long cell
                        cdef unsigned thread_index
                        cdef int* thread_row_sums_buffer
                        cdef int* thread_mt_sums_buffer
                        cdef int[::1] thread_row_sums, thread_mt_sums
                        
                        if num_threads == 1:
                            row_sums[:] = 0
                            mt_sums[:] = 0
                            for gene in range(num_genes):
                                if mt_genes[gene]:
                                    for cell in range(indptr[gene], indptr[gene + 1]):
                                        row_sums[indices[cell]] += <int> data[cell]
                                        mt_sums[indices[cell]] += <int> data[cell]
                                else:
                                    for cell in range(indptr[gene], indptr[gene + 1]):
                                        row_sums[indices[cell]] += <int> data[cell]
                            for cell in range(num_cells):
                                mito_mask[cell] = \
                                    (<double> mt_sums[cell] /
                                    row_sums[cell]) <= max_mito_fraction
                        else:
                            # Store total counts per cell and total
                            # mitochondrial counts per cell for each thread in
                            # a temporary buffer, then aggregate at the end. As
                            # an optimization, put the counts for the last
                            # thread (`thread_index == num_threads - 1`)
                            # directly into the final `row_sums` and `mt_sums`
                            # arrays.
                            chunk_size = num_genes // num_threads
                            thread_row_sums_buffer = <int*> malloc(
                                num_cells * (num_threads - 1) * sizeof(int))
                            if not thread_row_sums_buffer:
                                raise MemoryError
                            try:
                                thread_row_sums = \
                                    <int[:num_cells * (num_threads - 1):]> \
                                    thread_row_sums_buffer
                                thread_mt_sums_buffer = <int*> malloc(
                                    num_cells * (num_threads - 1) * sizeof(int))
                                if not thread_mt_sums_buffer:
                                    raise MemoryError
                                try:
                                    thread_mt_sums = \
                                        <int[:num_cells * (num_threads - 1):]> \
                                        thread_mt_sums_buffer
                                    with nogil:
                                        for thread_index in prange(
                                                num_threads, num_threads=num_threads,
                                                schedule='static', chunksize=1):
                                            start = thread_index * chunk_size
                                            if thread_index == num_threads - 1:
                                                end = num_genes
                                                row_sums[:] = 0
                                                mt_sums[:] = 0
                                                for gene in range(start, end):
                                                    if mt_genes[gene]:
                                                        for cell in range(indptr[gene], indptr[gene + 1]):
                                                            row_sums[indices[cell]] += <int> data[cell]
                                                            mt_sums[indices[cell]] += <int> data[cell]
                                                    else:
                                                        for cell in range(indptr[gene], indptr[gene + 1]):
                                                            row_sums[indices[cell]] += <int> data[cell]
                                            else:
                                                end = start + chunk_size
                                                dest_start = thread_index * num_cells
                                                dest_end = dest_start + num_cells
                                                thread_row_sums[dest_start:dest_end] = 0
                                                thread_mt_sums[dest_start:dest_end] = 0
                                                for gene in range(start, end):
                                                    if mt_genes[gene]:
                                                        for cell in range(indptr[gene], indptr[gene + 1]):
                                                            thread_row_sums[dest_start + indices[cell]] += \
                                                                <int> data[cell]
                                                            thread_mt_sums[dest_start + indices[cell]] += \
                                                                <int> data[cell]
                                                    else:
                                                        for cell in range(indptr[gene], indptr[gene + 1]):
                                                            thread_row_sums[dest_start + indices[cell]] += \
                                                                <int> data[cell]
                                        
                                        # Aggregate counts from all threads except the last
                                        for thread_index in range(num_threads - 1):
                                            dest_start = thread_index * num_cells
                                            for cell in range(num_cells):
                                                row_sums[cell] += thread_row_sums[dest_start + cell]
                                                mt_sums[cell] += thread_mt_sums[dest_start + cell]
                                    
                                        # Populate the mask
                                        for cell in prange(num_cells,
                                                           num_threads=num_threads):
                                            mito_mask[cell] = \
                                                (<double> mt_sums[cell] /
                                                row_sums[cell]) <= max_mito_fraction
                                finally:
                                    free(thread_mt_sums_buffer)
                            finally:
                                free(thread_row_sums_buffer)
                        ''')['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            row_sums=row_sums, mt_sums=mt_sums,
                            mito_mask=mito_mask, num_threads=num_threads)
            mito_mask = pl.Series(mito_mask)
            if not mito_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'≤{100 * max_mito_fraction}% mitochondrial counts')
                raise ValueError(error_message)
            if mask is None:
                mask = mito_mask
            else:
                mask &= mito_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with ≤{100 * max_mito_fraction}% mitochondrial '
                      f'counts.')
        # Filter to cells with ≥ `min_genes` genes detected, if specified
        if min_genes is not None:
            if verbose:
                print(f'Filtering to cells with ≥{min_genes:,} genes '
                      f'detected (with non-zero count)...')
            gene_mask = pl.Series(getnnz(X, axis=1, num_threads=num_threads) >=
                                  min_genes)
            if not gene_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'≥{min_genes:,} genes detected')
                raise ValueError(error_message)
            if mask is None:
                mask = gene_mask
            else:
                mask &= gene_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with ≥{min_genes:,} genes detected.')
        # Filter to cells with non-zero MALAT1 expression, if
        # `MALAT1_filter=True`
        if MALAT1_filter:
            if verbose:
                print(f'Filtering to cells with non-zero MALAT1 expression...')
            MALAT1_index = self._var\
                .select(pl.arg_where(pl.col(self.var_names.name)
                                     .is_in(('MALAT1', 'Malat1'))))
            if len(MALAT1_index) == 0:
                error_message = (
                    f"neither 'MALAT1' nor 'Malat1' was found in var_names; "
                    f"this may happen if your var_names are Ensembl IDs "
                    f"(ENSG) rather than gene symbols (in which case you "
                    f"should set the gene symbols as the var_names with "
                    f"set_var_names()). Alternatively, set "
                    f"MALAT1_filter=False to disable filtering on MALAT1 "
                    f"expression.")
                raise ValueError(error_message)
            if len(MALAT1_index) == 2:
                error_message = (
                    "both 'MALAT1' and 'Malat1' were found in var_names; if "
                    "this is intentional, rename one of them before running "
                    "qc(), or set MALAT1_filter=False to disable filtering "
                    "on MALAT1 expression")
                raise ValueError(error_message)
            MALAT1_index = MALAT1_index.item()
            # The code below is a faster version of:
            # MALAT1_mask = (X[:, [MALAT1_index]] != 0).toarray().squeeze()
            # More specifically,
            MALAT1_mask = np.zeros(X.shape[0], dtype=bool)
            has_sorted_indices = getattr(X, '_has_sorted_indices', None)
            if isinstance(X, csr_array):
                if has_sorted_indices:
                    # Use binary search
                    cython_inline(rf'''
                        from cython.parallel cimport prange
                        
                        def get_MALAT1_mask_csr(
                                const {cython_type(X.dtype)}[::1] data,
                                const {cython_type(X.indices.dtype)}[::1]
                                    indices,
                                const {cython_type(X.indptr.dtype)}[::1]
                                    indptr,
                                const int MALAT1_index,
                                char[::1] MALAT1_mask,
                                const unsigned num_threads):
                            
                            cdef int row, num_genes = indptr.shape[0] - 1
                            cdef long low, high, mid
                            
                            if num_threads == 1:
                                for row in range(num_genes):
                                    low = indptr[row]
                                    high = indptr[row + 1] - 1
                                    while True:
                                        mid = (low + high) // 2
                                        if indices[mid] < MALAT1_index:
                                            low = mid + 1
                                        else:
                                            high = mid
                                        if low >= high:
                                            break
                                    MALAT1_mask[row] = \
                                        indices[low] == MALAT1_index
                            else:
                                for row in prange(num_genes, nogil=True,
                                                  num_threads=num_threads):
                                    low = indptr[row]
                                    high = indptr[row + 1] - 1
                                    while True:
                                        mid = (low + high) // 2
                                        if indices[mid] < MALAT1_index:
                                            low = mid + 1
                                        else:
                                            high = mid
                                        if low >= high:
                                            break
                                    MALAT1_mask[row] = \
                                        indices[low] == MALAT1_index
                    ''')['get_MALAT1_mask_csr'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
                else:
                    if verbose and has_sorted_indices is False:
                        print('Warning: X does not have sorted indices, so '
                              'some operations may be slower. You may want to '
                              'sort indices with `X.sort_indices()` (an '
                              'in-place operation) as the first step after '
                              'loading, though be aware that this may take a '
                              'while.')
                    # Use brute-force search
                    cython_inline(rf'''
                        from cython.parallel cimport prange
                        
                        def get_MALAT1_mask_csr(
                                const {cython_type(X.dtype)}[::1] data,
                                const {cython_type(X.indices.dtype)}[::1]
                                    indices,
                                const {cython_type(X.indptr.dtype)}[::1]
                                    indptr,
                                const int MALAT1_index,
                                char[::1] MALAT1_mask,
                                const unsigned num_threads):
                            
                            cdef int row, num_genes = indptr.shape[0] - 1
                            cdef long col
                            
                            if num_threads == 1:
                                for row in range(num_genes):
                                    for col in range(indptr[row],
                                                     indptr[row + 1]):
                                        if indices[col] == MALAT1_index:
                                            MALAT1_mask[row] = True
                                            break
                            else:
                                for row in prange(num_genes, nogil=True,
                                                  num_threads=num_threads):
                                    for col in range(indptr[row],
                                                     indptr[row + 1]):
                                        if indices[col] == MALAT1_index:
                                            MALAT1_mask[row] = True
                                            break
                    ''')['get_MALAT1_mask_csr'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
            else:
                start = X.indptr[MALAT1_index]
                end = X.indptr[MALAT1_index + 1]
                MALAT1_mask[X.indices[start:end]] = True
            MALAT1_mask = pl.Series(MALAT1_mask)
            if mask is None:
                mask = MALAT1_mask
            else:
                mask &= MALAT1_mask
            if verbose:
                print(f'{mask.sum():,} cells remain after filtering to cells '
                      f'with non-zero MALAT1 expression.')
        # Remove predicted doublets, if `remove_doublets=True`
        if remove_doublets:
            if verbose:
                print('Removing predicted doublets...')
            singlets = ~pl.Series(SingleCell._find_doublets(
                X=self._X, batch_column=batch_column, QC_column=None,
                num_genes=num_doublet_genes, num_threads=num_threads,
                verbose=False)[0])
            if mask is None:
                mask = singlets
            else:
                mask &= singlets
            if verbose:
                print(f'{mask.sum():,} cells remain after removing predicted '
                      f'doublets.')
        # Add the mask of QCed cells as a column, or subset if `subset=True`
        if mask is None:
            error_message = 'no QC filters were specified'
            raise ValueError(error_message)
        if subset:
            if verbose:
                print(f'Subsetting to cells passing QC (note: you can reduce '
                      f'memory usage by specifying subset=False)...')
            sc = self.filter_obs(mask)
        else:
            if verbose:
                print(f'Adding a Boolean column, obs[{QC_column!r}], '
                      f'indicating which cells passed QC...')
            sc = SingleCell(X=X, obs=self._obs.with_columns(
                pl.lit(mask).alias(QC_column)), var=self._var, obsm=self._obsm,
                              varm=self._varm, uns=self._uns)
        sc._uns['QCed'] = True
        return sc
        
    @staticmethod
    def _find_doublets(X: csr_array | csc_array,
                       batch_column: SingleCellColumn | None,
                       QC_column: SingleCellColumn | None = 'passed_QC',
                       num_genes: int | np.integer = 500,
                       verbose: bool = False,
                       num_threads: int | np.integer | None = None) -> \
            tuple[np.ndarray[1, np.dtype[np.bool_]],
                  np.ndarray[1, np.dtype[np.float64]]]:
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507). Used by
        `qc()` (when `remove_doublets=True`) and `find_doublets()`.
        
        Args:
            X: the count matrix; may be either normalized or unnormalized
            batch_column: an optional String, Enum, Categorical, or integer
                          column of obs indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       doublet labels and doublet scores set to null.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            verbose: whether to print debug information about timings at each
                     step (will be removed for the final version)
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.

        Returns:
            A tuple of two NumPy arrays with the binary doublet calls and
            doublet scores.
        """
        # Define Cython functions
        is_csr = isinstance(X, csr_array)
        cython_functions = cython_inline(rf'''
            from cython.parallel cimport prange
            from libcpp.cmath cimport erfc, exp, fabs, floor, log, log1p, sqrt
            
            cdef extern from *:
                """
                #define atomic_add(x,y) _Pragma("omp atomic") x += y
                """
                void atomic_add(int &x, int y) nogil
            
            cdef double log_one_half = -0.6931471805599453
            cdef double log_sqrt_2_pi = 0.91893853320467274
            cdef double one_over_sqrt_2 = 0.70710678118654752
            cdef double one_over_sqrt_pi = 0.5641895835477563
            
            cdef double[5] gamma_A = [
                8.11614167470508450300E-4, -5.95061904284301438324E-4,
                7.93650340457716943945E-4, -2.77777777730099687205E-3,
                8.33333333333331927722E-2]
            
            cdef double[6] gamma_B = [
                -1.37825152569120859100E3, -3.88016315134637840924E4,
                -3.31612992738871184744E5, -1.16237097492762307383E6,
                -1.72173700820839662146E6, -8.53555664245765465627E5]
            
            cdef double[6] gamma_C = [
                -3.51815701436523470549E2, -1.70642106651881159223E4,
                -2.20528590553854454839E5, -1.13933444367982507207E6,
                -2.53252307177582951285E6, -2.01889141433532773231E6]
            
            cdef double[6] P = [
                0.5641895835477550741253201704,
                1.275366644729965952479585264,
                5.019049726784267463450058,
                6.1602098531096305440906,
                7.409740605964741794425,
                2.97886562639399288862]
            
            cdef double[6] Q = [
                2.260528520767326969591866945,
                9.396034016235054150430579648,
                12.0489519278551290360340491,
                17.08144074746600431571095,
                9.608965327192787870698,
                3.3690752069827527677]
            
            cdef inline double p1evl(const double x,
                                     const double* coef,
                                     const int N) noexcept nogil:
                cdef double ans
                cdef int i
                
                ans = x + coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef inline double polevl(const double x,
                                      const double* coef,
                                      const int N) noexcept nogil:
                cdef double ans
                cdef int i
                
                ans = coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef double log_erfc(const double x) noexcept nogil:
                # Based on GSL's gsl_sf_log_erfc_e at
                # github.com/ampl/gsl/blob/master/specfunc/erfc.c#L306
                
                cdef double y, series
                
                if x * x < 0.02460783300575925:
                    y = x * one_over_sqrt_pi
                    series = 0.00048204
                    series = y * series - 0.00142906
                    series = y * series + 0.0013200243174
                    series = y * series + 0.0009461589032
                    series = y * series - 0.0045563339802
                    series = y * series + 0.00556964649138
                    series = y * series + 0.00125993961762116
                    series = y * series - 0.01621575378835404
                    series = y * series + 0.02629651521057465
                    series = y * series - 0.001829764677455021
                    series = y * series - 0.09439510239319526
                    series = y * series + 0.28613578213673563
                    series = y * series + 1
                    series = y * series + 1
                    return -2 * y * series
                elif x > 8:
                    return log(polevl(x, &P[0], 6) / p1evl(x, &Q[0], 6)) - \
                        x * x
                else:
                    return log(erfc(x))
            
            cdef inline double gammaln(double x) noexcept nogil:
                # Simplified from github.com/scipy/scipy/blob/main/scipy/
                # special/xsf/cephes/gamma.h, based on the knowledge that `x`
                # will always be positive and finite when calculating terms in
                # the binomial distribution
                
                cdef double p, q, u, z
                cdef int i
                
                if x < 13:
                    z = 1
                    p = 0
                    u = x
                    while u >= 3:
                        p -= 1
                        u = x + p
                        z *= u
                    while u < 2:
                        z /= u
                        p += 1
                        u = x + p
                    z = fabs(z)
                    if u == 2:
                        return log(z)
                    p -= 2
                    x = x + p
                    p = x * polevl(x, &gamma_B[0], 6) / \
                        p1evl(x, &gamma_C[0], 6)
                    return log(z) + p
                elif x >= 1000:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    if x > 1e8:
                        return q
                    p = 1.0 / (x * x)
                    p = ((7.9365079365079365079365e-4 * p -
                          2.7777777777777777777778e-3) *
                         p + 0.0833333333333333333333) / x
                    return q + p
                else:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    p = 1.0 / (x * x)
                    return q + polevl(p, &gamma_A[0], 5) / x
            
            cdef inline double binom_logsf_term(
                    const int j,
                    const int n,
                    const double log_p,
                    const double log1p_q,
                    const double gammaln_n_plus_1) noexcept nogil:
                return gammaln_n_plus_1 - gammaln(j + 1) - \
                    gammaln(n - j + 1) + j * log_p + (n - j) * log1p_q
            
            cdef inline double binom_logsf(const int k,
                                           const int n,
                                           const double p) noexcept nogil:
                cdef int j, j_max
                cdef double mu, sigma, z
                cdef double sum_exp, max_term, term, log_p, log1p_q, \
                    gammaln_n_plus_1
                
                # Use the normal approximation when n * p and n * (1 - p) are
                # both greater than 500; add 0.5 for continuity correction
                
                if n * p > 500 and n * (1 - p) > 500:
                    mu = n * p
                    sigma = sqrt(mu * (1 - p))
                    z = (k + 0.5 - mu) / sigma
                    return log_erfc(z * one_over_sqrt_2) + log_one_half
                
                # Otherwise, compute the exact binomial p-value
            
                log_p = log(p)
                log1p_q = log1p(-p)
                gammaln_n_plus_1 = gammaln(n + 1)
            
                # Sum the terms of the binomial via the logsumexp trick. This
                # improves numerical stability by subtracting off the max term
                # from each term before exponentiation, then adding back the
                # max term at the end. So we need to find `j_max`, the value of
                # `j` for which `term` is maximized below. For a binomial
                # distribution, the mode (maximum probability) occurs at
                # `floor((n + 1) * p)`. However, since we are summing from
                # `k + 1` to `n`, not 0 to `n`, we need to ensure `j_max` is at
                # least `k + 1`.
                
                j_max = <int> floor((n + 1) * p)
                if j_max <= k:
                    j_max = k + 1
                
                max_term = binom_logsf_term(j_max, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                
                sum_exp = 0
                for j in range(k + 1, n + 1):
                    term = binom_logsf_term(j, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                    sum_exp += exp(term - max_term)
            
                return log(sum_exp) + max_term
            
            def compute_obs(
                    const int[::1] detection_count,
                    const {cython_type(X.indices.dtype)}[::1] indices,
                    const {cython_type(X.indptr.dtype)}[::1] indptr,
                    int[:, ::1] obs,
                    const unsigned num_threads):
                
                # obs[i, j] is the number of cells in which exactly one of the
                # two genes i and j is expressed. If we define:
                # - (1) as the number of cells where gene i is expressed
                # - (2) as the number of cells where gene j is expressed
                # - (3) as the number of cells where both genes i and j are
                #   expressed
                # then obs[i, j] = (1) + (2) - 2 * (3)
                
                cdef int cell, gene_i, gene_j, \
                    num_genes = detection_count.shape[0], \
                    num_cells = indptr.shape[0] - 1
                cdef long i, j, i_start, i_end, j_end
                
                if num_threads == 1:
                    # Initialize the upper diagonal of obs to (1) + (2)
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            obs[i, j] = detection_count[i] + detection_count[j]
                    
                    # Now subtract off 2 * (3) for the upper diagonal: iterate
                    # over all pairs of genes i and j within each cell, and
                    # subtract 2 from `obs[i, j]` for each pair
                    for cell in range(num_cells):
                        for i in range(indptr[cell], indptr[cell + 1]):
                            for j in range(i + 1, indptr[cell + 1]):
                                obs[indices[i], indices[j]] -= 2
                else:
                    with nogil:
                        for i in prange(num_genes, num_threads=num_threads):
                            for j in range(i + 1, num_genes):
                                obs[i, j] = \
                                    detection_count[i] + detection_count[j]
                        
                        for cell in prange(num_cells, num_threads=num_threads):
                            for i in range(indptr[cell], indptr[cell + 1]):
                                for j in range(i + 1, indptr[cell + 1]):
                                    atomic_add(obs[indices[i], indices[j]], -2)
            
            def compute_S(const int[:, ::1] obs,
                          const double[::1] p,
                          const int num_cells,
                          double[:, ::1] S,
                          const unsigned num_threads):
                
                cdef int num_genes = obs.shape[0]
                cdef int i, j
                
                if num_threads == 1:
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
                else:
                    for i in prange(num_genes, nogil=True,
                                    num_threads=num_threads):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
            
            def compute_cxds(
                    const {cython_type(X.indices.dtype)}[::1] indices,
                    const {cython_type(X.indptr.dtype)}[::1] indptr,
                    const double[:, ::1] S,
                    double[::1] cxds_scores,
                    const unsigned num_threads):
                
                cdef int cell, gene_i, gene_j, num_genes, \
                    num_cells = indptr.shape[0] - 1
                cdef long i, j, i_start, i_end, j_end
                
                # Iterate over all pairs of genes i and j within each cell, and
                # subtract `S[i, j]` from the cell's cxds score for each pair
                if num_threads == 1:
                    for cell in range(num_cells):
                        for gene_i in range(indptr[cell], indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1, indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
                else:
                    for cell in prange(num_cells, nogil=True,
                                       num_threads=num_threads):
                        for gene_i in range(indptr[cell], indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1, indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
            
            cdef inline int rand(long* state) noexcept nogil:
                cdef long x = state[0]
                state[0] = x * 6364136223846793005L + 1442695040888963407L
                cdef int s = (x ^ (x >> 18)) >> 27
                cdef int rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline long srand(const long seed) noexcept nogil:
                cdef long state = seed + 1442695040888963407L
                rand(&state)
                return state
            
            cdef inline int randint(const int bound, long* state) \
                    noexcept nogil:
                cdef int r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            def simulate_doublets(
                    const {cython_type(X.dtype)}[::1] data,
                    const {cython_type(X.indices.dtype)}[::1] indices,
                    const {cython_type(X.indptr.dtype)}[::1] indptr,
                    {cython_type(X.indices.dtype)}[::1] sim_indices,
                    {cython_type(X.indptr.dtype)}[::1] sim_indptr,
                    const int num_cells,
                    const long seed):
                
                cdef long i, j, i_end, j_end, nnz = 0, state = srand(seed)
                cdef int sim_index, cell_i, cell_j
                
                sim_indptr[0] = 0
                for sim_index in range(1, num_cells + 1):
                    cell_i = randint(num_cells, &state)
                    cell_j = randint(num_cells, &state)
                    i = indptr[cell_i]
                    i_end = indptr[cell_i + 1]
                    j = indptr[cell_j]
                    j_end = indptr[cell_j + 1]
                    while True:
                        if indices[i] == indices[j]:
                            sim_indices[nnz] = indices[i]
                            nnz = nnz + 1
                            i = i + 1
                            j = j + 1
                            if i == i_end or j == j_end:
                                break
                        elif indices[i] < indices[j]:
                            # `rand(&state) & 1` gives a random Boolean; only
                            # coin-flip if `data[i] == 1`
                            if data[i] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[i]
                                nnz = nnz + 1
                            i = i + 1
                            if i == i_end:
                                break
                        else:
                            if data[j] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[j]
                                nnz = nnz + 1
                            j = j + 1
                            if j == j_end:
                                break
                    sim_indptr[sim_index] = nnz
        ''')
        compute_obs = cython_functions['compute_obs']
        compute_S = cython_functions['compute_S']
        compute_cxds = cython_functions['compute_cxds']
        simulate_doublets = cython_functions['simulate_doublets']
        
        # If `batch_column` is not `None`, get the row indices of each batch,
        # ignoring cells failing QC when `QC_column` is present in obs. If
        # no batches were specified but `QC_column` is present, use `QC_column`
        # as the batch labels.
        if QC_column is not None and batch_column is None:
            batch_column = QC_column
        if batch_column is not None:
            batch_column_name = batch_column.name
            # noinspection PyTypeChecker
            batches = (batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      if QC_column is None else
                      batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      .filter(QC_column.name))\
                .group_by(batch_column_name, maintain_order=True)\
                .agg('_SingleCell_batch_indices')\
                .select('_SingleCell_batch_indices')\
                .collect()\
                .to_series()
        else:
            batches = None,
        
        # Preallocate
        if batch_column is not None:
            doublets = np.empty(X.shape[0], dtype=bool)
            doublet_scores = np.empty(X.shape[0])
        obs = np.empty((num_genes, num_genes), dtype=np.int32)
        S = np.empty((num_genes, num_genes))
        
        # noinspection PyUnresolvedReferences
        original_num_threads = X._num_threads
        try:
            X._num_threads = num_threads
            # For each batch...
            for batch_index, batch_indices in enumerate(batches):
                # Subset to cells in that batch
                with Timer('subset to batch', verbose=verbose):
                    X_batch = \
                        X[batch_indices] if batch_column is not None else X
                # Get the detection count of each gene
                with Timer('getnnz', verbose=verbose):
                    detection_count = \
                        getnnz(X_batch, axis=0, num_threads=num_threads)
                # Subset to the `num_genes` genes with detection rates closest
                # to 50%. Exclude genes with detection rates of 0% or 100%.
                with Timer('hvg_indices and misc subsetting', verbose=verbose):
                    num_cells = X_batch.shape[0]
                    p = detection_count / num_cells
                    hvg_indices = \
                        np.argsort(p * (1 - p), kind='stable')[-num_genes:]
                    least_variable_p = p[hvg_indices[-num_genes]]
                    if least_variable_p == 0 or least_variable_p == 1:
                        hvg_p = p[hvg_indices]
                        hvg_indices = hvg_indices[(hvg_p > 0) & (hvg_p < 1)]
                    hvg_indices.sort()
                    detection_count = detection_count[hvg_indices]
                    p = p[hvg_indices]
                    num_genes = len(hvg_indices)  # in case not enough hvgs
                with Timer('subsetting X_batch to hvgs', verbose=verbose):
                    X_batch = X_batch[:, hvg_indices]
                # Convert `X_batch` to CSR, if CSC
                if not is_csr:
                    with Timer('converting to CSR', verbose=verbose):
                        X_batch = X_batch.tocsr()
                # Sort indices, if not already sorted (necessary for
                # `simulate_doublets`)
                with Timer('sorting indices if not already sorted',
                           verbose=verbose):
                    if not X.has_sorted_indices:
                        X_batch.sort_indices()
                # Get `obs`, where `obs[i, j]` is the number of cells that
                # express exactly one of genes `i` and `j`
                with Timer('compute obs', verbose=verbose):
                    compute_obs(detection_count=detection_count,
                                indices=X_batch.indices, indptr=X_batch.indptr,
                                obs=obs, num_threads=num_threads)
                # Get `S`, the upper-tail log binomial p-values of `obs`
                with Timer('compute S', verbose=verbose):
                    compute_S(obs=obs, p=p, num_cells=num_cells, S=S,
                              num_threads=num_threads)
                # Calculate each cell's cxds score: the sum of `-S[i, j]`
                # across all gene pairs `i` and `j` that are both expressed by
                # the cell
                with Timer('compute cxds', verbose=verbose):
                    cxds_scores_batch = np.zeros(num_cells)
                    compute_cxds(indices=X_batch.indices,
                                 indptr=X_batch.indptr, S=S,
                                 cxds_scores=cxds_scores_batch,
                                 num_threads=num_threads)
                # Now simulate doublets within the batch and compute their cxds
                # scores, using the original `S` matrix derived from the real
                # data
                with Timer('simulating doublets', verbose=verbose):
                    sim_indptr = np.empty_like(X_batch.indptr)
                    # conservatively allocate twice as much memory for the
                    # indices as the original indices, since in the worst-case
                    # scenario none of the indices will match up and all
                    # coinflips will be 1
                    sim_indices = np.empty(2 * len(X_batch.indices),
                                           dtype=X_batch.indices.dtype)
                    simulate_doublets(
                        data=X_batch.data, indices=X_batch.indices,
                        indptr=X_batch.indptr, sim_indices=sim_indices,
                        sim_indptr=sim_indptr, num_cells=num_cells,
                        seed=batch_index)
                    sim_indices = sim_indices[:sim_indptr[-1]]
                with Timer('compute cxds simulated', verbose=verbose):
                    cxds_scores_sim = np.zeros(num_cells)
                    compute_cxds(indices=sim_indices, indptr=sim_indptr, S=S,
                                 cxds_scores=cxds_scores_sim,
                                 num_threads=num_threads)
                # Get the doublet threshold: the median of the cxds scores for
                # the simulated doublets
                with Timer('get doublet threshold', verbose=verbose):
                    threshold = np.median(cxds_scores_sim)
                # Call doublets based on whether their cxds score is above the
                # threshold. If `batch_column` is not `None`, map doublet
                # labels and scores back to the full dataset.
                with Timer('mapping back to full arrays', verbose=verbose):
                    if batch_column is not None:
                        doublets[batch_indices] = \
                            cxds_scores_batch >= threshold
                        doublet_scores[batch_indices] = cxds_scores_batch
                    else:
                        doublets = cxds_scores_batch >= threshold
                        doublet_scores = cxds_scores_batch
        finally:
            X._num_threads = original_num_threads
        return doublets, doublet_scores
    
    def find_doublets(self,
                      batch_column: SingleCellColumn | None,
                      *,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      num_genes: int | np.integer = 500,
                      doublet_column: str = 'doublet',
                      doublet_score_column: str = 'doublet_score',
                      overwrite: bool = False,
                      verbose: bool = False,
                      num_threads: int | np.integer | None = None):
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507).
        
        Doublets are already filtered out in `qc()` by default, so this
        function should only be used when `remove_doublets=False` in `qc()`.
        This function gives the same result regardless of whether it is run
        before or after normalization.
        
        Doublets cannot occur across sequencing batches, so make sure to
        specify `batch_column` if your dataset has multiple batches! Doublet
        detection will be done independently within each batch.
        
        Since the cxds score is continuous, it needs to be converted into a
        binary classification of doublets versus non-doublets. This problem can
        be framed as finding a cxds score threshold above which a cell is
        deemed to be a doublet. To determine this threshold, we simulate
        doublets by combining the counts from randomly selected pairs of cells,
        via the following steps:
        
        1) Sample as many random pairs of cells (with replacement) as there are
           real cells.
        2) Combine the counts from each pair of cells into a simulated doublet.
           Because cxds operates on binarized count matrices, we average the
           two cells' count matrices in a binary sense: if a gene is expressed
           in either cell, it is deemed to be expressed in the simulated
           doublet, but if it has a count of 1 in one cell and 0 in the other,
           it is randomly chosen to be either expressed or not expressed with
           equal probability (since the average count would be 0.5).
        3) Calculate cxds scores for these simulated doublets, based on the
           coexpression patterns (the `S` matrix from cxds) learned from the
           real data.
        4) Take the median cxds score of the simulated doublets as the
           threshold. In other words, if a real cell has a higher doublet score
           than the average simulated doublet, we call it a doublet.
           
        Args:
            batch_column: an optional String, Enum, Categorical, or integer
                          column of obs indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       doublet labels and doublet scores set to null.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            doublet_column: the name of a Boolean column to be added to obs
                            containing the doublet labels, i.e. whether each
                            cell is predicted to be a doublet
            doublet_score_column: the name of a column to be added to obs
                                  containing each cell's doublet score. Higher
                                  scores indicate greater likelihood of being a
                                  doublet. Scores are not normalized and are
                                  not comparable across datasets or batches,
                                  but are guaranteed to be positive (since they
                                  are sums of log p-values).
            overwrite: if `True`, overwrite `doublet_column` and/or
                       `doublet_score_column` if already present in obs,
                       instead of raising an error
            verbose: whether to print debug information about timings at each
                     step (will be removed for the final version)
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.

        Returns:
            A new SingleCell dataset where var contains two additional columns,
            `doublet_column` (default: `doublet`), indicating whether each cell
            is predicted to be a doublet, and `doublet_score_column` (default:
            `'doublet_score'`), containing each cell's doublet score.
        
        Note:
            This function's cxds scores are almost exactly half the original
            implementation's, because it avoids double-counting the two genes
            in each gene pair. Slight deviations from this one-half (usually
            by less than one part in a million) may occur because cxds uses
            a normal approximation to the binomial p-value to avoid long
            runtimes on large datasets.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_doublets()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get `QC_column` and `batch_column`, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if batch_column is not None:
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column)
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `doublet_column` and `doublet_score_column` are strings
        # and, unless `overwrite=True`, not already in obs
        for column, column_name in (
                (doublet_column, 'doublet_column'),
                (doublet_score_column, 'doublet_score_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column of obs; '
                    f'did you already run find_doublets()? Set overwrite=True '
                    f'to overwrite.')
                raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Run doublet detection
        doublets, doublet_scores = SingleCell._find_doublets(
            X=self._X, batch_column=batch_column, QC_column=QC_column,
            num_genes=num_genes, num_threads=num_threads, verbose=verbose)
        # Convert doublet labels and scores to polars Series to add to obs. If
        # `QC_column` exists, set doublet labels and scores to null for cells
        # failing QC.
        doublet_column = pl.Series(doublet_column, doublets)
        doublet_score_column = pl.Series(doublet_score_column, doublet_scores)
        if QC_column is None:
            return self.with_columns_obs(doublet_column, doublet_score_column)
        else:
            return self.with_columns_obs(
                pl.when(QC_column).then(doublet_column),
                pl.when(QC_column).then(doublet_score_column))
    
    def make_obs_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make obs names unique by appending `'-1'` to the second occurence of
        a given obs name, `'-2'` to the third occurrence, and so on, where
        `'-'` can be switched to a different string via the `separator`
        argument. Raises an error if any obs_names already contain `separator`.
        
        Args:
            separator: the string connecting the original obs name and the
                       integer suffix

        Returns:
            A new SingleCell dataset with the obs names made unique.
        """
        check_type(separator, 'separator', str, 'a string')
        if self.obs_names.str.contains(separator).any():
            error_message = (
                f'some obs_names already contain the separator {separator!r}; '
                f'did you already run make_obs_names_unique()? If not, set '
                f'the separator argument to a different string.')
            raise ValueError(error_message)
        obs_names = pl.col(self.obs_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(obs_names)
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(obs_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(obs_names)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def make_var_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make var names unique by appending `'-1'` to the second occurence of
        a given var name, `'-2'` to the third occurrence, and so on, where
        `'-'` can be switched to a different string via the `separator`
        argument. Raises an error if any var_names already contain `separator`.
        
        Args:
            separator: the string connecting the original var name and the
                       integer suffix

        Returns:
            A new SingleCell dataset with the var names made unique.
        """
        var_names = pl.col(self.var_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(var_names)
        return SingleCell(X=self._X,
                          obs=self._obs,
                          var=self._var.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(var_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(var_names)),
                          obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    def get_sample_covariates(self,
                              ID_column: SingleCellColumn,
                              *,
                              QC_column: SingleCellColumn |
                                         None = 'passed_QC') -> pl.DataFrame:
        """
        Get a DataFrame of sample-level covariates, i.e. the columns of obs
        that are the same for all cells within each sample.
        
        Args:
            ID_column: a column of obs containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored.
        
        Returns:
            A DataFrame of the sample-level covariates, with ID_column (sorted)
            as the first column.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column)
        ID_column_name = ID_column.name
        obs = self._obs
        if QC_column is not None:
            obs = obs.filter(QC_column)
            ID_column = ID_column.filter(QC_column)
        return obs\
            .select(ID_column,
                    *obs
                    .group_by(ID_column)
                    .n_unique()
                    .pipe(filter_columns,
                          (pl.exclude(ID_column_name)
                           if ID_column_name in obs else pl.all()).max().eq(1))
                    .columns)\
            .unique(ID_column_name)\
            .sort(ID_column_name)
    
    def pseudobulk(self,
                   ID_column: SingleCellColumn,
                   cell_type_column: SingleCellColumn,
                   *,
                   QC_column: SingleCellColumn | None = 'passed_QC',
                   additional_obs: pl.DataFrame | None = None,
                   sort_genes: bool = False,
                   num_threads: int | np.integer | None = None) -> Pseudobulk:
        """
        Pseudobulks a single-cell dataset with sample ID and cell type columns,
        after filtering to cells passing QC according to `QC_column`. Returns a
        Pseudobulk dataset.
        
        You can run this function multiple times at different cell type
        resolutions by setting a different cell_type_column each time.
        
        Args:
            ID_column: a column of obs containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            cell_type_column: a column of obs containing cell-type labels. Can
                              be a column name, a polars expression, a polars
                              Series, a 1D NumPy array, or a function that
                              takes in this SingleCell dataset and returns a
                              polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be excluded from the
                       pseudobulk.
            additional_obs: an optional DataFrame of additional sample-level
                            covariates, which will be joined to the
                            pseudobulk's obs for each cell type
            sort_genes: whether to sort genes in alphabetical order in the
                        pseudobulk
            num_threads: the number of threads to use when pseudobulking;
                         parallelism happens across {sample, cell type} pairs
                         (or just samples, if `cell_type_column` is `None`).
                         Set `num_threads=-1` to use all available cores
                         (as determined by `os.cpu_count()`), or leave unset to
                         use `single_cell.get_num_threads()` cores. For count
                         matrices stored in the usual CSR format,
                         parallelization takes place across cell types and
                         samples, so specifying more cores than the number of
                         cell type-sample pairs may not improve performance.
        
        Returns:
            A Pseudobulk dataset with X (counts), obs (metadata per sample),
            and var (metadata per gene) fields, each dicts across cell types.
            The columns of each cell type's obs will be:
            - 'ID' (a renamed version of `ID_column`)
            - 'num_cells' (the number of cells for that sample and cell type)
            followed by whichever columns of the SingleCell dataset's obs are
            constant across samples.
        """
        X = self._X
        # Check that `self` is QCed and not normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "pseudobulk()? Set uns['QCed'] = True or run "
                ".with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = (
                "uns['normalized'] is True; did you already run normalize()?")
            raise ValueError(error_message)
        # Check inputs
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        original_ID_column = ID_column
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column)
        ID_column_name = ID_column.name
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        cell_type_column_name = cell_type_column.name
        num_cells_column_name = 'num_cells'
        for column_description, column_name in ('ID_column', ID_column_name), \
                ('cell_type_column', cell_type_column_name):
            if column_name == num_cells_column_name:
                error_message = (
                    f'{column_description} has the name '
                    f'{num_cells_column_name!r}, which conflicts with the '
                    f'name of the column to be added to the Pseudobulk '
                    f'dataset containing the number of cells of each cell '
                    f'type')
                raise ValueError(error_message)
        check_type(sort_genes, 'sort_genes', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        if additional_obs is not None:
            check_type(additional_obs, 'additional_obs', pl.DataFrame,
                       'a polars DataFrame')
            if ID_column_name not in additional_obs:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f'{ID_column_description} is not a column of '
                    f'additional_obs')
                raise ValueError(error_message)
            if ID_column.dtype != additional_obs[ID_column_name].dtype:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f"{ID_column_description} has a different data type in "
                    f"additional_obs than in this SingleCell dataset's obs")
                raise TypeError(error_message)
        # Check that the first column of var is String, Enum, or Categorical:
        # this is a requirement of the Pseudobulk class. (The first column of
        # obs must be as well, but this will always be true by construction,
        # since it will always be the sample ID.)
        if self.var_names.dtype not in (pl.Categorical, pl.Enum, pl.String):
            error_message = (
                f'the first column of var (var_names) has data type '
                f'{self.obs_names.dtype!r}, but must be String, Enum, '
                f'or Categorical')
            raise ValueError(error_message)
        # Get the row indices that will be pseudobulked across for each group
        # (cell type-sample pair), ignoring cells failing QC when `QC_column`
        # is present in obs
        # noinspection PyUnboundLocalVariable
        groups = (pl.LazyFrame((cell_type_column, ID_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, ID_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name, ID_column_name)\
            .agg('_SingleCell_group_indices',
                 pl.len().alias(num_cells_column_name))\
            .sort(cell_type_column_name, ID_column_name)\
            .collect()
        # Pseudobulk, storing the result in a preallocated NumPy array
        result = np.zeros((len(groups), X.shape[1]), dtype=np.int32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = groups[num_cells_column_name].cum_sum().to_numpy()
            cython_inline(rf'''
                from cython.parallel cimport prange
                
                def groupby_sum_csr(
                        const {cython_type(X.dtype)}[::1] data,
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int num_groups = group_ends.shape[0]
                    cdef int group, row
                    cdef long gene
                    cdef unsigned cell
                    
                    if num_threads == 1:
                        # For each group (cell type-sample pair)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse
                                # matrix
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for
                                # this cell...
                                for gene in range(indptr[row],
                                                  indptr[row + 1]):
                                    # Add the value at this cell and gene to
                                    # the total for this group and gene
                                    result[group, indices[gene]] += \
                                        int(data[gene])
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(indptr[row],
                                                  indptr[row + 1]):
                                    result[group, indices[gene]] += \
                                        int(data[gene])
                ''')['groupby_sum_csr'](
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    group_indices=group_indices, group_ends=group_ends,
                    result=result, num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = QC_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            cython_inline(f'''
                from cython.parallel cimport prange
                
                def groupby_sum_csc(
                        const {cython_type(X.dtype)}[::1] data,
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_map,
                        const bint has_missing,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int gene, group, num_genes = result.shape[1]
                    cdef long cell
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene (column of the sparse matrix)...
                            for gene in range(num_genes):
                                # For each cell (row) that's non-zero for this
                                # gene...
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    # Get the group index for this cell (-1 if it
                                    # failed QC)
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    # Add the value at this cell and gene to the
                                    # total for this group and gene
                                    result[group, gene] += <int> data[cell]
                        else:
                            for gene in range(num_genes):
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, gene] += <int> data[cell]
                    else:
                        if has_missing:
                            for gene in prange(num_genes, nogil=True,
                                               num_threads=num_threads):
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    result[group, gene] += <int> data[cell]
                        else:
                            for gene in prange(num_genes, nogil=True,
                                               num_threads=num_threads):
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, gene] += <int> data[cell]
                    ''')['groupby_sum_csc'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=result, num_threads=num_threads)
        # Sort genes, if `sort_genes=True`
        cell_type_var = self._var
        if sort_genes:
            result = result[:, cell_type_var[:, 0].arg_sort().to_numpy()]
            cell_type_var = cell_type_var.sort(cell_type_var.columns[0])
        # Break up the results by cell type
        sample_covariates = self.get_sample_covariates(ID_column,
                                                       QC_column=QC_column)
        X, obs, var = {}, {}, {}
        start_index = 0
        for cell_type, count in groups[cell_type_column_name]\
                .value_counts().sort(cell_type_column_name).iter_rows():
            end_index = start_index + count
            X[cell_type] = result[start_index:end_index]
            obs[cell_type] = groups.lazy()\
                .select(ID_column_name, num_cells_column_name)\
                .slice(start_index, count)\
                .join(sample_covariates.lazy(), on=ID_column_name, how='left')\
                .pipe(lambda df: df.join(additional_obs.lazy(),
                                         on=ID_column_name, how='left')
                      if additional_obs is not None else df)\
                .rename({ID_column_name: 'ID'})\
                .pipe(lambda df: df if QC_column is None else
                                 df.drop(QC_column.name))\
                .collect()
            var[cell_type] = cell_type_var
            start_index = end_index
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def hvg(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            batch_column: SingleCellColumn | None |
                          Sequence[SingleCellColumn | None] = None,
            num_genes: int | np.integer = 2000,
            min_cells: int | np.integer = 3,
            flavor: Literal['seurat_v3', 'seurat_v3_paper'] = 'seurat_v3',
            span: int | float | np.integer | np.floating = 0.3,
            hvg_column: str = 'highly_variable',
            rank_column: str = 'highly_variable_rank',
            allow_float: bool = False,
            overwrite: bool = False,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Select highly variable genes using Seurat's algorithm. Operates on
        raw counts.
        
        By default, uses the same approach as Scanpy's
        `scanpy.pp.highly_variable_genes` function with the `flavor` argument
        set to the non-default value `'seurat_v3'`, and Seurat's
        `FindVariableFeatures` function with the `selection.method` argument
        set to the default value `'vst'`.

        Requires the scikit-misc package; install with:
        pip install --no-deps --no-build-isolation scikit-misc
        
        The general idea is that since genes with higher mean expression tend
        to have higher variance in expression (because they have more non-zero
        values), we want to select genes that have a high variance *relative to
        their mean expression*. Otherwise, we'd only be picking highly
        expressed genes! To correct for the mean-variance relationship, fit a
        LOESS curve fit to the mean-variance trend.
        
        Args:
            others: optional SingleCell datasets to jointly compute highly
                    variable genes across, alongside this one. Each dataset
                    will be treated as a separate batch. If `batch_column` is
                    not `None`, each dataset AND each distinct value of
                    `batch_column` within each dataset will be treated as a
                    separate batch. Variances will be computed per batch and
                    then aggregated (see `flavor`) across batches.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored. When `others`
                       is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of obs indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `hvg()`, except that the
                          `min_cells` filter will always be calculated
                          per-dataset rather than per-batch. Variances will be
                          computed per batch and then aggregated (see `flavor`)
                          across batches. Set to `None` to treat each dataset
                          as having a single batch. When `others` is specified,
                          `batch_column` can be a length-`1 + len(others)`
                          sequence of columns, expressions, Series, functions,
                          or `None` for each dataset (for `self`, followed by
                          each dataset in `others`).
            num_genes: the number of highly variable genes to select. The
                       default of 2000 matches Seurat and Scanpy's recommended
                       value. Fewer than `num_genes` genes will be selected if
                       not enough genes have non-zero count in >= `min_cells`
                       cells (or when `min_cells` is `None`, if not enough
                       genes are present).
            min_cells: if not `None`, filter to genes detected (with non-zero
                       count) in >= this many cells in every dataset, before
                       calculating highly variable genes. The default value of
                       3 matches Seurat and Scanpy's recommended value. Note
                       that genes with zero variance in any dataset will always
                       be filtered out, even if `min_cells` is 0.
            flavor: the highly variable gene algorithm to use. Must be one of
                    `seurat_v3` and `seurat_v3_paper`, both of which match the
                    algorithms with the same name in scanpy. Both algorithms
                    select genes based on two criteria: 1) which genes are
                    ranked as most variable (taking the median of the ranks
                    across batches where the gene is among the top `num_genes`
                    highly variable genes) and 2) the number of batches in
                    which a gene is ranked in among the top `num_genes` in
                    variability. `seurat_v3` ranks genes by 1) and uses 2) to
                    tiebreak, whereas `seurat_v3_paper` ranks genes by 2) and
                    uses 1) to tiebreak. When there is only one batch, both
                    algorithms are the same and only rank based on 1).
            span: the span of the LOESS fit; higher values will lead to more
                  smoothing
            hvg_column: the name of a Boolean column to be added to (each
                        dataset's) var indicating the highly variable genes
            rank_column: the name of an integer column to be added to (each
                         dataset's) var with the rank of each highly variable
                         gene's variance (1 = highest variance, 2 =
                         next-highest, etc.); will be null for non-highly
                         variable genes. In the very unlikely event of ties,
                         the gene that appears first in var will get the lowest
                         rank.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            overwrite: if `True`, overwrite `hvg_column` and/or `rank_column`
                       if already present in var, instead of raising an error
            num_threads: the number of threads to use when finding highly
                         variable genes. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use `single_cell.get_num_threads()`
                         cores.
        
        Returns:
            A new SingleCell dataset where var contains an additional Boolean
            column, `hvg_column` (default: `'highly_variable'`), indicating the
            `num_genes` most highly variable genes, and `rank_column` (default:
            'highly_variable_rank') indicating the (one-based) rank of each
            highly variable gene's variance. Or, if additional SingleCell
            dataset(s) are specified via the `others` argument, a
            length-`1 + len(others)` tuple of SingleCell datasets with these
            two columns added: `self`, followed by each dataset in `others`.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may not give identical results to Seurat and Scanpy.
            It avoids floating-point summation, which is more numerically
            stable than Scanpy and Seurat's calculations. If multiple genes are
            tied as the `num_genes`-th most highly variable gene in a batch or
            dataset, this function includes all of them, whereas Seurat and
            Scanpy arbitrarily pick one (or a subset) of them. Also, this
            function uses the ordering from a stable sort to break ties when
            selecting the final list of highly variable genes, instead of the
            unstable sort used by Seurat and Scanpy.
        """
        # noinspection PyUnresolvedReferences
        from skmisc.loess import loess
        # Check that all elements of `others` are SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Check that all datasets are QCed and not normalized
        if not all(dataset._uns['QCed'] for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f"uns['QCed'] is False{suffix}; did you forget to run qc() "
                f"before hvg()? Set uns['QCed'] = True or run "
                f"with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if any(dataset._uns['normalized'] for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f"hvg() requires raw counts but uns['normalized'] is "
                f"True{suffix}; did you already run normalize()?")
            raise ValueError(error_message)
        # Check that there are at least three cells in each dataset (since
        # LOESS seems to need at least three observations to converge)
        if any(len(dataset._obs) < 3 for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f'there are fewer than three cells{suffix}, so '
                f'highly variable genes cannot be calculated')
            raise ValueError(error_message)
        # Get `QC_column` and `batch_column` from every dataset, if not `None`
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        # Check that `min_cells` is a positive integer and at least as large as
        # the number of cells in each dataset
        check_type(min_cells, 'min_cells', int, 'a non-negative integer')
        check_bounds(min_cells, 'min_cells', 0)
        if any(len(dataset._obs) < min_cells for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f'the number of cells in this dataset ({len(self._obs)}) '
                f'is less than min_cells ({min_cells}){suffix}; increase '
                f'min_cells')
            raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `hvg_column` and `rank_column` are strings and, unless
        # `overwrite=True`, not already in var for any dataset
        for column, column_name in (hvg_column, 'hvg_column'), \
                (rank_column, 'rank_column'):
            check_type(column, column_name, str, 'a string')
            if not overwrite and \
                    any(column in dataset._var for dataset in datasets):
                suffix = ' for at least one dataset' if others else ''
                error_message = (
                    f'{column_name} {column!r} is already a column of '
                    f'var{suffix}; did you already run hvg()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # If `allow_float=False`, raise an error if `X` is floating-point
        # for any dataset
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float:
            for dataset in datasets:
                X = dataset._X
                if np.issubdtype(X.dtype, np.floating):
                    error_message = (
                        f'hvg() requires raw counts but X.dtype is '
                        f'{str(X.dtype)!r}, a floating-point data type. If '
                        f'you are sure that all values are raw integer '
                        f'counts, i.e. that (X.data == X.data.astype(int))'
                        f'.all(), then set allow_float=True.')
                    raise TypeError(error_message)
        # Get the universe of genes we'll be considering: those present in any
        # dataset. If there are multiple datasets, also get the indices of
        # these genes in each dataset (with nulls for genes not present in that
        # particular dataset).
        if others:
            # The use of `align_frames` here is a bit wasteful memory-wise,
            # because it creates an identical `'gene'` column for every
            # DataFrame in `genes_and_indices`. Fortunately, it's only one
            # small string column per dataset.
            genes_and_indices = pl.align_frames(*(
                dataset.var[:, 0].to_frame('gene')
                .with_columns(index=pl.int_range(pl.len(), dtype=pl.Int32))
                for dataset in datasets), on='gene')
            # noinspection PyTypeChecker
            genes_in_any_dataset = genes_and_indices[0]['gene']
            # noinspection PyTypeChecker
            dataset_gene_indices = [df['index'] for df in genes_and_indices]
            del genes_and_indices
        else:
            genes_in_any_dataset = self.var_names.rename('gene')
        # Get the batches to calculate variance across (datasets + batches
        # within each dataset)
        if others:
            if batch_column is None:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X, QC_column.to_numpy()
                            if QC_column is not None else None, gene_indices)
                           for dataset, QC_column, gene_indices in
                           zip(datasets, QC_columns, dataset_gene_indices))
            else:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X,
                            (batch_column.eq(batch) if QC_column is None else
                             batch_column.eq(batch) & QC_column).to_numpy()
                            if batch is not None else
                            (QC_column.to_numpy() if QC_column is not None else
                             None), gene_indices)
                           for dataset, QC_column, batch_column, gene_indices
                           in zip(datasets, QC_columns, batch_columns,
                                  dataset_gene_indices)
                           for batch in ((None,) if batch_column is None else
                                         batch_column.unique()))
        else:
            X = self._X
            batch_column = batch_columns[0]
            if batch_column is None:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = (X, QC_columns[0].to_numpy(), None),
                else:
                    batches = (X, None, None),
            else:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = ((X, (batch_column.eq(batch) & QC_columns[0])
                                   .to_numpy(), None)
                               for batch in batch_column.unique())
                else:
                    batches = ((X, batch_column.eq(batch).to_numpy(), None)
                               for batch in batch_column.unique())
        # Get the variance of each gene in each batch across cells passing QC
        norm_gene_vars = []
        for X, cell_mask, gene_indices in batches:
            num_dataset_genes = X.shape[1]
            mean = np.empty(num_dataset_genes)
            var = np.empty(num_dataset_genes)
            nonzero_count = np.empty(num_dataset_genes, dtype=np.int32)
            is_CSR = isinstance(X, csr_array)
            if is_CSR:
                if cell_mask is None:
                    cell_indices = np.array([], dtype=int)
                    num_cells = X.shape[0]
                else:
                    cell_indices = np.flatnonzero(cell_mask)
                    num_cells = len(cell_indices)
                # noinspection PyShadowingBuiltins
                sum = np.empty(num_dataset_genes * num_threads, dtype=int)
                sum_of_squares = \
                    np.empty(num_dataset_genes * num_threads, dtype=int)
                cython_inline(rf'''
                from cython.parallel cimport prange
                from libc.stdlib cimport free, malloc
                
                def sparse_mean_var_minor_axis(
                        const {cython_type(X.dtype)}[::1] data,
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const long[::1] cell_indices,
                        const long num_cells,
                        const int num_dataset_genes,
                        long[::1] sum,
                        long[::1] sum_of_squares,
                        double[::1] mean,
                        double[::1] var,
                        int[::1] nonzero_count,
                        const unsigned num_threads):
                        
                    cdef long num_elements, i, j, value, start, end, \
                        chunk_size, total_sum, total_sum_of_squares, \
                        total_nonzero_count
                    cdef int gene, cell, dest_start, dest_end, gene_index
                    cdef double inv_num_cells = 1.0 / num_cells, \
                        inv_num_pairs_of_cells = \
                        1.0 / (num_cells * (num_cells - 1))
                    cdef int* nonzero_count_buffer
                    cdef int[::1] temp_nonzero_count
                    cdef unsigned thread_index
                    
                    if num_threads == 1:
                        sum[:] = 0
                        sum_of_squares[:] = 0
                        nonzero_count[:] = 0
                        if cell_indices.shape[0] == 0:
                            # Iterate over all elements of the count
                            # matrix, ignoring which cell they're from
                            num_elements = indices.shape[0]
                            for i in range(num_elements):
                                gene = indices[i]
                                value = <long> data[i]
                                sum[gene] += value
                                sum_of_squares[gene] += value * value
                                nonzero_count[gene] += 1
                        else:
                            # Only iterate over the elements from cells
                            # in `cell_indices` (i.e. cells in this
                            # batch, and/or passing QC)
                            for j in range(num_cells):
                                cell = cell_indices[j]
                                for i in range(indptr[cell],
                                               indptr[cell + 1]):
                                    gene = indices[i]
                                    value = <long> data[i]
                                    sum[gene] += value
                                    sum_of_squares[gene] += \
                                        value * value
                                    nonzero_count[gene] += 1
                        
                        # Calculate means and variances from the sums
                        # and squared sums, including the contribution
                        # from zero elements
                        for gene in range(num_dataset_genes):
                            mean[gene] = sum[gene] * inv_num_cells
                            var[gene] = inv_num_pairs_of_cells * (
                                num_cells * sum_of_squares[gene] -
                                sum[gene] * sum[gene])
                    else:
                        nonzero_count_buffer = <int*> malloc(
                             num_dataset_genes * num_threads *
                             sizeof(int))
                        if not nonzero_count_buffer:
                            raise MemoryError
                        try:
                            temp_nonzero_count = \
                                <int[:num_dataset_genes *
                                     num_threads:]> \
                                nonzero_count_buffer
                            with nogil:
                                if cell_indices.shape[0] == 0:
                                    # Partition the work by elements, not
                                    # cells, for better load-balancing in
                                    # case cells have substantially
                                    # different library sizes
                                    num_elements = indices.shape[0]
                                    chunk_size = \
                                        num_elements // num_threads
                                    for thread_index in prange(
                                            num_threads,
                                            num_threads=num_threads,
                                            schedule='static',
                                            chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_elements \
                                            if thread_index == \
                                            num_threads - 1 else \
                                            start + chunk_size
                                        dest_start = thread_index * \
                                            num_dataset_genes
                                        dest_end = \
                                            dest_start + num_dataset_genes
                                        sum[dest_start:dest_end] = 0
                                        sum_of_squares[dest_start:dest_end] = 0
                                        temp_nonzero_count[dest_start:dest_end] = 0
                                        for i in range(start, end):
                                            gene = indices[i]
                                            gene_index = dest_start + gene
                                            value = <long> data[i]
                                            sum[gene_index] += value
                                            sum_of_squares[gene_index] += \
                                                value * value
                                            temp_nonzero_count[gene_index] += 1
                                else:
                                    # Partition the work by cells
                                    chunk_size = num_cells // num_threads
                                    for thread_index in prange(
                                            num_threads,
                                            num_threads=num_threads,
                                            schedule='static',
                                            chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_cells \
                                            if thread_index == \
                                            num_threads - 1 else \
                                            start + chunk_size
                                        dest_start = thread_index * \
                                            num_dataset_genes
                                        dest_end = \
                                            dest_start + num_dataset_genes
                                        sum[dest_start:dest_end] = 0
                                        sum_of_squares[
                                            dest_start:dest_end] = 0
                                        temp_nonzero_count[dest_start:dest_end] = 0
                                        for j in range(start, end):
                                            cell = cell_indices[j]
                                            for i in range(
                                                    indptr[cell],
                                                    indptr[cell + 1]):
                                                gene = indices[i]
                                                gene_index = \
                                                    dest_start + gene
                                                value = <long> data[i]
                                                sum[gene_index] += value
                                                sum_of_squares[
                                                    gene_index] += \
                                                    value * value
                                                temp_nonzero_count[gene_index] += 1
                                
                                # Calculate means and variances by
                                # aggregating the sums and squared sums
                                # across threads
                                for gene in prange(
                                        num_dataset_genes,
                                        num_threads=num_threads):
                                    total_sum = 0
                                    total_sum_of_squares = 0
                                    total_nonzero_count = 0
                                    for thread_index in range(num_threads):
                                        gene_index = thread_index * \
                                            num_dataset_genes + gene
                                        total_sum = \
                                            total_sum + sum[gene_index]
                                        total_sum_of_squares = \
                                            total_sum_of_squares + \
                                            sum_of_squares[gene_index]
                                        total_nonzero_count = \
                                            total_nonzero_count + \
                                            temp_nonzero_count[gene_index]
                                    mean[gene] = total_sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * total_sum_of_squares -
                                        total_sum * total_sum)
                                    nonzero_count[gene] = total_nonzero_count
                        finally:
                            free(nonzero_count_buffer)
                    ''')['sparse_mean_var_minor_axis'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_indices=cell_indices, num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes, sum=sum,
                        sum_of_squares=sum_of_squares, mean=mean, var=var,
                        nonzero_count=nonzero_count, num_threads=num_threads)
            else:
                if cell_mask is None:
                    cell_mask = np.array([], dtype=bool)
                    num_cells = X.shape[0]
                else:
                    num_cells = cell_mask.sum()
                cython_inline(rf'''
                    from cython.parallel cimport prange
                    
                    def sparse_mean_var_major_axis(
                            const {cython_type(X.dtype)}[::1] data,
                            const {cython_type(X.indices.dtype)}[::1] indices,
                            const {cython_type(X.indptr.dtype)}[::1] indptr,
                            char[::1] cell_mask,
                            const long num_cells,
                            const int num_dataset_genes,
                            double[::1] mean,
                            double[::1] var,
                            int[::1] nonzero_count,
                            const unsigned num_threads):
                    
                        cdef long i, value, sum, sum_of_squares
                        cdef int gene
                        cdef double inv_num_cells = 1.0 / num_cells, \
                            inv_num_pairs_of_cells = \
                            1.0 / (num_cells * (num_cells - 1))
                        
                        if num_threads == 1:
                            if cell_mask.shape[0] == 0:
                                for gene in range(num_dataset_genes):
                                    # Calculate the sum and squared sum for
                                    # this gene, across cells with non-zero
                                    # counts for the gene
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        value = <long> data[i]
                                        sum += value
                                        sum_of_squares += value * value
                                        nonzero_count[gene] += 1
                                    # Calculate the mean and variance from the
                                    # sum and squared sum, including the
                                    # contribution from zero elements
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                            else:
                                # As above, but only include cells where
                                # `cell_mask` is `True`
                                for gene in range(num_dataset_genes):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        if cell_mask[indices[i]]:
                                            value = <long> data[i]
                                            sum += value
                                            sum_of_squares += value * value
                                            nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                        else:
                            if cell_mask.shape[0] == 0:
                                for gene in prange(num_dataset_genes,
                                                   nogil=True,
                                                   num_threads=num_threads):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        value = <long> data[i]
                                        sum = sum + value
                                        sum_of_squares = \
                                            sum_of_squares + value * value
                                        nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                            else:
                                for gene in prange(num_dataset_genes,
                                                   nogil=True,
                                                   num_threads=num_threads):
                                    sum = 0
                                    sum_of_squares = 0
                                    nonzero_count[gene] = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        if cell_mask[indices[i]]:
                                            value = <long> data[i]
                                            sum = sum + value
                                            sum_of_squares = \
                                                sum_of_squares + value * value
                                            nonzero_count[gene] += 1
                                    mean[gene] = sum * inv_num_cells
                                    var[gene] = inv_num_pairs_of_cells * (
                                        num_cells * sum_of_squares - sum * sum)
                    ''')['sparse_mean_var_major_axis'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_mask=cell_mask, num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes, mean=mean,
                        var=var, nonzero_count=nonzero_count,
                        num_threads=num_threads)
            
            not_constant = var > 0
            y = np.log10(var[not_constant])
            x = np.log10(mean[not_constant])
            model = loess(x, y, span=span)
            model.fit()
            
            estimated_variance = np.empty(num_dataset_genes)
            estimated_variance[not_constant] = model.outputs.fitted_values
            estimated_variance[~not_constant] = 0
            estimated_stddev = np.sqrt(10 ** estimated_variance)
            clip_val = mean + estimated_stddev * np.sqrt(num_cells)
            
            batch_counts_sum = np.zeros(num_dataset_genes)
            squared_batch_counts_sum = np.zeros(num_dataset_genes)
            if is_CSR:
                batch_counts_sum = np.empty(num_dataset_genes)
                squared_batch_counts_sum = np.empty(num_dataset_genes)
                batch_counts_sum_int = \
                    np.empty(num_dataset_genes * num_threads, dtype=int)
                squared_batch_counts_sum_int = \
                    np.empty(num_dataset_genes * num_threads, dtype=int)
                num_out_of_range = \
                    np.empty(num_dataset_genes * num_threads, dtype=int)
                # noinspection PyUnboundLocalVariable
                cython_inline(rf'''
                    from cython.parallel cimport prange
                    
                    def clipped_sum(const {cython_type(X.dtype)}[::1] data,
                                    const {cython_type(X.indices.dtype)}[::1]
                                        indices,
                                    const {cython_type(X.indptr.dtype)}[::1]
                                        indptr,
                                    const int num_cells,
                                    const int num_dataset_genes,
                                    const long[::1] cell_indices,
                                    const double[::1] clip_val,
                                    long[::1] batch_counts_sum_int,
                                    long[::1] squared_batch_counts_sum_int,
                                    long[::1] num_out_of_range,
                                    double[::1] batch_counts_sum,
                                    double[::1] squared_batch_counts_sum,
                                    const unsigned num_threads):
                        cdef long i, j, value, start, end, chunk_size, \
                            total_batch_counts_sum, \
                            total_squared_batch_counts_sum, \
                            total_num_out_of_range
                        cdef int cell, gene, dest_start, dest_end, gene_index
                        cdef unsigned thread_index
                        
                        # Key insight: the things we're summing are integers
                        # except when `value > clip_val[gene]`, in which case
                        # we are adding a (floating-point) constant, so keep
                        # track of this case separately and add it at the end,
                        # to minimize floating-point error
                        
                        if num_threads == 1:
                            batch_counts_sum_int[:] = 0
                            squared_batch_counts_sum_int[:] = 0
                            num_out_of_range[:] = 0
                            if cell_indices.shape[0] == 0:
                                num_cells = indptr.shape[0] - 1
                                for cell in range(num_cells):
                                    for i in range(indptr[cell], indptr[cell + 1]):
                                        gene = indices[i]
                                        value = <long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range[gene] += 1
                                        else:
                                            batch_counts_sum_int[gene] += value
                                            squared_batch_counts_sum_int[gene] += \
                                                value ** 2
                            else:
                                for j in range(cell_indices.shape[0]):
                                    cell = cell_indices[j]
                                    for i in range(indptr[cell], indptr[cell + 1]):
                                        gene = indices[i]
                                        value = <long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range[gene] += 1
                                        else:
                                            batch_counts_sum_int[gene] += value
                                            squared_batch_counts_sum_int[gene] += \
                                                value ** 2
                            for gene in range(num_dataset_genes):
                                batch_counts_sum[gene] = \
                                    batch_counts_sum_int[gene] + \
                                    num_out_of_range[gene] * clip_val[gene]
                                squared_batch_counts_sum[gene] = \
                                    squared_batch_counts_sum_int[gene] + \
                                    num_out_of_range[gene] * clip_val[gene] * \
                                    clip_val[gene]
                        else:
                            chunk_size = num_cells // num_threads
                            with nogil:
                                if cell_indices.shape[0] == 0:
                                    for thread_index in prange(
                                            num_threads, num_threads=num_threads,
                                            schedule='static', chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_cells if thread_index == num_threads - 1 \
                                            else start + chunk_size
                                        dest_start = thread_index * num_dataset_genes
                                        dest_end = dest_start + num_dataset_genes
                                        batch_counts_sum_int[dest_start:dest_end] = 0
                                        squared_batch_counts_sum_int[dest_start:dest_end] = 0
                                        num_out_of_range[dest_start:dest_end] = 0
                                        for cell in range(start, end):
                                            for i in range(indptr[cell], indptr[cell + 1]):
                                                gene = indices[i]
                                                gene_index = dest_start + gene
                                                value = <long> data[i]
                                                if value > clip_val[gene]:
                                                    num_out_of_range[gene_index] += 1
                                                else:
                                                    batch_counts_sum_int[gene_index] += value
                                                    squared_batch_counts_sum_int[gene_index] += value ** 2
                                else:
                                    for thread_index in prange(
                                            num_threads, num_threads=num_threads,
                                            schedule='static', chunksize=1):
                                        start = thread_index * chunk_size
                                        end = num_cells if thread_index == num_threads - 1 \
                                            else start + chunk_size
                                        dest_start = thread_index * num_dataset_genes
                                        dest_end = dest_start + num_dataset_genes
                                        batch_counts_sum_int[dest_start:dest_end] = 0
                                        squared_batch_counts_sum_int[dest_start:dest_end] = 0
                                        num_out_of_range[dest_start:dest_end] = 0
                                        for j in range(start, end):
                                            cell = cell_indices[j]
                                            for i in range(indptr[cell], indptr[cell + 1]):
                                                gene = indices[i]
                                                gene_index = dest_start + gene
                                                value = <long> data[i]
                                                if value > clip_val[gene]:
                                                    num_out_of_range[gene_index] += 1
                                                else:
                                                    batch_counts_sum_int[gene_index] += value
                                                    squared_batch_counts_sum_int[gene_index] += value ** 2
                                
                                for gene in prange(num_dataset_genes,
                                                   num_threads=num_threads):
                                    total_batch_counts_sum = 0
                                    total_squared_batch_counts_sum = 0
                                    total_num_out_of_range = 0
                                    for thread_index in range(num_threads):
                                        gene_index = thread_index * \
                                            num_dataset_genes + gene
                                        total_batch_counts_sum = \
                                            total_batch_counts_sum + \
                                            batch_counts_sum_int[gene_index]
                                        total_squared_batch_counts_sum = \
                                            total_squared_batch_counts_sum + \
                                            squared_batch_counts_sum_int[gene_index]
                                        total_num_out_of_range = \
                                            total_num_out_of_range + \
                                            num_out_of_range[gene_index]
                                    batch_counts_sum[gene] = \
                                        total_batch_counts_sum + \
                                        total_num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        total_squared_batch_counts_sum + \
                                        total_num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                    ''')['clipped_sum'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        num_cells=num_cells,
                        num_dataset_genes=num_dataset_genes,
                        cell_indices=cell_indices, clip_val=clip_val,
                        batch_counts_sum_int=batch_counts_sum_int,
                        squared_batch_counts_sum_int=squared_batch_counts_sum_int,
                        num_out_of_range=num_out_of_range,
                        batch_counts_sum=batch_counts_sum,
                        squared_batch_counts_sum=squared_batch_counts_sum,
                        num_threads=num_threads)
            else:
                cython_inline(rf'''
                    from cython.parallel cimport prange
                    
                    def clipped_sum(const {cython_type(X.dtype)}[::1] data,
                                    const {cython_type(X.indices.dtype)}[::1]
                                        indices,
                                    const {cython_type(X.indptr.dtype)}[::1]
                                        indptr,
                                    char[::1] cell_mask,
                                    const double[::1] clip_val,
                                    double[::1] batch_counts_sum,
                                    double[::1] squared_batch_counts_sum,
                                    const unsigned num_threads):
                        cdef int cell, gene, num_genes = indptr.shape[0] - 1
                        cdef long i, value, batch_counts_sum_int, \
                            squared_batch_counts_sum_int, num_out_of_range
                        
                        if num_threads == 1:
                            if cell_mask.shape[0] == 0:
                                for gene in range(num_genes):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        value = <long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range += 1
                                        else:
                                            batch_counts_sum_int += value
                                            squared_batch_counts_sum_int += \
                                                value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                            else:
                                for gene in range(num_genes):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        cell = indices[i]
                                        if cell_mask[cell]:
                                            value = <long> data[i]
                                            if value > clip_val[gene]:
                                                num_out_of_range += 1
                                            else:
                                                batch_counts_sum_int += value
                                                squared_batch_counts_sum_int += \
                                                    value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                        else:
                            if cell_mask.shape[0] == 0:
                                for gene in prange(num_genes, nogil=True,
                                                   num_threads=num_threads):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        value = <long> data[i]
                                        if value > clip_val[gene]:
                                            num_out_of_range = num_out_of_range + 1
                                        else:
                                            batch_counts_sum_int = \
                                                batch_counts_sum_int + value
                                            squared_batch_counts_sum_int = \
                                                squared_batch_counts_sum_int + \
                                                value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                            else:
                                for gene in prange(num_genes, nogil=True,
                                                   num_threads=num_threads):
                                    batch_counts_sum_int = 0
                                    squared_batch_counts_sum_int = 0
                                    num_out_of_range = 0
                                    for i in range(indptr[gene], indptr[gene + 1]):
                                        cell = indices[i]
                                        if cell_mask[cell]:
                                            value = <long> data[i]
                                            if value > clip_val[gene]:
                                                num_out_of_range = num_out_of_range + 1
                                            else:
                                                batch_counts_sum_int = \
                                                    batch_counts_sum_int + value
                                                squared_batch_counts_sum_int = \
                                                    squared_batch_counts_sum_int + \
                                                    value ** 2
                                    batch_counts_sum[gene] = \
                                        batch_counts_sum_int + \
                                        num_out_of_range * clip_val[gene]
                                    squared_batch_counts_sum[gene] = \
                                        squared_batch_counts_sum_int + \
                                        num_out_of_range * \
                                        clip_val[gene] * clip_val[gene]
                    ''')['clipped_sum'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        cell_mask=cell_mask, clip_val=clip_val,
                        batch_counts_sum=batch_counts_sum,
                        squared_batch_counts_sum=squared_batch_counts_sum,
                        num_threads=num_threads)
            norm_gene_var = pl.Series(
                (1 / ((num_cells - 1) * np.square(estimated_stddev))) *
                ((num_cells * np.square(mean)) + squared_batch_counts_sum -
                 2 * batch_counts_sum * mean))
            # If `min_cells` is non-zero, set variances to null for genes
            # with a non-zero count less than `min_cells`
            if min_cells:
                norm_gene_var = norm_gene_var\
                    .set(pl.Series(nonzero_count < min_cells), None)
            # If there are multiple datasets, `norm_gene_var` is currently with
            # respect to the genes in `dataset.var_names`; map back to the
            # genes in `genes_in_any_dataset`, filling with nulls
            if others:
                norm_gene_var = norm_gene_var[gene_indices]
            norm_gene_vars.append(norm_gene_var)
        
        rank = pl.exclude('gene').rank('min', descending=True)
        final_rank = pl.struct(
            ('median_rank', 'nbatches') if flavor == 'seurat_v3' else
            ('nbatches', 'median_rank')).rank('ordinal')
        # note: the expression for `median_rank` can be replaced by
        # `pl.median_horizontal(pl.exclude('gene'))` once polars implements it
        hvgs = pl.DataFrame([genes_in_any_dataset] + norm_gene_vars)\
            .lazy()\
            .pipe(lambda df: df.drop_nulls(pl.selectors.exclude('gene'))
                             if min_cells or others else df)\
            .with_columns(pl.when(rank <= num_genes).then(rank))\
            .with_columns(nbatches=pl.sum_horizontal(pl.exclude('gene')
                                                     .is_null()),
                          median_rank=pl.concat_list(pl.exclude('gene'))
                                      .explode()
                                      .median().over(pl.int_range(
                                          pl.len(), dtype=pl.Int32)))\
            .select('gene', (final_rank <= num_genes).alias(hvg_column),
                    pl.when(final_rank <= num_genes).then(final_rank)
                    .alias(rank_column))\
            .collect()
        # Return a new SingleCell dataset (or a tuple of datasets, if others
        # is non-empty) containing the highly variable genes
        for dataset_index, dataset in enumerate(datasets):
            new_var = dataset._var\
                .join(hvgs.rename({'gene': dataset.var_names.name}),
                      on=dataset.var_names.name, how='left')\
                .with_columns(pl.col(hvg_column).fill_null(False))
            datasets[dataset_index] = \
                SingleCell(X=dataset._X, obs=dataset._obs, var=new_var,
                           obsm=dataset._obsm, varm=dataset._varm,
                           uns=dataset._uns)
        return tuple(datasets) if others else datasets[0]
    
    def normalize(self,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  method: Literal['PFlog1pPF', 'log1pPF',
                                  'logCP10k'] = 'PFlog1pPF',
                  allow_float: bool = False,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Normalize this SingleCell dataset's counts.
        
        By default, uses the PFlog1pPF method introduced in Booeshaghi et al.
        2022 (biorxiv.org/content/10.1101/2022.05.06.490859v1.full). With
        `method='logCP10k'`, it matches the default settings of Seurat's
        `NormalizeData` function, aside from differences in floating-point
        error.
        
        PFlog1pPF is a three-step process:
        1. Divide each cell's counts by a "size factor", namely the total
        number of counts for that cell, divided by the mean number of counts
        across all cells. Booeshaghi et al. call this process, which performs
        rowwise division of a matrix `X` by the vector
        `X.sum(axis=1) / X.sum(axis=1).mean()`, "proportional fitting" (PF).
        2. Take the logarithm of each entry plus 1, i.e. `log1p()`.
        3. Run an additional round of proportional fitting.
        
        If method='log1pPF', only performs steps 1 and 2 and leaves out step 3.
        Booeshaghi et al. call this method "log1pPF". Ahlmann-Eltze and Huber
        2023 (nature.com/articles/s41592-023-01814-1) recommend this method and
        argue that it outperforms log(CPM) normalization. However, Booeshaghi
        et al. note that log1pPF does not fully normalize for read depth,
        because the log transform of step 2 partially undoes the normalization
        introduced by step 1. This is the reasoning behind their use of step 3:
        to restore full depth normalization. By default, scanpy's
        normalize_total() uses a variation of proportional fitting that divides
        by the median instead of the mean, so it's closest to method='log1pPF'.
        
        If method='logCP10k', uses 10,000 for the denominator of the size
        factors instead of `X.sum(axis=1).mean()`, and leaves out step 3. This
        method is not recommended because it implicitly assumes an
        unrealistically large amount of overdispersion, and performs worse than
        log1pPF and PFlog1pPF in Ahlmann-Eltze and Huber and Booeshaghi et
        al.'s benchmarks. Seurat's NormalizeData() uses logCP10k normalization.
        
        Args:
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will still be normalized, but
                       will not count towards the calculation of the mean total
                       count across cells when `method` is `'PFlog1pPF'` or
                       `'log1pPF'`. Has no effect when `method` is
                       `'logCP10k'`.
            method: the normalization method to use (see above)
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            num_threads: the number of threads to use when normalizing. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        
        Returns:
            A new SingleCell dataset with the normalized counts, and
            `uns['normalized']` set to `True`.
        """
        # Check that `self` is QCed and not already normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() (and "
                "possibly hvg()) before normalize()? Set uns['QCed'] = True "
                "or run with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = \
                "uns['normalized'] is True; did you already run normalize()?"
            raise ValueError(error_message)
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `method` is one of the three valid methods
        if method not in ('PFlog1pPF', 'log1pPF', 'logCP10k'):
            error_message = \
                "method must be one of 'PFlog1pPF', 'log1pPF', or 'logCP10k'"
            raise ValueError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # If `allow_float=False`, raise an error if `X` is floating-point
        X = self._X
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'normalize() requires raw counts but X.dtype is '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)
        # Step 1
        rowsums = X.sum(axis=1)
        inverse_size_factors = np.empty_like(rowsums, dtype=float) \
            if np.issubdtype(rowsums.dtype, np.integer) else rowsums
        # Note: QCed cells will have null as the batch, and over() treats
        # null as its own category, so effectively all cells failing QC
        # will be treated as their own batch. This doesn't matter since we
        # never use the counts for these cells anyway.
        np.divide(10_000 if method == 'logCP10k' else
                  rowsums.mean() if QC_column is None else
                  rowsums[QC_column].mean(), rowsums, inverse_size_factors)
        X = sparse_matrix_vector_op(X, '*', inverse_size_factors, axis=0,
                                    return_dtype=float,
                                    num_threads=num_threads)
        # Step 2
        if num_threads == 1:
            np.log1p(X.data, X.data)
        else:
            cython_inline(f'''
                from cython.parallel cimport prange
                from libc.math cimport log1p
                
                def log1p_parallel(double[::1] data,
                                   const unsigned num_threads):
                    cdef long i
                    
                    if num_threads == 1:
                        for i in range(data.shape[0]):
                            data[i] = log1p(data[i])
                    else:
                        for i in prange(data.shape[0], nogil=True,
                                        num_threads=num_threads):
                            data[i] = log1p(data[i])
                ''')['log1p_parallel'](X.data, num_threads)
        # Step 3
        if method == 'PFlog1pPF':
            rowsums = X.sum(axis=1)
            inverse_size_factors = rowsums
            np.divide(rowsums.mean() if QC_column is None else
                      rowsums[QC_column].mean(), rowsums, inverse_size_factors)
            sparse_matrix_vector_op(X, '*', inverse_size_factors, axis=0,
                                    inplace=True, return_dtype=float,
                                    num_threads=num_threads)
        sc = SingleCell(X=X, obs=self._obs, var=self._var, obsm=self._obsm,
                        varm=self._varm, uns=self._uns)
        sc._uns['normalized'] = True
        return sc
    
    def PCA(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            seed: int | np.integer = 0,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components using irlba, the package used by Seurat.
        Operates on normalized counts (see `normalize()`).
        
        Install irlba with:
        
        from ryp import r
        r('install.packages("irlba", type="source")')
        
        IMPORTANT: if you already have a copy of irlba from CRAN (e.g.
        installed with Seurat), you will get the error:
        
        RuntimeError: in irlba(X, 50, verbose = FALSE) :
          function 'as_cholmod_sparse' not provided by package 'Matrix'
        
        This error will go away if you install irlba from source as described
        above.
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       PCs set to NaN. When `others` is specified, `QC_column`
                       can be a length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            hvg_column: a Boolean column of var indicating the highly variable
                        genes. Can be a column name, a polars expression, a
                        polars Series, a 1D NumPy array, or a function that
                        takes in this SingleCell dataset and returns a polars
                        Series or 1D NumPy array. Set to `None` to use all
                        genes. When `others` is specified, `hvg_column`
                        can be a length-`1 + len(others)` sequence of columns,
                        expressions, Series, functions, or `None` for each
                        dataset (for `self`, followed by each dataset in
                        `others`).
            PC_key: the key of obsm where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            seed: the random seed to use for irlba when computing PCs, via R's
                  set.seed() function
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to set the verbose flag in irlba
            num_threads: the number of threads to use when subsetting the count
                         matrix/matrices prior to PCA. PCA will run
                         single-threaded regardless of the value of
                         `num_threads`, because it does not parallelize
                         efficiently. Set `num_threads=-1` to use all
                         available cores (as determined by `os.cpu_count()`),
                         or leave unset to use `single_cell.get_num_threads()`
                         cores.
        
        Returns:
            A new SingleCell dataset where obsm contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA` function, which requires `ScaleData` to be
            run first, this function does not require the data to be scaled
            beforehand. Instead, it scales the data implicitly. It does this by
            providing the standard deviation and mean of the data to `irlba()`
            via its `scale` and `center` arguments, respectively. This approach
            is much more computationally efficient than explicit scaling, and
            is also taken by Seurat's internal (and currently unused)
            `RunPCA_Sparse` function, which this function is based on.
        """
        from ryp import r, to_py, to_r
        from sklearn.utils.sparsefuncs import mean_variance_axis
        r('suppressPackageStartupMessages(library(irlba))')
        # Check that all elements of `others` are SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Check that all datasets are normalized
        suffix = ' for at least one dataset' if others else ''
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        # Raise an error if `X` has an integer data type for any dataset
        for dataset in datasets:
            if np.issubdtype(dataset._X.dtype, np.integer):
                error_message = (
                    f'PCA() requires raw counts, but X.dtype is '
                    f'{str(dataset._X.dtype)!r}, an integer data type'
                    f'{suffix}; did you forget to run normalize() before '
                    f'PCA()?')
                raise TypeError(error_message)
        # Get `QC_column` (if not `None`) and `hvg_column` from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            allow_None=False,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and normalize()) '
                         f'before PCA()?')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `PC_key` is not already in obsm, unless `overwrite=True`
        check_type(PC_key, 'PC_key', str, 'a string')
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Get the matrix to compute PCA across: a CSC array of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use X[np.ix_(rows, columns)] as a faster,
        # more memory-efficient alternative to X[rows][:, columns]. Use CSC
        # rather than CSR because irlba has a fast C-based implementation for
        # CSC.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = self._var.filter(hvg_columns[0]).to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [dataset._X[np.ix_(QC_col.to_numpy(),
                                                hvg_col.to_numpy())]
                              if QC_col is not None else
                              dataset._X[:, hvg_col.to_numpy()]
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        X = vstack(Xs, format='csc')
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        del Xs
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        # Run PCA with irlba (github.com/bwlewis/irlba/blob/master/R/irlba.R)
        # This section is adapted from
        # github.com/satijalab/seurat/blob/master/R/integration.R#L7276-L7317
        # Note: totalvar doesn't seem to be used by irlba, maybe a Seurat bug?
        # Note: mean_variance_axis() can be replaced by Welford's algorithm
        center, feature_var = mean_variance_axis(X, axis=0)
        scale = np.sqrt(feature_var)
        scale.clip(min=1e-8, out=scale)
        to_r(X, '.SingleCell.X')
        try:
            to_r(center, '.SingleCell.center')
            try:
                to_r(scale, '.SingleCell.scale')
                try:
                    r(f'set.seed({seed})')
                    r(f'.SingleCell.PCs = irlba(.SingleCell.X, {num_PCs}, '
                      f'verbose={str(verbose).upper()}, '
                      f'scale=.SingleCell.scale, '
                      f'center=.SingleCell.center)')
                    try:
                        PCs = to_py('.SingleCell.PCs$u', format='numpy') * \
                              to_py('.SingleCell.PCs$d', format='numpy')
                    finally:
                        r('rm(.SingleCell.PCs)')
                finally:
                    r('rm(.SingleCell.scale)')
            finally:
                r('rm(.SingleCell.center)')
        finally:
            r('rm(.SingleCell.X)')
        # Store each dataset's PCs in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with NaN
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            else:
                dataset_PCs = np.ascontiguousarray(dataset_PCs)
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns)
        return tuple(datasets) if others else datasets[0]
    
    def neighbors(self,
                  *,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  PC_key: str = 'PCs',
                  neighbors_key: str = 'neighbors',
                  num_neighbors: int | np.integer = 20,
                  num_clusters: int | np.integer | None = None,
                  num_probes: int | np.integer | None = None,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate the k-nearest neighbors of each cell.
        
        This function is intended to be run after `PCA()`; by default, it uses
        `obsm['PCs']` as the input to the nearest-neighbors calculation. It
        uses an approximate algorithm based on an inverted file (IVF), as
        implemented in the Facebook AI Similarity Search (FAISS) library.
        
        It stores the output in `obsm['neighbors']` as a `len(obs)` ×
        `num_neighbors` NumPy array, where `obsm['neighbors'][i, j]` stores the
        index of the `i`th cell's `j + 1`th nearest neighbor. This differs from
        Scanpy and Seurat, which use a less compact sparse matrix
        representation instead.
        
        Args:
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       nearest neighbors set to NaN.
            PC_key: the key of obsm containing the principal components
                    calculated with `PCA()`, to use as the input for the
                    nearest-neighbors calculation
            neighbors_key: the key of obsm where the nearest neighbors will be
                           stored
            num_neighbors: the number of nearest neighbors to report for each
                           cell
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be between 1 and the number of cells. If
                          `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            seed: the random seed to use for nearest-neighbor finding
            overwrite: if `True`, overwrite `neighbors_key` if already present
                       in obsm, instead of raising an error
            verbose: whether to print details of the nearest-neighbors search
            num_threads: the number of cores to run nearest-neighbor finding
                         on. Set `num_threads=-1` to use all available cores
                         (as determined by `os.cpu_count()`), or leave unset to
                         use `single_cell.get_num_threads()` cores.
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors added as `obsm[neighbors_key]`. For instance, if
            `num_neighbors=2` and the 0th cell's nearest neighbors are the 4th
            cell and the 6th cell, then `obsm[neighbors_key][0]` will be
            `np.array([4, 6])`. Note that if `QC_column` is not `None`, these
            integer indices are with respect to QCed cells, not all cells.
        """
        with ignore_sigint():
            import faiss
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `PC_key` is the name of a key in obsm
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += \
                    '; did you forget to run PCA() before neighbors()?'
            raise ValueError(error_message)
        # Get PCs, and check that they are float64 and C-contiguous
        PCs = self._obsm[PC_key]
        if PCs.dtype != float:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float64'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
            raise ValueError(error_message)
        # Subset PCs to QCed cells only, if `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `neighbors_key` is a string and, unless `overwrite=True`,
        # not already a key in obsm
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if not overwrite and neighbors_key in self._obsm:
            error_message = (
                f'neighbors_key {neighbors_key!r} is already a key of obsm; '
                f'did you already run neighbors()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Check that `num_neighbors` is between 1 and the number of cells
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        num_cells = len(PCs)
        if not 1 <= num_neighbors <= num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be ≥ 1 and ≤ '
                f'the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells),
                                           num_cells / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters <= num_cells:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and ≤ '
                    f'the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be ≥ 1 and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = SingleCell._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        # Calculate each cell's `num_neighbors + 1`-nearest neighbors with
        # faiss, where the `+ 1` is for the cell itself
        dim = PCs.shape[1]
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        # noinspection PyArgumentList
        index.train(PCs)
        # noinspection PyArgumentList
        index.add(PCs)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = index.search(PCs, num_neighbors + 1)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = PCs[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} cells '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) did '
                          f'not have enough neighbors with {index.nprobe:,} '
                          f'probes; re-running nearest-neighbors finding for '
                          f'these cells with {num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = \
                    index.search(needs_update_X, num_neighbors + 1)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        if verbose:
            # noinspection PyUnresolvedReferences
            percent = \
                (nearest_neighbor_indices[:, 0] == range(num_cells)).mean()
            print(f'{100 * percent:.3f}% of cells are correctly detected as '
                  f'their own nearest neighbors (a measure of the quality of '
                  f'the k-nearest neighbors search)')
        # Remove self-neighbors from each cell's list of nearest neighbors.
        # These are almost always in the 0th column, but occasionally later due
        # to the inaccuracy of the nearest-neighbors search. This leaves us
        # with `num_neighbors + num_extra_neighbors` nearest neighbors.
        remove_self_neighbors = cython_inline(r'''
            from cython.parallel cimport prange
        
            def remove_self_neighbors(long[:, ::1] neighbors,
                                      const unsigned num_threads):
                cdef int i, j, num_cells = neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1]
                
                if num_threads == 1:
                    for i in range(num_cells):
                        # If the cell is its own nearest neighbor (almost always), skip
                        if neighbors[i, 0] == i:
                            continue
                        # Find the position where the cell is listed as its own
                        # self-neighbor
                        for j in range(1, num_neighbors):
                            if neighbors[i, j] == i:
                                break
                        # Shift all neighbors before it to the right, overwriting it
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                else:
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        if neighbors[i, 0] == i:
                            continue
                        for j in range(1, num_neighbors):
                            if neighbors[i, j] == i:
                                break
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                ''')['remove_self_neighbors']
        remove_self_neighbors(nearest_neighbor_indices, num_threads)
        nearest_neighbor_indices = nearest_neighbor_indices[:, 1:]
        # If `QC_column` is not `None`, back-project from QCed cells to all
        # cells, filling with NaN
        if QC_column is not None:
            nearest_neighbor_indices_QCed = nearest_neighbor_indices
            nearest_neighbor_indices = np.full(
                (len(self), nearest_neighbor_indices_QCed.shape[1]), np.nan)
            # noinspection PyUnboundLocalVariable
            nearest_neighbor_indices[QCed_NumPy] = \
                nearest_neighbor_indices_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm |
                               {neighbors_key: nearest_neighbor_indices},
                          varm=self._varm, uns=self._uns)
    
    def cluster(self,
                *,
                QC_column: SingleCellColumn | None |
                           Sequence[SingleCellColumn | None] = 'passed_QC',
                neighbors_key: str = 'neighbors',
                cell_type_column: str = 'cell_type',
                overwrite: bool = False,
                verbose: bool = True,
                num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Loosely based on the build_snn_graph function from
        github.com/libscran/scran_graph_cluster.
        
        Args:
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       cell-type labels set to NaN.
            neighbors_key: the key of obsm containing the nearest-neighbor
                           indices calculated with `neighbors()`, to use as the
                           input for cell-type clustering
            cell_type_column: the name of an integer column to be added to
                              obs indicating the cell-type labels
            overwrite: if `True`, overwrite `embedding_key` if already present
                       in obsm, instead of raising an error
            verbose: whether to print details of the cell-type clustering
            num_threads: the number of cores to run cell-type clustering on.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        Returns:
            A new SingleCell dataset where `obs[cell_type_column]` contains an
            integer cell-type label for each cell.
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += \
                    '; did you forget to run neighbors() before cluster()?'
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}].dtype is '
                f'{nearest_neighbor_indices.dtype!r}, but must be integer')
            raise TypeError(error_message)
        # Subset neighbor indices to QCed cells only, if `QC_column` is not
        # `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `cell_type_column` is a string and, unless
        # `overwrite=True`, not already in obs
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        if not overwrite and 'cell_type_column' in self._obs:
            error_message = (
                f'cell_type_column {cell_type_column!r} is already a column '
                f'of obs; did you already run cluster()? Set overwrite=True '
                f'to overwrite.')
            raise ValueError(error_message)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Compute the SNN graph from the KNN graph
        num_cells, num_neighbors = nearest_neighbor_indices.shape
        indptr = np.empty(num_cells + 1, dtype=int)
        num_shared_neighbors = np.zeros((num_threads, num_cells),
                                        dtype=np.int32)
        indices, data = cython_inline(r'''
            import numpy as np
            cimport numpy as np
            from cython.parallel cimport threadid, prange
            from libc.string cimport memcpy
            from libcpp.algorithm cimport sort
            from libcpp.vector cimport vector
            
            def compute_SNN(const long[:, :] neighbors,
                            long[::1] indptr,
                            int[:, ::1] num_shared_neighbors,
                            const unsigned num_threads):
                cdef int i, j, k, thread_id, num_unique_ks, size, \
                    num_cells = neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1], \
                    twice_num_neighbors = 2 * num_neighbors
                cdef long nnz, start
                cdef vector[vector[int]] reverse_neighbors, shared_neighbors
                cdef vector[vector[double]] shared_neighbor_weights
                cdef np.ndarray[np.int32_t, ndim=1] indices
                cdef np.ndarray[np.float64_t, ndim=1] data
                cdef int[::1] indices_view
                cdef double[::1] data_view
                
                # Allocate space for shared nearest neighbors and weights
                shared_neighbors.resize(num_cells)
                shared_neighbor_weights.resize(num_cells)
                
                # Build the "reverse" mapping: which cells have each cell as a neighbor
                reverse_neighbors.resize(num_cells)
                for i in range(num_cells):
                    for j in range(num_neighbors):
                        reverse_neighbors[neighbors[i, j]].push_back(i)
                
                # Process each cell in parallel TODO make single-threaded version
                for i in prange(num_cells, num_threads=num_threads, nogil=True):
                    thread_id = threadid()
                    
                    # For this cell `i`, find all pairs of cells `j` and `k`
                    # where `j` is a shared nearest neighbor of `i` and `k`.
                    # Do this by looking up all neighbors (`j`) of this cell
                    # (`i`), then looking up each of these neighbors' reverse
                    # neighbors (`k`). Require `k > i` to avoid duplicates due
                    # to symmetry.
                    for j in range(num_neighbors):
                        for k in reverse_neighbors[neighbors[i, j]]:
                            if k > i:
                                # Make a list of the unique `k`s...
                                if num_shared_neighbors[thread_id, k] == 0:
                                    shared_neighbors[i].push_back(k)
                                # ...and the number of times each `k` has a
                                # shared nearest neighbor with `i`. Note:
                                # `num_shared_neighbors[thread_id]` has one
                                # element per cell, and almost all elements are
                                # 0!
                                num_shared_neighbors[thread_id, k] += 1
                    
                    # Sort the `k`s
                    sort(shared_neighbors[i].begin(), shared_neighbors[i].end())
                    
                    # Store the number of unique `k`s in `indptr` (we will take
                    # the cumsum later)
                    num_unique_ks = shared_neighbors[thread_id].size()
                    indptr[i + 1] = num_unique_ks
                    
                    # Calculate the SNN weight between `i` and each `k`: the
                    # number of shared neighbors, divided by the total number
                    # of unique cells in `i` and `k`'s neighbor lists (which is
                    # twice the number of total neighbors, minus the number of
                    # shared neighbors). Note that since `shared_neighbors` is
                    # sorted, the elements of `shared_neighbor_weights` will
                    # correspond to the unique `k`s in sorted order.
                    shared_neighbor_weights[i].reserve(num_unique_ks)
                    for k in shared_neighbors[i]:
                        shared_neighbor_weights[i].push_back(
                            num_shared_neighbors[thread_id, k] /
                            (twice_num_neighbors -
                             num_shared_neighbors[thread_id, k]))
                        num_shared_neighbors[thread_id, k] = 0  # reset
            
                # Take the cumulative sum of the values in `indptr`; initialize the
                # first element to 0
                indptr[0] = 0
                for i in range(2, indptr.shape[0]):
                    indptr[i] += indptr[i - 1]
                
                # Allocate `indices` and `data`: their length is the sum of the
                # numbers of unique `k`s across all cells
                nnz = indptr[num_cells]
                indices = np.empty(nnz, dtype=np.int32)
                data = np.empty(nnz, dtype=np.float64)
                
                # Populate `indices` and `data` with `shared_neighbors` and
                # `shared_neighbor_weights`, respectively. Access the arrays via
                # memoryviews with C-contiguity specified to force Cython to avoid
                # generating slower code that accounts for stride (not sure if this
                # is necessary).
                indices_view = indices
                data_view = data
                for i in prange(num_cells, num_threads=num_threads, nogil=True):
                    start = indptr[i]
                    size = shared_neighbors[i].size()
                    # indices_view[start:end] = shared_neighbors[i][:]
                    memcpy(&indices_view[start], shared_neighbors[i].data(),
                           size * sizeof(int))
                    # data_view[start:end] = shared_neighbor_weights[i][:]
                    memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                           size * sizeof(double))
                
                return indices, data
            ''', boundscheck=True, initializedcheck=True,  # TODO
                                      warn_undeclared=False)['compute_SNN'](
                neighbors=nearest_neighbor_indices, indptr=indptr,
                num_shared_neighbors=num_shared_neighbors,
                num_threads=num_threads)
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(cell_type_labels),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          uns=self._uns)
    
    def harmonize(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  max_iter_harmony: int | np.integer = 10,
                  max_iter_clustering: int | np.integer | None = 20,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tol_harmony: int | float | np.integer | np.floating = 1e-4,
                  tol_clustering: int | float | np.integer |
                                  np.floating = 1e-5,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  theta: int | float | np.integer | np.floating = 2,
                  tau: int | float | np.integer | np.floating = 0,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0). Harmony was originally written
        in R (github.com/immunogenomics/harmony) but has two Python ports,
        harmony-pytorch (github.com/lilab-bcb/harmony-pytorch), which our
        implementation is based on, and harmonypy
        (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       Harmony embeddings set to NaN. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of obs indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of obsm containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of obsm where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm.
                          If not specified, take the minimum of 100 and
                          floor(number of cells / 30).
            max_iter_harmony: the maximum number of iterations to run Harmony
                              for, if convergence is not achieved. Defaults to
                              10, like the original harmony package,
                              harmony-pytorch, and harmonypy. Set to `None` to
                              use as many iterations as necessary to achieve
                              convergence.
            max_iter_clustering: the maximum number of iterations to run the
                                 clustering step within each Harmony iteration
                                 for, if convergence is not achieved. Defaults
                                 to 20 iterations, like the original harmony
                                 package and harmonypy; this differs from
                                 the default of 200 iterations used by
                                 harmony-pytorch. Set to `None` to use as many
                                 iterations as necessary to achieve
                                 convergence.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tol_harmony: the relative tolerance used to determine whether to
                         stop Harmony before `max_iter_harmony` iterations;
                         must be positive
            tol_clustering: the relative tolerance used to determine whether to
                            stop clustering before `max_iter_clustering`
                            iterations; must be positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            theta: the weight of the diversity penalty term in the Harmony
                   objective function; must be non-negative
            tau: the discounting factor on theta; must be non-negative. By
                 default, `tau = 0`, so there is no discounting.
            seed: the random seed for Harmony
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings added as obsm[Harmony_key]: `self`, followed by
            each dataset in `others`.
        """
        with ignore_sigint():
            import faiss
        # Check `others`
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `PC_key` is a key of obsm for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `Harmony_key` is a string and, unless `overwrite=True`,
        # not already in obsm for any dataset
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Check that `num_clusters`, `max_iter_harmony`, and
        # `max_iter_clustering` are `None` or a positive integer; if either max
        # iter argument is `None`, set it to `INT32_MAX`
        for parameter, parameter_name in (
                (num_clusters, 'num_clusters'),
                (max_iter_harmony, 'max_iter_harmony'),
                (max_iter_clustering, 'max_iter_clustering')):
            if parameter is not None:
                check_type(parameter, parameter_name, int,
                           'a positive integer')
                check_bounds(parameter, parameter_name, 1)
        if max_iter_harmony is None:
            max_iter_harmony = 2147483647
        if max_iter_clustering is None:
            max_iter_clustering = 2147483647
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        # Check that `tol_harmony` and `tol_clustering` are positive numbers,
        # and that `ridge_lambda`, `sigma`, `theta`, and `tau` are non-negative
        # numbers. If any is an integer, cast it to a float.
        for parameter, parameter_name in (
                (tol_harmony, 'tol_harmony'),
                (tol_clustering, 'tol_clustering')):
            check_type(parameter, parameter_name, (int, float),
                       'a positive number')
            check_bounds(parameter, parameter_name, 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma'),
                (theta, 'theta'), (tau, 'tau')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tol_harmony = float(tol_harmony)
        tol_clustering = float(tol_clustering)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        theta = float(theta)
        tau = float(tau)
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Concatenate PCs (`Z`) across datasets; get labels indicating which
        # rows of these concatenated PCs come from each dataset or batch.
        # Check that the PCs are float64 and C-contiguous.
        Z = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in Z:
            dtype = PCs.dtype
            if dtype != float:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float64')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
                raise ValueError(error_message)
        if QC_column is not None:
            Z = [PCs[QCed] if QCed is not None else PCs
                 for PCs, QCed in zip(Z, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, Z)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum()))
                    batch_index += 1
            batch_labels = np.concatenate(batch_labels)
        Z = np.concatenate(Z)
        
        # Run Harmony
        
        cython_functions = cython_inline(rf'''
            from cpython.exc cimport PyErr_CheckSignals
            from libcpp.cmath cimport abs, exp, pow, log, sqrt
            from scipy.linalg.cython_blas cimport dgemm, dgemv
            
            cdef inline void matrix_multiply(const double[:, ::1] A,
                                             const double[:, ::1] B,
                                             double[:, ::1] C,
                                             const bint transpose_A,
                                             const bint transpose_B,
                                             const double alpha,
                                             const double beta) noexcept nogil:
                # Flip A <-> B and shape[0] <-> shape[1] since our matrices are
                # C-major
                cdef int m, n, k, lda, ldb
                cdef char transA, transB
                if transpose_B:
                    m = B.shape[0]
                    k = B.shape[1]
                    lda = k
                    transA = b'T'
                else:
                    m = B.shape[1]
                    k = B.shape[0]
                    lda = m
                    transA = b'N'
                if transpose_A:
                    n = A.shape[1]
                    ldb = n
                    transB = b'T'
                else:
                    n = A.shape[0]
                    ldb = k
                    transB = b'N'
                dgemm(&transA, &transB, &m, &n, &k, <double*> &alpha,
                      <double*> &B[0, 0], &lda, <double*> &A[0, 0], &ldb,
                      <double*> &beta, &C[0, 0], &m)
            
            cdef inline void matrix_vector_multiply(
                    const double[:, ::1] A,
                    const double[::1] X,
                    double[::1] Y,
                    const bint transpose,
                    const double alpha,
                    const double beta) noexcept nogil:
                # Flip trans since our matrix is C-major
                cdef int m = A.shape[1], n = A.shape[0], incx = 1, incy = 1
                cdef char trans = b'N' if transpose else b'T'
                dgemv(&trans, &m, &n, <double*> &alpha, <double*> &A[0,0],
                      &m, <double*> &X[0], &incx, <double*> &beta, &Y[0],
                      &incy)
            
            cdef inline int rand(long* state) noexcept nogil:
                cdef long x = state[0]
                state[0] = x * 6364136223846793005L + 1442695040888963407L
                cdef int s = (x ^ (x >> 18)) >> 27
                cdef int rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline long srand(const long seed) noexcept nogil:
                cdef long state = seed + 1442695040888963407L
                rand(&state)
                return state
            
            cdef inline int randint(const int bound, long* state) \
                    noexcept nogil:
                cdef int r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            cdef inline void shuffle_array(int[::1] arr, long* state) \
                    noexcept nogil:
                cdef int i, j, temp
                for i in range(arr.shape[0] - 1, 0, -1):
                    j = randint(i + 1, state)
                    temp = arr[i]
                    arr[i] = arr[j]
                    arr[j] = temp
            
            cdef double compute_objective(double[:, ::1] Z_norm_times_Y_norm,
                                          const double[:, ::1] R,
                                          const double[:, ::1] E,
                                          const double[:, ::1] O,
                                          double[:, ::1] ratio,
                                          const double[::1] theta,
                                          double[::1] theta_times_ratio,
                                          const double sigma,
                                          const int num_cells,
                                          const int num_clusters,
                                          const int num_batches) \
                    noexcept nogil:
                cdef int i, j
                cdef double kmeans_error, entropy_term, diversity_penalty
                kmeans_error = entropy_term = diversity_penalty = 0
                for i in range(num_cells):
                    for j in range(num_clusters):
                        kmeans_error += \
                            R[i, j] * (1 - Z_norm_times_Y_norm[i, j])
                        entropy_term += R[i, j] * log(R[i, j])
                kmeans_error *= 2
                entropy_term *= sigma
                for i in range(num_batches):
                    for j in range(num_clusters):
                        ratio[i, j] = O[i, j] * log(
                            (O[i, j] + 1) / (E[i, j] + 1))
                matrix_vector_multiply(ratio, theta, theta_times_ratio,
                                       transpose=True, alpha=1, beta=0)
                for i in range(num_clusters):
                    diversity_penalty += theta_times_ratio[i]
                diversity_penalty *= sigma
                return kmeans_error + entropy_term + diversity_penalty
            
            def initialize(const double[:, ::1] Z_norm,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const int[::1] N_b,
                           double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[::1] R_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           const double sigma,
                           double[:, ::1] ratio,
                           double[::1] theta,
                           double[::1] theta_times_ratio,
                           const double tau):
                cdef int num_cells = Z_norm.shape[0]
                cdef int num_batches = E.shape[0]
                cdef int num_clusters = E.shape[1]
                cdef int i, j
                cdef unsigned batch_label
                cdef double norm, objective, base, two_over_sigma = 2 / sigma
                
                # Initialize Pr_b
                for i in range(num_batches):
                    Pr_b[i] = <double> N_b[i] / num_cells
                
                # Initialize R (and R_sum) and O
                matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                transpose_A=False, transpose_B=True, alpha=1,
                                beta=0)
                for i in range(num_cells):
                    batch_label = batch_labels[i]
                    norm = 0
                    for j in range(num_clusters):
                        R[i, j] = exp(two_over_sigma *
                                      (Z_norm_times_Y_norm[i, j] - 1))
                        norm += R[i, j]
                    norm = 1 / norm
                    for j in range(num_clusters):
                        R[i, j] *= norm
                        O[batch_label, j] += R[i, j]
                        R_sum[j] += R[i, j]
                
                # Initialize E
                for i in range(num_batches):
                    for j in range(num_clusters):
                        E[i, j] = Pr_b[i] * R_sum[j]
                
                # Apply discounting to theta, if specified
                if tau > 0:
                    for i in range(num_batches):
                        base = exp(-N_b[i] / (num_clusters * tau))
                        theta[i] = theta[i] * (1 - base * base)
                
                # Compute and return the initial value of the objective
                # function
                objective = compute_objective(
                    Z_norm_times_Y_norm, R, E, O, ratio, theta,
                    theta_times_ratio, sigma, num_cells, num_clusters,
                    num_batches)
                
                return objective
            
            def clustering(const double[:, ::1] Z_norm,
                           double[:, ::1] Z_norm_in,
                           double[:, ::1] Y_norm,
                           double[:, ::1] Z_norm_times_Y_norm,
                           const double[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           double[:, ::1] R,
                           double[:, ::1] R_in,
                           double[::1] R_in_sum,
                           double[:, ::1] E,
                           double[:, ::1] O,
                           double[:, ::1] ratio,
                           const double[::1] theta,
                           double[::1] theta_times_ratio,
                           int[::1] idx_list,
                           const double tol,
                           const int max_iter,
                           const double sigma,
                           const int block_size):
                cdef int num_cells = Z_norm.shape[0]
                cdef int num_PCs = Z_norm.shape[1]
                cdef int num_batches = E.shape[0]
                cdef int num_clusters = E.shape[1]
                cdef int i, j, k, iter, num_cells_in_block, pos
                cdef unsigned batch_label
                cdef long state
                cdef double two_over_sigma = 2 / sigma
                cdef double exp_neg_two_over_sigma = exp(-two_over_sigma)
                cdef double norm, objective = -1, old, new
                cdef double[:, ::1] Z_norm_block, R_block
                cdef double past_clustering_objectives[3]
                
                for iter in range(max_iter):
                    # Compute Cluster Centroids
                    matrix_multiply(R, Z_norm, Y_norm, transpose_A=True,
                                    transpose_B=False, alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm = norm + Y_norm[i, j] * Y_norm[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y_norm[i, j] = Y_norm[i, j] * norm
                    for i in range(num_cells):
                        idx_list[i] = i
                    state = srand(iter)
                    shuffle_array(idx_list, &state)
                    
                    # Update cells blockwise
                    pos = 0
                    Z_norm_block = Z_norm_in
                    R_block = R_in
                    while pos < num_cells:
                        if pos + block_size > num_cells:
                            num_cells_in_block = num_cells - pos
                            Z_norm_block = Z_norm_block[:num_cells_in_block]
                            R_block = R_block[:num_cells_in_block]
                        else:
                            num_cells_in_block = block_size
                        
                        # Remove the cells in this block from E and O
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[pos + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_in_sum[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_norm_block[i, j] = Z_norm[k, j]
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] -= Pr_b[i] * R_in_sum[j]
                        
                        # Recompute R for the removed cells
                        # Note: the original formula is
                        # exp(-2 / sigma * (1 - Z_norm_block @ Y_norm.T)),
                        # which expands to exp(-2 / sigma) *
                        # exp(2 / sigma * Z_norm_block @ Y_norm.T)). Since
                        # exp(-2 / sigma) is a constant, we fold it into
                        # `ratio`.
                        matrix_multiply(Z_norm_block, Y_norm, R_block,
                                        transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    pow((E[i, j] + 1) / (O[i, j] + 1),
                                        theta[i])
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[i + pos]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                R[k, j] = exp(R_block[i, j]) * \
                                          ratio[batch_label, j]
                                norm += R[k, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[k, j] *= norm
                                R_in_sum[j] += R[k, j]
                                # Add the removed cells back into O
                                O[batch_label, j] += R[k, j]
                        
                        # Add the removed cells back into E
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] += Pr_b[i] * R_in_sum[j]
                        
                        # Move to the next block
                        pos += block_size
                    
                    # Compute the objective and decide whether we've converged
                    matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                    transpose_A=False, transpose_B=True,
                                    alpha=1, beta=0)
                    objective = compute_objective(
                        Z_norm_times_Y_norm, R, E, O, ratio, theta,
                        theta_times_ratio, sigma, num_cells, num_clusters,
                        num_batches)
                    if iter < 3:
                        past_clustering_objectives[iter] = objective
                    else:
                        old = past_clustering_objectives[0] + \
                            past_clustering_objectives[1] + \
                            past_clustering_objectives[2]
                        new = past_clustering_objectives[1] + \
                            past_clustering_objectives[2] + objective
                        if old - new < tol * abs(old):
                            break
                        else:
                            past_clustering_objectives[0] = \
                                past_clustering_objectives[1]
                            past_clustering_objectives[1] = \
                                past_clustering_objectives[2]
                            past_clustering_objectives[2] = objective
                    PyErr_CheckSignals()
                return objective
            
            def correction(const double[:, ::1] Z,
                           double[:, ::1] Z_hat,
                           const double[:, ::1] R,
                           const double[:, ::1] O,
                           const double ridge_lambda,
                           const unsigned[::1] batch_labels,
                           double[::1] factor,
                           double[:, ::1] P,
                           double[:, ::1] P_t_B_inv,
                           double[:, ::1] inv_mat,
                           double[:, ::1] Phi_t_diag_R_by_X,
                           double[:, ::1] W):
                cdef int num_cells = Z.shape[0]
                cdef int num_PCs = Z.shape[1]
                cdef int num_batches = O.shape[0]
                cdef int num_clusters = O.shape[1]
                cdef int i, j, k
                cdef unsigned batch_label
                cdef double c, c_inv
                
                Z_hat[:] = Z[:]
                
                # Initialize P to the identity matrix
                P[:] = 0
                for i in range(num_batches + 1):
                    P[i, i] = 1
            
                for k in range(num_clusters):
                    # Compute factor, c_inv and P
                    c = 0
                    for i in range(num_batches):
                        factor[i] = 1 / (O[i, k] + ridge_lambda)
                        c += O[i, k] * (1 - factor[i] * O[i, k])
                        P[0, i + 1] = -factor[i] * O[i, k]
                    c_inv = 1 / c
                    
                    # Compute P_t_B_inv
                    P_t_B_inv[:] = 0
                    P_t_B_inv[0, 0] = c_inv
                    for i in range(1, num_batches + 1):
                        P_t_B_inv[i, i] = factor[i - 1]
                        P_t_B_inv[i, 0] = P[0, i] * c_inv
                    
                    # Compute inv_mat
                    matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                    transpose_B=False, alpha=1, beta=0)
                    
                    # Compute Phi_t_diag_R @ X
                    Phi_t_diag_R_by_X[:] = 0
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Phi_t_diag_R_by_X[0, j] += Z[i, j] * R[i, k]
                            Phi_t_diag_R_by_X[batch_label + 1, j] += \
                                Z[i, j] * R[i, k]
                            
                    # Compute W
                    matrix_multiply(inv_mat, Phi_t_diag_R_by_X, W,
                                    transpose_A=False, transpose_B=False,
                                    alpha=1, beta=0)
                     
                    # Update Z_hat
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Z_hat[i, j] = Z_hat[i, j] - \
                                W[batch_label + 1, j] * R[i, k]
                        
            def normalize_rows(const double[:, ::1] arr, double[:, ::1] out):
                cdef int i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

            def normalize_rows_inplace(double[:, ::1] arr):
                cdef int i, j
                cdef double norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        arr[i, j] = arr[i, j] * norm
        ''')
        initialize = cython_functions['initialize']
        clustering = cython_functions['clustering']
        correction = cython_functions['correction']
        normalize_rows = cython_functions['normalize_rows']
        normalize_rows_inplace = cython_functions['normalize_rows_inplace']
        
        # Get dimensions of everything
        num_cells, num_PCs = Z.shape
        block_size = int(num_cells * block_proportion)
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        N_b = bincount(batch_labels, num_bins=batch_labels[-1] + 1)
        num_batches = len(N_b)
        
        # Allocate arrays
        Z_norm = np.empty((num_cells, num_PCs))
        Z_norm_in = np.empty((block_size, num_PCs))
        Z_norm_times_Y_norm = np.empty((num_cells, num_clusters))
        Pr_b = np.empty(num_batches)
        R = np.empty((num_cells, num_clusters))
        R_in = np.empty((block_size, num_clusters))
        R_in_sum = np.zeros(num_clusters)
        E = np.empty((num_batches, num_clusters))
        O = np.zeros((num_batches, num_clusters))
        ratio = np.empty((num_batches, num_clusters))
        theta = np.repeat(theta, num_batches)
        theta_times_ratio = np.empty(num_clusters)
        idx_list = np.empty(num_cells, dtype=np.int32)
        factor = np.empty(num_cells)
        P = np.empty((num_batches + 1, num_batches + 1))
        P_t_B_inv = np.empty((num_batches + 1, num_batches + 1))
        inv_mat = np.empty((num_batches + 1, num_batches + 1))
        Phi_t_diag_R_by_X = np.empty((num_batches + 1, num_PCs))
        W = np.empty((num_batches + 1, num_PCs))
        
        # Run k-means
        normalize_rows(Z, Z_norm)
        kmeans = faiss.Kmeans(num_PCs, num_clusters, seed=seed)
        kmeans.train(Z_norm)
        Y_norm = kmeans.centroids.astype(float)
        normalize_rows_inplace(Y_norm)
        
        # Complete initialization in Cython
        objective = initialize(
            Z_norm=Z_norm, Y_norm=Y_norm,
            Z_norm_times_Y_norm=Z_norm_times_Y_norm, N_b=N_b, Pr_b=Pr_b,
            batch_labels=batch_labels, R=R, R_sum=R_in_sum, E=E, O=O,
            sigma=sigma, ratio=ratio, theta=theta,
            theta_times_ratio=theta_times_ratio, tau=tau)
        
        if verbose:
            print(f'Initialization is complete: objective = {objective:.2f}')
        
        iteration_string = plural('iteration', max_iter_harmony)
        
        for i in range(1, max_iter_harmony + 1):
            prev_objective = objective
            objective = clustering(
                Z_norm=Z_norm, Z_norm_in=Z_norm_in, Y_norm=Y_norm,
                Z_norm_times_Y_norm=Z_norm_times_Y_norm, Pr_b=Pr_b,
                batch_labels=batch_labels, R=R, R_in=R_in, R_in_sum=R_in_sum,
                E=E, O=O, ratio=ratio, theta=theta,
                theta_times_ratio=theta_times_ratio, idx_list=idx_list,
                tol=tol_clustering, max_iter=max_iter_clustering, sigma=sigma,
                block_size=block_size)
            correction(Z=Z, Z_hat=Z_norm, R=R, O=O, ridge_lambda=ridge_lambda,
                       batch_labels=batch_labels, factor=factor, P=P,
                       P_t_B_inv=P_t_B_inv, inv_mat=inv_mat,
                       Phi_t_diag_R_by_X=Phi_t_diag_R_by_X, W=W)
            
            if verbose:
                print(f'Completed {i} of {max_iter_harmony} '
                      f'{iteration_string}: objective = {objective:.2f}')
            
            if prev_objective - objective < tol_harmony * abs(prev_objective):
                if verbose:
                    print(f'Reached convergence after {i} {iteration_string}')
                break
            
            if i == max_iter_harmony:
                if verbose:
                    print(f'Failed to converge after {i} {iteration_string}')
                break
            
            normalize_rows_inplace(Z_norm)
        
        del batch_labels, Z, Pr_b, theta, R, E, O, Z_norm_in, Y_norm, \
            Z_norm_times_Y_norm, R_in, R_in_sum, ratio, theta_times_ratio, \
            idx_list, factor, P, P_t_B_inv, inv_mat, Phi_t_diag_R_by_X, W
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z_norm[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with NaN
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns)
        return tuple(datasets) if others else datasets[0]
    
    def label_transfer_from(
            self,
            other: SingleCell,
            original_cell_type_column: SingleCellColumn,
            *,
            QC_column: SingleCellColumn | None = 'passed_QC',
            other_QC_column: SingleCellColumn | None = 'passed_QC',
            Harmony_key: str = 'Harmony_PCs',
            cell_type_column: str = 'cell_type',
            confidence_column: str = 'cell_type_confidence',
            next_best_cell_type_column: str = 'next_best_cell_type',
            next_best_confidence_column: str =
                'next_best_cell_type_confidence',
            num_neighbors: int | np.integer = 20,
            num_clusters: int | np.integer | None = None,
            num_probes: int | np.integer | None = None,
            seed: int | np.integer = 0,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Transfer cell-type labels from another dataset to this one, by running
        approximate k-nearest neighbors on the Harmony embeddings from
        `harmonize()`.
        
        For each cell in `self`, the transferred cell-type label is the most
        common cell-type label among the `num_neighbors` cells in `other` with
        the nearest Harmony embeddings. The cell-type confidence is the
        fraction of these neighbors that share this most common cell-type
        label.
        
        Args:
            other: the dataset to transfer cell-type labels from
            original_cell_type_column: a column of `other.obs` containing
                                       cell-type labels. Can be a column name,
                                       a polars expression, a polars Series, a
                                       1D NumPy array, or a function that takes
                                       in `other` and returns a polars Series
                                       or 1D NumPy array.
            QC_column: an optional Boolean column of `self.obs` indicating
                       which cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in `self` and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will have their cell-type labels
                       and confidences set to null.
            other_QC_column: an optional Boolean column of `other.obs`
                             indicating which cells passed QC. Can be a column
                             name, a polars expression, a polars Series, a 1D
                             NumPy array, or a function that takes in `other`
                             and returns a polars Series or 1D NumPy array. Set
                             to `None` to include all cells. Cells failing QC
                             will be ignored during the label transfer.
            Harmony_key: the key of `self.obsm` and `other.obsm` containing the
                         Harmony embeddings for each dataset
            cell_type_column: the name of a column to be added to `self.obs`
                              indicating each cell's most likely cell type,
                              i.e. the most common cell-type label among the
                              cell's `num_neighbors` nearest neighbors in obs
            confidence_column: the name of a column to be added to `self.obs`
                               indicating each cell's cell-type confidence,
                               i.e. the fraction of the cell's `num_neighbors`
                               nearest neighbors in obs that share the most
                               common cell-type label. If multiple cell types
                               are equally common among the nearest neighbors,
                               tiebreak based on which of them is most common
                               in `other`.
            next_best_cell_type_column: the name of a column to be added to
                                        `self.obs` indicating each cell's
                                        second-most likely cell type, i.e. the
                                        second-most common cell-type label
                                        among the cell's `num_neighbors`
                                        nearest neighbors in obs
            next_best_confidence_column: the name of a column to be
                                                   added to
                                         `self.obs` indicating each cell's
                                         cell-type confidence, i.e. the
                                         fraction of the cell's `num_neighbors`
                                         nearest neighbors in obs that share
                                         the most common cell-type label. If
                                         multiple cell types are equally common
                                         among the nearest neighbors, tiebreak
                                         based on which of them is most common
                                         in `other`.
            num_neighbors: the number of nearest neighbors to use when
                           determining a cell's label. All cell-type
                           confidences will be multiples of
                           `1 / num_neighbors`.
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be between 1 and the number of cells. If
                          `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            seed: the random seed to use when finding nearest neighbors
            overwrite: if `True`, overwrite `cell_type_column` and/or
                       `confidence_column` if already present in this dataset's
                       obs, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use for the nearest-neighbors
                         tree construction and search. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        
        Returns:
            `self`, but with two additional columns: `cell_type_column`,
            containing the transferred cell-type labels, and
            `confidence_column`, containing the cell-type confidences.
        """
        with ignore_sigint():
            import faiss
        # Check that `other` is a SingleCell dataset
        check_type(other, 'other', SingleCell, 'a SingleCell dataset')
        # Get `QC_column` from `self` and `other_QC_column` from `other`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if other_QC_column is not None:
            other_QC_column = other._get_column(
                'obs', other_QC_column, 'other_QC_column', pl.Boolean,
                allow_missing=other_QC_column == 'passed_QC')
        # Get the number of cells in `self` and `other`
        num_cells_in_self = len(self._obs) if QC_column is None else \
            QC_column.sum()
        num_cells_in_other = len(other._obs) if other_QC_column is None else \
            other_QC_column.sum()
        # Get `original_cell_type_column` from `other`
        original_original_cell_type_column = original_cell_type_column
        original_cell_type_column = other._get_column(
            'obs', original_cell_type_column, 'original_cell_type_column',
            (pl.Categorical, pl.Enum, pl.String), QC_column=other_QC_column)
        # If `other_QC_column` is not `None`, filter the cell type labels in
        # `original_cell_type_column` to cells passing QC
        if other_QC_column is not None:
            original_cell_type_column = \
                original_cell_type_column.filter(other_QC_column)
        # Check that `original_cell_type_column` has at least two distinct cell
        # types
        most_common_cell_types = \
            original_cell_type_column.value_counts(sort=True).to_series()
        if len(most_common_cell_types) == 1:
            original_cell_type_column_description = \
                SingleCell._describe_column('original_cell_type_column',
                                            original_original_cell_type_column)
            error_message = (
                f'{original_cell_type_column_description} must have at least '
                f'two distinct cell types')
            if other_QC_column is not None:
                error_message += ' after filtering to cells passing QC'
            raise ValueError(error_message)
        # Check that `Harmony_key` is a string and in both `self.obsm` and
        # `other.obsm`
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        datasets = (self, 'self'), (other, 'other')
        for dataset, dataset_name in datasets:
            if Harmony_key not in dataset._obsm:
                error_message = (
                    f'Harmony_key {Harmony_key!r} is not a column of '
                    f'{dataset_name}.obs; did you forget to run harmonize() '
                    f'before label_transfer_from()? Set overwrite=True to '
                    f'overwrite.')
                raise ValueError(error_message)
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `cell_type_column`, `confidence_column`,
        # `next_best_cell_type_column` and `next_best_confidence_column` are
        # strings and, unless `overwrite=True`, not already columns of
        # `self.obs`
        for column, column_name in (
                (cell_type_column, 'cell_type_column'),
                (confidence_column, 'confidence_column'),
                (next_best_cell_type_column, 'next_best_cell_type_column'),
                (next_best_confidence_column, 'next_best_confidence_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column '
                    f'of obs; did you already run label_transfer_from()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `num_neighbors` is a positive integer
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        check_bounds(num_neighbors, 'num_neighbors', 1)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells_in_other),
                                           num_cells_in_other / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters <= num_cells_in_other:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and ≤ '
                    f'the number of cells in other ({num_cells_in_other:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be ≥ 1 and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = SingleCell._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Recode cell types so the most common is 0, the next-most common 1,
        # etc. This has the effect of breaking ties by taking the most common
        # cell type: we pick the first element in case of ties.
        cell_type_to_code = dict(zip(most_common_cell_types, range(
            len(most_common_cell_types))))
        original_cell_type_column = original_cell_type_column\
            .replace_strict(cell_type_to_code, return_dtype=pl.Int32)
        # Get the Harmony embeddings for self and other
        self_Harmony_embeddings = self._obsm[Harmony_key] \
            if QC_column is None else \
            self._obsm[Harmony_key][QC_column.to_numpy()]
        other_Harmony_embeddings = other._obsm[Harmony_key] \
            if other_QC_column is None else \
            other._obsm[Harmony_key][other_QC_column.to_numpy()]
        dim = self_Harmony_embeddings.shape[1]
        if other_Harmony_embeddings.shape[1] != dim:
            error_message = (
                f"the two datasets' Harmony embeddings have different numbers "
                f"of dimensions ({dim:,} vs "
                f"{other_Harmony_embeddings.shape[1]:,}")
            raise ValueError(error_message)
        # Use faiss to get the indices of the num_neighbors nearest
        # neighbors in other for each cell in self
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        # noinspection PyArgumentList
        index.train(other_Harmony_embeddings)
        # noinspection PyArgumentList
        index.add(other_Harmony_embeddings)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = \
            index.search(self_Harmony_embeddings, num_neighbors)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = self_Harmony_embeddings[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} cells '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) '
                          f'did not have enough neighbors with '
                          f'{index.nprobe:,} probes; re-running '
                          f'nearest-neighbors finding for these cells with '
                          f'{num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = index.search(needs_update_X, num_neighbors)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        # Get the cell-type labels of these nearest neighbors (using our
        # integer encoding where the most common cell type is 0, the next-most
        # common 1, etc.)
        nearest_neighbor_cell_types = \
            original_cell_type_column.to_numpy()[nearest_neighbor_indices]
        # Get the two most common cell types for each cell in `self` among its
        # `num_neighbors` nearest neighbors in `other`. Pick the first element
        # in case of ties, which according to our encoding is the most common
        # cell type. Also get the cell-type confidence of these two cell types,
        # i.e. their frequency among the nearest neighbors.
        cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        confidences = np.empty(num_cells_in_self, dtype=float)
        next_best_cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        next_best_confidences = np.empty(num_cells_in_self, dtype=float)
        cython_inline(rf'''
            from cython.parallel cimport threadid, prange
            from libc.stdlib cimport free, malloc
            
            def label_transfer(const int[:, ::1] nearest_neighbor_cell_types,
                               const int num_cell_types,
                               unsigned[::1] cell_types,
                               double[::1] confidences,
                               unsigned[::1] next_best_cell_types,
                               double[::1] next_best_confidences,
                               const unsigned num_threads):
                cdef int i, j, start, thread_id, cell_type, count, \
                    most_common_cell_type, second_most_common_cell_type, \
                    max_count, second_max_count
                cdef int num_cells = nearest_neighbor_cell_types.shape[0]
                cdef int num_neighbors = nearest_neighbor_cell_types.shape[1]
                cdef double inv_num_neighbors = 1.0 / num_neighbors
                cdef int* counts_buffer
                cdef int[::1] counts
                
                counts_buffer = <int*> malloc(
                    num_cell_types * num_threads * sizeof(int))
                if not counts_buffer:
                    raise MemoryError
                try:
                    counts = \
                        <int[:num_cell_types * num_threads:]> counts_buffer
                    if num_threads == 1:
                        for i in range(num_cells):
                            counts[:] = 0
                            for j in range(num_neighbors):
                                cell_type = nearest_neighbor_cell_types[i, j]
                                counts[cell_type] += 1
                            if counts[0] >= counts[1]:
                                max_count = counts[0]
                                second_max_count = counts[1]
                                most_common_cell_type = 0
                                second_most_common_cell_type = 1
                            else:
                                max_count = counts[1]
                                second_max_count = counts[0]
                                most_common_cell_type = 1
                                second_most_common_cell_type = 0
                            for cell_type in range(2, num_cell_types):
                                count = counts[cell_type]
                                if count > max_count:
                                    second_max_count = max_count
                                    second_most_common_cell_type = \
                                        most_common_cell_type
                                    max_count = count
                                    most_common_cell_type = cell_type
                                elif count > second_max_count:
                                    second_max_count = count
                                    second_most_common_cell_type = cell_type
                            cell_types[i] = most_common_cell_type
                            confidences[i] = max_count * inv_num_neighbors
                            next_best_cell_types[i] = \
                                second_most_common_cell_type
                            next_best_confidences[i] = \
                                second_max_count * inv_num_neighbors
                    else:
                        for i in prange(num_cells, nogil=True,
                                        num_threads=num_threads):
                            thread_id = threadid()
                            start = num_cell_types * thread_id
                            counts[start:start + num_cell_types] = 0
                            for j in range(num_neighbors):
                                cell_type = nearest_neighbor_cell_types[i, j]
                                counts[start + cell_type] += 1
                            if counts[start] >= counts[start + 1]:
                                max_count = counts[start]
                                second_max_count = counts[start + 1]
                                most_common_cell_type = 0
                                second_most_common_cell_type = 1
                            else:
                                max_count = counts[start + 1]
                                second_max_count = counts[start]
                                most_common_cell_type = 1
                                second_most_common_cell_type = 0
                            for cell_type in range(2, num_cell_types):
                                count = counts[start + cell_type]
                                if count > max_count:
                                    second_max_count = max_count
                                    second_most_common_cell_type = \
                                        most_common_cell_type
                                    max_count = count
                                    most_common_cell_type = cell_type
                                elif count > second_max_count:
                                    second_max_count = count
                                    second_most_common_cell_type = cell_type
                            cell_types[i] = most_common_cell_type
                            confidences[i] = max_count * inv_num_neighbors
                            next_best_cell_types[i] = \
                                second_most_common_cell_type
                            next_best_confidences[i] = \
                                second_max_count * inv_num_neighbors
                finally:
                    free(counts_buffer)
            ''')['label_transfer'](
                nearest_neighbor_cell_types=nearest_neighbor_cell_types,
                num_cell_types=len(cell_type_to_code), cell_types=cell_types,
                confidences=confidences,
                next_best_cell_types=next_best_cell_types,
                next_best_confidences=next_best_confidences,
                num_threads=num_threads)
        # Map the cell-type codes back to their labels by constructing a polars
        # Series from the codes, then casting it to an Enum. Also convert
        # cell-type confidences to Series.
        cell_types = pl.Series(cell_type_column, cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        confidences = pl.Series(confidence_column, confidences)
        next_best_cell_types = \
            pl.Series(next_best_cell_type_column, next_best_cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        next_best_confidences = \
            pl.Series(next_best_confidence_column, next_best_confidences)
        # Add the four columns to obs. If `QC_column` is not `None`, back-project
        # from QCed cells to all cells, filling with nulls.
        columns = cell_types, confidences, next_best_cell_types, \
            next_best_confidences
        if QC_column is None:
            obs = self._obs.with_columns(columns)
        else:
            # noinspection PyTypeChecker
            expand = lambda series: pl.when(QC_column.name)\
                .then(pl.lit(series).gather(pl.col(QC_column.name).cum_sum()
                                            .cast(pl.Int32) - 1))
            obs = self._obs.with_columns(map(expand, columns))
        # Return a new SingleCell dataset containing the cell-type labels and
        # confidences
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, uns=self._uns)
    
    # noinspection PyUnresolvedReferences
    @staticmethod
    def _get_rocket_r() -> 'LinearSegmentedColormap':
        """
        Define Seaborn's rocket_r colormap using base Matplotlib.
        
        Returns:

        """
        import matplotlib.pyplot as plt
        rocket_colors = [
            [0.01060815, 0.01808215, 0.10018654],
            [0.01428972, 0.02048237, 0.10374486],
            [0.01831941, 0.0229766, 0.10738511],
            [0.02275049, 0.02554464, 0.11108639],
            [0.02759119, 0.02818316, 0.11483751],
            [0.03285175, 0.03088792, 0.11863035],
            [0.03853466, 0.03365771, 0.12245873],
            [0.04447016, 0.03648425, 0.12631831],
            [0.05032105, 0.03936808, 0.13020508],
            [0.05611171, 0.04224835, 0.13411624],
            [0.0618531, 0.04504866, 0.13804929],
            [0.06755457, 0.04778179, 0.14200206],
            [0.0732236, 0.05045047, 0.14597263],
            [0.0788708, 0.05305461, 0.14995981],
            [0.08450105, 0.05559631, 0.15396203],
            [0.09011319, 0.05808059, 0.15797687],
            [0.09572396, 0.06050127, 0.16200507],
            [0.10132312, 0.06286782, 0.16604287],
            [0.10692823, 0.06517224, 0.17009175],
            [0.1125315, 0.06742194, 0.17414848],
            [0.11813947, 0.06961499, 0.17821272],
            [0.12375803, 0.07174938, 0.18228425],
            [0.12938228, 0.07383015, 0.18636053],
            [0.13501631, 0.07585609, 0.19044109],
            [0.14066867, 0.0778224, 0.19452676],
            [0.14633406, 0.07973393, 0.1986151],
            [0.15201338, 0.08159108, 0.20270523],
            [0.15770877, 0.08339312, 0.20679668],
            [0.16342174, 0.0851396, 0.21088893],
            [0.16915387, 0.08682996, 0.21498104],
            [0.17489524, 0.08848235, 0.2190294],
            [0.18065495, 0.09009031, 0.22303512],
            [0.18643324, 0.09165431, 0.22699705],
            [0.19223028, 0.09317479, 0.23091409],
            [0.19804623, 0.09465217, 0.23478512],
            [0.20388117, 0.09608689, 0.23860907],
            [0.20973515, 0.09747934, 0.24238489],
            [0.21560818, 0.09882993, 0.24611154],
            [0.22150014, 0.10013944, 0.2497868],
            [0.22741085, 0.10140876, 0.25340813],
            [0.23334047, 0.10263737, 0.25697736],
            [0.23928891, 0.10382562, 0.2604936],
            [0.24525608, 0.10497384, 0.26395596],
            [0.25124182, 0.10608236, 0.26736359],
            [0.25724602, 0.10715148, 0.27071569],
            [0.26326851, 0.1081815, 0.27401148],
            [0.26930915, 0.1091727, 0.2772502],
            [0.27536766, 0.11012568, 0.28043021],
            [0.28144375, 0.11104133, 0.2835489],
            [0.2875374, 0.11191896, 0.28660853],
            [0.29364846, 0.11275876, 0.2896085],
            [0.29977678, 0.11356089, 0.29254823],
            [0.30592213, 0.11432553, 0.29542718],
            [0.31208435, 0.11505284, 0.29824485],
            [0.31826327, 0.1157429, 0.30100076],
            [0.32445869, 0.11639585, 0.30369448],
            [0.33067031, 0.11701189, 0.30632563],
            [0.33689808, 0.11759095, 0.3088938],
            [0.34314168, 0.11813362, 0.31139721],
            [0.34940101, 0.11863987, 0.3138355],
            [0.355676, 0.11910909, 0.31620996],
            [0.36196644, 0.1195413, 0.31852037],
            [0.36827206, 0.11993653, 0.32076656],
            [0.37459292, 0.12029443, 0.32294825],
            [0.38092887, 0.12061482, 0.32506528],
            [0.38727975, 0.12089756, 0.3271175],
            [0.39364518, 0.12114272, 0.32910494],
            [0.40002537, 0.12134964, 0.33102734],
            [0.40642019, 0.12151801, 0.33288464],
            [0.41282936, 0.12164769, 0.33467689],
            [0.41925278, 0.12173833, 0.33640407],
            [0.42569057, 0.12178916, 0.33806605],
            [0.43214263, 0.12179973, 0.33966284],
            [0.43860848, 0.12177004, 0.34119475],
            [0.44508855, 0.12169883, 0.34266151],
            [0.45158266, 0.12158557, 0.34406324],
            [0.45809049, 0.12142996, 0.34540024],
            [0.46461238, 0.12123063, 0.34667231],
            [0.47114798, 0.12098721, 0.34787978],
            [0.47769736, 0.12069864, 0.34902273],
            [0.48426077, 0.12036349, 0.35010104],
            [0.49083761, 0.11998161, 0.35111537],
            [0.49742847, 0.11955087, 0.35206533],
            [0.50403286, 0.11907081, 0.35295152],
            [0.51065109, 0.11853959, 0.35377385],
            [0.51728314, 0.1179558, 0.35453252],
            [0.52392883, 0.11731817, 0.35522789],
            [0.53058853, 0.11662445, 0.35585982],
            [0.53726173, 0.11587369, 0.35642903],
            [0.54394898, 0.11506307, 0.35693521],
            [0.5506426, 0.11420757, 0.35737863],
            [0.55734473, 0.11330456, 0.35775059],
            [0.56405586, 0.11235265, 0.35804813],
            [0.57077365, 0.11135597, 0.35827146],
            [0.5774991, 0.11031233, 0.35841679],
            [0.58422945, 0.10922707, 0.35848469],
            [0.59096382, 0.10810205, 0.35847347],
            [0.59770215, 0.10693774, 0.35838029],
            [0.60444226, 0.10573912, 0.35820487],
            [0.61118304, 0.10450943, 0.35794557],
            [0.61792306, 0.10325288, 0.35760108],
            [0.62466162, 0.10197244, 0.35716891],
            [0.63139686, 0.10067417, 0.35664819],
            [0.63812122, 0.09938212, 0.35603757],
            [0.64483795, 0.0980891, 0.35533555],
            [0.65154562, 0.09680192, 0.35454107],
            [0.65824241, 0.09552918, 0.3536529],
            [0.66492652, 0.09428017, 0.3526697],
            [0.67159578, 0.09306598, 0.35159077],
            [0.67824099, 0.09192342, 0.3504148],
            [0.684863, 0.09085633, 0.34914061],
            [0.69146268, 0.0898675, 0.34776864],
            [0.69803757, 0.08897226, 0.3462986],
            [0.70457834, 0.0882129, 0.34473046],
            [0.71108138, 0.08761223, 0.3430635],
            [0.7175507, 0.08716212, 0.34129974],
            [0.72398193, 0.08688725, 0.33943958],
            [0.73035829, 0.0868623, 0.33748452],
            [0.73669146, 0.08704683, 0.33543669],
            [0.74297501, 0.08747196, 0.33329799],
            [0.74919318, 0.08820542, 0.33107204],
            [0.75535825, 0.08919792, 0.32876184],
            [0.76145589, 0.09050716, 0.32637117],
            [0.76748424, 0.09213602, 0.32390525],
            [0.77344838, 0.09405684, 0.32136808],
            [0.77932641, 0.09634794, 0.31876642],
            [0.78513609, 0.09892473, 0.31610488],
            [0.79085854, 0.10184672, 0.313391],
            [0.7965014, 0.10506637, 0.31063031],
            [0.80205987, 0.10858333, 0.30783],
            [0.80752799, 0.11239964, 0.30499738],
            [0.81291606, 0.11645784, 0.30213802],
            [0.81820481, 0.12080606, 0.29926105],
            [0.82341472, 0.12535343, 0.2963705],
            [0.82852822, 0.13014118, 0.29347474],
            [0.83355779, 0.13511035, 0.29057852],
            [0.83850183, 0.14025098, 0.2876878],
            [0.84335441, 0.14556683, 0.28480819],
            [0.84813096, 0.15099892, 0.281943],
            [0.85281737, 0.15657772, 0.27909826],
            [0.85742602, 0.1622583, 0.27627462],
            [0.86196552, 0.16801239, 0.27346473],
            [0.86641628, 0.17387796, 0.27070818],
            [0.87079129, 0.17982114, 0.26797378],
            [0.87507281, 0.18587368, 0.26529697],
            [0.87925878, 0.19203259, 0.26268136],
            [0.8833417, 0.19830556, 0.26014181],
            [0.88731387, 0.20469941, 0.25769539],
            [0.89116859, 0.21121788, 0.2553592],
            [0.89490337, 0.21785614, 0.25314362],
            [0.8985026, 0.22463251, 0.25108745],
            [0.90197527, 0.23152063, 0.24918223],
            [0.90530097, 0.23854541, 0.24748098],
            [0.90848638, 0.24568473, 0.24598324],
            [0.911533, 0.25292623, 0.24470258],
            [0.9144225, 0.26028902, 0.24369359],
            [0.91717106, 0.26773821, 0.24294137],
            [0.91978131, 0.27526191, 0.24245973],
            [0.92223947, 0.28287251, 0.24229568],
            [0.92456587, 0.29053388, 0.24242622],
            [0.92676657, 0.29823282, 0.24285536],
            [0.92882964, 0.30598085, 0.24362274],
            [0.93078135, 0.31373977, 0.24468803],
            [0.93262051, 0.3215093, 0.24606461],
            [0.93435067, 0.32928362, 0.24775328],
            [0.93599076, 0.33703942, 0.24972157],
            [0.93752831, 0.34479177, 0.25199928],
            [0.93899289, 0.35250734, 0.25452808],
            [0.94036561, 0.36020899, 0.25734661],
            [0.94167588, 0.36786594, 0.2603949],
            [0.94291042, 0.37549479, 0.26369821],
            [0.94408513, 0.3830811, 0.26722004],
            [0.94520419, 0.39062329, 0.27094924],
            [0.94625977, 0.39813168, 0.27489742],
            [0.94727016, 0.4055909, 0.27902322],
            [0.94823505, 0.41300424, 0.28332283],
            [0.94914549, 0.42038251, 0.28780969],
            [0.95001704, 0.42771398, 0.29244728],
            [0.95085121, 0.43500005, 0.29722817],
            [0.95165009, 0.44224144, 0.30214494],
            [0.9524044, 0.44944853, 0.3072105],
            [0.95312556, 0.45661389, 0.31239776],
            [0.95381595, 0.46373781, 0.31769923],
            [0.95447591, 0.47082238, 0.32310953],
            [0.95510255, 0.47787236, 0.32862553],
            [0.95569679, 0.48489115, 0.33421404],
            [0.95626788, 0.49187351, 0.33985601],
            [0.95681685, 0.49882008, 0.34555431],
            [0.9573439, 0.50573243, 0.35130912],
            [0.95784842, 0.51261283, 0.35711942],
            [0.95833051, 0.51946267, 0.36298589],
            [0.95879054, 0.52628305, 0.36890904],
            [0.95922872, 0.53307513, 0.3748895],
            [0.95964538, 0.53983991, 0.38092784],
            [0.96004345, 0.54657593, 0.3870292],
            [0.96042097, 0.55328624, 0.39319057],
            [0.96077819, 0.55997184, 0.39941173],
            [0.9611152, 0.5666337, 0.40569343],
            [0.96143273, 0.57327231, 0.41203603],
            [0.96173392, 0.57988594, 0.41844491],
            [0.96201757, 0.58647675, 0.42491751],
            [0.96228344, 0.59304598, 0.43145271],
            [0.96253168, 0.5995944, 0.43805131],
            [0.96276513, 0.60612062, 0.44471698],
            [0.96298491, 0.6126247, 0.45145074],
            [0.96318967, 0.61910879, 0.45824902],
            [0.96337949, 0.6255736, 0.46511271],
            [0.96355923, 0.63201624, 0.47204746],
            [0.96372785, 0.63843852, 0.47905028],
            [0.96388426, 0.64484214, 0.4861196],
            [0.96403203, 0.65122535, 0.4932578],
            [0.96417332, 0.65758729, 0.50046894],
            [0.9643063, 0.66393045, 0.5077467],
            [0.96443322, 0.67025402, 0.51509334],
            [0.96455845, 0.67655564, 0.52251447],
            [0.96467922, 0.68283846, 0.53000231],
            [0.96479861, 0.68910113, 0.53756026],
            [0.96492035, 0.69534192, 0.5451917],
            [0.96504223, 0.7015636, 0.5528892],
            [0.96516917, 0.70776351, 0.5606593],
            [0.96530224, 0.71394212, 0.56849894],
            [0.96544032, 0.72010124, 0.57640375],
            [0.96559206, 0.72623592, 0.58438387],
            [0.96575293, 0.73235058, 0.59242739],
            [0.96592829, 0.73844258, 0.60053991],
            [0.96612013, 0.74451182, 0.60871954],
            [0.96632832, 0.75055966, 0.61696136],
            [0.96656022, 0.75658231, 0.62527295],
            [0.96681185, 0.76258381, 0.63364277],
            [0.96709183, 0.76855969, 0.64207921],
            [0.96739773, 0.77451297, 0.65057302],
            [0.96773482, 0.78044149, 0.65912731],
            [0.96810471, 0.78634563, 0.66773889],
            [0.96850919, 0.79222565, 0.6764046],
            [0.96893132, 0.79809112, 0.68512266],
            [0.96935926, 0.80395415, 0.69383201],
            [0.9698028, 0.80981139, 0.70252255],
            [0.97025511, 0.81566605, 0.71120296],
            [0.97071849, 0.82151775, 0.71987163],
            [0.97120159, 0.82736371, 0.72851999],
            [0.97169389, 0.83320847, 0.73716071],
            [0.97220061, 0.83905052, 0.74578903],
            [0.97272597, 0.84488881, 0.75440141],
            [0.97327085, 0.85072354, 0.76299805],
            [0.97383206, 0.85655639, 0.77158353],
            [0.97441222, 0.86238689, 0.78015619],
            [0.97501782, 0.86821321, 0.78871034],
            [0.97564391, 0.87403763, 0.79725261],
            [0.97628674, 0.87986189, 0.8057883],
            [0.97696114, 0.88568129, 0.81430324],
            [0.97765722, 0.89149971, 0.82280948],
            [0.97837585, 0.89731727, 0.83130786],
            [0.97912374, 0.90313207, 0.83979337],
            [0.979891, 0.90894778, 0.84827858],
            [0.98067764, 0.91476465, 0.85676611],
            [0.98137749, 0.92061729, 0.86536915]]
        return plt.matplotlib.colors.LinearSegmentedColormap.from_list(
            'rocket_r', rocket_colors[::-1])
    
    # noinspection PyUnresolvedReferences
    def plot_heatmap(self,
                     x: SingleCellColumn,
                     y: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     normalize_rows: bool = False,
                     normalize_columns: bool = False,
                     ax: 'Axes' | None = None,
                     colormap: str | 'Colormap' | None = None,
                     heatmap_kwargs: dict[str, Any] | None = None,
                     label: bool = False,
                     label_format: str | None = None,
                     label_kwargs: dict[str, Any] | None = None,
                     colorbar: bool = True,
                     colorbar_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | Literal[True] | None = True,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | Literal[True] | None = True,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot a heatmap of the count of each combination of two categorical
        columns, `x` and `y`. If `normalize_rows` or `normalize_columns` is
        specified, plot percentages instead of counts, so that each row or
        column sums to 100%.
        
        Args:
            x: the first column; must be String, Enum or Categorical
            y: the second column; must be String, Enum or Categorical
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored.
            normalize_rows: whether to plot percentages instead of counts, so
                            that each row sums to 100%. Mutually exclusive with
                            `normalize_columns`.
            normalize_columns: whether to plot percentages instead of counts,
                               so that each column sums to 100%. Mutually
                               exclusive with `normalize_rows`.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in the heatmap, or `None` to use
                      Seaborn's `'rocket_r'` colormap.
            heatmap_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.pcolormesh()` when generating the heatmap, such
                            as:
                            - `rasterized`: whether to convert the heatmap
                              cells to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `norm`, `vmin`, and `vmax`: control how the
                              colormap maps counts or percentages to heatmap
                              colors
                            - `edgecolors`: the border color of each heatmap
                              cell; defaults to `'none'`, meaning no borders.
                              Specifying `cmap` will raise an error, since it
                              conflicts with the `colormap` argument.
            label: whether to label each cell of the heatmap with its count
                   (or percentage, if `normalize_rows=True` or
                    `normalize_columns=True`)
            label_format: a format string to apply to the label for each count
                          or percentage. If `None`, use `'{:,}'` for counts and
                          `'{:.2f}%'` for percentages. Can only be specified
                          when `label=True`.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size.
                             By default, the color is dark gray for
                             light-colored cells, and white for dark-colored
                             ones.
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                          Can only be specified when `label=True`.
            colorbar: whether to add a colorbar
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to
                               0.5 / log1p(number of heatmap rows), instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, `True` to use the name of `x` as the
                    x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, `True` to use the name of `y` as the
                    y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the spines (borders of the plot area)
                     from the plot; unlike the other plotting functions in this
                     library, this also removes the left and bottom spines
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get `x` and `y`, and check that they are String, Enum, or Categorical
        x = self._get_column('obs', x, 'x',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        y = self._get_column('obs', y, 'y',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `normalize_rows`, `normalize_columns`, `label`,
        # `colorbar`, and `despine` are Boolean
        check_type(normalize_rows, 'normalize_rows', bool, 'Boolean')
        check_type(normalize_columns, 'normalize_columns', bool, 'Boolean')
        check_type(label, 'label', bool, 'Boolean')
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # Check that `normalize_rows` and `normalize_columns` are mutually
        # exclusive
        if normalize_rows and normalize_columns:
            error_message = \
                'only one of normalize_rows and normalize_columns can be True'
            raise ValueError(error_message)
        # Check that `colormap` is a string in `plt.colormaps`, a Colormap
        # object, or `None`; if `None`, default to Seaborn's rocket_r colormap
        if colormap is None:
            colormap = SingleCell._get_rocket_r()
        else:
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap),
                       'a string or matplotlib Colormap object')
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
        # If `label=False`, check that `label_format` and `label_kwargs` are
        # `None`.
        if not label:
            if label_format is not None:
                error_message = 'label_format must be None when label=False'
                raise ValueError(error_message)
            if label_kwargs is not None:
                error_message = 'label_kwargs must be None when label=False'
                raise ValueError(error_message)
        # If not `None`, check that `label_format` is a valid format string.
        # For simplicity, just check that it has curly braces and that all
        # braces are matched. If `label_format` is `None`, use `'{:,}'` for
        # counts or, if `normalize_rows=True` or `normalize_columns=True`,
        # `'{:.2f}%'` for percentages.
        if label_format is None:
            label_format = \
                '{:.2f}%' if normalize_rows or normalize_columns else '{:,}'
        else:
            check_type(label_format, 'label_format', str, 'a string')
            open_braces = 0
            has_braces = False
            for char in label_format:
                if char == '{':
                    has_braces = True
                    open_braces += 1
                elif char == '}':
                    open_braces -= 1
                if open_braces < 0:
                    error_message = \
                        'label_format contains mismatched curly braces'
                    raise ValueError(error_message)
            if open_braces == 0:
                error_message = 'label_format contains mismatched curly braces'
                raise ValueError(error_message)
            if not has_braces:
                error_message = 'label_format must contain curly braces'
                raise ValueError(error_message)
        # If `colorbar=False`, check that `colorbar_kwargs` is None
        if not colorbar and colorbar_kwargs is not None:
            error_message = 'colorbar_kwargs must be None when colorbar=False'
            raise ValueError(error_message)
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well.
        if title is not None:
            check_type(title, 'title', str, 'a string')
        elif title_kwargs is not None:
            error_message = 'title_kwargs must be None when title is None'
            raise ValueError(error_message)
        # Check that `xlabel` is a string, `True` (in which case set it to
        # `x.name`), or `None`; if `None`, check that `xlabel_kwargs` is `None`
        # as well. Ditto for `ylabel`.
        if xlabel is not None:
            if xlabel is True:
                xlabel = x.name
            else:
                check_type(xlabel, 'xlabel', str, 'a string')
        elif xlabel_kwargs is not None:
            error_message = 'xlabel_kwargs must be None when xlabel is None'
            raise ValueError(error_message)
        if ylabel is not None:
            if ylabel is True:
                ylabel = y.name
            else:
                check_type(ylabel, 'ylabel', str, 'a string')
        elif ylabel_kwargs is not None:
            error_message = 'ylabel_kwargs must be None when ylabel is None'
            raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((heatmap_kwargs, 'heatmap_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Override the defaults for certain values of `heatmap_kwargs`; if
        # specified, check that `heatmap_kwargs` does not contain the `cmap`
        # argument
        default_heatmap_kwargs = dict(rasterized=True)
        if heatmap_kwargs is None:
            heatmap_kwargs = default_heatmap_kwargs
        else:
            if 'cmap' in heatmap_kwargs:
                error_message = (
                    f"'cmap' cannot be specified as a key in heatmap_kwargs; "
                    f"specify the colormap argument instead")
                raise ValueError(error_message)
            heatmap_kwargs = heatmap_kwargs | default_heatmap_kwargs
        # Get the heatmap data
        count = pl.DataFrame((x, y))\
            .group_by(pl.all(), maintain_order=True)\
            .len(name='_SingleCell_count')\
            .pivot(index=y.name, columns=x.name, values='_SingleCell_count')\
            .fill_null(0)
        heatmap_data = count[:, 1:].to_numpy()
        # Normalize, if `normalize_rows=True` or `normalize_columns=True`
        if normalize_rows:
            heatmap_data /= heatmap_data.sum(axis=1, keepdims=True)
        elif normalize_columns:
            heatmap_data /= heatmap_data.sum(axis=0, keepdims=True)
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            height, width = heatmap_data.shape
            if make_new_figure:
                plt.figure(figsize=(3 * np.log1p(width), 3 * np.log1p(height)),
                           constrained_layout=True)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make the heatmap
            xticks = np.arange(0.5, width)
            yticks = np.arange(0.5, height)
            heatmap = \
                ax.pcolormesh(heatmap_data, cmap=colormap, **heatmap_kwargs)
            ax.set_xticks(xticks, count.columns[1:], rotation=90)
            ax.set_yticks(yticks, count[:, 0].to_numpy())
            ax.set_aspect('equal')
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`. If normalizing rows or columns, make the
            # colorbar ticks percentages and set the range to be 0-100%.
            if colorbar:
                default_colorbar_kwargs = dict(shrink=0.5 / np.log1p(height),
                                               pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(heatmap, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
                if normalize_rows or normalize_columns:
                    cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1])
                    cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%',
                                         '100%'])
            # Add labels; this code is edited from `_annotate_heatmap()` at
            # github.com/mwaskom/seaborn/blob/master/seaborn/matrix.py
            if label:
                heatmap.update_scalarmappable()
                xpos, ypos = np.meshgrid(xticks, yticks)
                if label_kwargs is None:
                    label_kwargs = {}
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')))
                if 'c' in label_kwargs or 'color' in label_kwargs:
                    # Use the same color for all labels
                    for x, y, val in zip(xpos.ravel(), ypos.ravel(),
                                         heatmap_data.ravel()):
                        ax.text(x, y, s=label_format.format(val),
                                **label_kwargs)
                else:
                    # Use either dark gray or white for the label, depending on
                    # the cell's luminance
                    rgb_weights = np.array([0.2126, 0.7152, 0.0722])
                    for x, y, color, val in zip(
                            xpos.ravel(), ypos.ravel(),
                            heatmap.get_facecolors(), heatmap_data.ravel()):
                        rgb = plt.matplotlib.colors.colorConverter\
                            .to_rgba_array(color)[:, :3]
                        rgb = np.where(rgb <= 0.03928, rgb / 12.92,
                                       ((rgb + 0.055) / 1.055) ** 2.4)
                        lum = rgb.dot(rgb_weights).item()
                        ax.text(x, y, s=label_format.format(val),
                                c='.15' if lum > .408 else 'w', **label_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                for direction in 'top', 'bottom', 'left', 'right':
                    spines[direction].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    def find_markers(self,
                     cell_type_column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     min_detection_rate: int | float | np.integer |
                                         np.floating = 0.25,
                     min_fold_change: int | float | np.integer |
                                      np.floating = 2,
                     pareto: bool = True,
                     all_genes: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            pl.DataFrame:
        """
        Find "marker genes" that distinguish each cell type from all other cell
        types. This function gives the same result regardless of whether it is
        run before or after normalization.
        
        Marker genes are chosen via an adaptation of the strategy of Fischer
        and Gillis 2021 (ncbi.nlm.nih.gov/pmc/articles/PMC8571500). For a given
        cell type, genes are scored based on a) their "detection rate" in that
        cell type (the fraction of cells of that type that have non-zero count
        for that gene), as well as b) the fold change in detection rate between
        that cell type and every other cell type. Genes must also have a
        detection rate of at least `min_detection_rate` (25% by default) and a
        minimum fold change of at least `min_fold_change` (2-fold by default)
        to be considered as markers.
        
        There is an inherent tradeoff between these two metrics. For instance,
        candidate marker genes with high enough expression to be expressed in
        every cell of a given type (i.e. to have a high detection rate) tend to
        also have at least some expression in other cell types (i.e. a low fold
        change in detection rate).
        
        Thus, marker genes are selected to optimally trade off between these
        two metrics: all genes on the Pareto front of the two metrics (i.e.
        genes for which there is no other gene that does better on both metrics
        simultaneously) are selected as marker genes.
        
        Note that Fischer and Gillis use AUROC versus log2 fold change in
        detection rate, instead of detection rate versus fold change in
        detection rate. However, detection rate is much faster to compute than
        AUROC, and is a very accurate proxy for AUROC: as Figure 1D in their
        paper shows, AUROC is almost perfectly correlated with detection rate
        across marker genes.
        
        Args:
            cell_type_column: a column of obs containing cell-type labels. Can
                              be a column name, a polars expression, a polars
                              Series, a 1D NumPy array, or a function that
                              takes in this SingleCell dataset and returns a
                              polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored.
            min_detection_rate: the minimum detection rate required to select
                                a gene as a marker gene; must be positive and
                                less than or equal to 1
            min_fold_change: the minimum fold change in detection rate required
                             to select a gene as a marker gene; must be greater
                             than 1
            pareto: if `True`, include only genes on the Pareto front of
                    detection rate and fold change as markers; if `False`,
                    include all genes that pass the `min_detection_rate` and
                    `min_fold_change` thresholds as markers
            all_genes: if `True`, include all genes in the output, not just
                       marker genes. An additional Boolean column will be
                       included to specify which genes are the marker genes.
                       Note that this option does not change which marker genes
                       are selected, only which information is returned.
            num_threads: the number of threads to use for marker-gene finding.
                         Set `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores. For count
                         matrices stored in the usual CSR format, the most
                         time-consuming step (calculating detection counts) is
                         parallelized across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.
        
        Returns:
            By default, a DataFrame with one row per marker gene, with columns:
            - `'cell_type'`: a cell-type name from `cell_type_column`
            - `'gene'`: a gene symbol from var_names
            - `'detection_rate'`: the gene's detection rate in that cell type
            - `'fold_change'`, the gene's fold change in detection rate
              between that cell type and all other cell types
            If `all_genes=True`, a DataFrame with one row per cell type-gene
            pair, with those four columns plus one other:
            - `'marker'`, a Boolean column listing whether the gene is a marker
              for that cell type
            If `all_genes=False`, marker genes within each cell type will be
            sorted in increasing order of detection rate, and decreasing order
            of fold change.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        X = self._X
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        cell_type_column_name = cell_type_column.name
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Check that `min_detection_rate` and `min_fold_change` are numeric and
        # have the correct ranges: 0 < min_detection_rate <= 1,
        # min_fold_change > 1
        check_type(min_detection_rate, 'min_detection_rate',
                   (int, float), 'a positive number less than or equal to 1')
        check_bounds(min_detection_rate, 'min_detection_rate', 0, 1,
                     left_open=True)
        check_type(min_fold_change, 'min_fold_change', (int, float),
                   'a number greater than 1')
        check_bounds(min_fold_change, 'min_fold_change', 1, left_open=True)
        # Check that `all_genes` is Boolean
        check_type(all_genes, 'all_genes', bool, 'Boolean')
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in obs
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene, i.e. the gene's detection
        # count in that cell type
        num_cell_types = len(groups)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = X.shape[1]
        detection_count = np.zeros((num_cell_types, num_genes), dtype=np.int32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            cython_inline(f'''
                from cython.parallel cimport prange
                
                def groupby_getnnz_csr(
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int num_groups = group_ends.shape[0]
                    cdef int group, row
                    cdef long gene
                    cdef unsigned cell
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse matrix
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(indptr[row], indptr[row + 1]):
                                    # Add 1 to the total for this group and gene
                                    result[group, indices[gene]] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(indptr[row], indptr[row + 1]):
                                    result[group, indices[gene]] += 1
            ''')['groupby_getnnz_csr'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                result=detection_count, num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = QC_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            cython_inline(f'''
                from cython.parallel cimport prange
                
                def groupby_getnnz_csc(
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_map,
                        const bint has_missing,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int column, group, num_columns = result.shape[1]
                    cdef long row
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene (column of the sparse matrix)...
                            for column in range(num_columns):
                                # For each cell (row) that's non-zero for this gene...
                                for row in range(indptr[column], indptr[column + 1]):
                                    # Get the group index for this row (-1 if it failed
                                    # QC)
                                    group = group_map[indices[row]]
                                    if group == -1: continue
                                    # Add 1 to the total for this group and column
                                    result[group, column] += 1
                        else:
                            for column in range(num_columns):
                                for row in range(indptr[column], indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    result[group, column] += 1
                    else:
                        if has_missing:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                for row in range(indptr[column], indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    if group == -1: continue
                                    result[group, column] += 1
                        else:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                for row in range(indptr[column], indptr[column + 1]):
                                    group = group_map[indices[row]]
                                    result[group, column] += 1
                    ''')['groupby_getnnz_csc'](
                        indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=detection_count, num_threads=num_threads)
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.int32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes), dtype=float)
        fold_change = np.empty((num_cell_types, num_genes), dtype=float)
        is_pareto = np.empty((num_cell_types, num_genes), dtype=bool)
        cython_inline(rf'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const int[:, ::1] detection_count,
                    const int[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    const double min_detection_rate,
                    const double min_fold_change,
                    double[:, ::1] detection_rate,
                    double[:, ::1] fold_change,
                    char[:, ::1] is_pareto,
                    const unsigned num_threads):
                cdef int cell_type, gene, count, background_count, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef unsigned num_cells, background_num_cells
                cdef double pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells,
                min_detection_rate=min_detection_rate,
                min_fold_change=min_fold_change, detection_rate=detection_rate,
                fold_change=fold_change, is_pareto=is_pareto,
                num_threads=num_threads)
        # If `pareto=True`, get the set of genes on the Pareto front of the two
        # metrics for each cell type; these are the marker genes.
        # If `pareto=False`, just include the genes we've initialized
        # `is_pareto` to `True` for: all those passing the `min_detection_rate`
        # and `min_fold_change` thresholds.
        if pareto:
            cython_inline(rf'''
                from cython.parallel cimport prange
                
                def pareto_front(double[:, ::1] detection_rate,
                                 double[:, ::1] fold_change,
                                 char[:, ::1] is_pareto,
                                 int num_threads):
                    cdef int gene, other_gene, cell_type, \
                        num_cell_types = detection_count.shape[0], \
                        num_genes = detection_count.shape[1]
                    cdef double gene_detection_rate, gene_fold_change
                    
                    if num_threads == 1:
                        for cell_type in range(num_cell_types):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                    else:
                        for cell_type in prange(num_cell_types, nogil=True,
                                                num_threads=num_threads):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                ''')['pareto_front'](
                    detection_rate=detection_rate, fold_change=fold_change,
                    is_pareto=is_pareto, num_threads=num_threads)
        # Return a DataFrame of the selected marker genes, or all genes if
        # `all_genes=True`
        cell_types = groups[cell_type_column_name].rename('cell_type')
        genes = self._var[:, 0].rename('gene')
        if all_genes:
            cell_types = pl.select(pl.lit(cell_types).repeat_by(num_genes))\
                .explode('cell_type')\
                .to_series()
            genes = pl.concat([genes] * num_cell_types)
            return pl.DataFrame((
                cell_types, genes,
                pl.Series('marker', is_pareto.ravel()),
                pl.Series('detection_rate', detection_rate.ravel()),
                pl.Series('fold_change', fold_change.ravel())))
        else:
            cell_type_indices, gene_indices = is_pareto.nonzero()
            return pl.DataFrame((
                cell_types[cell_type_indices], genes[gene_indices],
                pl.Series('detection_rate', detection_rate[
                    cell_type_indices, gene_indices].ravel()),
                pl.Series('fold_change', fold_change[
                    cell_type_indices, gene_indices].ravel())))\
                .select(pl.all().sort_by('detection_rate').over('cell_type'))
    
    # noinspection PyUnresolvedReferences
    def plot_markers(self,
                     genes: str | Iterable[str],
                     cell_type_column: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     colormap: str | 'Colormap' = 'RdBu_r',
                     swap_axes: bool = False,
                     scatter_kwargs: dict[str, Any] | None = None,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = None,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = None,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None,
                     num_threads: int | np.integer | None = None) -> None:
        """
        Make a dot plot of the detection rate and fold change of a set of genes
        across cell types - the same metrics used to select marker genes in
        `find_markers()`. This function gives the same result regardless of
        whether it is run before or after normalization.
        
        The size of a gene's dot for a cell type represents its
        "detection rate" in that cell type (the fraction of cells of that type
        that have non-zero count for that gene), while its color represents the
        gene's fold change in detection rate between that cell type and every
        other cell type (by default: more positive fold changes are redder,
        while more negative fold changes are bluer).
        
        Unlike the other plotting functions, this is a figure-level rather than
        an axis-level function, and does not take an `axis` argument.
        
        Args:
            genes: a list of genes to plot: for instance, marker genes found by
                   `find_markers()`, or marker genes from the literature
            cell_type_column: a column of obs containing cell-type labels. Can
                              be a column name, a polars expression, a polars
                              Series, a 1D NumPy array, or a function that
                              takes in this SingleCell dataset and returns a
                              polars Series or 1D NumPy array.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in `ax.scatter()` for representing fold
                      changes
            swap_axes: if `True`, plot genes on the y-axis and cell types on
                       the x-axis, instead of the other way around
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since the size and color of each point
                            are set automatically, and `cmap` conflicts with
                            the `colormap` argument.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. The legend will be placed in its
                             own axis in the top right of the plot, and by
                             default, `loc` is set to `'center'`.
                           - `ncols` to set its number of columns
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border. `frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`)
                           - `title` to modify the legend title
                             (`'Detection rate'` by default)
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`), instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
            num_threads: the number of threads to use when tabulating each
                         gene's detection rate and fold change. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores. For count
                         matrices stored in the usual CSR format, the most
                         time-consuming step (calculating detection counts) is
                         parallelized across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        import matplotlib.pyplot as plt
        X = self._X
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "plot_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        # Get `genes` as a polars Series of the same dtype as var_names; make
        # sure all its entries are unique and present in var_names
        genes = to_tuple_checked(genes, 'genes', str, 'strings')
        genes = pl.Series(genes)
        if genes.n_unique() < len(genes):
            error_message = 'genes contains duplicates'
            raise ValueError(error_message)
        var_names = self._var[:, 0]
        if not genes.is_in(var_names).all():
            if not genes.is_in(var_names).any():
                error_message = \
                    'none of the specified genes were found in var_names'
                raise ValueError(error_message)
            else:
                for gene in genes:
                    if gene not in var_names:
                        error_message = (
                            f'one of the specified genes, {gene!r}, was not '
                            f'found in var_names')
                        raise ValueError(error_message)
        if var_names.dtype != pl.String:
            genes = genes.cast(var_names.dtype)
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        cell_type_column_name = cell_type_column.name
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `colormap` is a string in `plt.colormaps` or a Colormap
        # object
        check_type(colormap, 'colormap', (str, plt.matplotlib.colors.Colormap),
                   'a string or matplotlib Colormap object')
        if isinstance(colormap, str):
            colormap = plt.colormaps[colormap]
        # Check that `swap_axes` and `despine` are Boolean
        check_type(swap_axes, 'swap_axes', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((scatter_kwargs, 'scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in obs
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        cell_types = groups[cell_type_column_name]
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene in `genes`, i.e. the gene's
        # detection count in that cell type
        num_cell_types = len(cell_types)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = len(genes)
        detection_count = np.zeros((num_cell_types, num_genes), dtype=np.int32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            # Get an array mapping each gene in var_names to its position in
            # `genes` (-1 if missing from `genes`)
            gene_map = var_names\
                .to_frame()\
                .join(genes
                      .to_frame(var_names.name)
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.Int32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()
            gene_map = gene_map.fill_null(-1).to_numpy()
            cython_inline(f'''
                from cython.parallel cimport prange
                
                def groupby_getnnz_csr(
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_indices,
                        const unsigned[::1] group_ends,
                        const int[::1] gene_map,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int num_groups = group_ends.shape[0]
                    cdef int group, row, column
                    cdef long gene
                    cdef unsigned cell
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse matrix
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(indptr[row], indptr[row + 1]):
                                    # Get this gene's column index in `result`
                                    # (-1 if the gene is not in `genes`)
                                    column = gene_map[indices[gene]]
                                    if column == -1: continue
                                    # Add 1 to the total for this group and gene
                                    result[group, column] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(indptr[row], indptr[row + 1]):
                                    column = gene_map[indices[gene]]
                                    if column == -1: continue
                                    result[group, column] += 1
            ''')['groupby_getnnz_csr'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                gene_map=gene_map, result=detection_count,
                num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.Int32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = QC_column is not None
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            # Get an array mapping each gene in `genes` to its position in
            # var_names
            gene_map = genes\
                .to_frame(var_names.name)\
                .join(var_names
                      .to_frame()
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.Int32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()\
                .to_numpy()
            cython_inline(f'''
                from cython.parallel cimport prange
                
                def groupby_getnnz_csc(
                        const {cython_type(X.indices.dtype)}[::1] indices,
                        const {cython_type(X.indptr.dtype)}[::1] indptr,
                        const int[::1] group_map,
                        const int[::1] gene_map,
                        const bint has_missing,
                        int[:, ::1] result,
                        const unsigned num_threads):
                    cdef int column, gene, group, num_columns = result.shape[1]
                    cdef long cell
                    
                    if num_threads == 1:
                        if has_missing:
                            # For each gene...
                            for column in range(num_columns):
                                # Get the index of this gene in the count matrix
                                gene = gene_map[column]
                                # For each cell (row) that's non-zero for this gene...
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    # Get the group index for this cell (-1 if it
                                    # failed QC)
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    # Add 1 to the total for this group and gene
                                    result[group, column] += 1
                        else:
                            for column in range(num_columns):
                                gene = gene_map[column]
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, column] += 1
                    else:
                        if has_missing:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                gene = gene_map[column]
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    if group == -1: continue
                                    result[group, column] += 1
                        else:
                            for column in prange(num_columns, nogil=True,
                                                 num_threads=num_threads):
                                gene = gene_map[column]
                                for cell in range(indptr[gene], indptr[gene + 1]):
                                    group = group_map[indices[cell]]
                                    result[group, column] += 1
                    ''')['groupby_getnnz_csc'](
                        indices=X.indices, indptr=X.indptr,
                        group_map=group_map, gene_map=gene_map,
                        has_missing=has_missing, result=detection_count,
                        num_threads=num_threads)
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.int32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes), dtype=float)
        fold_change = np.empty((num_cell_types, num_genes), dtype=float)
        cython_inline(rf'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const int[:, ::1] detection_count,
                    const int[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    double[:, ::1] detection_rate,
                    double[:, ::1] fold_change,
                    const unsigned num_threads):
                cdef int cell_type, gene, count, background_count, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef unsigned num_cells, background_num_cells
                cdef double pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <double> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells, detection_rate=detection_rate,
                fold_change=fold_change, num_threads=num_threads)
        
        # If `swap_axes=True`, swap cell types and genes
        if swap_axes:
            cell_types, genes = genes, cell_types
            num_cell_types, num_genes = num_genes, num_cell_types
            detection_rate = detection_rate.T
            fold_change = fold_change.T
        
        # Calculate the range of the legend, and the multiplier to multiply
        # each point's size by
        max_detection_rate = detection_rate.max()
        interval = 0.2 if max_detection_rate > 0.5 else \
            0.1 if max_detection_rate > 0.2 else 0.05
        max_detection_rate = \
            np.ceil(max_detection_rate / interval) * interval
        legend_point_sizes = \
            np.arange(interval, max_detection_rate + interval / 2, interval)
        point_size_multiplier = 180 / max_detection_rate
        
        # Calculate the plot dimensions
        # noinspection PyTypeChecker
        width_ratio = max(4, 0.2 * num_genes)
        width = 6.4 / (1 + width_ratio) + \
                6.4 * width_ratio / (1 + width_ratio) * \
                max(num_genes, 5) / 20
        height = max(4.8, 1 + 3.8 * num_cell_types / 20)
        
        try:
            # Make the figure, including separate portions on the left for the
            # legend and colorbar
            fig = plt.figure(figsize=(width, height), constrained_layout=True)
            gs = fig.add_gridspec(2, 2, width_ratios=[width_ratio, 1],
                                  height_ratios=[1, 1])
            
            # Plot the circles; override the defaults for certain keys of
            # `scatter_kwargs`
            ax_main = fig.add_subplot(gs[:, 0])  # the main plot spans all rows
            point_size = detection_rate * point_size_multiplier
            x, y = np.meshgrid(range(len(genes)), range(len(cell_types)))
            default_scatter_kwargs = dict(linewidths=0)
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
            scatter = ax_main.scatter(
                x.ravel(), y.ravel(), s=point_size.ravel(),
                c=np.log2(fold_change), cmap=colormap, **scatter_kwargs)
            ax_main.set_aspect('equal')
            ax_main.invert_yaxis()
            
            # Set x and y limits
            padding = 0.6
            ax_main.set_xlim([-padding, len(genes) - 1 + padding])
            ax_main.set_ylim([-padding, len(cell_types) - 1 + padding])
            
            # Add x and y ticks and tick labels
            ax_main.set_xticks(range(len(genes)), genes, rotation=90)
            ax_main.set_yticks(range(len(cell_types)), cell_types)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax_main.set_xlabel(xlabel)
                else:
                    ax_main.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax_main.set_ylabel(ylabel)
                else:
                    ax_main.set_ylabel(ylabel, **ylabel_kwargs)
            
            # Add a legend for detection rate; markers should be at intervals
            # of X% (X%, 2X%, 3X%, ...) up to the maximum detection rate
            # (rounded up to the nearest X%). Override the defaults for
            # certain keys of `legend_kwargs`.
            ax_legend = fig.add_subplot(gs[0, 1])
            ax_legend.axis('off')
            legend_elements = [
                plt.Line2D([0], [0], label=f'{100 * size:.0f}%',
                           markersize=np.sqrt(size * point_size_multiplier),
                           marker='o', linestyle='None',
                           markerfacecolor='black', markeredgecolor='None')
                for size in legend_point_sizes]
            default_legend_kwargs = dict(title='Detection rate', loc='center',
                                         frameon=False)
            legend_kwargs = default_legend_kwargs | legend_kwargs \
                if legend_kwargs is not None else default_legend_kwargs
            ax_legend.legend(handles=legend_elements, **legend_kwargs)
            
            # Add a colorbar for fold change, with labels at powers of 2
            ax_colorbar = fig.add_subplot(gs[1, 1])
            cbar = plt.colorbar(scatter, cax=ax_colorbar)
            cbar.outline.set_visible(False)
            cbar.ax.set_box_aspect(12)
            cbar.ax.set_title('Fold change', size='medium')
            cbar.ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))
            cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(
                lambda x, pos: f'{2 ** x:.4f}'.rstrip('0').rstrip('.')))
            
            # Add the title
            if title is not None:
                if title_kwargs is None:
                    ax.set_title(title)
                else:
                    ax.set_title(title, **title_kwargs)
        
            # Despine, if specified
            if despine:
                spines = ax_main.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                plt.close()
        except:
            # Since we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            plt.close()
            raise
    
    def embed(self,
              *,
              QC_column: SingleCellColumn | None = 'passed_QC',
              PC_key: str = 'PCs',
              neighbors_key: str = 'neighbors',
              embedding_key: str = 'PaCMAP',
              num_neighbors: int | np.integer = 10,
              num_extra_neighbors: int | np.integer = 10,
              num_mid_near_pairs: int | np.integer = 5,
              num_further_pairs: int | np.integer = 20,
              num_iterations: int | np.integer |
                              tuple[int | np.integer, int | np.integer,
                                    int | np.integer] | None = None,
              learning_rate: int | float | np.integer | np.floating = 1.0,
              seed: int | np.integer = 0,
              faster_single_threading: bool = False,
              overwrite: bool = False,
              verbose: bool = True,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate a two-dimensional embedding of this SingleCell dataset
        suitable for plotting with `plot_embedding()`.
        
        Uses PaCMAP (Pairwise Controlled Manifold Approximation;
        github.com/YingfanWang/PaCMAP; arxiv.org/pdf/2012.04456), a faster
        alternative to UMAP that also captures global structure better.
        
        This function is intended to be run after `PCA()` and `neighbors()`; by
        default, it uses `obsm['PCs']` and `obsm['neighbors']` as the inputs to
        PaCMAP, and stores the output in `obsm['PaCMAP']` as a `len(obs)` × 2
        NumPy array. It can also be run on Harmony embeddings by running
        `harmonize()` and then specifying `PC_key='Harmony_PCs'`.
        
        Args:
            QC_column: an optional Boolean column of obs indicating which cells
                       passed QC. Can be a column name, a polars expression, a
                       polars Series, a 1D NumPy array, or a function that
                       takes in this SingleCell dataset and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will be ignored and have their
                       embeddings set to NaN.
            PC_key: the key of obsm containing the principal components
                    calculated with `PCA()`, to use as an input for the
                    embedding calculation. Can also be set to the Harmony
                    embeddings calculated by `harmonize()`, by specifying
                    `PC_key='Harmony_PCs'`.
            neighbors_key: the key of obsm containing the nearest-neighbor
                           indices for each cell, to use as an input for the
                           embedding calculation
            embedding_key: the key of obsm where the embeddings will be stored
            num_neighbors: the number of nearest neighbors to consider for
                           local structure preservation. `neighbors_key` must
                           contain at least
                           `num_neighbors + num_extra_neighbors` nearest
                           neighbors.
            num_extra_neighbors: the number of extra nearest neighbors (on top
                                 of `num_neighbors`) to search for initially,
                                 before pruning to the `num_neighbors` of these
                                 `num_neighbors + num_extra_neighbors` cells
                                 with the smallest scaled distances. For a pair
                                 of cells `i` and `j`, the scaled distance
                                 between `i` and `j` is its squared Euclidean
                                 distance, divided by `i`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors, divided by `j`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors. Must be a positive integer or 0.
                                 Defaults to 10, instead of PaCMAP's original
                                 default of 50. `neighbors_key` must contain at
                                 least `num_neighbors + num_extra_neighbors`
                                 nearest neighbors.
            num_mid_near_pairs: the number of mid-near pairs to consider for
                                global structure preservation
            num_further_pairs: the number of further pairs to consider for
                               local and global structure preservation
            num_iterations: the number of iterations/epochs to run PaCMAP for.
                            Can be a length-3 tuple of the number of iterations
                            for each of the 3 stages of PaCMAP, or a single
                            integer of the number of iterations for the third
                            stage (in which case the number of iterations for
                            the first two stages will be set to 100).
            learning_rate: the learning rate of the Adam optimizer for PaCMAP
            seed: the random seed to use for nearest-neighbor finding
            faster_single_threading: if `True`, use a different order of
                                     operations for single-threaded PaCMAP,
                                     which gives a modest (~15%) boost in
                                     single-threaded performance at the cost of
                                     no longer matching the embedding produced
                                     by the multithreaded version (due to
                                     differences in floating-point round-off
                                     arising from the different order of
                                     operations). Must be `False` unless
                                     `num_threads=1`.
            overwrite: if `True`, overwrite `embedding_key` if already present
                       in obsm, instead of raising an error
            verbose: whether to print details of the PaCMAP construction
            num_threads: the number of cores to run PaCMAP on. Set
                         `num_threads=-1` to use all available cores (as
                         determined by `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores.
        
        Returns:
            A new SingleCell dataset with the PaCMAP embedding added as
            `obsm[embedding_key]`.
        
        Note:
            PaCMAP's original implementation assumes generic input data, so it
            initializes the embedding by standardizing the input data, running
            PCA on it, and taking the first two PCs. Because our input data is
            already PCs (or harmonized PCs), we avoid redundancy by omitting
            this step and initializing the embedding with the first two columns
            of our input data, i.e. the first two PCs.
        """
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get PCs, and check that they are float64 and C-contiguous
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = \
                f'PC_key {PC_key!r} is not a key of obsm'
            if neighbors_key == 'PCs':
                error_message += (
                    '; did you forget to run PCA() (and neighbors()) before '
                    'embed()?')
            raise ValueError(error_message)
        PCs = self._obsm[PC_key]
        if PCs.dtype != float:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float64'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with np.ascontiguousarray(dataset.obsm[{PC_key!r}])')
            raise ValueError(error_message)
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += \
                    '; did you forget to run neighbors() before embed()?'
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}].dtype is '
                f'{nearest_neighbor_indices.dtype!r}, but must be integer')
            raise TypeError(error_message)
        # Subset PCs and nearest-neighbor indices to QCed cells only, if
        # `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `embedding_key` is a string and, unless `overwrite=True`,
        # not already a key in obsm
        check_type(embedding_key, 'embedding_key', str, 'a string')
        if not overwrite and embedding_key in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is already a key of obsm; '
                f'did you already run embed()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Check that `num_neighbors`, `num_mid_near_pairs` and
        # `num_further_pairs` are positive integers
        for variable, variable_name in (
                (num_neighbors, 'num_neighbors'),
                (num_mid_near_pairs, 'num_mid_near_pairs'),
                (num_further_pairs, 'num_further_pairs')):
            check_type(variable, variable_name, int, 'a positive integer')
            check_bounds(variable, variable_name, 1)
        # Check that `num_extra_neighbors` is a positive integer or 0
        check_type(num_extra_neighbors, 'num_extra_neighbors', int,
                   'a positive integer or 0')
        check_bounds(num_extra_neighbors, 'num_extra_neighbors', 0)
        # Check that `num_iterations` is an integer or length-3 tuple of
        # integers, or `None`
        if num_iterations is not None:
            check_type(num_iterations, 'num_iterations', (int, tuple),
                       'a positive integer or length-3 tuple of positive '
                       'integers')
            if isinstance(num_iterations, tuple):
                if len(num_iterations) != 3:
                    error_message = (
                        f'num_iterations must be a positive integer or '
                        f'length-3 tuple of positive integers, but has length '
                        f'{len(num_iterations):,}')
                    raise ValueError(error_message)
                for step, step_num_iterations in enumerate(num_iterations):
                    check_type(step_num_iterations,
                               f'num_iterations[{step!r}]', int,
                               'a positive integer')
                    check_bounds(step_num_iterations,
                                 f'num_iterations[{step!r}]', 1)
            else:
                check_bounds(num_iterations, 'num_iterations', 1)
                num_iterations = 100, 100, num_iterations
        else:
            num_iterations = 100, 100, 250
        # Check that `learning_rate` is a positive floating-point number
        check_type(learning_rate, 'learning_rate', (int, float),
                   'a positive number')
        check_bounds(learning_rate, 'learning_rate', 0, left_open=True)
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()`, and if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads(num_threads)
        # Check that `faster_single_threading` is Boolean, and `False` unless
        # `num_threads` is 1
        check_type(faster_single_threading, 'faster_single_threading', bool,
                   'Boolean')
        if faster_single_threading and num_threads != 1:
            error_message = \
                'faster_single_threading must be False unless num_threads is 1'
            raise ValueError(error_message)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Define Cython functions
        cython_functions = cython_inline(rf'''
        from cython.parallel cimport threadid, prange
        from libc.math cimport sqrt
        from libc.stdlib cimport free, malloc
        from libc.string cimport memcpy
        
        cdef extern from "limits.h":
            cdef int INT_MAX
        
        cdef inline int rand(long* state) noexcept nogil:
            cdef long x = state[0]
            state[0] = x * 6364136223846793005L + 1442695040888963407L
            cdef int s = (x ^ (x >> 18)) >> 27
            cdef int rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline long srand(const long seed) noexcept nogil:
            cdef long state = seed + 1442695040888963407L
            rand(&state)
            return state
        
        cdef inline int randint(const int bound, long* state) noexcept nogil:
            cdef int r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound

        cdef void quicksort(const double[::1] arr,
                            int[::1] indices,
                            int left,
                            int right) noexcept nogil:
            cdef double pivot_value
            cdef int pivot_index, mid, i, temp
    
            while left < right:
                mid = left + (right - left) // 2
                if arr[indices[mid]] < arr[indices[left]]:
                    temp = indices[left]
                    indices[left] = indices[mid]
                    indices[mid] = temp
                if arr[indices[right]] < arr[indices[left]]:
                    temp = indices[left]
                    indices[left] = indices[right]
                    indices[right] = temp
                if arr[indices[right]] < arr[indices[mid]]:
                    temp = indices[mid]
                    indices[mid] = indices[right]
                    indices[right] = temp
    
                pivot_value = arr[indices[mid]]
                temp = indices[mid]
                indices[mid] = indices[right]
                indices[right] = temp
                pivot_index = left
    
                for i in range(left, right):
                    if arr[indices[i]] < pivot_value:
                        temp = indices[i]
                        indices[i] = indices[pivot_index]
                        indices[pivot_index] = temp
                        pivot_index += 1
    
                temp = indices[right]
                indices[right] = indices[pivot_index]
                indices[pivot_index] = temp
    
                if pivot_index - left < right - pivot_index:
                    quicksort(arr, indices, left, pivot_index - 1)
                    left = pivot_index + 1
                else:
                    quicksort(arr, indices, pivot_index + 1, right)
                    right = pivot_index - 1
    
        cdef inline void argsort(const double[::1] arr,
                                 int[::1] indices,
                                 const int n) noexcept nogil:
            cdef int i
            for i in range(n):
                indices[i] = i
            quicksort(arr, indices, 0, n - 1)
        
        def get_scaled_distances(const double[:, ::1] X,
                                 const long[:, :] neighbors,
                                 double[:, ::1] scaled_distances,
                                 const unsigned num_threads):
            cdef int i, j, k, num_cells = scaled_distances.shape[0], \
                num_total_neighbors = scaled_distances.shape[1], \
                num_PCs = X.shape[1]
            cdef double* sig
            
            sig = <double*> malloc(num_cells * sizeof(double))
            if not sig:
                raise MemoryError
            try:
                if num_threads == 1:
                    for i in range(num_cells):
                        for j in range(num_total_neighbors):
                            scaled_distances[i, j] = 0
                            for k in range(num_PCs):
                                scaled_distances[i, j] = \
                                    scaled_distances[i, j] + \
                                    (X[i, k] - X[j, k]) ** 2
                        sig[i] = (sqrt(scaled_distances[i, 3]) +
                                  sqrt(scaled_distances[i, 4]) +
                                  sqrt(scaled_distances[i, 5])) / 3
                        if sig[i] < 1e-10:
                            sig[i] = 1e-10
                    
                    for i in range(num_cells):
                        for j in range(num_neighbors):
                            scaled_distances[i, j] = scaled_distances[i, j] / \
                                sig[i] / sig[neighbors[i, j]]
                else:
                    with nogil:
                        for i in prange(num_cells, num_threads=num_threads):
                            for j in range(num_neighbors):
                                scaled_distances[i, j] = 0
                                for k in range(num_PCs):
                                    scaled_distances[i, j] = \
                                        scaled_distances[i, j] + \
                                        (X[i, k] - X[j, k]) ** 2
                            sig[i] = (sqrt(scaled_distances[i, 3]) +
                                      sqrt(scaled_distances[i, 4]) +
                                      sqrt(scaled_distances[i, 5])) / 3
                            if sig[i] < 1e-10:
                                sig[i] = 1e-10
                        
                        for i in prange(num_cells,
                                        num_threads=num_threads):
                            for j in range(num_neighbors):
                                scaled_distances[i, j] = scaled_distances[i, j] / \
                                    sig[i] / sig[neighbors[i, j]]
            finally:
                free(sig)
    
        def get_neighbor_pairs(const double[:, ::1] X,
                               const double[:, ::1] scaled_distances,
                               const long[:, :] neighbors,
                               int[:, ::1] neighbor_pairs,
                               const unsigned num_threads):
            cdef int i, j, thread_id
            cdef int num_cells = X.shape[0]
            cdef int num_neighbors = neighbor_pairs.shape[1]
            cdef int num_total_neighbors = scaled_distances.shape[1]
            cdef int* indices_buffer
            cdef int[::1] indices
            
            indices_buffer = \
                <int*> malloc(num_total_neighbors * num_threads * sizeof(int))
            if not indices_buffer:
                raise MemoryError
            try:
                indices = \
                    <int[:num_total_neighbors * num_threads:]> indices_buffer
                if num_threads == 1:
                    for i in range(num_cells):
                        argsort(scaled_distances[i], indices, indices.shape[0])
                        for j in range(num_neighbors):
                            neighbor_pairs[i, j] = neighbors[i, indices[j]]
                else:
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        thread_id = threadid()
                        argsort(scaled_distances[i],
                                indices[thread_id * num_total_neighbors:
                                        thread_id * num_total_neighbors +
                                        num_total_neighbors],
                                indices.shape[0])
                        for j in range(num_neighbors):
                            neighbor_pairs[i, j] = neighbors[i, indices[
                                thread_id * num_total_neighbors + j]]
            finally:
                free(indices_buffer)
        
        def sample_mid_near_pairs(const double[:, ::1] X,
                                  int[:, ::1] mid_near_pairs,
                                  const int seed,
                                  const unsigned num_threads):
            cdef int i, j, k, l, thread_id, n = X.shape[0], \
                closest_cell = -1, second_closest_cell = -1, \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_PCs = X.shape[1]
            cdef double squared_distance, smallest, second_smallest
            cdef long state
            cdef int* sampled
            
            sampled = <int*> malloc(6 * num_threads * sizeof(int))
            if not sampled:
                raise MemoryError
            try:
                if num_threads == 1:
                    for i in range(n):
                        state = srand(seed + i)
                        for j in range(num_mid_near_pairs):
                            # Randomly sample 6 cells (which are not the
                            # current cell) and select the 2nd-closest
                            smallest = INT_MAX
                            second_smallest = INT_MAX
                            for k in range(6):
                                while True:
                                    # Sample a random cell...
                                    sampled[k] = randint(n, &state)
                                    # ...that is not this cell...
                                    if sampled[k] == i:
                                        continue
                                    # ...nor a previously sampled cell
                                    for l in range(k):
                                        if sampled[k] == sampled[l]:
                                            break
                                    else:
                                        break
                            for k in range(6):
                                squared_distance = 0
                                for l in range(num_PCs):
                                    squared_distance = squared_distance + \
                                        (X[i, l] - X[sampled[k], l]) ** 2
                                if squared_distance < smallest:
                                    second_smallest = smallest
                                    second_closest_cell = closest_cell
                                    smallest = squared_distance
                                    closest_cell = sampled[k]
                                elif squared_distance < second_smallest:
                                    second_smallest = squared_distance
                                    second_closest_cell = sampled[k]
                            mid_near_pairs[i, j] = second_closest_cell
                else:
                    for i in prange(n, nogil=True,
                                    num_threads=num_threads):
                        thread_id = threadid()
                        state = srand(seed + i)
                        for j in range(num_mid_near_pairs):
                            smallest = INT_MAX
                            second_smallest = INT_MAX
                            for k in range(6 * thread_id, 6 * thread_id + 6):
                                while True:
                                    sampled[k] = randint(n, &state)
                                    if sampled[k] == i:
                                        continue
                                    for l in range(6 * thread_id, k):
                                        if sampled[k] == sampled[l]:
                                            break
                                    else:
                                        break
                            for k in range(6 * thread_id, 6 * thread_id + 6):
                                squared_distance = 0
                                for l in range(num_PCs):
                                    squared_distance = squared_distance + \
                                        (X[i, l] - X[sampled[k], l]) ** 2
                                if squared_distance < smallest:
                                    second_smallest = smallest
                                    second_closest_cell = closest_cell
                                    smallest = squared_distance
                                    closest_cell = sampled[k]
                                elif squared_distance < second_smallest:
                                    second_smallest = squared_distance
                                    second_closest_cell = sampled[k]
                            mid_near_pairs[i, j] = second_closest_cell
            finally:
                free(sampled)
        
        def sample_further_pairs(const double[:, ::1] X,
                                 const int[:, ::1] neighbor_pairs,
                                 int[:, ::1] further_pairs,
                                 const int seed,
                                 const unsigned num_threads):
            """Sample Further pairs using the given seed."""
            cdef int i, j, k, further_pair_index, n = X.shape[0], \
                num_further_pairs = further_pairs.shape[1], \
                num_neighbors = neighbor_pairs.shape[1]
            cdef long state
            
            if num_threads == 1:
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            # Sample a random cell...
                            further_pair_index = randint(n, &state)
                            # ...that is not this cell...
                            if further_pair_index == i:
                                continue
                            # ...nor one of its nearest neighbors...
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                # ...nor a previously sampled cell
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
            else:
                for i in prange(n, nogil=True, num_threads=num_threads):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            further_pair_index = randint(n, &state)
                            if further_pair_index == i:
                                continue
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
        
        def reformat_for_parallel(const int[:, ::1] pairs,
                                  int[::1] pair_indices, int[::1] pair_indptr):
            cdef int i, j, k, dest_index, num_cells = pairs.shape[0], num_pairs_per_cell = pairs.shape[1]
            cdef int* dest_indices
            # Tabulate how often each cell appears in pairs; at a minimum, it
            # will appear `pairs.shape[1]` times (i.e. the number of
            # neighbors), as the `i` in the pair, but it will also appear a
            # variable number of times as the `j` in the pair.
            pair_indptr[0] = 0
            pair_indptr[1:] = pairs.shape[1]
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indptr[j + 1] += 1
            # Take the cumulative sum of the values in `pair_indptr`
            for i in range(2, pair_indptr.shape[0]):
                pair_indptr[i] += pair_indptr[i - 1]
            # Now that we know how many pairs each cell is a part of, do a
            # second pass over `pairs` to populate `pair_indices` with the
            # pairs' indices. Use a temporary buffer, `dest_indices`, to keep
            # track of the index within `pair_indptr` to write each cell's next
            # pair to.
            dest_indices = <int*> malloc(pairs.shape[0] * sizeof(int))
            if not dest_indices:
                raise MemoryError
            try:
                memcpy(dest_indices, &pair_indptr[0],
                       pairs.shape[0] * sizeof(int))
                for i in range(num_cells):
                    for k in range(num_pairs_per_cell):
                        j = pairs[i, k]
                        pair_indices[dest_indices[i]] = j
                        pair_indices[dest_indices[j]] = i
                        dest_indices[i] += 1
                        dest_indices[j] += 1
            finally:
                free(dest_indices)

        def get_gradients(const double[:, ::1] embedding,
                          const int[:, ::1] neighbor_pairs,
                          const int[:, ::1] mid_near_pairs,
                          const int[:, ::1] further_pairs,
                          const double w_neighbors,
                          const double w_mid_near,
                          double[:, ::1] gradients):
            cdef int i, j, k, num_cells = neighbor_pairs.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_further_pairs = further_pairs.shape[1]
            cdef double distance_ij, embedding_ij_0, embedding_ij_1, w
            gradients[:] = 0
            # Nearest-neighbor pairs
            for i in range(num_cells):
                for k in range(num_neighbors):
                    j = neighbor_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
            # Mid-near pairs
            for i in range(num_cells):
                for k in range(num_mid_near_pairs):
                    j = mid_near_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
            # Further pairs
            for i in range(num_cells):
                for k in range(num_further_pairs):
                    j = further_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = 2 / (1 + distance_ij) ** 2
                    gradients[i, 0] -= w * embedding_ij_0
                    gradients[j, 0] += w * embedding_ij_0
                    gradients[i, 1] -= w * embedding_ij_1
                    gradients[j, 1] += w * embedding_ij_1
        
        def get_gradients_parallel(const double[:, ::1] embedding,
                                   const int[::1] neighbor_pair_indices,
                                   const int[::1] neighbor_pair_indptr,
                                   const int[::1] mid_near_pair_indices,
                                   const int[::1] mid_near_pair_indptr,
                                   const int[::1] further_pair_indices,
                                   const int[::1] further_pair_indptr,
                                   const double w_neighbors,
                                   const double w_mid_near,
                                   double[:, ::1] gradients,
                                   const unsigned num_threads):
            cdef int i, j, k, num_cells = embedding.shape[0]
            cdef double distance_ij, embedding_ij_0, embedding_ij_1, w
            
            if num_threads == 1:
                for i in range(num_cells):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    # Nearest-neighbor pairs
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                    # Mid-near pairs
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                    # Further pairs
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] - w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] - w * embedding_ij_1
            else:
                for i in prange(num_cells, nogil=True, num_threads=num_threads):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    # Nearest-neighbor pairs
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                    # Mid-near pairs
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] + w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] + w * embedding_ij_1
                    # Further pairs
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] = gradients[i, 0] - w * embedding_ij_0
                        gradients[i, 1] = gradients[i, 1] - w * embedding_ij_1
        
        def update_embedding_adam(double[:, ::1] embedding,
                                  const double[:, ::1] gradients,
                                  double[:, ::1] momentum,
                                  double[:, ::1] velocity,
                                  const double beta1,
                                  const double beta2,
                                  double learning_rate,
                                  const int iteration,
                                  const unsigned num_threads):
            cdef int i, num_cells = embedding.shape[0]
            
            learning_rate = \
                learning_rate * sqrt(1 - beta2 ** (iteration + 1)) / \
                (1 - beta1 ** (iteration + 1))
            if num_threads == 1:
                for i in range(num_cells):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            else:
                for i in prange(num_cells, nogil=True,
                                num_threads=num_threads):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            ''')
        get_scaled_distances = cython_functions['get_scaled_distances']
        get_neighbor_pairs = cython_functions['get_neighbor_pairs']
        sample_mid_near_pairs = cython_functions['sample_mid_near_pairs']
        sample_further_pairs = cython_functions['sample_further_pairs']
        update_embedding_adam = cython_functions['update_embedding_adam']
        # Get scaled distances between each cell and its nearest neighbors
        scaled_distances = np.empty_like(nearest_neighbor_indices, dtype=float)
        get_scaled_distances(PCs, nearest_neighbor_indices, scaled_distances,
                             num_threads)
        # Select the `num_neighbors` of the
        # `num_neighbors + num_extra_neighbors` nearest-neighbor pairs with the
        # lowest scaled distances
        num_cells = PCs.shape[0]
        neighbor_pairs = np.empty((num_cells, num_neighbors), dtype=np.int32)
        get_neighbor_pairs(PCs, scaled_distances, nearest_neighbor_indices,
                           neighbor_pairs, num_threads)
        del scaled_distances, nearest_neighbor_indices
        # Sample mid-near pairs
        mid_near_pairs = np.empty((num_cells, num_mid_near_pairs),
                                  dtype=np.int32)
        sample_mid_near_pairs(PCs, mid_near_pairs, seed, num_threads)
        # Sample further pairs
        further_pairs = np.empty((num_cells, num_further_pairs),
                                 dtype=np.int32)
        sample_further_pairs(PCs, neighbor_pairs, further_pairs,
                             seed + mid_near_pairs.size, num_threads)
        # If multithreaded, or single-threaded with
        # `faster_single_threading=False`, reformat the three lists of pairs to
        # allow deterministic parallelism. Specifically, transform pairs of
        # cell indices from the original format of a 2D array `pairs` where
        # `pairs[i]` contains all js for which (i, j) is a pair, to a pair of
        # 1D arrays `pair_indices` and `pair_indptr` forming a sparse matrix,
        # where `pair_indices[pair_indptr[i]:pair_indptr[i + 1]]` contains all
        # js for which (i, j) is a pair or (j, i) is a pair. `pair_indices`
        # must have length `2 * pairs.size`, since each pair will appear twice,
        # once for (i, j) and once for (j, i). `pair_indptr` must have length
        # equal to the number of cells plus one, just like for scipy sparse
        # matrices.
        if faster_single_threading:
            get_gradients = cython_functions['get_gradients']
        else:
            reformat_for_parallel = cython_functions['reformat_for_parallel']
            
            neighbor_pair_indices = np.empty(2 * neighbor_pairs.size,
                                              dtype=np.int32)
            neighbor_pair_indptr = np.empty(num_cells + 1, dtype=np.int32)
            reformat_for_parallel(neighbor_pairs, neighbor_pair_indices,
                                  neighbor_pair_indptr)
            del neighbor_pairs
            mid_near_pair_indices = \
                np.empty(2 * mid_near_pairs.size, dtype=np.int32)
            mid_near_pair_indptr = np.empty(num_cells + 1, dtype=np.int32)
            reformat_for_parallel(mid_near_pairs, mid_near_pair_indices,
                                  mid_near_pair_indptr)
            del mid_near_pairs
            further_pair_indices = \
                np.empty(2 * further_pairs.size, dtype=np.int32)
            further_pair_indptr = np.empty(num_cells + 1, dtype=np.int32)
            reformat_for_parallel(further_pairs, further_pair_indices,
                                  further_pair_indptr)
            del further_pairs
            get_gradients = cython_functions['get_gradients_parallel']
        # Initialize the embedding, gradients, and other optimizer parameters
        embedding = 0.01 * PCs[:, :2]
        gradients = np.zeros_like(embedding, dtype=float)
        momentum = np.zeros_like(embedding, dtype=float)
        velocity = np.zeros_like(embedding, dtype=float)
        w_mid_near_init = 1000
        beta1 = 0.9
        beta2 = 0.999
        # Optimize the embedding
        for iteration in range(sum(num_iterations)):
            num_phase_1_iterations, num_phase_2_iterations = num_iterations[:2]
            if iteration < num_phase_1_iterations:
                w_mid_near = \
                    (1 - iteration / num_phase_1_iterations) * \
                    w_mid_near_init + iteration / num_phase_1_iterations * 3
                w_neighbors = 2
            elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                w_mid_near = 3
                w_neighbors = 3
            else:
                w_mid_near = 0
                w_neighbors = 1
            # Calculate gradients
            if faster_single_threading:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pairs, mid_near_pairs,
                              further_pairs, w_neighbors, w_mid_near,
                              gradients)
            else:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pair_indices,
                              neighbor_pair_indptr, mid_near_pair_indices,
                              mid_near_pair_indptr, further_pair_indices,
                              further_pair_indptr, w_neighbors, w_mid_near,
                              gradients, num_threads)
            # Update the embedding based on the gradients, via the Adam
            # optimizer
            update_embedding_adam(embedding, gradients, momentum, velocity,
                                  beta1, beta2, learning_rate, iteration,
                                  num_threads)
        # If `QC_column` is not `None`, back-project from QCed cells to all
        # cells, filling with NaN
        if QC_column is not None:
            embedding_QCed = embedding
            embedding = np.full((len(self), embedding_QCed.shape[1]), np.nan)
            # noinspection PyUnboundLocalVariable
            embedding[QCed_NumPy] = embedding_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {embedding_key: embedding},
                          varm=self._varm, uns=self._uns)
    
    # noinspection PyUnresolvedReferences
    def plot_embedding(
            self,
            color_column: SingleCellColumn | None,
            filename: str | Path | None = None,
            *,
            cells_to_plot_column: SingleCellColumn | None = 'passed_QC',
            embedding_key: str = 'PaCMAP',
            ax: 'Axes' | None = None,
            point_size: int | float | np.integer | np.floating | str |
                        None = None,
            sort_by_frequency: bool = False,
            colormap: str | 'Colormap' | dict[Any, Color] = None,
            colormap_kwargs: dict[str, Any] | None = None,
            default_color: Color = 'lightgray',
            scatter_kwargs: dict[str, Any] | None = None,
            label: bool = False,
            label_kwargs: dict[str, Any] | None = None,
            legend: bool = True,
            legend_kwargs: dict[str, Any] | None = None,
            colorbar: bool = True,
            colorbar_kwargs: dict[str, Any] | None = None,
            title: str | None = None,
            title_kwargs: dict[str, Any] | None = None,
            xlabel: str | None = 'Component 1',
            xlabel_kwargs: dict[str, Any] | None = None,
            ylabel: str | None = 'Component 2',
            ylabel_kwargs: dict[str, Any] | None = None,
            xlim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            ylim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            despine: bool = True,
            savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot an embedding created by `embed()`, using Matplotlib.
        
        Requires the colorspacious package. Install via:
        mamba install -y colorspacious
        
        Args:
            color_column: an optional column of obs indicating how to color
                          each cell in the plot. Can be a column name, a polars
                          expression, a polars Series, a 1D NumPy array, or a
                          function that takes in this SingleCell dataset and
                          returns a polars Series or 1D NumPy array. Can be
                          discrete (e.g. cell-type labels), specified as a
                          String/Categorical/Enum column, or quantitative (e.g.
                          the number of UMIs per cell), specified as an
                          integer/floating-point column. Missing (null) cells
                          will be plotted with the color `default_color`. Set
                          to `None` to use `default_color` for all cells.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of obs indicating
                                  which cells to plot. Can be a column name, a
                                  polars expression, a polars Series, a 1D
                                  NumPy array, or a function that takes in this
                                  SingleCell dataset and returns a polars
                                  Series or 1D NumPy array. Set to `None` to
                                  plot all cells passing QC.
            embedding_key: the key of obsm containing the embedding to plot,
                           calculated with `embed()`
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            point_size: the size of the points for each cell; defaults to
                        30,000 divided by the number of cells, one quarter of
                        scanpy's default. Can be a single number, or the name
                        of a column of obs to make each point a different size.
            sort_by_frequency: if `True`, assign colors and sort the legend in
                               order of decreasing frequency; if `False` (the
                               default), use natural sorted order
                               (en.wikipedia.org/wiki/Natural_sort_order).
                               Cannot be `True` unless `colormap` is `None` and
                               `color_column` is discrete; if `colormap` is
                               not `None`, the plot order is determined by the
                               order of the keys in `colormap`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use; or, if `color_column` is discrete, a
                      dictionary mapping values in `color_column` to Matplotlib
                      colors (cells with values of `color_column` that are not
                      in the dictionary will be plotted in the color
                      `default_color`). Defaults to
                      `plt.rcParams['image.cmap']` (`'viridis'` by default) if
                      `color_column` is continous, or the colors from
                      `generate_palette()` if `color_column` is discrete (with
                      colors assigned in decreasing order of frequency). Cannot
                      be specified if `color_column` is `None`.
            colormap_kwargs: a dictionary of keyword arguments to be passed to
                             `generate_palette()`. Can only be specified when
                             `color_column` is discrete and `colormap` is
                             `None`.
            default_color: the default color to plot cells in when
                           `color_column` is `None`, or when certain cells have
                           missing (null) values for `color_column`, or when
                           `colormap` is a dictionary and some cells have
                           values of `color_column` that are not in the
                           dictionary. Can be any valid Matplotlib color, like
                           a hex string (e.g. `'#FF0000'`), a named color (e.g.
                           'red'), a 3- or 4-element RGB/RGBA tuple of integers
                           0-255 or floats 0-1, or a single float 0-1 for
                           grayscale.
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since these arguments conflict with the
                            `point_size`, `color_column`, and `colormap`
                            arguments, respectively.
            label: whether to label cells with each distinct value of
                   `color_column`. Labels will be placed at the median x and y
                   position of the points with that color. Can only be `True`
                   when `color_column` is discrete. When set to `True`, you may
                   also want to set `legend=False` to avoid redundancy.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                           - `path_effects` to set properties for the border
                             around the text. By default, set to
                             `matplotlib.patheffects.withStroke(
                                  linewidth=3, foreground='white', alpha=0.75)`
                             instead of Matplotlib's default of `None`, to put
                             a semi-transparent white border around the labels
                             for better contrast.
                          Can only be specified when `label=True`.
            legend: whether to add a legend for each value in `color_column`.
                    Ignored unless `color_column` is discrete.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. By default, `loc` is set to
                             `'center left'` and `bbox_to_anchor` to `(1, 0.5)`
                             to put the legend to the right of the plot,
                             anchored at the middle.
                           - `ncols` to set its number of columns. By
                             default, set to
                             `obs[color_column].n_unique() // 16 + 1` to have
                             at most 16 items per column.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border (`frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`)
                           - `title` to add a legend title
                           Can only be specified when `color_column` is
                           discrete and `legend=True`.
            colorbar: whether to add a colorbar. Ignored unless `color_column`
                      is quantitative.
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `color_column` is
                             quantitative and `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            xlim: a length-2 tuple of the left and right x-axis limits, or
                 `None` to set the limits based on the data
            ylim: a length-2 tuple of the bottom and top y-axis limits, or
                 `None` to set the limits based on the data
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's 
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Get `cells_to_plot_column`, if not `None`
        original_cells_to_plot_column = cells_to_plot_column
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean,
                custom_error='cells_to_plot_column {} is not a column of obs; '
                             'set cells_to_plot_column=None to include all '
                             'cells')
        # If `color_column` is not `None`, check that it either discrete
        # (Categorical, Enum, or String) or quantitative (integer or
        # floating-point). If discrete, check that `color_column` has at least
        # two distinct values.
        original_color_column = color_column
        if color_column is not None:
            color_column = self._get_column(
                'obs', color_column, 'color_column',
                (pl.Categorical, pl.Enum, pl.String, 'integer',
                 'floating-point'), allow_null=True,
                QC_column=cells_to_plot_column)
            unique_color_column = color_column.unique().drop_nulls()
            dtype = color_column.dtype
            discrete = dtype in (pl.Categorical, pl.Enum, pl.String)
            if discrete and len(unique_color_column) == 1:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} must have at least two '
                    f'distinct values when its data '
                    f'type is {dtype.base_type()!r}')
                raise ValueError(error_message)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `embedding_key` is the name of a key in obsm
        check_type(embedding_key, 'embedding_key', str, 'a string')
        if embedding_key not in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is not a key of obsm; '
                f'did you forget to run embed() before plot_embedding()?')
            raise ValueError(error_message)
        # Check that the embedding `embedding_key` references is 2D.
        embedding = self._obsm[embedding_key]
        if embedding.shape[1] != 2:
            error_message = (
                f'the embedding at obsm[{embedding_key!r}] is '
                f'{embedding.shape[1]:,}-dimensional, but must be '
                f'2-dimensional to be plotted')
            raise ValueError(error_message)
        # If `cells_to_plot_column` is not `None`, subset to these cells
        if cells_to_plot_column is not None:
            embedding = embedding[cells_to_plot_column.to_numpy()]
            if color_column is not None:
                color_column = color_column.filter(cells_to_plot_column)
                unique_color_column = color_column.unique().drop_nulls()
        # Check that the embedding does not contain NaNs
        if np.isnan(embedding).any():
            error_message = \
                f'the embedding at obsm[{embedding_key!r}] contains NaNs; '
            if cells_to_plot_column is None and QC_column is not None:
                error_message += (
                    'did you forget to set QC_column to None in embed(), to '
                    'match the fact that you set cells_to_plot_column to '
                    'None in plot_embedding()?')
            else:
                cells_to_plot_column_description = \
                    SingleCell._describe_column('cells_to_plot_column',
                                                original_cells_to_plot_column)
                error_message += (
                    f'does your {cells_to_plot_column_description} contain '
                    f'cells that were excluded by the QC_column used in '
                    f'embed()?')
            raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((colormap_kwargs, 'colormap_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # If `point_size` is `None`, default to 30,000 / num_cells; otherwise,
        # check that it is a positive number or the name of a numeric column of
        # obs with all-positive numbers
        num_cells = \
            len(self) if cells_to_plot_column is None else len(embedding)
        if point_size is None:
            # noinspection PyUnboundLocalVariable
            point_size = 30_000 / num_cells
        else:
            check_type(point_size, 'point_size', (int, float, str),
                       'a positive number or string')
            if isinstance(point_size, (int, float)):
                check_bounds(point_size, 'point_size', 0, left_open=True)
            else:
                if point_size not in self._obs:
                    error_message = \
                        f'point_size {point_size!r} is not a column of obs'
                    raise ValueError(error_message)
                point_size = self._obs[point_size]
                if not (point_size.dtype.is_integer() or
                        point_size.dtype.is_float()):
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], must '
                        f'have an integer or floating-point data type, but '
                        f'has data type {point_size.dtype.base_type()!r}')
                    raise TypeError(error_message)
                if point_size.min() <= 0:
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], does '
                        f'not have all-positive elements')
                    raise ValueError(error_message)
        # If `sort_by_frequency=True`, ensure `colormap` is `None` and
        # `color_column` is discrete
        check_type(sort_by_frequency, 'sort_by_frequency', bool, 'Boolean')
        if sort_by_frequency:
            if colormap is not None:
                error_message = (
                    f'sort_by_frequency must be False when colormap is '
                    f'specified')
                raise ValueError(error_message)
            if color_column is None:
                error_message = \
                    'sort_by_frequency must be False when color_column is None'
                raise ValueError(error_message)
            # noinspection PyUnboundLocalVariable
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'sort_by_frequency must be False when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
        # If `colormap` is not `None`, check that it is a string in
        # `plt.colormaps`, Colormap object, or dictionary where all keys are 
        # in `color_column` and all values are valid Matplotlib colors (and 
        # normalize these to hex codes). If `None` and `color_column` is
        # discrete, assign colors via `generate_palette()`, in natural sort 
        # order, or decreasing order of frequency if `sort_by_frequency=True`.
        # Also make sure `colormap` and `colormap_kwargs` are `None` when
        # `color_column` is `None`.
        if colormap is not None:
            if color_column is None:
                error_message = \
                    'colormap must be None when color_column is None'
                raise ValueError(error_message)
            if colormap_kwargs is not None:
                error_message = \
                    'colormap_kwargs must be None when colormap is specified'
                raise ValueError(error_message)
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap, dict),
                       'a string, matplotlib Colormap object, or dictionary')
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
            elif isinstance(colormap, dict):
                # noinspection PyUnboundLocalVariable
                if not discrete:
                    color_column_description = \
                        SingleCell._describe_column('color_column',
                                                    original_color_column)
                    error_message = (
                        f'colormap cannot be a dictionary when '
                        f'{color_column_description} is continuous')
                    raise ValueError(error_message)
                for key, value in colormap.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of colormap must be strings, but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    # noinspection PyUnboundLocalVariable
                    if key not in unique_color_column:
                        error_message = (
                            f'colormap is a dictionary containing the key '
                            f'{key!r}, which is not one of the values in '
                            f'obs[{color_column!r}]')
                        raise ValueError(error_message)
                    if not plt.matplotlib.colors.is_color_like(value):
                        error_message = (
                            f'colormap[{key!r}] is not a valid Matplotlib '
                            f'color')
                        raise ValueError(error_message)
                    colormap[key] = plt.matplotlib.colors.to_hex(value)
        else:
            # noinspection PyUnboundLocalVariable
            if color_column is not None and discrete:
                if colormap_kwargs is None:
                    colormap_kwargs = {}
                # noinspection PyUnboundLocalVariable
                color_order = color_column\
                    .value_counts(sort=True)\
                    .to_series()\
                    .drop_nulls() if sort_by_frequency else \
                        sorted(unique_color_column,
                               key=lambda color_label: [
                                   int(text) if text.isdigit() else
                                   text.lower() for text in
                                   re.split('([0-9]+)', color_label)])
                colormap = generate_palette(len(color_order),
                                            **colormap_kwargs)
                colormap = dict(zip(color_order, colormap))
            elif colormap_kwargs is not None:
                if color_column is None:
                    error_message = (
                        'colormap_kwargs must be None when color_column is '
                        'None')
                    raise ValueError(error_message)
                else:
                    color_column_description = \
                        SingleCell._describe_column('color_column',
                                                    original_color_column)
                    error_message = (
                        f'colormap_kwargs must be None when '
                        f'{color_column_description} is continuous')
                    raise ValueError(error_message)
        # Check that `default_color` is a valid Matplotlib color, and convert
        # it to hex
        if not plt.matplotlib.colors.is_color_like(default_color):
            error_message = 'default_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        default_color = plt.matplotlib.colors.to_hex(default_color)
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
            if scatter_kwargs is not None else default_scatter_kwargs
        # Check that `scatter_kwargs` does not contain the `s`, `c`/`color`, or
        # `cmap` keys
        if 's' in scatter_kwargs:
            error_message = (
                "'s' cannot be specified as a key in scatter_kwargs; specify "
                "the point_size argument instead")
            raise ValueError(error_message)
        for key in 'c', 'color', 'cmap':
            if key in scatter_kwargs:
                error_message = (
                    f'{key!r} cannot be specified as a key in '
                    f'scatter_kwargs; specify the color_column, colormap, '
                    f'colormap_kwargs, and/or default_color arguments instead')
                raise ValueError(error_message)
        # If `label=True`, check that `color_column` is discrete.
        # If `label=False`, check that `label_kwargs` is `None`.
        check_type(label, 'label', bool, 'Boolean')
        if label:
            if color_column is None:
                error_message = 'color_column cannot be None when label=True'
                raise ValueError(error_message)
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} cannot be continuous when '
                    f'label=True')
                raise ValueError(error_message)
        elif label_kwargs is not None:
            error_message = 'label_kwargs must be None when label=False'
            raise ValueError(error_message)
        # Only add a legend if `legend=True` and `color_column` is discrete.
        # If not adding a legend, check that `legend_kwargs` is `None`.
        check_type(legend, 'legend', bool, 'Boolean')
        add_legend = legend and color_column is not None and discrete
        if not add_legend and legend_kwargs is not None:
            if color_column is None:
                error_message = \
                    'legend_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = \
                    _describe_color('color_column', original_color_column)
                error_message = (
                    f'legend_kwargs must be None when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
        # Only add a colorbar if `colorbar=True` and `color_column` is
        # continuous. If not adding a colorbar, check that `colorbar_kwargs` is
        # `None`.
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        add_colorbar = colorbar and color_column is not None and not discrete
        if not add_colorbar and colorbar_kwargs is not None:
            if color_column is None:
                error_message = \
                    'colorbar_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = \
                    _describe_color('color_column', original_color_column)
                error_message = (
                    f'colorbar_kwargs must be None when '
                    f'{color_column_description} is discrete')
                raise ValueError(error_message)
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # Check that `xlim` and `ylim` are be length-2 tuples or `None`, with
        # the first element less than the second
        for arg, arg_name in (xlim, 'xlim'), (ylim, 'ylim'):
            if arg is not None:
                check_type(arg, arg_name, tuple, 'a length-2 tuple')
                if len(arg) != 2:
                    error_message = (
                        f'{arg_name} must be a length-2 tuple, but has length '
                        f'{len(arg):,}')
                    raise ValueError(error_message)
                if arg[0] >= arg[1]:
                    error_message = \
                        f'{arg_name}[0] must be less than {arg_name}[1]'
                    raise ValueError(error_message)
        # If `color_column` is `None`, plot all cells in `default_color`. If
        # `colormap` is a dictionary, generate an explicit list of colors to
        # plot each cell in. If `colormap` is a Colormap, just pass it as the
        # cmap` argument. If `colormap` is missing and `color_column` is
        # continuous, set it to `plt.rcParams['image.cmap']` ('viridis' by
        # default)
        if color_column is None:
            c = default_color
            cmap = None
        elif isinstance(colormap, dict):
            # Note: `replace_strict(..., default=default_color)` fills both
            # missing values and values missing from `colormap` with
            # `default_color`
            c = color_column\
                .replace_strict(colormap, default=default_color,
                                return_dtype=pl.String)\
                .to_numpy()
            cmap = None
        else:
            # Need to `copy()` because `set_bad()` is in-place
            c = color_column.to_numpy()
            if colormap is not None:
                cmap = colormap.copy()
                # noinspection PyUnresolvedReferences
                cmap.set_bad(default_color)
            else:  # `color_column` is continuous
                cmap = plt.rcParams['image.cmap']
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                plt.figure(constrained_layout=True)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make a scatter plot of the embedding with equal x-y aspect ratios
            scatter = ax.scatter(embedding[:, 0], embedding[:, 1],
                                 s=point_size, c=c, cmap=cmap,
                                 **scatter_kwargs)
            ax.set_aspect('equal')
            # Add the title, axis labels and axis limits
            if title is not None:
                if title_kwargs is None:
                    ax.set_title(title)
                else:
                    ax.set_title(title, **title_kwargs)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax.set_xlabel(xlabel)
                else:
                    ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax.set_ylabel(ylabel)
                else:
                    ax.set_ylabel(ylabel, **ylabel_kwargs)
            if xlim is not None:
                ax.set_xlim(*xlim)
            if ylim is not None:
                ax.set_ylim(*ylim)
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if add_legend:
                default_legend_kwargs = dict(
                    loc='center left', bbox_to_anchor=(1, 0.5), frameon=False,
                    ncols=len(unique_color_column) // 16 + 1)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                if isinstance(colormap, dict):
                    for color_label, color in colormap.items():
                        ax.scatter([], [], c=color, label=color_label,
                                   **scatter_kwargs)
                    plt.legend(**legend_kwargs)
                else:
                    plt.legend(*scatter.legend_elements(), **legend_kwargs)
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`
            if add_colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(scatter, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
            # Label cells; override the defaults for certain keys of
            # `label_kwargs`
            if label:
                from matplotlib.patheffects import withStroke
                if label_kwargs is None:
                    label_kwargs = {}
                # noinspection PyUnresolvedReferences
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')),
                    path_effects=[withStroke(linewidth=3, foreground='white',
                                             alpha=0.75)])
                for color_label in unique_color_column:
                    ax.text(*np.median(embedding[color_column == color_label],
                                       axis=0), color_label, **label_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise


class Pseudobulk:
    """
    A pseudobulked single-cell dataset resulting from calling `pseudobulk()`
    on a SingleCell dataset.
    
    Has slots for:
    - `X`: a dict of NumPy arrays of counts per cell and gene for each cell
      type
    - `obs`: a dict of polars DataFrames of sample metadata for each cell type
    - `var`: a dict of polars DataFrames of gene metadata for each cell type
    as well as `obs_names` and `var_names`, aliases for a dict of obs[:, 0] and
    var[:, 0] for each cell type, and `cell_types`, a tuple of cell types.
    
    Supports iteration:
    - `for cell_type in pseudobulk:` yields the cell type names, as does
      `for cell_type in pseudobulk.keys():`
    - `for X, obs, var in pseudobulk.values():` yields the X, obs and var for
       each cell type
    - `for cell_type, (X, obs, var) in pseudobulk.items():` yields both the
      name and the X, obs and var for each cell type
    - `for X in pseudobulk.iter_X():` yields just the X for each cell type
    - `for X in pseudobulk.iter_obs():` yields just the obs for each cell type
    - `for X in pseudobulk.iter_var():` yields just the var for each cell type
    """
    def __init__(self,
                 X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                     np.floating]]] |
                    str | Path,
                 obs: dict[str, pl.DataFrame] = None,
                 var: dict[str, pl.DataFrame] = None) -> None:
        """
        Load a saved Pseudobulk dataset, or create one from an in-memory count
        matrix + metadata for each cell type. The latter functionality is
        mainly for internal use; most users will create new pseudobulk datasets
        by calling `pseudobulk()` on a SingleCell dataset.
        
        Args:
            X: a {cell type: NumPy array} dict of counts or log CPMs, or a
               directory to load a saved Pseudobulk dataset from (see save())
            obs: a {cell type: polars DataFrame} dict of metadata per sample.
                 The first column must be String, Enum, or Categorical.
            var: a {cell type: polars DataFrame} dict of metadata per gene.
                 The first column must be String, Enum, or Categorical.
        """
        if isinstance(X, dict):
            if obs is None:
                error_message = (
                    'obs is None, but since X is a dictionary, obs must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if var is None:
                error_message = (
                    'var is None, but since X is a dictionary, var must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if not X:
                error_message = 'X is an empty dictionary'
                raise ValueError(error_message)
            if X.keys() != obs.keys():
                error_message = (
                    'X and obs must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            if X.keys() != var.keys():
                error_message = (
                    'X and var must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            for cell_type in X:
                if not isinstance(cell_type, str):
                    error_message = (
                        f'all keys of X (cell types) must be strings, but X '
                        f'contains a key of type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                check_type(X[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                if X[cell_type].ndim != 2:
                    error_message = (
                        f'X[{cell_type!r}] is a {X[cell_type].ndim:,}-'
                        f'dimensional NumPy array, but must be 2-dimensional')
                    raise ValueError(error_message)
                check_type(obs[cell_type], f'obs[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
                check_type(var[cell_type], f'var[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
            self._X = X
            self._obs = obs
            self._var = var
        elif isinstance(X, (str, Path)):
            X = str(X)
            if not os.path.exists(X):
                error_message = f'Pseudobulk directory {X!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{X}/cell_types.txt')]
            self._X = {cell_type: np.load(
                os.path.join(X, f'{cell_type.replace("/", "-")}.X.npy'))
                for cell_type in cell_types}
            self._obs = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.obs.parquet'))
                for cell_type in cell_types}
            self._var = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.var.parquet'))
                for cell_type in cell_types}
        else:
            error_message = (
                f'X must be a dictionary of NumPy arrays or a directory '
                f'containing a saved Pseudobulk dataset, but has type '
                f'{type(X).__name__!r}')
            raise ValueError(error_message)
        for cell_type in self._X:
            dtype = self._X[cell_type].dtype
            if dtype != np.int64 and dtype != np.int32 and \
                    dtype != np.float64 and dtype != np.float32:
                error_message = (
                    f'X must be int32/int64 or float32/float64, but '
                    f'X[{cell_type!r}] has data type {str(dtype)}')
                raise TypeError(error_message)
            if len(self._obs[cell_type]) == 0:
                error_message = \
                    f'len(obs[{cell_type!r}]) is 0: no samples remain'
                raise ValueError(error_message)
            if len(self._var[cell_type]) == 0:
                error_message = \
                    f'len(var[{cell_type!r}]) is 0: no genes remain'
                raise ValueError(error_message)
            if len(self._obs[cell_type]) != len(self._X[cell_type]):
                error_message = (
                    f'len(obs[{cell_type!r}]) is '
                    f'{len(self._obs[cell_type]):,}, but '
                    f'len(X[{cell_type!r}]) is {len(X[cell_type]):,}')
                raise ValueError(error_message)
            if len(self._var[cell_type]) != self._X[cell_type].shape[1]:
                error_message = (
                    f'len(var[{cell_type!r}]) is '
                    f'{len(self._var[cell_type]):,}, but '
                    f'X[{cell_type!r}].shape[1] is '
                    f'{self._X[cell_type].shape[1]:,}')
                raise ValueError(error_message)
            if self._obs[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of obs[{cell_type!r}] '
                    f'({self._obs[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._obs[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
            if self._var[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of var[{cell_type!r}] '
                    f'({self._var[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._var[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
    
    @staticmethod
    def _setter_check(new: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      old: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      name: str) -> None:
        """
        When setting X, obs or var, raise an error if the new value is not a
        dictionary, the new cell types (keys) differ from the old ones, or the
        new values differ in length (or shape, in the case of X) from the old
        ones. For obs and var, also check that the first column is String,
        Enum, or Categorical.
        
        Args:
            new: the new X, obs or var
            old: the old X, obs or var
            name: the name of the field: 'X', 'obs' or 'var'
        """
        if not isinstance(new, dict):
            error_message = (
                f'new {name} must be a dictionary, but has type '
                f'{type(new).__name__!r}')
            raise TypeError(error_message)
        if new.keys() != old.keys():
            error_message = (
                f'new {name} has different cell types (keys) from the old '
                f'{name}, or has the same cell types in a different order')
            raise ValueError(error_message)
        if name == 'X':
            for cell_type in new:
                check_type(new[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                new_shape = new[cell_type].shape
                old_shape = old[cell_type].shape
                if new_shape != old_shape:
                    error_message = (
                        f'new X[{cell_type!r}] is {new_shape.shape[0]:,} × '
                        f'{new_shape.shape[1]:,}, but old X is '
                        f'{old_shape.shape[0]:,} × {old_shape.shape[1]:,}')
                    raise ValueError(error_message)
                dtype = new[cell_type].dtype
                if dtype != np.int64 and dtype != np.int32 and \
                        dtype != np.float64 and dtype != np.float32:
                    error_message = (
                        f'X must be int32/int64 or float32/float64, but new '
                        f'X[{cell_type!r}] data type {str(dtype)}')
                    raise TypeError(error_message)
        else:
            for cell_type in new:
                check_type(new[cell_type], f'{name}[{cell_type!r}]',
                           pl.DataFrame, 'a polars DataFrame')
                if new[cell_type][:, 0].dtype not in (
                        pl.String, pl.Categorical, pl.Enum):
                    error_message = (
                        f'the first column of {name}[{cell_type!r}] '
                        f'({new[cell_type].columns[0]!r}) must be String, '
                        f'Enum, or Categorical, but the first column of the '
                        f'new {name} has data type '
                        f'{new[cell_type][:, 0].dtype.base_type()!r}')
                    raise ValueError(error_message)
                if len(new) != len(old):
                    error_message = (
                        f'new {name}[{cell_type!r}] has length {len(new):,}, '
                        f'but old {name} has length {len(old):,}')
                    raise ValueError(error_message)
    
    @property
    def X(self) -> dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]:
        return self._X
    
    @X.setter
    def X(self, X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]) -> \
            None:
        self._setter_check(X, self._X, 'X')
        self._X = X
    
    @property
    def obs(self) -> dict[str, pl.DataFrame]:
        return self._obs
    
    @obs.setter
    def obs(self, obs: dict[str, pl.DataFrame]) -> None:
        self._setter_check(obs, self._obs, 'obs')
        self._obs = obs

    @property
    def var(self) -> dict[str, pl.DataFrame]:
        return self._var
    
    @var.setter
    def var(self, var: dict[str, pl.DataFrame]) -> None:
        self._setter_check(var, self._var, 'var')
        self._var = var

    @property
    def obs_names(self) -> dict[str, pl.Series]:
        return {cell_type: obs[:, 0] for cell_type, obs in self._obs.items()}
    
    @property
    def var_names(self) -> dict[str, pl.Series]:
        return {cell_type: var[:, 0] for cell_type, var in self._var.items()}

    def _process_cell_types(self,
                            cell_types: str | Iterable[str] | None,
                            excluded_cell_types: str | Iterable[str] | None,
                            *,
                            return_description: bool = False) -> \
            tuple[str, ...] | tuple[tuple[str, ...], str]:
        """
        Process the `cell_types` and `excluded_cell_types` arguments of various
        Pseudobulk functions.
        
        Args:
            cell_types: one or more cell types to include in the calling
                        function's operation
            excluded_cell_types: one or more cell types to exclude from the
                                 calling function's operation
            return_description: whether to return a description of the cell
                                types, to use in error messages

        Returns:
            A tuple of cell-type names, or a two-element tuple of
            (cell-type names, cell-type description) if
            `return_description=True`.
        """
        if cell_types is not None:
            if excluded_cell_types is not None:
                error_message = (
                    'cell_types and excluded_cell_types cannot both be '
                    'specified')
                raise ValueError(error_message)
            is_string = isinstance(cell_types, str)
            cell_types = \
                to_tuple_checked(cell_types, 'cell_types', str, 'strings')
            for cell_type in cell_types:
                if cell_type not in self._X:
                    if is_string:
                        error_message = (
                            f'cell_types is {cell_type!r}, which is not a '
                            f'cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            if return_description:
                cell_type_description = 'the cell_types argument'
        elif excluded_cell_types is not None:
            excluded_cell_types = to_tuple_checked(
                excluded_cell_types, 'cell_types', str, 'strings')
            for cell_type in excluded_cell_types:
                if cell_type not in self._X:
                    if excluded_cell_types:
                        error_message = (
                            f'excluded_cell_types is {cell_type!r}, which is '
                            f'not a cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of excluded_cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            cell_types = tuple(cell_type for cell_type in self._X
                               if cell_type not in excluded_cell_types)
            if len(cell_types) == 0:
                error_message = \
                    'all cell types were excluded by excluded_cell_types'
                raise ValueError(error_message)
            if return_description:
                cell_type_description = (
                    'this Pseudobulk dataset (after excluding the cell types '
                    'in excluded_cell_types)')
        else:
            cell_types = tuple(self._X)
            if return_description:
                cell_type_description = 'this Pseudobulk dataset'
        # noinspection PyUnboundLocalVariable
        return (cell_types, cell_type_description) \
            if return_description else cell_types
    
    def set_obs_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of obs, i.e. the obs_names.
        
        Args:
            column: the column name in obs; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's obs. If `column` is already the first column for every
            cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._obs[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                if column not in cell_type_obs:
                    error_message = \
                        f'{column!r} is not a column of obs[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_obs, f'obs[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                obs[cell_type] = \
                    cell_type_obs.select(column, pl.exclude(column))
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def set_var_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of var, i.e. the var_names.
        
        Args:
            column: the column name in var; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's var. If `column` is already the first column for every
            cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._var[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                if column not in cell_type_var:
                    error_message = \
                        f'{column!r} is not a column of var[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_var, f'var[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                var[cell_type] = \
                    cell_type_var.select(column, pl.exclude(column))
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var)

    def keys(self) -> KeysView[str]:
        """
        Get a KeysView (like you would get from `dict.keys()`) of this
        Pseudobulk dataset's cell types. `for cell_type in pb.keys()` is
        equivalent to `for cell_type in pb`.
        
        Returns:
            A KeysView of the cell types.
        """
        return self._X.keys()
    
    def values(self) -> ValuesView[tuple[np.ndarray[2, np.dtype[np.integer |
                                                                np.floating]],
                                       pl.DataFrame, pl.DataFrame]]:
        """
        Get a ValuesView (like you would get from `dict.values()`) of
        `(X, obs, var)` tuples for each cell type in this Pseudobulk dataset.
        
        Returns:
            A ValuesView of `(X, obs, var)` tuples for each cell type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.values()
    
    def items(self) -> ItemsView[str, tuple[np.ndarray[2, np.dtype[
            np.integer | np.floating]], pl.DataFrame, pl.DataFrame]]:
        """
        Get an ItemsView (like you would get from `dict.items()`) of
        `(cell_type, (X, obs, var))` tuples for each cell type in this
        Pseudobulk dataset.
        
        Yields:
            An ItemsView of `(cell_type, (X, obs, var))` tuples for each cell
            type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.items()
    
    def iter_X(self) -> Iterable[np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]]:
        """
        Iterate over each cell type's X.
        
        Yields:
            X for each cell type.
        """
        for X in self._X.values():
            yield X
    
    def iter_obs(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's obs.
        
        Yields:
            obs for each cell type.
        """
        for obs in self._obs.values():
            yield obs
    
    def iter_var(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's var.
        
        Yields:
            var for each cell type.
        """
        for var in self._var.values():
            yield var
    
    def __eq__(self, other: Pseudobulk) -> bool:
        """
        Test for equality with another Pseudobulk dataset.
        
        Args:
            other: the other Pseudobulk dataset to test for equality with

        Returns:
            Whether the two Pseudobulk datasets are identical.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `==` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return tuple(self.keys()) == tuple(other.keys()) and \
            all(obs.equals(other_obs) for obs, other_obs in
                zip(self._obs.values(), other._obs.values())) and \
            all(var.equals(other_var) for var, other_var in
                zip(self._var.values(), other._var.values())) and \
            all(array_equal(X, other_X) for X, other_X in
                zip(self._X.values(), other._X.values()))
    
    def __or__(self, other: Pseudobulk) -> Pseudobulk:
        """
        Combine the cell types of this Pseudobulk dataset with another. The
        two datasets must have non-overlapping cell types.
        
        Args:
            other: the other Pseudobulk dataset to combine with this one

        Returns:
            A Pseudobulk dataset with each of the cell types in the first
            Pseudobulk dataset, followed by each of the cell types in the
            second.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `|` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        if self.keys() & other.keys():
            error_message = (
                'the left- and right-hand operands of `|` are Pseudobulk '
                'datasets that share some cell types')
            raise ValueError(error_message)
        return Pseudobulk(X=self._X | other._X, obs=self._obs | other._obs,
                          var=self._var | other._var)
    
    def __contains__(self, cell_type: str) -> bool:
        """
        Check if this Pseudobulk dataset contains the specified cell type.
        
        Args:
            cell_type: the cell type

        Returns:
            Whether the cell type is present in the Pseudobulk dataset.
        """
        check_type(cell_type, 'cell_type', str, 'a string')
        return cell_type in self._X
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[str, Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'Pseudobulk indices must be a cell-type string, a length-1 tuple '
            f'of (cell_type,), a length-2 tuple of (cell_type, samples), or a '
            f'length-3 tuple of (cell_type, samples, genes). Samples and '
            f'genes must each be a string or integer; a slice of strings or '
            f'integers; or a list, NumPy array, or polars Series of strings, '
            f'integers, or Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (obs or var)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('__Pseudobulk_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    @staticmethod
    def _getitem_process(item: Indexer | tuple[str, Indexer, Indexer],
                         index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to __getitem__().
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (obs or var) to process the element with respect
                to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [Pseudobulk._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = Pseudobulk._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if isinstance(stop, str):
                stop = Pseudobulk._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            if not isinstance(subitem, pl.Series):
                subitem = pl.Series(subitem)
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            if subitem.dtype == pl.String or subitem.dtype == \
                    pl.Categorical or subitem.dtype == pl.Enum:
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_Pseudobulk_index=pl.int_range(
                              pl.len(), dtype=pl.Int32)),
                          on=df.columns[0], how='left')\
                    ['_Pseudobulk_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif subitem.dtype.is_integer() or subitem.dtype == pl.Boolean:
                return subitem
            else:
                Pseudobulk._getitem_error(item)
        else:
            Pseudobulk._getitem_error(item)
    
    def __getitem__(self, item: Indexer | tuple[str, Indexer, Indexer]) -> \
            Pseudobulk:
        """
        Subset to specific cell type(s), sample(s), and/or gene(s).
        
        Index with a tuple of `(cell_types, samples, genes)`. If `samples` and
        `genes` are integers, arrays/lists/slices of integers, or arrays/lists
        of Booleans, the result will be a Pseudobulk dataset subset to
        `X[samples, genes]`, `obs[samples]`, and `var[genes]` for each of the
        cell types in `cell_types`. However, `samples` and/or `genes` can
        instead be strings (or arrays or slices of strings), in which case they
        refer to the first column of obs and/or var, respectively.
        
        Examples:
        - Subset to one cell type:
          pseudobulk['Astro']
        - Subset to multiple cell types:
          pseudobulk[['Astro', 'Micro']]
        - Subset to one cell type and sample, for all genes:
          pb['Astro', 'H19.30.002']
          pb['Astro', 2]
        - Subset to one gene, for all cell types and samples:
          pb[:, :, 'APOE']
          pb[:, :, 13196]
        - Subset to one cell type, sample and gene:
          pb['Astro', 'H18.30.002', 'APOE']
          pb['Astro', 2, 13196]
        - Subset to one cell type and a range of samples and genes:
          pb['Astro', 'H18.30.002':'H19.33.004', 'APOE':'TREM2']
          pb['Astro', 'H18.30.002':'H19.33.004', 13196:34268]
        - Subset to one a cell type and specific samples and genes:
          pb['Astro', ['H18.30.002', 'H19.33.004']]
          pb['Astro', :, pl.Series(['APOE', 'TREM2'])]
          pb['Astro', ('H18.30.002', 'H19.33.004'),
             np.array(['APOE', 'TREM2'])]
        
        Args:
            item: the item to index with

        Returns:
            A new Pseudobulk dataset subset to the specified cell types,
            samples, and/or genes.
        """
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 3:
                self._getitem_error(item)
            cell_types = to_tuple(item[0])
        elif isinstance(item, list):
            cell_types = to_tuple(item)
        elif isinstance(item, str):
            cell_types = item,
        else:
            self._getitem_error(item)
        # noinspection PyUnboundLocalVariable
        for cell_type in cell_types:
            if cell_type not in self._X:
                if isinstance(cell_type, str):
                    error_message = (
                        f'tried to select {cell_type!r}, which is not a cell '
                        f'type in this Pseudobulk')
                    raise ValueError(error_message)
                else:
                    error_message = (
                        f'tried to select a non-existent cell type of type '
                        f'{type(cell_type).__name__!r}')
                    raise TypeError(error_message)
        if not isinstance(item, tuple) or len(item) == 1:
            return Pseudobulk(X={cell_type: self._X[cell_type]
                                 for cell_type in cell_types},
                              obs={cell_type: self._obs[cell_type]
                                   for cell_type in cell_types},
                              var={cell_type: self._var[cell_type]
                                   for cell_type in cell_types})
        X, obs, var = {}, {}, {}
        for cell_type in cell_types:
            rows = self._getitem_process(item, 1, self._obs[cell_type])
            if isinstance(rows, pl.Series):
                obs[cell_type] = self._obs[cell_type].filter(rows) \
                    if rows.dtype == pl.Boolean else self._obs[cell_type][rows]
                rows = rows.to_numpy()
            else:
                obs[cell_type] = self._obs[cell_type][rows]
            if len(item) == 2:
                X[cell_type] = self._X[cell_type][rows]
                var[cell_type] = self._var[cell_type]
            else:
                columns = self._getitem_process(item, 2, self._var[cell_type])
                if isinstance(columns, pl.Series):
                    var[cell_type] = self._var[cell_type].filter(columns) \
                        if columns.dtype == pl.Boolean \
                        else self._var[cell_type][columns]
                    columns = columns.to_numpy()
                else:
                    var[cell_type] = self._var[cell_type][columns]
                X[cell_type] = self._X[cell_type][rows, columns] \
                    if isinstance(rows, slice) or \
                       isinstance(columns, slice) else \
                    self._X[cell_type][np.ix_(rows, columns)]
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def sample(self, cell_type: str, sample: str) -> np.ndarray[1, Any]:
        """
        Get the row of `X[cell_type]` corresponding to a single sample, based
        on the sample's name in obs_names.
        
        Args:
            cell_type: the cell type to retrieve the row of X from
            sample: the name of the sample in obs_names
        
        Returns:
            The corresponding row of X[cell_type], as a dense 1D NumPy array
            with zeros included.
        """
        row_index = Pseudobulk._getitem_by_string(self._obs[cell_type], sample)
        return self._X[cell_type][[row_index]].toarray().squeeze()
    
    def gene(self, cell_type: str, gene: str) -> np.ndarray[1, Any]:
        """
        Get the column of `X[cell_type]` corresponding to a single gene, based
        on the gene's name in var_names.
        
        Args:
            cell_type: the cell type to retrieve the row of X from
            gene: the name of the gene in var_names
        
        Returns:
            The corresponding column of X[cell_type], as a dense 1D NumPy array
            with zeros included.
        """
        column_index = \
            Pseudobulk._getitem_by_string(self._var[cell_type], gene)
        return self._X[cell_type][:, [column_index]].toarray().squeeze()
    
    def __iter__(self) -> Iterable[str]:
        """
        Iterate over the cell types of this Pseudobulk dataset.
        `for cell_type in pb` is equivalent to `for cell_type in pb.keys()`.
        
        Returns:
            An iterator over the cell types.
        """
        return iter(self._X)
    
    def __len__(self) -> dict[str, int]:
        """
        Get the number of samples in each cell type of this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to its number of samples.
        """
        return {cell_type: len(X_cell_type)
                for cell_type, X_cell_type in self._X.items()}
    
    def __repr__(self) -> str:
        """
        Get a string representation of this Pseudobulk dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        min_num_samples = min(len(obs) for obs in self._obs.values())
        max_num_samples = max(len(obs) for obs in self._obs.values())
        min_num_genes = min(len(var) for var in self._var.values())
        max_num_genes = max(len(var) for var in self._var.values())
        samples_string = \
            f'{min_num_samples:,} {plural("sample", max_num_samples)}' \
            if min_num_samples == max_num_samples else \
            f'{min_num_samples:,}-{max_num_samples:,} samples'
        genes_string = \
            f'{min_num_genes:,} {plural("gene", max_num_genes)}' \
            if min_num_genes == max_num_genes else \
            f'{min_num_genes:,}-{max_num_genes:,} genes'
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        return f'Pseudobulk dataset with {len(self._X):,} cell ' \
               f'{"types, each" if len(self._X) > 1 else "type,"} with ' \
               f'{samples_string} (obs) and {genes_string} (var)\n' + \
            fill(f'    Cell types: {", ".join(self._X)}',
                 width=terminal_width, subsequent_indent=' ' * 17)
    
    @property
    def shape(self) -> dict[str, tuple[int, int]]:
        """
        Get the shape of each cell type in this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to a length-2 tuple where the
            first element is the number of samples, and the second is the
            number of genes.
        """
        return {cell_type: X_cell_type.shape
                for cell_type, X_cell_type in self._X.items()}
    
    def save(self,
             directory: str | Path,
             overwrite: bool = False,
             *,
             cell_types: str | Iterable[str] | None = None,
             excluded_cell_types: str | Iterable[str] | None = None) -> None:
        # noinspection GrazieInspection
        """
        Saves a Pseudobulk dataset to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with three files per cell type:
        the X at f'{cell_type}.X.npy', the obs at f'{cell_type}.obs.parquet',
        and the var at f'{cell_type}.var.parquet'. Also saves a text file,
        cell_types.txt, containing the cell types.
        
        Args:
            directory: the directory to save the Pseudobulk dataset to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
            cell_types: one or more cell types to save; if `None`, save all
                        cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from saving;
                                 mutually exclusive with `cell_types`
        """
        cell_types = self._process_cell_types(cell_types, excluded_cell_types)
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
            # noinspection PyTypeChecker
            print('\n'.join(cell_types), file=f)
        for cell_type in cell_types:
            escaped_cell_type = cell_type.replace('/', '-')
            np.save(os.path.join(directory, f'{escaped_cell_type}.X.npy'),
                    self._X[cell_type])
            self._obs[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.obs.parquet'))
            self._var[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.var.parquet'))
    
    def copy(self, *, deep: bool = False) -> Pseudobulk:
        """
        Make a copy of this Pseudobulk dataset.
        
        Args:
            deep: whether to make a deep copy instead of a shallow one. Since
                  polars DataFrames are immutable, `obs[cell_type]` and
                  `var[cell_type]` will always point to the same underlying
                  data as the original for all cell types. The only difference
                  when `deep=True` is that `X[cell_type]` will point to a fresh
                  copy of the data, rather than the same data. When
                  `deep=False`, any modifications to the underlying count
                  matrix will modify both the original and the copy.

        Returns:
            A copy of the Pseudobulk dataset.
        """
        check_type(deep, 'deep', bool, 'Boolean')
        return Pseudobulk(X={cell_type: cell_type_X.copy()
                             for cell_type, cell_type_X in self._X.items()}
                            if deep else self._X, obs=self._obs, var=self._var)

    def concat_obs(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate the samples of multiple Pseudobulk datasets. All datasets
        must have the same cell types.
        
        By default, all datasets must have the same var. They must also have
        the same columns in obs, with the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of var, i.e. the var_names) before
        concatenating. Subset to columns of var that are identical in all
        datasets after this subsetting. Also, subset to columns of obs that are
        present in all datasets, and have the same data types. All datasets'
        obs_names must have the same name and dtype, and similarly for their
        var_names.
        
        The one exception to the obs "same data type" rule: if a column is Enum
        in some datasets and Categorical in others, or Enum in all datasets but
        with different categories in each dataset, that column will be retained
        as an Enum column (with the union of the categories) in the
        concatenated obs.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes and columns of obs and var
                      common to all datasets before concatenating, rather than
                      raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        if isinstance(datasets, Pseudobulk):
            datasets = datasets,
        datasets = (self,) + datasets + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._obs:
            if flexible:
                # Check that obs_names and var_names have the same name and
                # data type for each cell type across all datasets
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0] == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0].name == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                # Subset to genes in common across all datasets
                genes_in_common = self._var[cell_type][:, 0]\
                    .filter(self._var[cell_type][:, 0]
                            .is_in(pl.concat([dataset._var[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(genes_in_common) == 0:
                    error_message = (
                        f'no genes are shared across all Pseudobulk datasets '
                        f'for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_var = []
                for dataset in datasets:
                    gene_indices = dataset._getitem_process(
                        genes_in_common, 1, dataset._var[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, gene_indices.to_numpy()])
                    cell_type_var.append(dataset._var[cell_type][gene_indices])
                # Subset to columns of var that are identical in all datasets
                # after this subsetting
                var_columns_in_common = [
                    column.name for column in cell_type_var[0][:, 1:]
                    if all(column.name in dataset_cell_type_var and
                           dataset_cell_type_var[column.name].equals(column)
                           for dataset_cell_type_var in cell_type_var[1:])]
                cell_type_var = cell_type_var[0]
                cell_type_var = cell_type_var.select(cell_type_var.columns[0],
                                                     var_columns_in_common)
                # Subset to columns of obs that are present in all datasets,
                # and have the same data types. Also include columns of obs
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                obs_mismatched_categoricals = {
                    column for column, dtype in self._obs[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._obs[cell_type] and
                           dataset._obs[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._obs[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                obs_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._obs[cell_type].schema.items(), 1, None)
                    if column in obs_mismatched_categoricals or
                       all(column in dataset[cell_type]._obs and
                           dataset._obs[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._obs[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in obs_mismatched_categoricals}
                cell_type_obs = [
                    dataset._obs[cell_type]
                    .cast(cast_dict)
                    .select(obs_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all var are identical
                cell_type_var = self._var[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._var[cell_type].equals(cell_type_var):
                        error_message = (
                            f'all Pseudobulk datasets must have the same var '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                # Check that all obs have the same columns and data types
                schema = self._obs[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._obs[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in obs for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_obs = [dataset._obs[cell_type]
                                 for dataset in datasets]
            # Concatenate
            X[cell_type] = np.vstack(cell_type_X)
            obs[cell_type] = pl.concat(cell_type_obs)
            var[cell_type] = cell_type_var
        return Pseudobulk(X=X, obs=obs, var=var)

    def concat_var(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate the genes of multiple Pseudobulk datasets. All datasets
        must have the same cell types.
        
        By default, all datasets must have the same obs. They must also have
        the same columns in var, with the same data types.
        
        Conversely, if `flexible=True`, subset to cells present in all
        datasets (according to the first column of obs, i.e. the obs_names)
        before concatenating. Subset to columns of obs that are identical in
        all datasets after this subsetting. Also, subset to columns of var that
        are present in all datasets, and have the same data types. All
        datasets' obs_names must have the same name and dtype, and similarly
        for their var_names.
        
        The one exception to the var "same data type" rule: if a column is Enum
        in some datasets and Categorical in others, or Enum in all datasets but
        with different categories in each dataset, that column will be retained
        as an Enum column (with the union of the categories) in the
        concatenated var.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to cells and columns of obs and var
                      common to all datasets before concatenating, rather than
                      raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        if isinstance(datasets, Pseudobulk):
            datasets = datasets,
        datasets = (self,) + datasets + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._var:
            if flexible:
                # Check that var_names and obs_names have the same name and
                # data type for each cell type across all datasets
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0] == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0].name == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                # Subset to genes in common across all datasets
                genes_in_common = self._obs[cell_type][:, 0]\
                    .filter(self._obs[cell_type][:, 0]
                            .is_in(pl.concat([dataset._obs[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(genes_in_common) == 0:
                    error_message = (
                        f'no genes are shared across all Pseudobulk datasets '
                        f'for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_obs = []
                for dataset in datasets:
                    gene_indices = dataset._getitem_process(
                        genes_in_common, 1, dataset._obs[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, gene_indices.to_numpy()])
                    cell_type_obs.append(dataset._obs[cell_type][gene_indices])
                # Subset to columns of obs that are identical in all datasets
                # after this subsetting
                obs_columns_in_common = [
                    column.name for column in cell_type_obs[0][:, 1:]
                    if all(column.name in dataset_cell_type_obs and
                           dataset_cell_type_obs[column.name].equals(column)
                           for dataset_cell_type_obs in cell_type_obs[1:])]
                cell_type_obs = cell_type_obs[0]
                cell_type_obs = cell_type_obs.select(cell_type_obs.columns[0],
                                                     obs_columns_in_common)
                # Subset to columns of var that are present in all datasets,
                # and have the same data types. Also include columns of var
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                var_mismatched_categoricals = {
                    column for column, dtype in self._var[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._var[cell_type] and
                           dataset._var[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._var[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                var_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._var[cell_type].schema.items(), 1, None)
                    if column in var_mismatched_categoricals or
                       all(column in dataset[cell_type]._var and
                           dataset._var[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._var[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in var_mismatched_categoricals}
                cell_type_var = [
                    dataset._var[cell_type]
                    .cast(cast_dict)
                    .select(var_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all obs are identical
                cell_type_obs = self._obs[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._obs[cell_type].equals(cell_type_obs):
                        error_message = (
                            f'all Pseudobulk datasets must have the same obs '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                # Check that all var have the same columns and data types
                schema = self._var[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._var[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in var for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_var = [dataset._var[cell_type]
                                 for dataset in datasets]
            # Concatenate
            X[cell_type] = np.hstack(cell_type_X)
            var[cell_type] = pl.concat(cell_type_var)
            obs[cell_type] = cell_type_obs
        return Pseudobulk(X=X, obs=obs, var=var)
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],       
                    column: PseudobulkColumn | None |
                            dict[str, PseudobulkColumn | None],
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                                  ...],
                    custom_error: str | None = None,
                    allow_None: bool = True,
                    allow_null: bool = False,
                    cell_types: Sequence[str] | None = None) -> \
            dict[str, pl.Series | None]:
        """
        Get a column of the same length as obs or var for each cell type.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of each cell type's obs/var, a
                    polars expression that evaluates to a single column when 
                    applied to each cell type's obs/var, a polars Series or 
                    NumPy array of the same length as each cell type's 
                    obs/var, or a function that takes in two arguments, `self`
                    and a cell type, and returns a polars Series or NumPy 
                    array of the same length as obs/var. Or, a dictionary
                    mapping cell-type names to any of the above; each cell type
                    in this Pseudobulk dataset must be present. May also be
                    `None` (or a dictionary containing `None` values) if 
                    `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the column
            custom_error: a custom error message for when (an element of)
                          `columns` is a string and is not found in obs/var;
                          use `{}` as a placeholder for the name of the column
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_null: whether to allow `columns` to contain null values
            cell_types: a list of cell types; if `None`, use all cell types. If
                        specified and `column` is a Sequence, `column` and
                        `cell_types` should have the same length.
        
        Returns:
            A dictionary mapping each cell type to a polars Series of the same
            length as the cell type's obs/var. Or, if `columns` is `None` (or
            if some elements are `None`), a dict where some or all values are
            `None`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if cell_types is None:
            cell_types = self._X
        if column is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return {cell_type: None for cell_type in cell_types}
        columns = {}
        if isinstance(column, str):
            for cell_type in cell_types:
                if column not in obs_or_var[cell_type]:
                    error_message = (
                        f'{variable_name} {column!r} is not a column of '
                        f'{obs_or_var_name}[{cell_type!r}]'
                        if custom_error is None else
                        custom_error.format(f'{column!r}'))
                    raise ValueError(error_message)
                columns[cell_type] = obs_or_var[cell_type][column]
        elif isinstance(column, pl.Expr):
            for cell_type in cell_types:
                columns[cell_type] = obs_or_var[cell_type].select(column)
                if columns[cell_type].width > 1:
                    error_message = (
                        f'{variable_name} is a polars expression that expands '
                        f'to {columns[cell_type].width:,} columns rather '
                        f'than 1 for cell type {cell_type!r}')
                    raise ValueError(error_message)
                # noinspection PyUnresolvedReferences
                columns[cell_type] = columns[cell_type].to_series()
        elif isinstance(column, pl.Series):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a polars Series of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = column
        elif isinstance(column, np.ndarray):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a NumPy array of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = pl.Series(variable_name, column)
        elif callable(column):
            function = column
            for cell_type in cell_types:
                columns = function(self, cell_type)
                if isinstance(columns, np.ndarray):
                    if columns.ndim != 1:
                        error_message = (
                            f'{variable_name} is a function that returns a '
                            f'{columns.ndim:,}D NumPy array, but must return '
                            f'a polars Series or 1D NumPy array')
                        raise ValueError(error_message)
                    columns = pl.Series(variable_name, columns)
                elif not isinstance(columns, pl.Series):
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'variable of type {type(columns).__name__}, but must '
                        f'return a polars Series or 1D NumPy array')
                    raise TypeError(error_message)
                if len(columns) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a function that returns a column '
                        f'of length {len(columns):,} for cell type '
                        f'{cell_type!r}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = columns
        elif isinstance(column, dict):
            if len(column) != len(cell_types):
                error_message = (
                    f'{variable_name} is a dictionary of length '
                    f'{len(column):,}, which differs from the number of cell '
                    f'types ({len(cell_types):,})')
                raise ValueError(error_message)
            column_set = set(column)
            cell_type_set = set(cell_types)
            if column_set != cell_type_set:
                overlap = len(column_set & cell_type_set)
                if overlap:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but only {overlap:,} '
                        f'of its {len(cell_types):,} keys '
                        f'{plural("correspond", overlap)} to cell types')
                    raise ValueError(error_message)
                else:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but None of its '
                        f'{len(cell_types):,} keys correspond to cell types')
                    raise ValueError(error_message)
            for cell_type, col in column.items():
                if col is None:
                    if allow_None:
                        columns[cell_type] = None
                    else:
                        error_message = \
                            f'{variable_name}[{cell_type!r}] is None'
                        raise TypeError(error_message)
                elif isinstance(col, str):
                    columns[cell_type] = col
                elif isinstance(col, pl.Expr):
                    col = obs_or_var[cell_type].select(col)
                    if col.width > 1:
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'expression that expands to {col.width:,} '
                            f'columns rather than 1')
                        raise ValueError(error_message)
                    columns[cell_type] = col.to_series()
                elif isinstance(col, pl.Series):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'Series of length {len(col):,}, which differs '
                            f'from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                elif isinstance(col, np.ndarray):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a NumPy array '
                            f'of length {len(col):,}, which differs from the '
                            f'length of {obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = pl.Series(variable_name, col)
                elif callable(col):
                    # noinspection PyCallingNonCallable
                    col = col(self, cell_type)
                    if isinstance(col, np.ndarray):
                        if col.ndim != 1:
                            error_message = (
                                f'{variable_name}[{cell_type!r}] is a '
                                f'function that returns a {col.ndim:,}D NumPy '
                                f'array, but must return a polars Series or '
                                f'1D NumPy array')
                            raise ValueError(error_message)
                        col = pl.Series(variable_name, col)
                    elif not isinstance(col, pl.Series):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a variable of type '
                            f'{type(col).__name__}, but must return a '
                            f'polars Series or 1D NumPy array')
                        raise TypeError(error_message)
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a column of length {len(col):,}, '
                            f'which differs from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                else:
                    error_message = (
                        f'{variable_name}[{cell_type!r}] must be a string '
                        f'column name, a polars expression or Series, a 1D '
                        f'NumPy array, or a function that returns any of '
                        f'these when applied to this Pseudobulk dataset and a '
                        f'given cell type, but has type '
                        f'{type(col).__name__!r}')
                    raise TypeError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression or Series, a 1D NumPy array, or a function that '
                f'returns any of these when applied to this Pseudobulk '
                f'dataset and a given cell type, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        # Check dtypes
        if not isinstance(dtypes, tuple):
            dtypes = dtypes,
        for cell_type, col in columns.items():
            base_type = col.dtype.base_type()
            for expected_type in dtypes:
                if base_type == expected_type or expected_type == 'integer' \
                        and base_type in pl.INTEGER_DTYPES or \
                        expected_type == 'floating-point' and \
                        base_type in pl.FLOAT_DTYPES:
                    break
            else:
                if len(dtypes) == 1:
                    dtypes = str(dtypes[0])
                elif len(dtypes) == 2:
                    dtypes = ' or '.join(map(str, dtypes))
                else:
                    dtypes = \
                        ', '.join(map(str, dtypes[:-1])) + f', or {dtypes[-1]}'
                if isinstance(columns, str):
                    error_message = (
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]'
                        f'[{columns!r}] must be {dtypes}, but has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
                else:
                    error_message = (
                        f'{variable_name} must be {dtypes}, but has data type '
                        f'{base_type!r} for cell type {cell_type!r}')
                    raise TypeError(error_message)
        # Check nulls, if `allow_null=False`
        if not allow_null:
            for cell_type, col in columns.items():
                null_count = col.null_count()
                if null_count > 0:
                    full_variable_name = \
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]' \
                        f'[{columns!r}]' if isinstance(columns, str) else \
                            variable_name
                    error_message = (
                        f'{full_variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cell type '
                        f'{cell_type!r}, but must not contain any')
                    raise ValueError(error_message)
        return columns
    
    def _describe_column(self,
                         column_name: str,
                         column: SingleCellColumn,
                         cell_type: str):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
            cell_type: the cell type where the error was triggered
    
        Returns:
            The column's description: just the argument's name unless the value
            (for this cell type) is a string (i.e. the column's name in obs or
            var), in which case also include the value.
        """
        if isinstance(column, Sequence):
            cell_type_index = next(i for i, cell_type_i in enumerate(self._obs)
                                   if cell_type_i == cell_type)
            column = column[cell_type_index]
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
        
    def filter_obs(self,
                   *predicates: str | pl.Expr | pl.Series |
                                Iterable[str | pl.Expr | pl.Series] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both obs and X
        for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to samples
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to samples passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        obs = {}
        for cell_type in self._obs:
            if cell_type in cell_types:
                obs[cell_type] = self._obs[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.Int32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    obs[cell_type]['__Pseudobulk_index'].to_numpy()]
                obs[cell_type] = obs[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                obs[cell_type] = self._obs[cell_type]
        return Pseudobulk(X=X, obs=obs, var=self._var)
    
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both var and X
        for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        var = {}
        for cell_type in self._var:
            if cell_type in cell_types:
                var[cell_type] = self._var[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.Int32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    var[cell_type]['__Pseudobulk_index'].to_numpy()]
                var[cell_type] = var[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                var[cell_type] = self._var[cell_type]
        return Pseudobulk(X=X, obs=self._obs, var=var)
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's obs. obs_names will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            obs[cell_type]=obs[cell_type].select(*exprs, **named_exprs) for all
            cell types in obs, and obs_names as the first column unless already
            included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                new_cell_type_obs = cell_type_obs.select(*exprs, **named_exprs)
                if cell_type_obs.columns[0] not in new_cell_type_obs:
                    new_cell_type_obs = \
                        new_cell_type_obs.select(cell_type_obs[:, 0], pl.all())
                obs[cell_type] = new_cell_type_obs
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's var. var_names will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            var[cell_type]=var[cell_type].select(*exprs, **named_exprs) for all
            cell types in var, and var_names as the first column unless already
            included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                new_cell_type_var = cell_type_var.select(*exprs, **named_exprs)
                if cell_type_var.columns[0] not in new_cell_type_var:
                    new_cell_type_var = \
                        new_cell_type_var.select(cell_type_var[:, 0], pl.all())
                var[cell_type] = new_cell_type_var
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var)
    
    def select_cell_types(self,
                          cell_types: str | Iterable[str],
                          *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset subset to the cell type(s) in
        `cell_types` and `more_cell_types`.
        
        Args:
            cell_types: cell type(s) to select
            *more_cell_types: additional cell types to select, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset subset to the specified cell type(s).
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types += more_cell_types
        for cell_type in cell_types:
            if cell_type not in self._X:
                error_message = (
                    f'tried to select {cell_type!r}, which is not a cell type '
                    f'in this Pseudobulk')
                raise ValueError(error_message)
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in cell_types})
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's obs.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            obs[cell_type]=obs[cell_type].with_columns(*exprs, **named_exprs)
            for all cell types in obs.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs={
            cell_type: obs.with_columns(*exprs, **named_exprs)
                       if cell_type in cell_types else obs
            for cell_type, obs in self._obs.items()}, var=self._var)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's var.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            var[cell_type]=var[cell_type].with_columns(*exprs, **named_exprs)
            for all cell types in var.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs, var={
            cell_type: var.with_columns(*exprs, **named_exprs)
                       if cell_type in cell_types else var
            for cell_type, var in self._var.items()})

    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from obs.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`, 
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the 
                                 operation; mutually exclusive with 
                                 `cell_types`               
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X, 
                          obs={cell_type: obs.drop(columns)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from var.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`, 
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the 
                                 operation; mutually exclusive with 
                                 `cell_types`           
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: var.drop(columns) 
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()})
    
    def drop_cell_types(self,
                        cell_types: str | Iterable[str],
                        *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset with `cell_types` and `more_cell_types`
        removed. Raises an error if all cell types would be dropped.
        
        Args:
            cell_types: cell type(s) to drop
            *more_cell_types: additional cell types to drop, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) removed.
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types = set(cell_types) | set(more_cell_types)
        # noinspection PyTypeChecker
        original_cell_types = set(self)
        if not cell_types < original_cell_types:
            if cell_types == original_cell_types:
                error_message = 'all cell types would be dropped'
                raise ValueError(error_message)
            for cell_type in cell_types:
                if cell_type not in original_cell_types:
                    error_message = (
                        f'tried to drop {cell_type!r}, which is not a cell '
                        f'type in this Pseudobulk')
                    raise ValueError(error_message)
        new_cell_types = \
            [cell_type for cell_type in self if cell_type not in cell_types]
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in new_cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in new_cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in new_cell_types})
    
    def rename_obs(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of obs renamed for each
        cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of obs renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs={
            cell_type: obs.rename(mapping) if cell_type in cell_types else obs
            for cell_type, obs in self._obs.items()}, var=self._var)
    
    def rename_var(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of var renamed for each
        cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of var renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs, var={
            cell_type: var.rename(mapping) if cell_type in cell_types else var
            for cell_type, var in self._var.items()})
    
    def rename_cell_types(self,
                          mapping: dict[str, str] | Callable[[str], str]) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with cell type(s) renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with old
                     cell type names as keys and new names as values, or a
                     function that takes an old name and returns a new name.
                     If `mapping` is a dictionary, cell types missing from its
                     keys will retain their original names.
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) renamed.
        """
        if isinstance(mapping, dict):
            new_cell_types = [mapping.get(cell_type, cell_type)
                              for cell_type in self._X]
        elif callable(mapping):
            new_cell_types = [mapping(cell_type) for cell_type in self._X]
        else:
            raise TypeError(f'mapping must be a dictionary or function, but '
                            f'has type {type(mapping).__name__!r}')
        return Pseudobulk(X={new_cell_type: X
                             for new_cell_type, X in
                             zip(new_cell_types, self._X.values())},
                          obs={new_cell_type: obs
                               for new_cell_type, obs in
                               zip(new_cell_types, self._obs.values())},
                          var={new_cell_type: var
                               for new_cell_type, var in
                               zip(new_cell_types, self._var.values())})
    
    def cast_X(self,
               dtype: np._typing.DTypeLike,
               *,
               cell_types: str | Iterable[str] | None = None,
               excluded_cell_types: str | Iterable[str] |
                                    None = None) -> Pseudobulk:
        """
        Cast each cell type's X to the specified data type.
        
        Args:
            dtype: a NumPy data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with each cell type's X cast to the
            specified data type.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X={cell_type: X.astype(dtype)
                                        if X in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's obs to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's obs
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs={cell_type: obs.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's var to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's var
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs=self._obs,
                          var={cell_type: var.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()},)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Left join each cell type's obs with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's obs with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in obs
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in obs or more
                        than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in obs.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include null as a valid value to join on.
                        By default, null values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from obs
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's obs.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in obs and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    f"either 'on' or both of 'left_on' and 'right_on' must be "
                    f"specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type not in cell_types:
                obs[cell_type] = cell_type_obs
                continue
            left = cell_type_obs
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'obs[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            obs[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(obs[cell_type]) > len(self._obs[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=obs, var=self._var)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Join each cell type's var with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's var with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in var
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in var or more
                        than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in var.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include null as a valid value to join on.
                        By default, null values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from obs
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's var.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in obs and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type not in cell_types:
                var[cell_type] = cell_type_var
                continue
            left = cell_type_var
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'var[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            var[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(var[cell_type]) > len(self._var[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=self._obs, var=var)
    
    def peek_obs(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of obs (the first row, by default) for a cell type (the
        first cell type, by default) with each column on its own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._obs))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def peek_var(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of var (the first row, by default) with each column on its
        own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._var))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of samples.
        
        Args:
            n: the number of samples to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of samples to return; mutually exclusive
                      with `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of obs to subsample by. Can be `None`, a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this Pseudobulk dataset and a
                       cell type and returns a polars Series or 1D NumPy array.
                       Or, a dictionary mapping cell-type names to any of the
                       above; each cell type in this Pseudobulk dataset must be
                       present. Specifying `by_column` ensures that the same
                       fraction of cells with each value of `by_column` are
                       subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups can be undersampled by one
                       element. Can contain null entries: the corresponding
                       samples will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in obs, instead of raising an error. Must be 
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new Pseudobulk dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to obs.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'obs', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._obs[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of obs[{cell_type!r}]')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = (
                'overwrite must be False when subsample_column is None; did '
                'you already run subsample_obs()? Set overwrite=True to '
                'overwrite.')
            raise ValueError(error_message)
        check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign n to be a vector of sample sizes per group, broadcast to
            # the length of obs. The total sample size should exactly match the
            # original n; if necessary, oversample the smallest groups or
            # undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            obs = {}
            for cell_type, cell_type_obs in self._obs.items():
                if cell_type in cell_types:
                    cell_type_obs = cell_type_obs\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.Int32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_obs['__Pseudobulk_index'].to_numpy()]
                    obs[cell_type] = cell_type_obs.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    obs[cell_type] = cell_type_obs
            return Pseudobulk(X=X, obs=obs, var=self._var)
        else:
            return Pseudobulk(X=self._X, obs={
                cell_type: obs.with_columns(expressions[cell_type]
                                            .alias(subsample_column)) 
                           if cell_type in cell_types else obs
                for cell_type, obs in self._obs.items()}, var=self._var)
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of var to subsample by. Can be `None`, a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this Pseudobulk dataset and a
                       cell type and returns a polars Series or 1D NumPy array.
                       Or, a dictionary mapping cell-type names to any of the
                       above; each cell type in this Pseudobulk dataset must be
                       present. Specifying `by_column` ensures that the same
                       fraction of cells with each value of `by_column` are
                       subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups may be undersampled by one
                       element. Can contain null entries: the corresponding
                       genes will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already 
                       present in var, instead of raising an error. Must be 
                       `False` when `subsample_column` is `None`.

        Returns:
            A new Pseudobulk dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to var.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'var', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._var[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of var[{cell_type!r}]')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = (
                'overwrite must be False when subsample_column is None; did '
                'you already run subsample_var()? Set overwrite=True to '
                'overwrite.')
            raise ValueError(error_message)
        check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign n to be a vector of sample sizes per group, broadcast to
            # the length of var. The total sample size should exactly match the
            # original n; if necessary, oversample the smallest groups or
            # undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n * pl.len() / len(by_column))
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            var = {}
            for cell_type, cell_type_var in self._var.items():
                if cell_type in cell_types:
                    cell_type_var = cell_type_var\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.Int32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_var['__Pseudobulk_index'].to_numpy()]
                    var[cell_type] = cell_type_var.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    var[cell_type] = cell_type_var
            return Pseudobulk(X=X, obs=self._obs, var=var)
        else:
            return Pseudobulk(X=self._X, obs=self._obs, var={
                cell_type: var.with_columns(expressions[cell_type]
                                            .alias(subsample_column))
                           if cell_type in cell_types else var
                for cell_type, var in self._var.items()})
    
    def pipe(self,
             function: Callable[[Pseudobulk, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a Pseudobulk dataset.
        
        Args:
            function: the function to apply
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this Pseudobulk dataset.
        """
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]], ...],
                                  dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]]],
               *args: Any,
               **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's X. To apply a function to
        each cell type's X, rather than to X as a whole, use `map_X()`.
        
        Args:
            function: the function to apply to X. It must take the old X as its
                      first argument and return the new X. The function may
                      also take other arguments after X, which can be specified
                      via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to X.
        """
        return Pseudobulk(X=function(self._X, *args, **kwargs),
                          obs=self._obs, var=self._var)
    
    def pipe_obs(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's obs. To apply a function to
        each cell type's obs, rather than to obs as a whole, use `map_obs()`.
        
        Args:
            function: the function to apply to obs. It must take the old obs as
                      its first argument and return the new obs. The function
                      may also take other arguments after obs, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to 
            obs.
        """
        return Pseudobulk(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var)
    
    def pipe_var(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's var. To apply a function to
        each cell type's var, rather than to var as a whole, use `map_var()`.
        
        Args:
            function: the function to apply to var. It must take the old var as
                      its first argument and return the new var. The function
                      may also take other arguments after var, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to 
            var.
        """
        return Pseudobulk(X=self._X, obs=self._obs, 
                          var=function(self._var, *args, **kwargs))

    def map_X(self,
              function: Callable[[np.ndarray[2, np.dtype[np.integer |
                                                         np.floating]], ...],
                                 np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]],
              *args: Any,
              cell_types: str | Iterable[str] | None = None,
              excluded_cell_types: str | Iterable[str] | None = None,
              **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's X. To apply a function to X as a
        whole, rather than each cell type's X, use `pipe_X()`.
        
        Args:
            function: the function to apply to each cell type's X. It must take
                      the old X for a cell type and return the new X. The 
                      function may also take other arguments after X, which can
                      be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's X.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X={cell_type: function(X, *args, **kwargs)
                                        if cell_type in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var)
    
    def map_obs(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's obs. To apply a function to obs as
        a whole, rather than each cell type's obs, use `pipe_obs()`.
        
        Args:
            function: the function to apply to each cell type's obs. It must 
                      take the old obs for a cell type and return the new obs.  
                      The function may also take other arguments after obs, 
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's obs.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs={cell_type: function(obs, *args, **kwargs) 
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var)
    
    def map_var(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's var. To apply a function to var as
        a whole, rather than each cell type's var, use `pipe_var()`.
        
        Args:
            function: the function to apply to each cell type's var. It must 
                      take the old var for a cell type and return the new var.  
                      The function may also take other arguments after var, 
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's var.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: function(var, *args, **kwargs) 
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()})
    
    @staticmethod
    def _too_few_samples(obs: pl.DataFrame,
                         group_column: pl.Series | None,
                         min_samples: int | np.integer,
                         cell_type: str,
                         verbose: bool,
                         after_filtering: bool = False) -> bool:
        """
        Skip cell types with fewer than `min_samples` samples, or with fewer
        than `min_samples` samples in any group if `group_column` is not None.
        
        Args:
            obs: the cell type's obs, after applying one or more QC filters
            group_column: the column with sample group information (e.g. which
                          samples are disease cases and which are controls),
                          after applying one or more QC filters. The samples
                          in this column must be the same as those in `obs`.
            min_samples: filter to cell types with at least this many samples
                         in each group, or with at least this many total
                         samples if `group_column` is `None`
            cell_type: the name of the cell type
            verbose: whether to explain why the cell type is being skipped, if
                     it is
            after_filtering: whether this function is being run after sample
                             filtering

        Returns:
            Whether this cell type has too few samples and should be skipped.
        """
        if group_column is not None:
            too_small_groups = group_column\
                .value_counts()\
                .filter(pl.col.count < min_samples)\
                .to_series()\
                .drop_nulls()
            if len(too_small_groups) > 0:
                if verbose:
                    group_description = (
                        f'{too_small_groups["count"][-1]:,} samples where '
                        f'group_column = {too_small_groups["group"][-1]}')
                    if len(too_small_groups) > 1:
                        group_description = (
                            ', '.join(f'{count:,} samples where '
                                      f'group_column = {group}'
                                      for group, count in
                                      too_small_groups[:-1].iter_rows()) +
                            f' and {group_description}')
                    print(f'[{cell_type}] Skipping this cell type because it '
                          f'has only {group_description} after filtering, '
                          f'which '
                          f'{"is" if len(too_small_groups) == 1 else "are"} '
                          f'fewer than min_samples ({min_samples:,})')
                return True
        else:
            num_samples = len(obs)
            if num_samples < min_samples:
                if verbose:
                    print(f'[{cell_type}] Skipping this cell type because '
                          f'it has only {num_samples:,} '
                          f'{plural("sample", num_samples)}'
                          f'{" after filtering" if after_filtering else ""}, '
                          f'which is fewer than min_samples ({min_samples:,})')
                return True
        return False
    
    def qc(self,
           group_column: PseudobulkColumn | None |
                         dict[str, PseudobulkColumn | None],
           *,
           custom_filter: PseudobulkColumn | None |
                          dict[str, PseudobulkColumn | None] = None,
           min_samples: int | np.integer = 2,
           min_cells: int | np.integer | None = 10,
           max_standard_deviations: int | float | np.integer | np.floating |
                                    None = 3,
           min_nonzero_fraction: int | float | np.integer | np.floating |
                                 None = 0.8,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           error_if_negative_counts: bool = True,
           allow_float: bool = False,
           verbose: bool = True) -> Pseudobulk:
        """
        Subsets each cell type to samples passing quality control (QC). If
        samples fall into discrete groups (e.g. disease cases versus controls),
        these should be specified via the `group_column` argument.
        
        Filters, in order, to:
        - samples that pass the `custom_filter` (if specified), have
          non-missing values for `group_column` (if specified), and have at
          least `min_cells` cells of that type (default: 10)
        - samples where the number of genes with 0 counts is at most
          `max_standard_deviations` standard deviations above the mean
          (default: 3)
        - genes with at least 1 count in `100 * min_nonzero_fraction`%
          (default: 80%) of samples (in every group, if `group_column` is
          specified)
        
        If at any point during this filtering process, there are fewer than
        `min_samples` samples (in any group, if `group_column` is specified),
        the cell type is filtered out entirely.
        
        Args:
            group_column: an optional String, Categorical, Enum, Boolean, or
                          integer column of obs with sample group information,
                          e.g. which samples are disease cases and which are
                          controls. If specified, the `min_nonzero_fraction`
                          and `min_samples` filters must pass for every group,
                          rather than merely passing for the dataset as a
                          whole. Set to `None` if samples do not fall into
                          discrete groups. Can be `None`, a column name, a
                          polars expression, a polars Series, a 1D NumPy array,
                          or a function that takes in this Pseudobulk dataset
                          and a cell type and returns a polars Series or 1D
                          NumPy array. Or, a dictionary mapping cell-type names
                          to any of the above; each cell type in this
                          Pseudobulk dataset must be present. Can contain null
                          entries: the corresponding samples will be deemed to
                          fail QC.
            custom_filter: an optional Boolean column of obs containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be `None`, a
                           column name, a polars expression, a polars Series, a
                           1D NumPy array, or a function that takes in this
                           Pseudobulk dataset and a cell type and returns a
                           polars Series or 1D NumPy array. Or, a dictionary
                           mapping cell-type names to any of the above; each
                           cell type in this Pseudobulk dataset must be
                           present.
            min_samples: filter to cell types with at least this many samples
                         in every group, or with at least this many total
                         samples if `group_column` is `None`
            min_cells: if not `None`, filter to samples with ≥ this many cells
                       of each cell type
            max_standard_deviations: if not `None`, filter to samples where the
                                     number of genes with 0 counts is at most
                                     this many standard deviations above the
                                     mean
            min_nonzero_fraction: if not `None`, filter to genes with at least
                                  one count in this fraction of samples in each
                                  group, or if `group_column` is `None`, at
                                  least one count in this fraction of samples
                                  overall. Note: `min_nonzero_fraction=0`
                                  filters out only genes with all-zero counts,
                                  while `min_nonzero_fraction=None` does not
                                  filter out any genes.
            cell_types: one or more cell types to QC; if `None`, QC all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from QC;
                                 mutually exclusive with `cell_types`
            error_if_negative_counts: if `True`, raise an error if any counts
                                      are negative
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            verbose: whether to print how many samples and genes were filtered
                     out at each step of the QC process
        
        Returns:
            A new Pseudobulk dataset with each cell type's X, obs and var
            subset to samples and genes passing QC.
        """
        # Check inputs
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        group_column = self._get_column(
            'obs', group_column, 'group_column',
            (pl.String, pl.Categorical, pl.Enum, pl.Boolean, 'integer'),
            allow_null=True)
        custom_filter = self._get_column(
            'obs', custom_filter, 'custom_filter', pl.Boolean)
        check_type(min_samples, 'min_samples', int,
                   'an integer greater than or equal to 2')
        check_bounds(min_samples, 'min_samples', 2)
        if min_cells is not None:
            check_type(min_cells, 'min_cells', int, 'a positive integer')
            check_bounds(min_cells, 'min_cells', 1)
        if max_standard_deviations is not None:
            check_type(max_standard_deviations, 'max_standard_deviations',
                       (int, float), 'a positive number')
            check_bounds(max_standard_deviations, 'max_standard_deviations', 0,
                         left_open=True)
        if min_nonzero_fraction is not None:
            check_type(min_nonzero_fraction, 'min_nonzero_fraction',
                       (int, float), 'a number between 0 and 1, inclusive')
            check_bounds(min_nonzero_fraction, 'min_nonzero_fraction', 0, 1)
        check_type(error_if_negative_counts, 'error_if_negative_counts', bool,
                   'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `group_column` is None when neither the
        # `min_nonzero_fraction` nor the `min_samples` filters will be applied
        if group_column is not None and min_nonzero_fraction is None and \
                min_samples is None:
            error_message = (
                'group_column must be None when min_nonzero_fraction and '
                'min_samples are both None')
            raise ValueError(error_message)
        # If `error_if_negative_counts=True`, raise an error if X has any
        # negative values
        if error_if_negative_counts:
            for cell_type in cell_types:
                if self._X[cell_type].ravel().min() < 0:
                    error_message = f'X[{cell_type!r}] has negative counts'
                    raise ValueError(error_message)
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float:
            for cell_type in cell_types:
                dtype = self._X[cell_type].dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"qc() requires raw counts but X[{cell_type!r}].dtype "
                        f"is {dtype!r}, a floating-point data type; if you "
                        f"are sure that all values are raw integer counts, "
                        f"i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        if verbose:
            print()
        X_qced, obs_qced, var_qced = {}, {}, {}
        for cell_type in self._X:
            X = self._X[cell_type]
            obs = self._obs[cell_type]
            var = self._var[cell_type]
            if cell_type not in cell_types:
                X_qced[cell_type] = X
                obs_qced[cell_type] = obs
                var_qced[cell_type] = var
                if verbose:
                    if cell_types is not None:
                        print(f'[{cell_type}] Skipping this cell type due to '
                              f'being absent from cell_types')
                    else:
                        print(f'[{cell_type}] Skipping this cell type due to '
                              f'being present in excluded_cell_types')
                continue
            if verbose:
                print(f'[{cell_type}] Starting with {len(obs):,} samples and '
                      f'{len(var):,} genes.')
            # Get the group column for this cell type.
            groups = \
                group_column[cell_type] if group_column is not None else None
            # Check if we have enough samples for this cell type
            if Pseudobulk._too_few_samples(obs, groups, min_samples, cell_type,
                                           verbose):
                continue
            # Get a mask of samples passing the custom filter, if specified
            if custom_filter is not None:
                if verbose:
                    print(f'[{cell_type}] Applying the custom filter...')
                sample_mask = custom_filter[cell_type]
                if sample_mask is not None:
                    if verbose:
                        print(f'[{cell_type}] {sample_mask.sum():,} samples '
                              f'remain after applying the custom filter.')
            else:
                sample_mask = None
            # If `groups` is not `None` and some samples have missing groups,
            # get a mask of samples with non-missing groups
            if groups is not None:
                null_count = groups.null_count()
                if null_count:
                    if verbose:
                        print(f'[{cell_type}] Filtering to samples with '
                              f'non-missing values for group_column...')
                    if sample_mask is None:
                        sample_mask = groups.is_not_null()
                    else:
                        sample_mask &= groups.is_not_null()
                    if verbose:
                        print(f'[{cell_type}] {sample_mask.sum():,} samples '
                              f'remain after filtering to samples with '
                              f'non-missing values for group_column.')
            # Get a mask of samples with at least `min_cells` cells of this
            # cell type, if `min_cells` was specified. Combine this with the
            # sample mask from `custom_filter` above, if both were specified.
            if min_cells is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples with at least '
                          f'{min_cells} {cell_type} cells...')
                if sample_mask is None:
                    sample_mask = obs['num_cells'] >= min_cells
                else:
                    sample_mask &= obs['num_cells'] >= min_cells
                if verbose:
                    print(f'[{cell_type}] {sample_mask.sum():,} samples '
                          f'remain after filtering to samples with at least '
                          f'{min_cells} {cell_type} cells.')
            # Now apply the sample mask, which contains the samples passing the
            # custom filter and/or `min_cells` filter
            if sample_mask is not None:
                # noinspection PyUnresolvedReferences
                X = X[sample_mask.to_numpy()]
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                # Check if we still have enough samples for this cell type,
                # after applying these three filters
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
            # Filter to samples where the number of genes with 0 counts is less
            # than `max_standard_deviations` standard deviations above the mean
            if max_standard_deviations is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples where the '
                          f'number of genes with 0 counts is '
                          f'<{max_standard_deviations} standard deviations '
                          f'above the mean...')
                # noinspection PyUnresolvedReferences
                num_zero_counts = (X == 0).sum(axis=1, dtype=np.int32)
                sample_mask = num_zero_counts < num_zero_counts.mean() + \
                              max_standard_deviations * num_zero_counts.std()
                X = X[sample_mask]
                sample_mask = pl.Series(sample_mask)
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                if verbose:
                    print(f'[{cell_type}] {len(obs):,} samples remain after '
                          f'filtering to samples where the number of genes '
                          f'with 0 counts is <{max_standard_deviations} '
                          f'standard deviations above the mean.')
                # Check if we have still enough samples for this cell type,
                # after applying this filter
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
            # Filter to genes with at least 1 count in
            # `100 * min_nonzero_fraction`% of samples (or samples in each
            # group, if `group_column` is not None for this cell type)
            if min_nonzero_fraction is not None:
                if groups is not None:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples in '
                              f'each group...')
                    gene_mask = np.logical_and.reduce([
                        np.quantile(X[mask.to_numpy()],
                                    1 - min_nonzero_fraction, axis=0) > 0
                        for mask in groups.to_dummies().cast(pl.Boolean)])
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        print(f'[{cell_type}] {len(var):,} genes remain '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples in each group.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples...')
                    gene_mask = np.quantile(X, 1 - min_nonzero_fraction,
                                            axis=0) > 0
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        print(f'[{cell_type}] {len(var):,} genes remain '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples.')
            X_qced[cell_type] = np.ascontiguousarray(X)
            obs_qced[cell_type] = obs
            var_qced[cell_type] = var
            if verbose:
                print()
        return Pseudobulk(X=X_qced, obs=obs_qced, var=var_qced)

    @staticmethod
    def _library_size(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                      cell_type: str,
                      *,
                      logratio_trim: int | float | np.integer |
                                     np.floating = 0.3,
                      sum_trim: int | float | np.integer | np.floating = 0.05,
                      A_cutoff: int | float | np.integer |
                                np.floating = -1e10) -> \
            np.ndarray[1, np.dtype[np.float64]]:
        """
        Calculate normalization factor-adjusted library sizes according to the
        method of edgeR's `calcNormFactors()` with the default `method='TMM'`.
        
        Results differ from edgeR due to the presence of a floating-point bug
        in the original `calcNormFactors()` implementation. When calculating
        `logR`, the log2 ratio of `count / library_size` for a gene between a
        particular sample and a "reference" sample, the numerator and
        denominator of the ratio both involve a division by their sample's
        library size. In principle, these divisions by library size are
        equivalent to multiplying by the same constant across genes, namely the
        ratio of the two samples' library sizes. But in practice, even if two
        genes have the same count ratio between the two samples, they may still
        have slightly different `count / library_size` ratios due to
        floating-point roundoff, leading to these genes erroneously being
        assigned different `logR` ranks instead of being treated as tied. Our
        implementation fixes this bug by changing the order of operations so
        that the library size ratio is calculated first, then multiplied by the
        count ratio.
        
        Because this bug affects which genes are included in the trimmed mean,
        its impact can be relatively large, sometimes leading to a >1% error in
        edgeR's estimated library size relative to our correct implementation.

        Does not support the `lib.size` and `refColumn` arguments to
        `calcNormFactors()`; these are both assumed to be `NULL` (the default)
        and will always be calculated internally. The `doWeighting` argument is
        also not supported and is assumed to be `TRUE` (the default), so
        asymptotic binomial precision weights will be used.

        Args:
            X: a matrix of raw (read) counts. `X` is assumed to have the
               opposite orientation from the original `calcNormFactors()`:
               samples are rows and genes are columns.
            cell_type: the cell type `X` is from, used in error messages
            logratio_trim: the amount of trim to use on log-ratios ("M"
                           values); must be between 0 and 1
            sum_trim: the amount of trim to use on the combined absolute levels
                      ("A" values); must be between 0 and 1
            A_cutoff: the cutoff on "A" values to use before trimming

        Returns:
            The norm factor-corrected library sizes: raw library sizes (column
            sums) times norm factors.
        """
        # Check inputs
        check_type(logratio_trim, 'logratio_trim', float,
                   'a floating-point number')
        check_bounds(logratio_trim, 'logratio_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(sum_trim, 'sum_trim', float, 'a floating-point number')
        check_bounds(sum_trim, 'sum_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(A_cutoff, 'A_cutoff', float, 'a floating-point number')
        
        # Degenerate cases
        num_samples, num_genes = X.shape
        if num_samples == 1 or num_genes == 0:
            return np.ones(num_samples)
        
        # Raise an error if there are any all-zero columns (genes)
        has_all_zero_columns = cython_inline(f'''
            def has_all_zero_columns(const {cython_type(X.dtype)}[:, :] X):
                cdef int i, j
                for j in range(X.shape[1]):
                    for i in range(X.shape[0]):
                        if X[i, j] != 0:
                            break
                    else:
                        return True
                return False
            ''')['has_all_zero_columns'](X=X)
        if has_all_zero_columns:
            error_message = (
                f'[{cell_type}] some genes have all-zero counts; did you '
                f'forget to run Pseudobulk.qc()?')
            raise ValueError(error_message)
        
        # Calculate raw library sizes
        library_size = X.sum(axis=1)
        
        # Raise an error if any raw library sizes are 0
        if library_size.min() == 0:
            error_message = (
                'some samples have all-zero counts; did you forget to run '
                'Pseudobulk.qc()?')
            raise ValueError(error_message)
        
        # Determine which sample is the reference sample
        f75 = np.quantile(X, 0.75, axis=1) / library_size
        if np.median(f75) < 1e-20:
            ref_sample = np.argmax(np.sqrt(X).sum(axis=1))
        else:
            ref_sample = np.argmin(np.abs(f75 - f75.mean()))
        
        # Preallocate arrays for norm factor calculation
        norm_factors = np.empty(num_samples)
        inv_relative_library_size = np.empty(num_samples)
        log_normalized_X_ref = np.empty(num_genes)
        logR = np.empty(num_genes)
        absE = np.empty(num_genes)
        counts = np.empty(num_genes, dtype=X.dtype)
        ref_counts = np.empty(num_genes, dtype=X.dtype)
        indices = np.empty(num_genes, dtype=np.int32)
        logR_rank = np.empty(num_genes)
        absE_rank = np.empty(num_genes)
        # Calculate norm factors
        cython_inline(rf'''
            from libcpp.cmath cimport abs, exp, log, log2
            
            cdef void quicksort(const double[::1] arr,
                                int[::1] indices,
                                int left,
                                int right) noexcept nogil:
                cdef double pivot_value
                cdef int pivot_index, mid, i, temp
        
                while left < right:
                    mid = left + (right - left) // 2
                    if arr[indices[mid]] < arr[indices[left]]:
                        temp = indices[left]
                        indices[left] = indices[mid]
                        indices[mid] = temp
                    if arr[indices[right]] < arr[indices[left]]:
                        temp = indices[left]
                        indices[left] = indices[right]
                        indices[right] = temp
                    if arr[indices[right]] < arr[indices[mid]]:
                        temp = indices[mid]
                        indices[mid] = indices[right]
                        indices[right] = temp
        
                    pivot_value = arr[indices[mid]]
                    temp = indices[mid]
                    indices[mid] = indices[right]
                    indices[right] = temp
                    pivot_index = left
        
                    for i in range(left, right):
                        if arr[indices[i]] < pivot_value:
                            temp = indices[i]
                            indices[i] = indices[pivot_index]
                            indices[pivot_index] = temp
                            pivot_index += 1
        
                    temp = indices[right]
                    indices[right] = indices[pivot_index]
                    indices[pivot_index] = temp
        
                    if pivot_index - left < right - pivot_index:
                        quicksort(arr, indices, left, pivot_index - 1)
                        left = pivot_index + 1
                    else:
                        quicksort(arr, indices, pivot_index + 1, right)
                        right = pivot_index - 1
        
            cdef inline void argsort(const double[::1] arr,
                                     int[::1] indices,
                                     const int n) noexcept nogil:
                cdef int i
                for i in range(n):
                    indices[i] = i
                quicksort(arr, indices, 0, n - 1)
            
            cdef inline void rankdata(const double[::1] data,
                                      int[::1] indices,
                                      double[::1] ranks,
                                      const int n):
                cdef int i = 0, start_pos = 0
                cdef double current_val, rank
                cdef bint end = False
                
                argsort(data, indices, n)
                
                while True:
                    current_val = data[indices[i]]
                    
                    # Count elements equal to current value
                    i += 1
                    if i == n:
                        end = True
                    else:
                        while data[indices[i]] == current_val:
                            i += 1
                            if i == n:
                                end = True
                                break
                        
                    # Assign average rank to all tied elements
                    rank = 0.5 * (start_pos + i) + 0.5
                    while start_pos < i:
                        ranks[indices[start_pos]] = rank
                        start_pos += 1
                    
                    if end:
                        break
            
            def calc_norm_factors(
                    const {cython_type(X.dtype)}[:, :] X,
                    const double logratio_trim,
                    const double sum_trim,
                    const double A_cutoff,
                    const int ref_sample,
                    double[::1] norm_factors,
                    {cython_type(library_size.dtype)}[::1] library_size,
                    double[::1] inv_relative_library_size,
                    double[::1] log_normalized_X_ref,
                    double[::1] logR,
                    double[::1] absE,
                    {cython_type(X.dtype)}[::1] counts,
                    {cython_type(X.dtype)}[::1] ref_counts,
                    int[::1] indices,
                    double[::1] logR_rank,
                    double[::1] absE_rank):
                    
                cdef {cython_type(X.dtype)} ref_count, count
                cdef {cython_type(library_size.dtype)} ref_library_size
                cdef int i, j, n, loL, hiL, loS, hiS, \
                    num_samples = X.shape[0], num_genes = X.shape[1]
                cdef double inverse_ref_library_size, logR_, absE_, \
                    inverse_library_size, total_inverse_library_size, \
                    norm_factor, total_weight, variance, weight, scale
                cdef bint large_enough_logR
                
                # Calculate each sample's library size relative to the
                # reference sample's (to use in the logR calculation)
                ref_library_size = library_size[ref_sample]
                for i in range(num_samples):
                    inv_relative_library_size[i] = \
                        <double> ref_library_size / library_size[i]
                
                # Calculate each gene's log normalized expression (to use in
                # the absE calculation)
                inverse_ref_library_size = 1. / ref_library_size
                for j in range(num_genes):
                    count = X[ref_sample, j]
                    log_normalized_X_ref[j] = \
                        log2(count * inverse_ref_library_size)
                
                # Calculate the normalization factor for each sample
                for i in range(num_samples):
                    inverse_library_size = 1. / library_size[i]
                    large_enough_logR = False
                    n = 0
                    for j in range(num_genes):
                        # Get the count and reference count for this gene; skip
                        # the gene if either are 0
                        ref_count = X[ref_sample, j]
                        if ref_count == 0:
                            continue
                    
                        count = X[i, j]
                        if count == 0:
                            continue
                        
                        # Calculate the log ratio of expression accounting for
                        # library size
                        logR_ = log2(inv_relative_library_size[i] * (
                            <double> count / ref_count))
                        
                        # Calculate "absolute expression": the average log2
                        # expression of this gene between this sample and the
                        # reference sample
                        absE_ = 0.5 * (log2(count * inverse_library_size) +
                                       log_normalized_X_ref[j])
                                       
                        # Cutoff based on `A_cutoff`
                        if absE_ <= A_cutoff:
                            continue
                        
                        # Store logR, absE, and the count for genes passing the
                        # infinite value and `A_cutoff` filters above
                        logR[n] = logR_
                        absE[n] = absE_
                        counts[n] = count
                        ref_counts[n] = ref_count
                        n += 1
                        
                        # Keep track of whether any gene's logR is above 1e-6
                        # in magnitude for this sample
                        large_enough_logR |= abs(logR_) >= 1e-6
                    
                    # If every gene's logR is below 1e-6 in magnitude for this
                    # sample (i.e. expression is extremely low across the
                    # board), set the sample's norm factor to 1
                    if not large_enough_logR:
                        norm_factors[i] = 1
                        continue
                    
                    # Rank genes by logR and absE
                    loL = <int> (n * logratio_trim)
                    hiL = n - loL
                    loS = <int> (n * sum_trim)
                    hiS = n - loS
                    rankdata(logR, indices, logR_rank, n)
                    rankdata(absE, indices, absE_rank, n)
                    
                    # Calculate the norm factors themselves. Find genes with
                    # intermediate ranks of both logR and absE (this is the
                    # "trimmed" part, the "T" in "TMM"). The norm factors are 2
                    # to the power of the weighted average of the logRs for
                    # these intermediate-ranked genes, where the weights are
                    # the inverse asymptotic variances.
                    total_inverse_library_size = \
                        inverse_library_size + inverse_ref_library_size
                    norm_factor = 0
                    total_weight = 0
                    for j in range(n):
                        if loL + 1 <= logR_rank[j] <= hiL and \
                                loS + 1 <= absE_rank[j] <= hiS:
                            variance = 1. / counts[j] + 1. / ref_counts[j] + \
                                total_inverse_library_size
                            weight = 1 / variance
                            norm_factor += weight * logR[j]
                            total_weight += weight
                    norm_factor = 2 ** (norm_factor / total_weight)
                    
                    # Results will be missing if the two libraries share no
                    # features with positive counts; in this case, set to unity
                    if norm_factor != norm_factor:  # i.e. NaN
                        norm_factor = 1
                    
                    norm_factors[i] = norm_factor
                
                # Normalize factors across samples so that they multiply to 1
                scale = 0
                for i in range(num_samples):
                    scale += log(norm_factors[i])
                scale = exp(-scale / num_samples)
                for i in range(num_samples):
                    norm_factors[i] *= scale
                
                # Multiply norm factors by library sizes
                for i in range(num_samples):
                    norm_factors[i] *= library_size[i]
                
            ''')['calc_norm_factors'](
                X=X, logratio_trim=logratio_trim, sum_trim=sum_trim, 
                A_cutoff=A_cutoff, ref_sample=ref_sample, 
                norm_factors=norm_factors, library_size=library_size,
                inv_relative_library_size=inv_relative_library_size,
                log_normalized_X_ref=log_normalized_X_ref,
                logR=logR, absE=absE, counts=counts, ref_counts=ref_counts,
                indices=indices, logR_rank=logR_rank, absE_rank=absE_rank)
        
        return norm_factors  # this is actually `library_size * norm_factors`
    
    def CPM(self) -> Pseudobulk:
        """
        Calculate counts per million for each cell type.

        Returns:
            A new Pseudobulk dataset containing the CPMs.
        """
        CPMs = {}
        for cell_type, X in self._X.items():
            library_size = self._library_size(X, cell_type)
            CPMs[cell_type] = X / library_size[:, None] * 1e6
        return Pseudobulk(X=CPMs, obs=self._obs, var=self._var)
    
    def log_CPM(self,
                *,
                prior_count: int | float | np.integer |
                             np.floating = 2) -> Pseudobulk:
        """
        Calculate log counts per million for each cell type.
        
        Do NOT run this before DE(), since DE() already runs it internally.
        
        Based on the R translation of edgeR's C++ cpm() code at
        bioinformatics.stackexchange.com/a/4990.
        
        Results were verified to match edgeR to within floating-point error.
        
        Args:
            prior_count: the pseudocount to add before log-transforming. In the
                         current version of edgeR, prior.count is now 2 instead
                         of the old value of 0.5: code.bioconductor.org/browse/
                         edgeR/blob/RELEASE_3_18/R/cpm.R
        
        Returns:
            A new Pseudobulk dataset containing the log(CPMs).
        """
        check_type(prior_count, 'prior_count', (int, float),
                   'a positive number')
        check_bounds(prior_count, 'prior_count', 0, left_open=True)
        log_CPMs = {}
        for cell_type, X in self._X.items():
            library_size = self._library_size(X, cell_type)
            pseudocount = prior_count * library_size / library_size.mean()
            library_size += 2 * pseudocount
            log_CPMs[cell_type] = np.log2(X + pseudocount[:, None]) - \
                np.log2(library_size[:, None]) + np.log2(1e6)
        return Pseudobulk(X=log_CPMs, obs=self._obs, var=self._var)
    
    @staticmethod
    def _get_unique_variables(formulas: str | Iterable[str],
                              composite: bool = False) -> list[str]:
        """
        Get a list of the unique variables referenced in one or more R
        formulas. Do not include R functions (e.g. `exp(x1)` adds `'x1'` to the
        list, but not `exp`) or numbers.
        
        Args:
            formulas: one or more R formulas, represented as Python strings
            composite: if `True`, avoid splitting "composite" variables like
                       `x1:x2`, so that the unique variables are columns of the
                       design matrix. If `False`, split these into their
                       components, so that the unique variables are columns of
                       obs.

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        if isinstance(formulas, str):
            formulas = formulas,
        seen = set()
        unique_variables = [
            token for formula in formulas for token, next_token in pairwise(
                re.findall(rf'[+\-*/^{":" if composite else ""}()]|[\w.]+',
                           formula) + [''])
            if token not in seen and not seen.add(token) and
               not token.isdigit() and re.fullmatch(r'[\w.]*', token) and
               next_token != '(']
        return unique_variables
    
    @staticmethod
    def _process_formula_variables(formula: str,
                                   cell_type: str,
                                   obs: pl.DataFrame) -> list[str]:
        """
        Check that `formula` starts with a tilde (`~`), and that all variables
        it references are Categorical, Enum, Boolean, integer, or
        floating-point columns of obs. Make a set of these variables. Used by
        `DE()` and `regress_out()`.
        
        Args:
            formula: the formula to process
            cell_type: the cell type the formula is for, used in error messages
            obs: the obs for this cell type

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        formula = formula.replace(' ', '')
        if formula[0] != '~':
            error_message = 'formula must start with a tilde (~)'
            raise ValueError(error_message)
        unique_formula_variables = \
            Pseudobulk._get_unique_variables(formula)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        # noinspection PyUnboundLocalVariable
        for variable in unique_formula_variables:
            if variable not in obs:
                error_message = (
                    f'formula contains the variable {variable!r}, which is '
                    f'not the name of a column of obs[{cell_type!r}]')
                raise ValueError(error_message)
            base_type = obs[variable].dtype.base_type()
            if base_type not in valid_dtypes:
                error_message = (
                    f'all columns of obs referenced in formula must be '
                    f'Categorical, Enum, Boolean, integer, or floating-point, '
                    f'but it contains the variable {variable!r} and '
                    f'obs[{cell_type!r}][{variable!r}] has data type '
                    f'{base_type!r}')
                raise TypeError(error_message)
        return unique_formula_variables
    
    @staticmethod
    def _create_design_matrix(formula: str,
                              cell_type: str,
                              obs: pl.DataFrame,
                              obs_columns: list[str],
                              categorical_columns: str | Iterable[str] | None,
                              prefix: str) -> None:
        """
        Create a design matrix from a formula. Used by `DE()` and
        `regress_out()`.
        
        Adds variables called `{prefix}.formula`, `{prefix}.obs`, and
        `{prefix}.design.matrix` to the ryp R workspace, which need to be
        deleted by the calling function.
        
        Args:
            formula: the formula to construct the design matrix from
            cell_type: the cell type the formula is for
            obs: the obs for this cell type
            obs_columns: the columns of obs that need to be converted to R
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal
            prefix: a prefix to use for the three variables to be added to the
                    ryp R workspace
        """
        from ryp import r, to_py, to_r, \
            _bytestring_to_character_vector, _rlib, _RMemory
        # Convert the formula to R
        to_r(formula, f'{prefix}.formula')
        r(f'{prefix}.formula = as.formula({prefix}.formula)')
        # Subset obs to just the columns we need, convert the integer columns
        # in `categorical_columns` to Enums (after checking that all elements
        # of `categorical_columns` are integer or Enum columns of obs), then
        # convert the selected columns of obs to R
        obs_names = obs[:, 0]
        obs = obs.select(*obs_columns)
        if categorical_columns is not None:
            for column in categorical_columns:
                if column not in obs:
                    error_message = (
                        f'one of the columns in categorical_columns, '
                        f'{column!r}, is not a column of obs[{cell_type!r}]')
                    raise ValueError(error_message)
                base_type = obs[column].dtype.base_type()
                if base_type != pl.Enum and base_type not in pl.INTEGER_DTYPES:
                    error_message = (
                        f'all columns in categorical_columns must be integer '
                        f'or Enum, but one of the columns is {column!r} and '
                        f'obs[{cell_type!r}][{column!r}] has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
            obs = obs\
                .cast({row[0]: pl.Enum(row[1]) for row in
                       obs.select(
                           (pl.selectors.integer() &
                            pl.selectors.by_name(categorical_columns))
                           .unique(maintain_order=True)
                           .implode()
                           .list.drop_nulls())
                      .unpivot()
                      .cast({'value': pl.List(pl.String)})
                      .rows()})
        obs_name = f'{prefix}.obs'
        to_r(obs, obs_name, rownames=obs_names)
        # If specified, convert the columns listed in `categorical_columns`
        # from ordered to unordered factors by removing the `'ordered'`
        # class and leaving only the `'factor'` class. This has to be done
        # through the C API to be in-place.
        if categorical_columns is not None:
            R_obs = _rlib.Rf_findVar(_rlib.Rf_install(obs_name.encode()),
                                     _rlib.R_GlobalEnv)
            with _RMemory(_rlib) as rmemory:
                new_class = \
                    _bytestring_to_character_vector(b'factor', rmemory)
                for column in categorical_columns:
                    column_index = obs.columns.index(column)
                    R_column = _rlib.VECTOR_ELT(R_obs, column_index)
                    _rlib.Rf_setAttrib(R_column, _rlib.R_ClassSymbol,
                                       new_class)
        # Create the design matrix
        r(f'{prefix}.design.matrix = '
          f'model.matrix({prefix}.formula, {prefix}.obs)')
        # Check that the design matrix has more rows than columns
        height = to_py(f'nrow({prefix}.design.matrix)')
        width = to_py(f'ncol({prefix}.design.matrix)')
        if width >= height:
            error_message = (
                f'the design matrix must have more rows (samples) than '
                f'columns (one plus the number of covariates), but has '
                f'{height:,} rows and {width:,} columns for cell type '
                f'{cell_type!r}. Either reduce the number of covariates, or '
                f'exclude this cell type with e.g. '
                f'excluded_cell_types={cell_type!r}.')
            raise ValueError(error_message)
    
    @staticmethod
    def _process_num_threads(num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        Pseudobulk function.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use. If `num_threads` is a positive
            integer, raise an error if the user lacks free-threaded Python,
            otherwise return it unchanged. If `num_threads` is `None`, return
            `single_cell.get_num_threads()` if free-threaded and 1 otherwise.
            If `num_threads` is -1, return `os.cpu_count()`. Otherwise, raise
            an error.
        """
        try:
            # noinspection PyUnresolvedReferences
            has_GIL = sys._is_gil_enabled()
        except AttributeError:
            has_GIL = False
        if num_threads is None and not has_GIL:
            num_threads = 1
        else:
            num_threads = SingleCell._process_num_threads(num_threads)
            # If `num_threads` is greater than 1, check that the user is using
            # free-threaded Python 3.13+
            if num_threads > 1 and not has_GIL:
                error_message = (
                    f'num_threads is {num_threads}, but multithreading in '
                    f'Pseudobulk methods is only supported for '
                    f'"free-threaded" builds of Python 3.13 and later with '
                    f'the global interpreter lock (GIL) disabled')
                raise ValueError(error_message)
        return num_threads
    
    @staticmethod
    def _regress_out(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                     obs: pl.DataFrame,
                     cell_type: str,
                     cell_type_index: int,
                     formula: str,
                     categorical_columns: str | Iterable[str] | None,
                     error_if_int: bool,
                     verbose: bool) -> np.ndarray[2, np.dtype[np.integer |
                                                              np.floating]]:
        """
        Regress out covariates from obs for a single cell type. Used by
        `regress_out()`.
        
        Args:
            X: the X for this cell type
            obs: the obs for this cell type
            cell_type: the cell type covariates will be regressed out for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of obs,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process

        Returns:
            `X` with covariates regressed out.
        """
        from ryp import r, to_py
        # If `error_if_int=True`, raise an error if X has an integer dtype
        if error_if_int and np.issubdtype(X.dtype, np.integer):
            error_message = (
                f'X[{cell_type!r}].dtype is {str(X.dtype)!r}, an integer '
                f'data type; did you forget to run log_CPM() before '
                f'regress_out()?')
            raise ValueError(error_message)
        # Check that `formula` starts with a tilde (`~`), and that all
        # variables it references are Categorical, Enum, Boolean, integer,
        # or floating-point columns of obs. Make a set of these variables.
        if verbose:
            print(f'[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        # Create the design matrix
        try:
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_columns,
                                             categorical_columns, prefix)
            design_matrix = to_py('design_matrix', format='numpy')
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix")))')
        # Regress out the design matrix; silence warnings with `rcond=None`
        if verbose:
            print(f'[{cell_type}] Regressing out...')
        beta, _, rank, _ = np.linalg.lstsq(design_matrix, X, rcond=None)
        # Check that the design matrix is full-rank
        if rank < design_matrix.width:
            error_message = (
                f'the design matrix is not full-rank for cell type '
                f'{cell_type!r} (rank {rank} with {design_matrix.width} '
                f'columns); some of your covariates are linear '
                f'combinations of other covariates')
            raise ValueError(error_message)
        # Calculate the residuals
        residuals = X - design_matrix @ beta
        return residuals
    
    def regress_out(self,
                    formula: str | dict[str, str],
                    *,
                    categorical_columns: str | Iterable[str] | None |
                                         dict[str, str | Iterable[str] |
                                                   None] = None,
                    cell_types: str | Iterable[str] | None = None,
                    excluded_cell_types: str | Iterable[str] | None = None,
                    error_if_int: bool = True,
                    verbose: bool = True,
                    num_threads: int | np.integer | None = None) -> Pseudobulk:
        """
        Regress out covariates from obs. Must be run after log_CPM().
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of obs referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of obs represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```r
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of obs,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to regress the covariates out
                        of; if `None`, regress covariates out of all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when
                                 regressing out covariates; mutually exclusive
                                 with `cell_types`
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process
            num_threads: the number of cores to use when regressing out.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores. Parallelization
                         takes place across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.

        Returns:
            A new Pseudobulk dataset with covariates regressed out.
        """
        # Get the list of cell types to regress out covariates for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings; the validity of the formula is checked later
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        # Check that `error_if_int` is Boolean
        check_type(error_if_int, 'error_if_int', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()` if free-threaded and 1
        # otherwise, and if -1, set to `os.cpu_count()`. Raise an error if the
        # user specified multiple threads but lacks free-threaded Python.
        num_threads = Pseudobulk._process_num_threads(num_threads)
        # Compute residuals for each cell type
        if num_threads == 1:
            # noinspection PyUnboundLocalVariable
            residuals = {
                cell_type: Pseudobulk._regress_out(
                    X=X, obs=obs, cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type] if formula_is_dict else
                            formula,
                    categorical_columns=categorical_columns,
                    error_if_int=error_if_int, verbose=verbose)
                    for cell_type_index, (cell_type, (X, obs, var))
                    in enumerate(self.items())}
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            args_list = [{
                'X': X,
                'obs': obs,
                'cell_type': 'cell_type',
                'cell_type_index': 'cell_type_index',
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'error_if_int': error_if_int,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                residuals = dict(zip(cell_types, executor.map(
                    lambda kwargs: self._DE(**kwargs), args_list)))
        # Return a new Pseudobulk datasets with the residuals
        return Pseudobulk(X=residuals, obs=self._obs, var=self._var)
    
    # A slightly reformatted version of the voomByGroup source code from
    # github.com/YOU-k/voomByGroup/blob/main/voomByGroup.R, which is available
    # under the MIT license. Copyright (c) 2023 Yue You. Also added 
    # `drop=FALSE` to the `countsi` subsetting to avoid an error with length-1
    # groups.
    _voomByGroup_source_code = r'''
    voomByGroup <- function (counts, group = NULL, design = NULL,
                             lib.size = NULL, dynamic = NULL,
                             normalize.method = "none", span = 0.5,
                             save.plot = FALSE, print = TRUE, plot = c("none",
                             "all", "separate", "combine"),
                             col.lines = NULL, pos.legend = c("inside",
                             "outside", "none"), fix.y.axis = FALSE, ...) {
      out <- list()
      if (is(counts, "DGEList")) {
        out$genes <- counts$genes
        out$targets <- counts$samples
        if(is.null(group))
          group <- counts$samples$group
        if (is.null(lib.size))
          lib.size <- with(counts$samples, lib.size * norm.factors)
        counts <- counts$counts
      }
      else {
        isExpressionSet <-
          suppressPackageStartupMessages(is(counts, "ExpressionSet"))
        if (isExpressionSet) {
          if (length(Biobase::fData(counts)))
            out$genes <- Biobase::fData(counts)
          if (length(Biobase::pData(counts)))
            out$targets <- Biobase::pData(counts)
          counts <- Biobase::exprs(counts)
        }
        else {
          counts <- as.matrix(counts)
        }
      }
      if (nrow(counts) < 2L)
        stop("Need at least two genes to fit a mean-variance trend")
      # Library size
      if(is.null(lib.size))
        lib.size <- colSums(counts)
      # Group
      if(is.null(group))
        group <- rep("Group1", ncol(counts))
      group <- as.factor(group)
      intgroup <- as.integer(group)
      levgroup <- levels(group)
      ngroups <- length(levgroup)
      # Design matrix
      if (is.null(design)) {
        design <- matrix(1L, ncol(counts), 1)
        rownames(design) <- colnames(counts)
        colnames(design) <- "GrandMean"
      }
      # Dynamic
      if (is.null(dynamic)) {
        dynamic <- rep(FALSE, ngroups)
      }
      # voom by group
      if(print)
        cat("Group:\n")
      E <- w <- counts
      xy <- line <- as.list(rep(NA, ngroups))
      names(xy) <- names(line) <- levgroup
      for (lev in 1L:ngroups) {
        if(print)
          cat(lev, levgroup[lev], "\n")
        i <- intgroup == lev
        countsi <- counts[, i, drop = FALSE]
        libsizei <- lib.size[i]
        designi <- design[i, , drop = FALSE]
        QR <- qr(designi)
        if(QR$rank<ncol(designi))
          designi <- designi[,QR$pivot[1L:QR$rank], drop = FALSE]
        if(ncol(designi)==ncol(countsi))
          designi <- matrix(1L, ncol(countsi), 1)
        voomi <- voom(counts = countsi, design = designi, lib.size = libsizei,
                      normalize.method = normalize.method, span = span,
                      plot = FALSE, save.plot = TRUE, ...)
        E[, i] <- voomi$E
        w[, i] <- voomi$weights
        xy[[lev]] <- voomi$voom.xy
        line[[lev]] <- voomi$voom.line
      }
      #voom overall
      if (TRUE %in% dynamic){
        voom_all <- voom(counts = counts, design = design, lib.size = lib.size,
                         normalize.method = normalize.method, span = span,
                         plot = FALSE, save.plot = TRUE, ...)
        E_all <- voom_all$E
        w_all <- voom_all$weights
        xy_all <- voom_all$voom.xy
        line_all <- voom_all$voom.line
        dge <- DGEList(counts)
        disp <- estimateCommonDisp(dge)
        disp_all <- disp$common
      }
      # Plot, can be "both", "none", "separate", or "combine"
      plot <- plot[1]
      if(plot!="none"){
        disp.group <- c()
        for (lev in levgroup) {
          dge.sub <- DGEList(counts[,group == lev])
          disp <- estimateCommonDisp(dge.sub)
          disp.group[lev] <- disp$common
        }
        if(plot %in% c("all", "separate")){
          if (fix.y.axis == TRUE) {
            yrange <- sapply(levgroup, function(lev){
              c(min(xy[[lev]]$y), max(xy[[lev]]$y))
            }, simplify = TRUE)
            yrange <- c(min(yrange[1,]) - 0.1, max(yrange[2,]) + 0.1)
          }
          for (lev in 1L:ngroups) {
            if (fix.y.axis == TRUE){
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25,
                   ylim = yrange)
            } else {
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25)
            }
            title(paste("voom: Mean-variance trend,", levgroup[lev]))
            lines(line[[lev]], col = "red")
            legend("topleft", bty="n", paste("BCV:",
              round(sqrt(disp.group[lev]), 3)), text.col="red")
          }
        }
        
        if(plot %in% c("all", "combine")){
          if(is.null(col.lines))
            col.lines <- 1L:ngroups
          if(length(col.lines)<ngroups)
            col.lines <- rep(col.lines, ngroups)
          xrange <- unlist(lapply(line, `[[`, "x"))
          xrange <- c(min(xrange)-0.3, max(xrange)+0.3)
          yrange <- unlist(lapply(line, `[[`, "y"))
          yrange <- c(min(yrange)-0.1, max(yrange)+0.3)
          plot(1L,1L, type="n", ylim=yrange, xlim=xrange,
               xlab = "log2( count size + 0.5 )",
               ylab = "Sqrt( standard deviation )")
          title("voom: Mean-variance trend")
          if (TRUE %in% dynamic){
            for (dy in which(dynamic)){
              line[[dy]] <- line_all
              disp.group[dy] <- disp_all
              levgroup[dy] <- paste0(levgroup[dy]," (all)")
            }
          }
          for (lev in 1L:ngroups)
            lines(line[[lev]], col=col.lines[lev], lwd=2)
          pos.legend <- pos.legend[1]
          disp.order <- order(disp.group, decreasing = TRUE)
          text.legend <-
            paste(levgroup, ", BCV: ", round(sqrt(disp.group), 3), sep="")
          if(pos.legend %in% c("inside", "outside")){
            if(pos.legend=="outside"){
              plot(1,1, type="n", yaxt="n", xaxt="n", ylab="", xlab="",
                   frame.plot=FALSE)
              legend("topleft", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            } else {
              legend("topright", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            }
          }
        }
      }
      # Output
      if (TRUE %in% dynamic){
        E[,intgroup %in% which(dynamic)] <-
          E_all[,intgroup %in% which(dynamic)]
        w[,intgroup %in% which(dynamic)] <-
          w_all[,intgroup %in% which(dynamic)]
      }
      out$E <- E
      out$weights <- w
      out$design <- design
      if(save.plot){
        out$voom.line <- line
        out$voom.xy <- xy
      }
      new("EList", out)
    }
    '''
    
    def DE_old(self,
           label_column: PseudobulkColumn | Sequence[PseudobulkColumn],
           covariate_columns: Sequence[PseudobulkColumn | None |
                                       Sequence[PseudobulkColumn | None]] |
                              None,
           *,
           formula: str | Iterable[str] | None = None,
           case_control: bool = True,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           library_size_as_covariate: bool = True,
           num_cells_as_covariate: bool = True,
           robust: bool = False,
           return_voom_info: bool = True,
           allow_float: bool = False,
           verbose: bool = True) -> DE:
        """
        Perform differential expression (DE) on a Pseudobulk dataset with
        limma-voom. Uses voomByGroup when case_control=True, which is better
        than regular voom for case-control DE.

        Loosely based on the `de_pseudobulk()` function from
        github.com/tluquez/utils/blob/main/utils.R, which is itself based on
        github.com/neurorestore/Libra/blob/main/R/pseudobulk_de.R.

        Args:
            label_column: the column of obs to calculate DE with respect to.
                          Can be a column name, a polars expression, a polars
                          Series, a 1D NumPy array, or a function that takes in
                          this Pseudobulk dataset and a cell type and returns a
                          polars Series or 1D NumPy array. Can also be a
                          sequence of any combination of these for each cell
                          type. If `case_control=True`, must be Boolean,
                          integer, floating-point, or Enum with
                          cases = `1`/`True` and controls = `0`/`False`. If
                          `case_control=False`, must be integer or
                          floating-point.
            covariate_columns: an optional sequence of columns of obs to use
                               as covariates, or `None` to not include
                               covariates. Each element of the sequence can be
                               a column name, a polars expression, a polars
                               Series, a 1D NumPy array, or a function that
                               takes in this Pseudobulk dataset and a cell type
                               and returns a polars Series or 1D NumPy array.
                               Each element of the sequence may also itself be
                               a sequence of any combination of these for each
                               cell type, or `None` to not include that
                               covariate for that cell type. Mutually exclusive
                               with `formula`.
            formula: a string representation of an R formula indicating the DE
                     design, which will be converted into an R formula object
                     with R's `as.formula()` function and then expanded into a
                     design matrix with R's `model.matrix()` function. Must
                     begin with a tilde (`~`). May also be a sequence of
                     strings, one per cell type. Mutually exclusive with
                     `covariate_columns`.
            case_control: whether the analysis is case-control or with respect
                          to a quantitative variable.
                          If `True`, uses voomByGroup instead of regular voom,
                          and uses `label_column` as the `group` argument to
                          calcNormFactors().
            cell_types: one or more cell types to test for differential
                        expression; if `None`, test all cell types. Mutually
                        exclusive with `excluded_cell_types`.
            excluded_cell_types: cell types to exclude when testing for
                                 differential expression; mutually exclusive
                                 with `cell_types`
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as anadditional covariate
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of obs, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         log_CPM() already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation

        Returns:
            A DE object with a `table` attribute containing a polars DataFrame
            of the DE results. If `return_voom_info=True`, also includes a
            `voom_weights` attribute containing a {cell_type: DataFrame}
            dictionary of voom weights, and a `voom_plot_data` attribute
            containing a {cell_type: DataFrame} dictionary of info necessary to
            construct a voom plot with `DE.plot_voom()`.
        """
        # Import required Python and R packages
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(edgeR))')
        # Source voomByGroup code
        if case_control:
            r(self._voomByGroup_source_code)
        # Check inputs
        original_label_column = label_column
        label_column = self._get_column(
            'obs', label_column, 'label_column',
            (pl.Boolean, 'integer', 'floating-point', pl.Enum)
            if case_control else ('integer', 'floating-point'),
            allow_None=False)
        if covariate_columns is not None and formula is not None:
            error_message = \
                'covariate_columns and formula cannot both be specified'
            raise ValueError(error_message)
        if covariate_columns is not None:
            covariate_columns = [
                self._get_column('obs', column, f'covariate_columns[{index}]',
                                 ('integer', 'floating-point', pl.Categorical,
                                  pl.Enum))
                for index, column in enumerate(covariate_columns)]
        check_type(case_control, 'case_control', bool, 'Boolean')
        if cell_types is not None:
            if excluded_cell_types is not None:
                error_message = (
                    'cell_types and excluded_cell_types cannot both be '
                    'specified')
                raise ValueError(error_message)
            is_string = isinstance(cell_types, str)
            cell_types = \
                to_tuple_checked(cell_types, 'cell_types', str, 'strings')
            for cell_type in cell_types:
                if cell_type not in self._X:
                    if is_string:
                        error_message = (
                            f'cell_types is {cell_type!r}, which is not a '
                            f'cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
        elif excluded_cell_types is not None:
            excluded_cell_types = to_tuple_checked(
                excluded_cell_types, 'cell_types', str, 'strings')
            for cell_type in excluded_cell_types:
                if cell_type not in self._X:
                    if excluded_cell_types:
                        error_message = (
                            f'excluded_cell_types is {cell_type!r}, which is '
                            f'not a cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of excluded_cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            cell_types = [cell_type for cell_type in self._X
                          if cell_type not in excluded_cell_types]
            if len(cell_types) == 0:
                error_message = \
                    'all cell types were excluded by excluded_cell_types'
                raise ValueError(error_message)
        else:
            cell_types = self._X
        if formula is not None:
            formulas = to_tuple_checked(formula, 'formula', str, 'a string')
            if len(formulas) == 1:
                formulas *= len(cell_types)
            else:
                if len(formulas) != len(cell_types):
                    error_message = (
                        f'formula has length {len(formulas):,}, which differs '
                        f'from the number of cell types ({len(cell_types)}); '
                        f'it can also be a single string')
                    raise ValueError(error_message)
        else:
            formulas = (None,) * len(cell_types)
        check_type(library_size_as_covariate, 'library_size_as_covariate',
                   bool, 'Boolean')
        check_type(num_cells_as_covariate, 'num_cells_as_covariate', bool,
                   'Boolean')
        if num_cells_as_covariate:
            for cell_type, obs in self._obs.items():
                if 'num_cells' not in obs:
                    error_message = (
                        f"num_cells_as_covariate is True, but 'num_cells' is "
                        f"not a column of obs[{cell_type!r}]")
                    raise ValueError(error_message)
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Compute DE for each cell type
        DE_results = {}
        if return_voom_info:
            voom_weights = {}
            voom_plot_data = {}
        for cell_type_index, cell_type in enumerate(cell_types):
            X = self._X[cell_type]
            obs = self._obs[cell_type]
            var = self._var[cell_type]
            formula = formulas[cell_type_index]
            # If `allow_float=False`, raise an error if `X` is floating-point
            if not allow_float and np.issubdtype(X.dtype, np.floating):
                error_message = (
                    f"DE() requires raw counts but X[{cell_type!r}].dtype is "
                    f"{str(X.dtype)!r}, a floating-point data type. If you "
                    f"are sure that all values are integers, i.e. that "
                    f"X[{cell_type!r}].data == X[{cell_type!r}].data"
                    f".astype(int)).all(), then set allow_float=True (or just "
                    f"cast X to an integer data type). Alternatively, did you "
                    f"accidentally run log_CPM() before DE()?")
                raise TypeError(error_message)
            # Get the DE labels for this cell type
            DE_labels = label_column[cell_type]
            if case_control:
                # If `case_control=True`, check that the DE labels have only
                # two unique values
                if DE_labels.dtype != pl.Boolean:
                    if DE_labels.dtype == pl.Enum:
                        categories = DE_labels.cat.get_categories()
                        if len(categories) != 2:
                            label_column_description = self._describe_column(
                                'label_column', original_label_column,
                                cell_type)
                            error_message = (
                                f'{label_column_description} is an Enum '
                                f'column with {len(categories):,} categor'
                                f'{"y" if len(categories) == 1 else "ies"} '
                                f'for cell type {cell_type!r}, but must have '
                                f'2 (cases and controls)')
                            raise ValueError(error_message)
                        DE_labels = DE_labels.to_physical()
                    else:
                        unique_labels = DE_labels.unique()
                        num_unique_labels = len(unique_labels)
                        if num_unique_labels != 2:
                            label_column_description = self._describe_column(
                                'label_column', original_label_column,
                                cell_type)
                            error_message = (
                                f'{label_column_description} is a numeric '
                                f'column with {num_unique_labels:,} '
                                f'{plural("unique value", num_unique_labels)} '
                                f'for cell type {cell_type!r}, but must have '
                                f'2 (cases = 1, controls = 0) unless '
                                f'case_control=False')
                            raise ValueError(error_message)
                        if not unique_labels.sort()\
                                .equals(pl.Series([0, 1])):
                            label_column_description = self._describe_column(
                                'label_column', original_label_column,
                                cell_type)
                            error_message = (
                                f'{label_column_description} is a numeric '
                                f'column with 2 unique values, '
                                f'{unique_labels[0]} and {unique_labels[1]}, '
                                f'for cell type {cell_type!r}, but must have '
                                f'cases = 1 and controls = 0')
                            raise ValueError(error_message)
            # Get the design matrix
            if verbose:
                print(f'[{cell_type}] Generating design matrix...')
            if formula is not None:
                # Check that formula starts with a tilde (`~`), and that all
                # variables referenced in `formula` are columns of obs
                # noinspection PyUnresolvedReferences
                formula = formula.replace(' ', '')
                if formula[0] != '~':
                    error_message = 'formula must start with a tilde (~)'
                    raise ValueError(error_message)
                tokens = re.findall(r'[+\-*/^:()]|\w+', formula[1:])
                seen = set()
                unique_variables = [
                    token for token, next_token in pairwise(tokens + [''])
                    if token not in seen and not seen.add(token) and
                    re.fullmatch(r'[\w.]*', token) and not token.isdigit() and
                    next_token != '(']
                for variable in unique_variables:
                    if variable not in obs:
                        error_message = (
                            f'formula contains the variable {variable!r}, '
                            f'which is not the name of a column of '
                            f'obs[{cell_type!r}]')
                        raise ValueError(error_message)
                to_r(obs.select(unique_variables), '.Pseudobulk.design.matrix',
                     rownames=obs[:, 0])
                to_r(formula, '.Pseudobulk.formula')
                r('.Pseudobulk.design.matrix = model.matrix('
                  'as.formula(.Pseudobulk.formula), '
                  '.Pseudobulk.design.matrix)')
                design_matrix = to_py('.Pseudobulk.design.matrix')
            else:
                design_matrix = \
                    obs.select(pl.lit(1).alias('intercept'), DE_labels)
                if covariate_columns is not None:
                    covariates = \
                        pl.DataFrame([column[cell_type]
                                      for column in covariate_columns])
                    if covariates.width:
                        design_matrix = pl.concat([
                            design_matrix,
                            covariates.to_dummies(covariates.select(
                                pl.col(pl.Categorical, pl.Enum)).columns,
                                drop_first=True)], how='horizontal')
            # Estimate library sizes
            if verbose:
                print(f'[{cell_type}] Estimating library sizes...')
            library_size = self._library_size(X, cell_type)
            # Add library size and/or `num_cells` as covariates, if specified
            if library_size_as_covariate:
                design_matrix = design_matrix\
                    .with_columns(library_size=np.log2(library_size))
            if num_cells_as_covariate:
                design_matrix = design_matrix\
                    .with_columns(obs['num_cells'].log(2))
            # Check that the design matrix has more rows than columns, and
            # is full-rank
            if verbose:
                print(f'[{cell_type}] Sanity-checking the design matrix')
            if design_matrix.width >= design_matrix.height:
                error_message = (
                    f'the design matrix must have more rows (samples) than '
                    f'columns (one plus the number of covariates) for cell '
                    f'type {cell_type!r}, but has {design_matrix.height:,} '
                    f'rows and {design_matrix.width:,} columns; either reduce '
                    f'the number of covariates or exclude this cell type with '
                    f'e.g. excluded_cell_types={cell_type!r}')
                raise ValueError(error_message)
            if np.linalg.matrix_rank(design_matrix.to_numpy()) < \
                    design_matrix.width:
                error_message = (
                    f'the design matrix is not full-rank for cell type '
                    f'{cell_type!r}; some of your covariates are linear '
                    f'combinations of other covariates')
                raise ValueError(error_message)
            try:
                # Convert the expression matrix, design matrix, and library
                # sizes to R
                if verbose:
                    print(f'[{cell_type}] Converting the expression matrix, '
                          f'design matrix and library sizes to R...')
                to_r(X.T, '.Pseudobulk.X.T', rownames=var[:, 0],
                     colnames=obs[:, 0])
                to_r(design_matrix, '.Pseudobulk.design.matrix',
                     rownames=obs[:, 0])
                to_r(library_size, '.Pseudobulk.library.size',
                     rownames=obs[:, 0])
                # Run voom
                to_r(return_voom_info, 'save.plot')
                if case_control:
                    if verbose:
                        print(f'[{cell_type}] Running voomByGroup...')
                    to_r(DE_labels, '.Pseudobulk.DE.labels',
                         rownames=obs[:, 0])
                    r('.Pseudobulk.voom.result = voomByGroup('
                      '.Pseudobulk.X.T, .Pseudobulk.DE.labels, '
                      '.Pseudobulk.design.matrix, .Pseudobulk.library.size, '
                      'save.plot=save.plot, print=FALSE)')
                else:
                    if verbose:
                        print(f'[{cell_type}] Running voom...')
                    r('.Pseudobulk.voom.result = voom(.Pseudobulk.X.T, '
                      '.Pseudobulk.design.matrix, .Pseudobulk.library.size, '
                      'save.plot=save.plot)')
                if return_voom_info:
                    # noinspection PyUnboundLocalVariable
                    voom_weights[cell_type] = \
                        to_py('.Pseudobulk.voom.result$weights', index='gene')
                    if case_control:
                        frames = [
                            pl.DataFrame({
                                'gene': to_py(
                                    f'names(.Pseudobulk.voom.result$voom.xy$'
                                    f'`{case_label}`$x)')} | {
                                f'{prop}_{dim}_{case}': to_py(
                                    f'.Pseudobulk.voom.result$voom.{prop}$'
                                    f'`{case_label}`${dim}', index=False)
                                for prop in ('xy', 'line')
                                for dim in ('x', 'y')})
                            for case, case_label in zip(
                                (False, True), ('FALSE', 'TRUE')
                                if DE_labels.dtype == pl.Boolean else (0, 1))]
                        # noinspection PyUnboundLocalVariable
                        voom_plot_data[cell_type] = frames[0]\
                            .join(frames[1], on='gene', how='outer',
                                  coalesce=True)
                    else:
                        voom_plot_data[cell_type] = pl.DataFrame({
                            'gene': to_py(
                                'rownames(.Pseudobulk.voom.result)')} | {
                            f'{prop}_{dim}': to_py(
                                f'.Pseudobulk.voom.result$voom.{prop}${dim}',
                                index=False)
                            for prop in ('xy', 'line') for dim in ('x', 'y')})
                # Run limma
                if verbose:
                    print(f'[{cell_type}] Running lmFit...')
                r('.Pseudobulk.lmFit.result = lmFit('
                  '.Pseudobulk.voom.result, .Pseudobulk.design.matrix)')
                if verbose:
                    print(f'[{cell_type}] Running eBayes...')
                to_r(robust, '.Pseudobulk.robust')
                r('.Pseudobulk.eBayes.result = eBayes('
                  '.Pseudobulk.lmFit.result, trend=FALSE, '
                  'robust=.Pseudobulk.robust)')
                # Get results table
                if verbose:
                    print(f'[{cell_type}] Running topTable...')
                to_r(DE_labels.name, '.Pseudobulk.coef')
                r('.Pseudobulk.topTable.result = topTable('
                  '.Pseudobulk.eBayes.result, coef=.Pseudobulk.coef, '
                  'number=Inf, adjust.method="none", sort.by="P", '
                  'confint=TRUE)')
                if verbose:
                    print(f'[{cell_type}] Collating results...')
                DE_results[cell_type] = \
                    to_py('.Pseudobulk.topTable.result', index='gene')\
                    .select('gene',
                            logFC=pl.col.logFC,
                            SE=to_py('.Pseudobulk.eBayes.result$s2.post')
                               .sqrt() *
                               to_py('.Pseudobulk.eBayes.result$stdev.'
                                     'unscaled[,.Pseudobulk.coef]',
                                     index=False),
                            LCI=pl.col('CI.L'),
                            UCI=pl.col('CI.R'),
                            AveExpr=pl.col.AveExpr,
                            P=pl.col('P.Value'),
                            Bonferroni=bonferroni(pl.col('P.Value')),
                            FDR=fdr(pl.col('P.Value')))
            finally:
                r('rm(list = Filter(exists, c(".Pseudobulk.X.T", '
                  '".Pseudobulk.DE.labels", ".Pseudobulk.design.matrix", '
                  '".Pseudobulk.formula", ".Pseudobulk.library.size", '
                  '".Pseudobulk.voom.result", ".Pseudobulk.lmFit.result", '
                  '".Pseudobulk.robust", ".Pseudobulk.eBayes.result", '
                  '".Pseudobulk.coef", ".Pseudobulk.topTable.result")))')
        # Concatenate across cell types
        table = pl.concat([
            cell_type_DE_results
            .select(pl.lit(cell_type).alias('cell_type'), pl.all())
            for cell_type, cell_type_DE_results in DE_results.items()])
        if return_voom_info:
            return DE(table, voom_weights, voom_plot_data)
        else:
            return DE(table)
    
    def _DE(self,
            cell_type: str,
            cell_type_index: int,
            formula: str,
            coefficient: str | int | np.integer |
                         Iterable[str | int | np.integer],
            contrasts: dict[str, str] | None,
            group: Literal[False] | PseudobulkColumn | None,
            categorical_columns: str | Iterable[str] | None,
            library_size_as_covariate: bool,
            num_cells_as_covariate: bool,
            robust: bool,
            return_voom_info: bool,
            allow_float: bool,
            verbose: bool) -> pl.DataFrame | \
                              tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """
        Compute differential expression for a single cell type. Used by `DE()`.
        
        Args:
            cell_type: the cell type DE will be calculated for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of obs, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Negative indices work in the
                         usual Python way. Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. If
                       specified, DE will be performed with respect to each
                       contrast by running limma's `makeContrasts()` and
                       `contrasts.fit()` functions after `lmFit()`. Mutually
                       exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of obs
                   referenced in `coefficient`, or the columns of obs
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   column (the name of a String, Categorical, Enum, Boolean, or
                   integer column of obs, a polars expression, a polars Series,
                   a 1D NumPy array, or a function that takes in this
                   Pseudobulk dataset and a cell type and returns a polars
                   Series or 1D NumPy array), force the use of voomByGroup and
                   group on the unique values of that column. `group` can also
                   be a dictionary mapping cell-type names to `False`, `None`,
                   or a column for each cell type. When using voomByGroup, the
                   same groups are also used as the `group` argument to
                   `calcNormFactors()` when normalizing by library size.
            categorical_columns: one or more names of integer or Enum columns
                                 of obs to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of obs, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         log_CPM() already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation

        Returns:
            A DataFrame of the DE results for this cell type. Or, if
            `return_voom_info=True`, a tuple of three DataFrames: the DE
            results, the voom weights, and the voom plot info for this cell
            type.
        """
        from ryp import r, to_py, to_r
        # Get the data for this cell type
        X = self._X[cell_type]
        obs = self._obs[cell_type]
        var = self._var[cell_type]
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f"DE() requires raw counts but X[{cell_type!r}].dtype is "
                f"{str(X.dtype)!r}, a floating-point data type. If you "
                f"are sure that all values are integers, i.e. that "
                f"X[{cell_type!r}].data == X[{cell_type!r}].data"
                f".astype(int)).all(), then set allow_float=True (or just "
                f"cast X to an integer data type). Alternatively, did you "
                f"accidentally run log_CPM() before DE()?")
            raise TypeError(error_message)
        # Check that `formula` starts with a tilde (`~`), and that all
        # variables it references are Categorical, Enum, Boolean, integer, or
        # floating-point columns of obs. Make a set of these variables so we
        # can subset to them before converting obs to R.
        if verbose:
            print(f'[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
        # Estimate library sizes
        if verbose:
            print(f'[{cell_type}] Estimating library sizes...')
        library_size = self._library_size(X, cell_type)
        # If `library_size_as_covariate=True`, check that `formula` does not
        # already contain a `'log_library_size'` variable, then add
        # `log2(library_size)` as a covariate
        if library_size_as_covariate:
            if verbose:
                print(f'[{cell_type}] Adding library size as a covariate...')
            if 'log_library_size' in obs_columns:
                error_message = (
                    'formula cannot contain a log_library_size column when '
                    'library_size_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            obs = obs.with_columns(
                log_library_size=pl.Series(library_size).log(2))
            obs_columns.append('log_library_size')
            formula += ' + log_library_size'
        # If `num_cells_as_covariate=True`, check that `formula` does not
        # already contain a `'num_cells'` or `'log_num_cells'` variable, then
        # add `log2(num_cells)` as a covariate
        if num_cells_as_covariate:
            if verbose:
                print(f'[{cell_type}] Adding num_cells as a covariate...')
            if 'num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain num_cells explicitly when '
                    'num_cells_as_covariate=True, to avoid collinearity')
                raise ValueError(error_message)
            if 'log_num_cells' in obs_columns:
                error_message = (
                    'formula cannot contain a log_num_cells column when '
                    'num_cells_as_covariate=True, to avoid a name clash')
                raise ValueError(error_message)
            obs = obs.with_columns(log_num_cells=obs['num_cells'].log(2))
            obs_columns.append('log_num_cells')
            formula += ' + log_num_cells'
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        try:
            # Create the design matrix
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            obs_names = obs[:, 0]
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_columns, categorical_columns,
                                             prefix)
            design_matrix_columns = to_py(f'colnames({prefix}.design.matrix)')
            # Check that the design matrix is full-rank
            rank = to_py(f'qr({prefix}.design.matrix)$rank')
            width = len(design_matrix_columns)
            if rank < width:
                error_message = (
                    f'the design matrix is not full-rank for cell type '
                    f'{cell_type!r} (rank {rank:,} with {width:,} columns); '
                    f'some of your covariates are linear combinations of '
                    f'other covariates')
                raise ValueError(error_message)
            # If `contrasts` was specified, validate `contrasts`; otherwise,
            # validate `coefficient`
            if contrasts is None:
                if verbose:
                    print(f'[{cell_type}] Validating coefficient...')
                # Check that all string entries of `coefficient` are names of
                # columns of the design matrix
                for coef in coefficient:
                    if isinstance(coef, str) and \
                            coef not in design_matrix_columns:
                        error_message = (
                            f'coefficient {coef!r} is not a column of the '
                            f'design matrix for cell type {cell_type!r}. The '
                            f'design matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns: '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
                # Extract the name of the design matrix column corresponding to
                # each integer in `coefficient`; make sure none of the integers
                # are >= the design matrix width
                for coef in coefficient:
                    if isinstance(coef, (int, np.integer)) and coef >= width:
                        error_message = (
                            f'coefficient '
                            f'{"is" if len(coefficient) == 1 else "contains"} '
                            f'the integer {coef}, which is more than the '
                            f'number of columns of the design matrix '
                            f'({width:,}) minus 1 for cell type {cell_type!r}')
                        raise ValueError(error_message)
                coefficient = [design_matrix_columns[coef]
                               if isinstance(coef, (int, np.integer)) else coef
                               for coef in coefficient]
                # Convert `coefficient` to R
                to_r(pl.Series(coefficient), f'{prefix}.coef')
            else:
                if verbose:
                    print(f'[{cell_type}] Validating contrasts...')
                # Check that all variables referenced in `contrasts` are in the
                # design matrix. Use `composite=True` to avoid splitting e.g.
                # `x1:x2` into `x1` and `x2`.
                contrasts = {contrast_name: contrast.replace(' ', '')
                             for contrast_name, contrast in contrasts.items()}
                valid_dtypes = pl.INTEGER_DTYPES | \
                               {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
                unique_contrast_variables = \
                    Pseudobulk._get_unique_variables(contrasts.values(),
                                                     composite=True)
                for variable in unique_contrast_variables:
                    if variable not in design_matrix_columns:
                        error_message = (
                            f'a contrast contains the variable {variable!r}, '
                            f'which is not the name of a column of the design '
                            f'matrix for cell type {cell_type!r}. The design '
                            f'matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
            if group is None or contrasts is not None:
                # Get the list of columns of obs referenced in `coefficient` or
                # `contrasts`. This can be done by:
                # 1. Getting the columns of the design matrix referenced in
                #    `coefficient` or `contrasts`. For `coefficient`, this is
                #    just the entries of `coefficient` themselves. For
                #    `contrasts`, this is the `unique_contrast_variables`
                #    variable we defined above.
                # noinspection PyUnboundLocalVariable
                referenced_design_matrix_columns = coefficient \
                    if contrasts is None else unique_contrast_variables
                # 2. Mapping each column of the design matrix listed in
                #    `coefficient` or `contrasts` back to the integer index of
                #    the term in `formula` it was derived from, using the
                #    design matrix's `assign` attribute. Subtract 1 to convert
                #    from 1-based to 0-based indexing.
                assign = to_py(f'attr({prefix}.design.matrix, "assign")')
                referenced_term_indices = assign\
                    .filter(design_matrix_columns
                            .is_in(referenced_design_matrix_columns)) - 1
                # 3. Using R's `terms()` function to expand the formula into
                #    the list of terms corresponding to these integer indices.
                term_labels = \
                    to_py(f'attr(terms({prefix}.formula), "term.labels")')
                # 4. Pulling out the terms corresponding to the indices from
                #    step 2, to get the terms of `formula` referenced in
                #    `coefficient` or `contrasts`.
                referenced_terms = term_labels[referenced_term_indices]
                # 5. Getting the unique variables in these terms. These are the
                #    columns of obs referenced in `coefficient` or `contrasts`.
                referenced_columns = \
                    Pseudobulk._get_unique_variables(referenced_terms)
            # If `contrasts` is not `None`, check that all columns of obs
            # referenced in `contrasts` are categorical, now that we know which
            # columns are referenced
            if contrasts is not None:
                # noinspection PyUnboundLocalVariable
                for column in referenced_columns:
                    base_type = obs[column].dtype.base_type()
                    # noinspection PyUnboundLocalVariable
                    if base_type not in valid_dtypes:
                        error_message = (
                            f'all columns of obs referenced in contrasts must '
                            f'be String, Categorical, Enum, Boolean, or '
                            f'integer, but a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}], which has data '
                            f'type {base_type!r}')
                        raise TypeError(error_message)
                    if (base_type == pl.Enum or
                        base_type in pl.INTEGER_DTYPES) and (
                            categorical_columns is None or
                            column not in categorical_columns):
                        error_message = (
                            f'a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}] with data type '
                            f'{base_type!r}, but all columns referenced in '
                            f'contrasts must be categorical and integer/Enum '
                            f'columns are not treated as categorical unless '
                            f'specified in categorical_columns; did you '
                            f'forget to add {column!r} to '
                            f'categorical_columns?')
                        raise TypeError(error_message)
            # If `group=None`, group on the unique combinations of values of
            # the categorical columns of obs referenced in `coefficient` or
            # `contrasts`
            if group is None:
                if verbose:
                    print(f'[{cell_type}] Defining groups...')
                if contrasts is not None:
                    # If using `contrasts`, always use voomByGroup, and group
                    # on the unique combinations of the columns referenced in
                    # the contrasts (recall that we already checked that they
                    # are categorical)
                    # noinspection PyUnboundLocalVariable
                    group_columns = referenced_columns
                else:
                    # If using `coefficient`, group on the unique combinations
                    # of the categorical columns referenced in the
                    # coefficients. If no columns are categorical, use voom
                    # instead of voomByGroup.
                    categorical_selector = \
                        pl.selectors.by_dtype(pl.String, pl.Categorical,
                                              pl.Boolean)
                    if categorical_columns is not None:
                        categorical_selector |= \
                            pl.selectors.by_name(categorical_columns)
                    group_columns = pl.selectors.expand_selector(
                        obs,
                        pl.selectors.by_name(referenced_columns) &
                        categorical_selector)
                if len(group_columns) > 0:                   
                    # Check that none of the group columns contain null values
                    for group_column in group_columns:
                        null_count = obs[group_column].null_count()
                        if null_count > 0:
                            error_message = (
                                f'group column {group_column!r} contains '
                                f'{null_count:,} '
                                f'{plural("null value", null_count)} for cell '
                                f'type {cell_type!r}, but must not contain '
                                f'any. Specify group_column={group_column!r} '
                                'in Pseudobulk.qc(), or specify a different '
                                'group column via the group argument to '
                                'Pseudobulk.DE().')
                            raise ValueError(error_message)
                    # Create a descriptive name for each combination of values
                    # in the `group_columns`
                    group = obs\
                        .select(pl.format(', '.join(f'{column} = {{}}'
                                                    for column in
                                                    group_columns),
                                          *group_columns))\
                        .to_series()
                    group = group\
                        .cast(pl.Enum(group.unique().sort().to_list()))
                    # If the group columns have the same values for every 
                    # sample, disable grouping
                    single_group = len(group.cat.get_categories())
                    if single_group:
                        group = None
                    # If `verbose=True`, print whether and how we're grouping
                    if verbose:
                        if len(group_columns) == 1:
                            group_column_description = f'{group_columns[0]!r}'
                        elif len(group_columns) == 2:
                            group_column_description = \
                                f'{group_columns[0]!r} and ' \
                                f'{group_columns[1]!r}'
                        else:
                            group_column_description = \
                                ', '.join(map(repr, group_columns[:-1])) + \
                                f', and {group_columns[-1]!r}'
                        if single_group:
                            if len(group_columns) == 1:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group column '
                                      f'{group_column_description} has the '
                                      f'same value for every sample')
                            else:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group columns '
                                      f'{group_column_description} have the '
                                      f'same values for every sample')
                        else:
                            print(f'[{cell_type}] Grouping on the '
                                  f'{group_column_description} '
                                  f'{plural("column", len(group_columns))} of '
                                  f'obs.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Not grouping since coefficient '
                              f'does not reference any categorical variables.')
                    group = None
            # If grouping, check that all groups have at least two samples
            if group is not None:
                group_counts = group.value_counts().sort('count')
                if group_counts['count'][0] == 1:
                    error_message = (
                        f'all groups must have at least two samples, but '
                        f'group {group_counts[0, 0]!r} has only one '
                        f'sample for cell type {cell_type}')
                    raise ValueError(error_message)
            # Convert the expression matrix and library sizes to R
            if verbose:
                if group is not None:
                    print(f'[{cell_type}] Converting the expression matrix, '
                          f'library sizes and groups to R...')
                else:
                    print(f'[{cell_type}] Converting the expression matrix '
                          f'and library sizes to R...')
            to_r(X.T, f'{prefix}.X.T', rownames=var[:, 0],
                 colnames=obs_names)
            to_r(library_size, f'{prefix}.library.size', rownames=obs_names)
            to_r(group, f'{prefix}.group')
            # Run voom
            to_r(return_voom_info, 'save.plot')
            if group is not None:
                if verbose:
                    print(f'[{cell_type}] Running voomByGroup...')
                r(f'{prefix}.voom.result = voomByGroup('
                  f'{prefix}.X.T, {prefix}.group, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot, print=FALSE)')
            else:
                if verbose:
                    print(f'[{cell_type}] Running voom...')
                r(f'{prefix}.voom.result = voom('
                  f'{prefix}.X.T, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot)')
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                voom_weights = \
                    to_py(f'{prefix}.voom.result$weights', index='gene')
                if group is not None:
                    frames = [
                        pl.DataFrame({
                            'gene': to_py(
                                f'names({prefix}.voom.result$voom.xy$'
                                f'`{group_name}`$x)')} | {
                            f'{prop}_{dim}_{group_name}': to_py(
                                f'{prefix}.voom.result$voom.{prop}$'
                                f'`{group_name}`${dim}', index=False)
                            for prop in ('xy', 'line')
                            for dim in ('x', 'y')})
                        for group_name in group.unique(maintain_order=True)]
                    frames = pl.align_frames(*frames, on='gene', how='outer')
                    voom_plot_data = pl.concat(
                        [frames[0]] + [frame[:, 1:] for frame in frames[1:]],
                        how='horizontal')
                else:
                    voom_plot_data = pl.DataFrame({
                        'gene': to_py(
                            f'names({prefix}.voom.result$voom.xy$x)')} | {
                        f'{prop}_{dim}': to_py(
                            f'{prefix}.voom.result$voom.{prop}${dim}',
                            index=False)
                        for prop in ('xy', 'line') for dim in ('x', 'y')})
            # Run lmFit
            if verbose:
                print(f'[{cell_type}] Running lmFit...')
            r(f'{prefix}.lmFit.result = lmFit('
              f'{prefix}.voom.result, {prefix}.design.matrix)')
            if contrasts is not None:
                # Convert contrasts to R
                to_r(pl.Series(contrasts.values()), f'{prefix}.contrasts')
                # Make the contrast terms and design matrix colnames into
                # valid R variable names by temporarily:
                # - escaping interaction terms (e.g. renaming `A:B` to `A.B`)
                #   in both the contrasts and the design matrix colnames
                # - renaming the design matrix column `(Intercept)` to
                #   `.Intercept` (not `Intercept` in case the user already has
                #   a column called `Intercept` for some reason)
                r(f'{prefix}.contrasts = gsub(":", ".", {prefix}.contrasts)')
                r(f'{prefix}.columns = colnames({prefix}.design.matrix)')
                r(rf'{prefix}.levels = gsub("\\(Intercept\\)", ".Intercept", '
                  rf'gsub(":", ".", {prefix}.columns))')
                r(f'colnames({prefix}.design.matrix) = {prefix}.levels')
                # Make contrasts
                r(f'{prefix}.contrasts = makeContrasts('
                  f'contrasts={prefix}.contrasts, levels={prefix}.levels)')
                # Rename the design matrix colnames (and the contrast rownames)
                # to the original design matrix colnames
                r(f'rownames({prefix}.contrasts) = {prefix}.columns')
                r(f'colnames({prefix}.design.matrix) = {prefix}.columns')
                # Set the colnames of the contrasts to `contrasts.keys()`, so
                # they display as those names in the output DE table
                to_r(pl.Series(contrasts.keys()), f'{prefix}.coef')
                r(f'colnames({prefix}.contrasts) = {prefix}.coef')
                # Fit contrasts
                r(f'{prefix}.lmFit.result = contrasts.fit('
                  f'{prefix}.lmFit.result, {prefix}.contrasts)')
            # Run eBayes
            if verbose:
                print(f'[{cell_type}] Running eBayes...')
            to_r(robust, f'{prefix}.robust')
            r(f'{prefix}.eBayes.result = eBayes('
              f'{prefix}.lmFit.result, trend=FALSE, robust={prefix}.robust)')
            # Make a table of the DE results
            if verbose:
                print(f'[{cell_type}] Collating results...')
            gene = to_py(f'rownames({prefix}.eBayes.result)')
            logFC = to_py(f'{prefix}.eBayes.result$coefficients['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            SE = to_py(f'{prefix}.eBayes.result$s2.post').sqrt() * \
                 to_py(f'{prefix}.eBayes.result$stdev.unscaled['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            margin_error = \
                SE * stdtrit(to_py(f'{prefix}.eBayes.result$df.total'), 0.975)
            LCI = logFC - margin_error
            UCI = logFC + margin_error
            AveExpr = to_py(f'{prefix}.eBayes.result$Amean', index=False)
            p = to_py(f'{prefix}.eBayes.result$p.value['
                      f',{prefix}.coef, drop=FALSE]', index=False)
            DE_results = pl.concat([
                pl.DataFrame({
                    'coefficient': coef, 'gene': gene, 'logFC': logFC[coef],
                    'SE': SE[coef], 'LCI': LCI[coef], 'UCI': UCI[coef],
                    'AveExpr': AveExpr, 'p': p[coef]})
                .with_columns(Bonferroni=bonferroni(pl.col.p),
                              FDR=fdr(pl.col.p))
                for coef in (
                    contrasts if contrasts is not None else coefficient)])
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix", '
              f'"{prefix}.library.size", "{prefix}.X.T", "{prefix}.group", '
              f'"{prefix}.voom.result", "{prefix}.lmFit.result", '
              f'"{prefix}.contrasts", "{prefix}.columns", "{prefix}.levels", '
              f'"{prefix}.robust", "{prefix}.eBayes.result", '
              f'"{prefix}.coef")))')
        # noinspection PyUnboundLocalVariable
        return DE_results, voom_weights, voom_plot_data \
            if return_voom_info else DE_results
    
    def DE(self,
           formula: str | dict[str, str],
           coefficient: str | int | np.integer |
                        Iterable[str | int | np.integer] |
                        dict[str, str | int | np.integer |
                                  Iterable[str | int | np.integer]] = 1,
           *,
           contrasts: dict[str, str] | None |
                      dict[str, dict[str, str] | None] = None,
           group: Literal[False] | PseudobulkColumn | None |
                  dict[str, Literal[False] | str | pl.Expr | pl.Series |
                            np.ndarray | None] = None,
           categorical_columns: str | Iterable[str] | None |
                                dict[str, str | Iterable[str] | None] = None,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           library_size_as_covariate: bool = True,
           num_cells_as_covariate: bool = True,
           robust: bool = False,
           return_voom_info: bool = True,
           allow_float: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> DE:
        """
        Perform differential expression (DE) on a Pseudobulk dataset with
        limma-voom. The DE design is specified via an R formula string that
        references columns from obs.
        
        By default, DE is reported with respect to the first column of the
        design matrix after the intercept, which is usually just the first term
        in the formula. This can be changed via the `coefficient` and
        `contrasts` arguments.
        
        By default, voomByGroup is used instead of voom when reporting DE with
        respect to a categorical variable, e.g. when comparing disease cases to
        healthy controls. This can be changed via the `group` argument.
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of obs referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of obs represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of obs, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Or, a dictionary mapping
                         cell-type names to any of the above; each cell type in
                         this Pseudobulk dataset must be present. Negative
                         indices work in the usual Python way. Defaults to
                         `coefficient=1`, the first variable in `formula`
                         (`coefficient=0` would refer to the intercept).
                         Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. Or, a
                       dictionary mapping cell-type names to these
                       dictionaries; each cell type in this Pseudobulk dataset
                       must be present. If specified, DE will be performed with
                       respect to each contrast by running limma's
                       `makeContrasts()` and `contrasts.fit()` functions after
                       `lmFit()`. Mutually exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of obs
                   referenced in `coefficient`, or the columns of obs
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   column (the name of a String, Categorical, Enum, Boolean, or
                   integer column of obs, a polars expression, a polars Series,
                   a 1D NumPy array, or a function that takes in this
                   Pseudobulk dataset and a cell type and returns a polars
                   Series or 1D NumPy array), force the use of voomByGroup and
                   group on the unique values of that column. `group` can also
                   be a dictionary mapping cell-type names to `False`, `None`,
                   or a column for each cell type. When using voomByGroup, the
                   same groups are also used as the `group` argument to
                   `calcNormFactors()` when normalizing by library size. All
                   groups must have at least two samples.
            categorical_columns: one or more names of integer or Enum columns
                                 of obs to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to test for DE; if `None`, test
                        all cell types. If specified and using dictionaries for
                        `formula`, `coefficient`, `contrasts`, or `group`, the
                        keys of these dictionaries must match the cell types
                        specified here. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when testing
                                 for DE. If specified and using dictionaries
                                 for `formula`, `coefficient`, `contrasts`, or
                                 `group`, the keys of these dictionaries must
                                 also exclude these cell types. Mutually
                                 exclusive with `cell_types`.
            library_size_as_covariate: whether to include the log2 of the
                                       library size, calculated according to
                                       the method of edgeR's calcNormFactors(),
                                       as an additional covariate called
                                       `'log_library_size'`. If
                                       `library_size_as_covariate=True`,
                                       `formula` cannot include a column called
                                       `'log_library_size'` to avoid a name
                                       clash.
            num_cells_as_covariate: whether to include the log2 of the
                                    `'num_cells'` column of obs, i.e. the
                                    number of cells that went into each
                                    sample's pseudobulk in each cell type, as
                                    an additional covariate called
                                    `log_num_cells`. If
                                    `num_cells_as_covariate=True`, `formula`
                                    cannot include the `num_cells` column
                                    explicitly to avoid collinearity, nor can
                                    it include a column called
                                    `'log_num_cells'` to avoid a name clash.
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         log_CPM() already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation
            num_threads: the number of cores to use for DE estimation.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores (as determined by
                         `os.cpu_count()`), or leave unset to use
                         `single_cell.get_num_threads()` cores. Parallelization
                         takes place across cell types, so specifying more
                         cores than the number of cell types may not improve
                         performance.

        Returns:
            A DE object with a `table` attribute containing a polars DataFrame
            of the DE results, with columns:
            - cell_type: the cell type in which DE was tested
            - coefficient: the coefficient (or contrast) for which DE was 
                           tested
            - gene: the gene for which DE was tested
            - logFC: the log2 fold change of the gene, i.e. its effect size
            - SE: the standard error of the effect size
            - LCI: the lower 95% confidence interval of the effect size
            - UCI: the upper 95% confidence interval of the effect size
            - AveExpr: the gene's average expression in this cell type, in log
                       CPM
            - p: the DE p-value
            - Bonferroni: the Bonferroni-corrected DE p-value
            - FDR: the FDR q-value for the DE
            If `return_voom_info=True`, the DE object also includes a
            `voom_weights` attribute containing a {cell_type: DataFrame}
            dictionary of voom weights, and a `voom_plot_data` attribute
            containing a {cell_type: DataFrame} dictionary of info necessary to
            construct a voom plot with `DE.plot_voom()`.
        """
        # Import required Python and R packages, and source voomByGroup code
        from ryp import r
        r('suppressPackageStartupMessages(library(limma))')
        r(self._voomByGroup_source_code)
        # Get the list of cell types to compute DE for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings; the validity of the formula is checked later
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        # Check that `coefficient` is one or more strings or integers, or a
        # dictionary mapping cell types to one or more strings or integers.
        # Convert it (or its values, if a dictionary) to tuples, storing the
        # result as a new variable, `coefficients`.
        coefficient_is_dict = isinstance(coefficient, dict)
        if coefficient_is_dict:
            coefficients = {}
            for cell_type, value in coefficient.items():
                if not isinstance(cell_type, str):
                    error_message = (
                        f'when coefficient is a dictionary, all its keys must '
                        f'be strings (cell types), but it contains a key of '
                        f'type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                coefficients[cell_type] = to_tuple_checked(
                    value, f'coefficient[{cell_type!r}]', (str, int),
                    'strings or integers')
            if tuple(coefficients) != cell_types:
                error_message = (
                    f'coefficient is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
        else:
            coefficients = to_tuple_checked(coefficient, 'coefficient',
                                            (str, int), 'strings or integers')
        # Check that `contrasts` is `None`, a dictionary mapping strings to
        # strings, or a dictionary mapping cell types to `None` or dictionaries
        # mapping strings to strings. If not a nested dictionary, make it one.
        contrasts_is_nested_dict = False
        if contrasts is not None:
            check_type(contrasts, 'contrasts', dict, 'a dictionary')
            if not isinstance(coefficient, (int, np.integer)) or \
                    coefficient != 1:
                error_message = \
                    'coefficient and contrasts cannot both be specified'
                raise ValueError(error_message)
            for key in contrasts:
                if not isinstance(key, str):
                    error_message = (
                        f'all keys of contrasts must be strings, but it '
                        f'contains a key of type {type(key).__name__!r}')
                    raise TypeError(error_message)
            if all(isinstance(value, str) for value in contrasts.values()):
                # `contrasts` is a dictionary mapping strings to strings
                pass
            elif all(isinstance(value, dict) or value is None
                     for value in contrasts.values()):
                # `contrasts` is a dictionary mapping cell types to `None` or
                # dictionaries mapping strings to strings
                contrasts_is_nested_dict = True
                if tuple(contrasts) != cell_types:
                    error_message = (
                        f'contrasts is a dictionary of dictionaries, but does '
                        f'not have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
                for key, value in contrasts.values():
                    if value is not None:
                        for inner_key, inner_value in value.items():
                            if not isinstance(inner_key, str):
                                error_message = (
                                    f'all keys of contrasts[{key!r}] must be '
                                    f'strings, but it contains a key of type '
                                    f'{type(inner_key).__name__!r}')
                                raise TypeError(error_message)
                            if not isinstance(inner_value, str):
                                error_message = (
                                    f'all values of contrasts[{key!r}] must '
                                    f'be strings, but it contains a value of '
                                    f'type {type(inner_value).__name__!r}')
                                raise TypeError(error_message)
                all_contrasts = contrasts
            else:
                error_message = (
                    'contrasts.values() must either be all strings or all '
                    'dictionaries/None')
                raise TypeError(error_message)
        # Check that `group` is `False`, `None`, a categorical column, or a
        # dictionary mapping cell types to `False`, `None`, or categorical
        # columns.
        if group is not None and group is not False:
            if group is True:
                error_message = \
                    'group must be None, False, or a column of obs, not True'
                raise TypeError(error_message)
            if isinstance(group, dict):
                for key, value in group.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when group is a dictionary, all its keys must '
                            f'be strings (cell types), but it contains a key '
                            f'of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None and value is not False:
                        if value is True:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, not True')
                            raise TypeError(error_message)
                        else:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, but has type '
                                f'{type(value).__name__!r}')
                            raise TypeError(error_message)
                # noinspection PyTypeChecker
                if tuple(group) != cell_types:
                    error_message = (
                        f'group is a dictionary, but does not have the same '
                        f'cell types (keys) as {cell_type_description}, or '
                        f'has the same cell types in a different order')
                    raise ValueError(error_message)
                groups = {}
                for cell_type, column in group.items():
                    obs = self._obs[cell_type]
                    if column is None:
                        groups[cell_type] = None
                    if isinstance(column, str):
                        if column not in obs:
                            error_message = (
                                f'group[{cell_type!r}] is {column!r}, which '
                                f'is not a column of obs[{cell_type!r}]')
                            raise ValueError(error_message)
                        groups[cell_type] = obs[column]
                    elif isinstance(column, pl.Expr):
                        groups[cell_type] = obs.select(column)
                        if groups[cell_type].width > 1:
                            # noinspection PyUnresolvedReferences
                            error_message = (
                                f'group[{cell_type!r}] is a polars expression '
                                f'that expands to {group[cell_type].width:,} '
                                f'columns rather than 1 for cell type ')
                            raise ValueError(error_message)
                        groups[cell_type] = groups[cell_type].to_series()
                    elif isinstance(column, pl.Series):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a polars Series of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = column
                    elif isinstance(column, np.ndarray):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a NumPy array of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = pl.Series('group', column)
                    else:
                        error_message = (
                            f'group[{cell_type!r}] must be None, False, a '
                            f'string column name, a polars expression or '
                            f'Series, or a 1D NumPy array, but has type '
                            f'{type(column).__name__!r}')
                        raise TypeError(error_message)
                    # Check dtype
                    base_type = column.dtype.base_type()
                    if base_type not in (pl.String, pl.Categorical, pl.Enum,
                                         pl.Boolean) and \
                            base_type not in pl.INTEGER_DTYPES:
                        error_message = (
                            f'group[{cell_type!r}] must be String, '
                            f'Categorical, Enum, Boolean, or integer, but has '
                            f'data type {base_type!r}')
                        raise TypeError(error_message)
                    # Check nulls
                    null_count = column.null_count()
                    if null_count > 0:
                        error_message = (
                            f'group[{cell_type!r}] contains {null_count:,} '
                            f'{plural("null value", null_count)}, but must '
                            f'not contain any')
                        raise ValueError(error_message)
            else:
                # noinspection PyTypeChecker
                groups = self._get_column(
                    'obs', group, 'group',
                    (pl.String, pl.Categorical, pl.Enum, pl.Boolean,
                     'integer'))
                for cell_type, group in groups.items():
                    if group is not None and group is not False:
                        null_count = group.null_count()
                        if null_count > 0:
                            error_message = (
                                f'group contains {null_count:,} '
                                f'{plural("null value", null_count)} for cell '
                                f'type {cell_type!r}, but must not contain '
                                f'any. Specify the same group column via the '
                                f'group_column argument to Pseudobulk.qc(), '
                                f'explicitly remove nulls from your group '
                                f'column with fill_null(), or specify a '
                                f'different group column (or none at all, to '
                                f'group automatically).')
                            raise ValueError(error_message)
        else:
            groups = None
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        # Check that Boolean arguments are Boolean
        check_type(library_size_as_covariate, 'library_size_as_covariate',
                   bool, 'Boolean')
        check_type(num_cells_as_covariate, 'num_cells_as_covariate', bool,
                   'Boolean')
        check_type(robust, 'robust', bool, 'Boolean')
        check_type(return_voom_info, 'return_voom_info', bool, 'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `single_cell.get_num_threads()` if free-threaded and 1
        # otherwise, and if -1, set to `os.cpu_count()`. Raise an error if the
        # user specified multiple threads but lacks free-threaded Python.
        num_threads = Pseudobulk._process_num_threads(num_threads)
        # If `num_cells_as_covariate=True`, check that `'num_cells'` is a
        # column of obs for every cell type
        if num_cells_as_covariate:
            for cell_type in cell_types:
                if 'num_cells' not in self._obs[cell_type]:
                    error_message = (
                        f"num_cells_as_covariate is True, but 'num_cells' is "
                        f"not a column of obs[{cell_type!r}]")
                    raise ValueError(error_message)
        # Compute DE for each cell type
        if num_threads == 1:
            DE_results = {}
            if return_voom_info:
                voom_weights = {}
                voom_plot_data = {}
            for cell_type_index, cell_type in enumerate(cell_types):
                # noinspection PyUnboundLocalVariable
                results = self._DE(
                    cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type]
                            if formula_is_dict else formula,
                    coefficient=coefficients[cell_type]
                                if coefficient_is_dict else coefficients,
                    contrasts=all_contrasts[cell_type]
                              if contrasts_is_nested_dict else contrasts,
                    group=groups[cell_type] if groups is not None else None,
                    categorical_columns=all_categorical_columns[cell_type]
                                        if categorical_columns_is_dict else
                                        categorical_columns,
                    library_size_as_covariate=library_size_as_covariate,
                    num_cells_as_covariate=num_cells_as_covariate,
                    robust=robust,
                    return_voom_info=return_voom_info,
                    allow_float=allow_float,
                    verbose=verbose)
                if return_voom_info:
                    # noinspection PyUnboundLocalVariable
                    DE_results[cell_type], voom_weights[cell_type], \
                        voom_plot_data[cell_type] = results
                else:
                    DE_results[cell_type] = results
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            args_list = [{
                'cell_type': cell_type,
                'cell_type_index': cell_type_index,
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'coefficient': coefficients[cell_type]
                               if coefficient_is_dict else coefficients,
                'contrasts': all_contrasts[cell_type]
                             if contrasts_is_nested_dict else contrasts,
                'group': groups[cell_type] if groups is not None else None,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'library_size_as_covariate': library_size_as_covariate,
                'num_cells_as_covariate': num_cells_as_covariate,
                'robust': robust,
                'return_voom_info': return_voom_info,
                'allow_float': allow_float,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                results = tuple(executor.map(
                    lambda kwargs: self._DE(**kwargs), args_list))
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                DE_results = {cell_type: result[0] for cell_type, result in
                              zip(cell_types, results)}
                voom_weights = {cell_type: result[1] for cell_type, result in
                                zip(cell_types, results)}
                voom_plot_data = {cell_type: result[2] for cell_type, result in
                                  zip(cell_types, results)}
            else:
                DE_results = dict(zip(cell_types, results))
        # Concatenate across cell types
        table = pl.concat([
            cell_type_DE_results
            .select(pl.lit(cell_type).alias('cell_type'), pl.all())
            for cell_type, cell_type_DE_results in DE_results.items()])
        if return_voom_info:
            return DE(table, voom_weights, voom_plot_data)
        else:
            return DE(table)


class DE:
    """
    Differential expression results returned by Pseudobulk.DE().
    """
    
    def __init__(self,
                 table: pl.DataFrame,
                 voom_weights: dict[str, pl.DataFrame] | None = None,
                 voom_plot_data: dict[str, pl.DataFrame] | None = None) -> \
            None:
        """
        Initialize the DE object.
        
        Args:
            table: a polars DataFrame containing the DE results, with columns:
                   - cell_type: the cell type in which DE was tested
                   - coefficient: the coefficient (or contrast) for which DE 
                                  was tested
                   - gene: the gene for which DE was tested
                   - logFC: the log2 fold change of the gene, i.e. its effect
                            size
                   - SE: the standard error of the effect size
                   - LCI: the lower 95% confidence interval of the effect size
                   - UCI: the upper 95% confidence interval of the effect size
                   - AveExpr: the gene's average expression in this cell type,
                              in log CPM
                   - p: the DE p-value
                   - Bonferroni: the Bonferroni-corrected DE p-value
                   - FDR: the FDR q-value for the DE
                   Or, a directory containing a DE object saved with `save()`.
            voom_weights: an optional {cell_type: DataFrame} dictionary of voom
                         weights, where rows are genes and columns are samples.
                         The first column of each cell type's DataFrame,
                         'gene', contains the gene names.
            voom_plot_data: an optional {cell_type: DataFrame} dictionary of
                            info necessary to construct a voom plot with
                            `DE.plot_voom()`
        """
        if isinstance(table, pl.DataFrame):
            if voom_weights is not None:
                if voom_plot_data is None:
                    error_message = (
                        'voom_plot_data must be specified when voom_weights '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_weights, 'voom_weights', dict, 'a dictionary')
                if voom_weights.keys() != voom_plot_data.keys():
                    error_message = (
                        'voom_weights and voom_plot_data must have matching '
                        'cell types (keys)')
                    raise ValueError(error_message)
                for key in voom_weights:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of voom_weights and voom_plot_data '
                            f'must be strings (cell types), but they contain '
                            f'a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
            if voom_plot_data is not None:
                if voom_weights is None:
                    error_message = (
                        'voom_weights must be specified when voom_plot_data '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_plot_data, 'voom_plot_data', dict,
                           'a dictionary')
        elif isinstance(table, (str, Path)):
            table = str(table)
            if not os.path.exists(table):
                error_message = f'DE object directory {table!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{table}/cell_types.txt')]
            voom_weights = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_weights.parquet'))
                for cell_type in cell_types}
            voom_plot_data = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_plot_data.parquet'))
                for cell_type in cell_types}
            table = pl.read_parquet(os.path.join(table, 'table.parquet'))
        else:
            error_message = (
                f'table must be a polars DataFrame or a directory (string or '
                f'pathlib.Path) containing a saved DE object, but has type '
                f'{type(table).__name__!r}')
            raise TypeError(error_message)
        self.table = table
        self.voom_weights = voom_weights
        self.voom_plot_data = voom_plot_data
    
    def __repr__(self) -> str:
        """
        Get a string representation of this DE object.
        
        Returns:
            A string summarizing the object.
        """
        num_cell_types = self.table['cell_type'].n_unique()
        descr = (
            f'DE object with {len(self.table):,} '
            f'{"entries" if len(self.table) != 1 else "entry"} across '
            f'{num_cell_types:,} {plural("cell type", num_cell_types)}:\n'
            f'{self.table}')
        return descr
    
    def __eq__(self, other: DE) -> bool:
        """
        Test for equality with another DE object.
        
        Args:
            other: the other DE object to test for equality with

        Returns:
            Whether the two DE objects are identical.
        """
        if not isinstance(other, DE):
            error_message = (
                f'the left-hand operand of `==` is a DE object, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        return self.table.equals(other.table) and \
            (other.voom_weights is None if self.voom_weights is None else
             self.voom_weights.keys() == other.voom_weights.keys() and
             all(self.voom_weights[cell_type].equals(
                     other.voom_weights[cell_type]) and
                 self.voom_plot_data[cell_type].equals(
                     other.voom_plot_data[cell_type])
                 for cell_type in self.voom_weights))
    
    @property
    def groups(self) -> dict[str, tuple[str, ...] | None] | None:
        """
        Get the groups used by voomByGroup for each cell type.
        
        Returns:
            A dictionary mapping cell type names to group names used by
            voomByGroup for that cell type, or None if voomByGroup was not used
            for that cell type. If `Pseudobulk.DE()` was called with
            `return_voom_info=False`, `groups` will be None instead of a
            dictionary.
        """
        return {cell_type: groups if groups else None
                for cell_type, groups in (
                    (cell_type, tuple(column[5:] for column in data.columns
                                      if column[:4] == 'xy_x'))
                    for cell_type, data in self.voom_plot_data.items())} \
            if self.voom_plot_data is not None else None
    
    def save(self, directory: str | Path, overwrite: bool = False) -> None:
        # noinspection GrazieInspection
        """
        Save a DE object to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with the table at table.parquet.
        
        If the DE object contains voom info (i.e. was created with
        `return_voom_info=True` in `Pseudobulk.DE()`, the default), also saves
        each cell type's voom weights and voom plot data to
        f'{cell_type}_voom_weights.parquet' and
        f'{cell_type}_voom_plot_data.parquet', as well as a text file,
        cell_types.txt, containing the cell types.
        
        Args:
            directory: the directory to save the DE object to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
        """
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        self.table.write_parquet(os.path.join(directory, 'table.parquet'))
        if self.voom_weights is not None:
            with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
                # noinspection PyTypeChecker
                print('\n'.join(self.voom_weights), file=f)
            for cell_type in self.voom_weights:
                escaped_cell_type = cell_type.replace('/', '-')
                self.voom_weights[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_weights.parquet'))
                self.voom_plot_data[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_plot_data.parquet'))
    
    def get_hits(self,
                 significance_column: str = 'FDR',
                 threshold: int | float | np.integer | np.floating = 0.05,
                 num_top_hits: int | np.integer | None = None) -> pl.DataFrame:
        """
        Get all (or the top) differentially expressed genes.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            num_top_hits: the number of top hits to report for each cell type;
                          if `None`, report all hits

        Returns:
            The `table` attribute of this DE object, subset to (top) DE hits.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        if num_top_hits is not None:
            check_type(num_top_hits, 'num_top_hits', int, 'a positive integer')
            check_bounds(num_top_hits, 'num_top_hits', 1)
        return self.table\
            .filter(pl.col(significance_column) < threshold)\
            .pipe(lambda df: df.group_by('cell_type', maintain_order=True)
                  .head(num_top_hits) if num_top_hits is not None else df)
    
    def get_num_hits(self,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer |
                                np.floating = 0.05) -> pl.DataFrame:
        """
        Get the number of differentially expressed genes in each cell type.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`

        Returns:
            A DataFrame with one row per cell type and two columns:
            'cell_type' and 'num_hits'.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        return self.table\
            .lazy()\
            .filter(pl.col(significance_column) < threshold)\
            .group_by('cell_type')\
            .agg(num_hits=pl.len())\
            .sort('cell_type')\
            .collect()
    
    # noinspection PyUnresolvedReferences
    def plot_voom(self,
                  cell_type: str,
                  filename: str | Path | None = None,
                  *,
                  ax: 'Axes' | None = None,
                  point_color: Color | dict[str, Color] | None = None,
                  point_size: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1,
                  line_color: Color | dict[str, Color] | None = None,
                  line_width: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1.5,
                  scatter_kwargs: dict[str, Any] | None |
                                  dict[str, dict[str, Any] | None] = None,
                  plot_kwargs: dict[str, Any] | None |
                               dict[str, dict[str, Any] | None] = None,
                  legend: bool = True,
                  legend_kwargs: dict[str, Any] | None = None,
                  title: str | None = None,
                  title_kwargs: dict[str, Any] | None = None,
                  xlabel: str | None = 'Average log2(count + 0.5)',
                  xlabel_kwargs: dict[str, Any] | None = None,
                  ylabel: str | None = 'sqrt(standard deviation)',
                  ylabel_kwargs: dict[str, Any] | None = None,
                  despine: bool = True,
                  savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a voom plot for a cell type that differential expression was
        calculated for.
        
        Voom plots consist of a scatter plot with one point per gene. They
        visualize how the mean expression of each gene across samples (x)
        relates to the gene's variation in expression across samples (y). The
        plot also includes a LOESS (also called LOWESS) curve, a type of
        non-linear curve fit, of the mean-variance (x-y) trend.
        
        Specifically, the x position of a gene's point is the average, across
        samples, of the base-2 logarithm of the gene's count in each sample,
        plus a pseudocount of 0.5: in other words, mean(log2(count + 0.5)).
        The y position is the square root of the standard deviation, across
        samples, of the gene's log counts per million after regressing out,
        across samples, the differential expression design matrix.
        
        When running differential expression with voomByGroup, voom is run
        separately within each group, so the voom plot will show a separate
        LOESS trendline for each group, with the points and trendlines for each
        group shown in distinct colors.
        
        Many arguments to this function can be either a single value or a
        dictionary mapping group names to values. The group names can be viewed
        with `self.groups[cell_type]`.
        
        Args:
            cell_type: the cell type to generate the voom plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            point_color: the color of the points in the voom plot. Can be a
                         single color or a dictionary mapping each of the group
                         names in `self.groups[cell_type]` to colors. When not
                         using voomByGroup, defaults to `'#666666'` (gray).
                         When using voomByGroup with two groups, defaults to
                         `'#666666'` for the first group in
                         `self.groups[cell_type]` and `'#FF6666'` (red) for the
                         second. When using voomByGroup with more than two
                         groups, must be specified manually. Can be any valid
                         Matplotlib color, like a hex string (e.g.
                         `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            point_size: the size of the points in the voom plot. Can be a
                        single number or a dictionary mapping each of the group
                        names in `self.groups[cell_type]` to numbers.
            line_color: the color of the LOESS trendline. Can be a single color
                        or a dictionary mapping each of the group names in
                        `self.groups[cell_type]` to colors. When not using
                        voomByGroup, defaults to `'#000000'` (black). When
                        using voomByGroup with two groups, defaults to
                        `'#000000'` for the first group and `'#FF0000'` (red).
                        for the second. When using voomByGroup with more than
                        two groups, must be specified manually. Can be any
                        valid Matplotlib color, like a hex string (e.g.
                        `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            line_width: the width of the LOESS trendline. Can be a single
                        number or a dictionary mapping each of the group names
                        in `self.groups[cell_type]` to numbers.
            scatter_kwargs: a dictionary (or dictionary mapping each of the
                            group names in `self.groups[cell_type]` to
                            dictionaries) of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              numbers in `color_column` are converted to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s` or `c`/`color`/`norm`/`vmin`/`vmax`
                            will raise an error, since these arguments conflict
                            with the `point_size` and `point_color` arguments,
                            respectively.
            plot_kwargs: a dictionary (or dictionary mapping each of the group
                         names in `self.groups[cell_type]` to dictionaries) of
                         keyword arguments to be passed to `ax.plot()` when
                         plotting the trendlines, such as `linestyle='--'` for
                         dashed trendlines. Specifying `color`/`c` or
                         `linewidth` will raise an error, since these arguments
                         conflict with the `line_color` and `line_width`
                         arguments, respectively.
            legend: whether to add a legend with the colors for each group when
                    using voomByGroup. Only `legend=False` has an effect, and
                    it can only be specified when using voomByGroup. Without 
                    groups, there will never be a legend, so specifying 
                    `legend=False` would be redundant.                     
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border. `frameon` is `False` by default,
                             instead of Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when using voomByGroup with
                           `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the voom plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Check that this DE object contains `voom_plot_data`, the data
        # necessary to generate the voom plot from (scatter-plot points and
        # LOESS trendlines)
        if self.voom_plot_data is None:
            error_message = (
                'this DE object does not contain the voom_plot_data '
                'attribute, which is necessary to generate voom plots; re-run '
                'Pseudobulk.DE() with return_voom_info=True to include this '
                'attribute')
            raise AttributeError(error_message)
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.voom_plot_data:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        # Get the voom plot data for this cell type
        voom_plot_data = self.voom_plot_data[cell_type]
        # Get the voomByGroup groups for this cell type (`None` if voomByGroup
        # was not used)
        groups = [column[5:] for column in voom_plot_data.columns
                  if column[:4] == 'xy_x']
        if len(groups) == 0:
            groups = None
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `point_color` and `line_color` are valid Matplotlib colors
        # or dictionaries thereof, and convert them to hex. Or, if `None`, set
        # to their default value if there are no groups or exactly two groups.
        point_color_is_dict = isinstance(point_color, dict)
        if point_color is None:
            if groups is None:
                point_color = '#666666'
            elif len(groups) == 2:
                point_color = {groups[0]: '#666666', groups[1]: '#FF6666'}
                point_color_is_dict = True
            else:
                error_message = (
                    f'point_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif point_color_is_dict:
            for group, group_point_color in point_color.items():
                if not plt.matplotlib.colors.is_color_like(group_point_color):
                    error_message = (
                        f'point_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            point_color = {
                group: plt.matplotlib.colors.to_hex(group_point_color)
                for group, group_point_color in point_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(point_color):
                error_message = (
                    f'point_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            point_color = plt.matplotlib.colors.to_hex(point_color)
        line_color_is_dict = isinstance(line_color, dict)
        if line_color is None:
            if groups is None:
                line_color = '#000000'
            elif len(groups) == 2:
                line_color = {groups[0]: '#000000', groups[1]: '#FF0000'}
                line_color_is_dict = True
            else:
                error_message = (
                    f'line_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif line_color_is_dict:
            for group, group_line_color in line_color.items():
                if not plt.matplotlib.colors.is_color_like(group_line_color):
                    error_message = (
                        f'line_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            line_color = {
                group: plt.matplotlib.colors.to_hex(group_line_color)
                for group, group_line_color in line_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(line_color):
                error_message = (
                    f'line_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            line_color = plt.matplotlib.colors.to_hex(line_color)
        # Check that `point_size` and `line_width` are positive numbers or
        # dicts thereof
        point_size_is_dict = isinstance(point_size, dict)
        line_width_is_dict = isinstance(line_width, dict)
        for number, number_name, is_dict in (
                (point_size, 'point_size', point_size_is_dict),
                (line_width, 'line_width', line_width_is_dict)):
            if is_dict:
                for group, group_number in number.items():
                    check_type(group_number, f'{number_name}[{group!r}]',
                               (int, float), 'a positive number')
                    check_bounds(group_number, f'{number_name}[{group!r}]', 0,
                                 left_open=True)
            else:
                check_type(number, number_name, (int, float),
                           'a positive number')
                check_bounds(number, number_name, 0, left_open=True)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((scatter_kwargs, 'scatter_kwargs'),
                                    (plot_kwargs, 'plot_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (title_kwargs, 'title_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # If using voomByGroup, for each of `scatter_kwargs` and `plot_kwargs`,
        # if the kwarg is not `None`, check that either all keys are group
        # names and in the correct order, or that no keys are group names. If
        # all keys are group names, check that all values are either `None` or
        # dictionaries with all-string keys, and make note that the kwargs is a
        # nested dict.
        scatter_kwargs_is_nested_dict = False
        plot_kwargs_is_nested_dict = False
        if groups is not None:
            for kwargs, kwargs_name in ((scatter_kwargs, 'scatter_kwargs'),
                                        (plot_kwargs, 'plot_kwargs')):
                if kwargs is not None:
                    if tuple(kwargs) == groups:
                        # The kwargs's keys exactly match the group names
                        for key, value in kwargs.items():
                            if value is not None:
                                check_type(value, f'{kwargs_name}[{key!r}]',
                                           dict, 'a dictionary')
                                for inner_key in value:
                                    if not isinstance(inner_key, str):
                                        error_message = (
                                            f'all keys of '
                                            f'{kwargs_name}[{key!r}] must be '
                                            f'strings, but it contains a key '
                                            f'of type '
                                            f'{type(inner_key).__name__!r}')
                                        raise TypeError(error_message)
                        if kwargs is scatter_kwargs:
                            scatter_kwargs_is_nested_dict = True
                        else:
                            plot_kwargs_is_nested_dict = True
                    else:
                        # Check that none of the kwargs's keys are group names
                        for group in groups:
                            if group in kwargs:
                                if set(groups) == set(kwargs):
                                    error_message = (
                                        f'{kwargs_name}.keys() does have the '
                                        f'same groups as '
                                        f'self.groups[{cell_type!r}], but '
                                        f'they are in a different order')
                                    raise ValueError(error_message)
                                else:
                                    error_message = (
                                        f'some keys of {kwargs_name}.keys() '
                                        f'are groups in '
                                        f'self.groups[{cell_type!r}], but '
                                        f'others are not')
                                    raise ValueError(error_message)
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        if scatter_kwargs_is_nested_dict:
            for key, value in scatter_kwargs.items():
                scatter_kwargs[key] = default_scatter_kwargs | value \
                    if value is not None else default_scatter_kwargs
        else:
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
        # Set `plot_kwargs` to `{}` if it is `None`, or set the `None` values
        # of `plot_kwargs` to `{}` if `plot_kwargs` is a nested dict
        if plot_kwargs is None:
            plot_kwargs = {}
        elif plot_kwargs_is_nested_dict:
            for key, value in plot_kwargs.items():
                if value is None:
                    plot_kwargs[key] = {}
        # Check that `scatter_kwargs` does not contain the `s` or
        # `c`/`color`/`norm`/`vmin`/`vmax` keys and that `plot_kwargs` does
        # not contain the `c`/`color`/`norm`/`vmin`/`vmax` or `linewidth` keys,
        # or that their non-`None` values do not contain these keys if a nested
        # dict
        for kwargs, kwargs_name, alternate_color, is_nested_dict in (
                (scatter_kwargs, 'scatter_kwargs', 'line_color',
                 scatter_kwargs_is_nested_dict),
                (plot_kwargs, 'plot_kwargs', 'point_color',
                 plot_kwargs_is_nested_dict)):
            bad_keys = (('linewidth', 'line_width')
                        if kwargs is plot_kwargs else ('s', 'point_size'),
                        ('c', alternate_color),
                        ('color', alternate_color),
                        ('norm', alternate_color),
                        ('vmin', alternate_color),
                        ('vmax', alternate_color))
            if is_nested_dict:
                for key, value in kwargs.items():
                    if value is not None:
                        for bad_key, alternate_argument in bad_keys:
                            if bad_key in value:
                                error_message = (
                                    f'{bad_key!r} cannot be specified as a '
                                    f'key in {kwargs_name}[{key}!r]; specify '
                                    f'the {alternate_argument} argument '
                                    f'instead')
                                raise ValueError(error_message)
            elif kwargs is not None:
                for bad_key, alternate_argument in bad_keys:
                    if bad_key in kwargs:
                        error_message = (
                            f'{bad_key!r} cannot be specified as a key in '
                            f'{kwargs_name}; specify the {alternate_argument} '
                            f'argument instead')
                        raise ValueError(error_message)
        # Check that `legend` is Boolean. If not using voomByGroup, check that 
        # the user did not specify `legend=False`.
        check_type(legend, 'legend', bool, 'Boolean')
        if groups is None:
            if not legend:
                error_message = (
                    'legend=False cannot be specified when there are no '
                    'groups, since it would be redundant: without groups, '
                    'there will never be a legend')
                raise ValueError(error_message)
        # Override the defaults for certain values of `legend_kwargs`; check
        # that it is `None` when not using a legend
        default_legend_kwargs = dict(frameon=False)
        if legend_kwargs is not None:
            if groups is None:
                error_message = (
                    'legend_kwargs cannot be specified when there are no '
                    'groups, since there will not be a legend')
                raise ValueError(error_message)
            if not legend:
                error_message = \
                    'legend_kwargs cannot be specified when legend=False'
                raise ValueError(error_message)
            legend_kwargs = default_legend_kwargs | legend_kwargs
        else:
            legend_kwargs = default_legend_kwargs
        # If `title` is not `None`, check that it is a string
        if title is not None:
            check_type(title, 'title', str, 'a string')
        # Check that `title_kwargs` is `None` when `title` is `None`
        if title is None and title_kwargs is not None:
            error_message = 'title_kwargs cannot be specified when title=None'
            raise ValueError(error_message)
        # Check that `xlabel` is a string or `None`; if `None`, check that
        # `xlabel_kwargs` is `None` as well. Ditto for `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
        # Override the defaults for certain values of `savefig_kwargs`
        default_savefig_kwargs = \
            dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                 transparent=filename is not None and
                             filename.endswith('.pdf'))
        savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
            if savefig_kwargs is not None else default_savefig_kwargs
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                plt.figure(constrained_layout=True)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            if groups is not None:
                if legend:
                    legend_patches = []
                for group in groups:
                    # Get this group's point size, point color, line color,
                    # line width, plot kwargs, and scatter kwargs
                    group_point_size = point_size[group] \
                        if point_size_is_dict else point_size
                    group_point_color = point_color[group] \
                        if point_color_is_dict else point_color
                    group_line_color = line_color[group] \
                        if line_color_is_dict else line_color
                    group_line_width = line_width[group] \
                        if line_width_is_dict else line_width
                    group_scatter_kwargs = scatter_kwargs[group] \
                        if scatter_kwargs_is_nested_dict else scatter_kwargs
                    group_plot_kwargs = plot_kwargs[group] \
                        if plot_kwargs_is_nested_dict else plot_kwargs
                    # Plot the scatter plot for this group
                    ax.scatter(voom_plot_data[f'xy_x_{group}'].drop_nulls(),
                               voom_plot_data[f'xy_y_{group}'].drop_nulls(),
                               s=group_point_size, c=group_point_color,
                               **group_scatter_kwargs)
                    # Plot the LOESS trendline for this group
                    ax.plot(voom_plot_data[f'line_x_{group}'].drop_nulls(),
                            voom_plot_data[f'line_y_{group}'].drop_nulls(),
                            c=group_line_color, linewidth=group_line_width,
                            **group_plot_kwargs)
                    # Create a rectangle for the legend for this group, where
                    # the border matches the color of the trendline and the
                    # fill matches the color of the scatter plot points
                    if legend:
                        # noinspection PyUnboundLocalVariable
                        # noinspection PyUnresolvedReferences
                        legend_patches.append(plt.matplotlib.patches.Patch(
                            facecolor=group_point_color,
                            edgecolor=group_line_color,
                            linewidth=group_line_width, label=group))
                # Add the legend
                if legend:
                    ax.legend(handles=legend_patches, **legend_kwargs)
            else:
                # Plot the scatter plot
                ax.scatter(voom_plot_data['xy_x'], voom_plot_data['xy_y'],
                           s=point_size, c=point_color, **scatter_kwargs)
                # Plot the LOESS trendline
                ax.plot(voom_plot_data['line_x'], voom_plot_data['line_y'],
                         c=line_color, linewidth=line_width, **plot_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not False:
                if title_kwargs is None:
                    title_kwargs = {}
                # noinspection PyCallingNonCallable
                ax.set_title(title[cell_type] if isinstance(title, dict)
                             else title if isinstance(title, str) else
                             title(cell_type) if isinstance(title, Callable)
                             else cell_type, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    # noinspection PyUnresolvedReferences
    def plot_volcano(self,
                     cell_type: str,
                     filename: str | Path | None = None,
                     *,
                     ax: 'Axes' | None = None,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer | np.floating = 0.05,
                     genes_to_label: int | np.integer | str |
                                     Iterable[str] = 10,
                     label_kwargs: dict[str, Any] | None = None,
                     upregulated_size: int | float | np.integer |
                                       np.floating = 6,
                     downregulated_size: int | float | np.integer |
                                         np.floating = 6,
                     non_significant_size: int | float | np.integer |
                                           np.floating = 4,
                     upregulated_color: Color = '#FC4E07',
                     downregulated_color: Color = '#00AFBB',
                     non_significant_color: Color = 'lightgray',
                     upregulated_scatter_kwargs: dict[str, Any] | None = None,
                     downregulated_scatter_kwargs: dict[str, Any] |
                                                   None = None,
                     non_significant_scatter_kwargs: dict[str, Any] |
                                                     None = None,
                     legend: bool = True,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = '$log_2(FC)$',
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = '$-log_{10}(FDR)$',
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a volcano plot of the DE hits, with negative log FDRs (or
        another `significance_column`) on the y-axis plotted against log fold
        changes on the x-axis. Upregulated, downregulated and non-significant
        genes are plotted in three different colors.
        
        When labeling genes (`genes_to_label != 0`), requires the textalloc
        package (github.com/ckjellson/textalloc) to make sure the labels don't
        overlap. Install it with:
        
        pip install --no-deps --no-build-isolation textalloc
        
        Args:
            cell_type: the cell type to generate the volcano plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            genes_to_label: an integer number of top DE genes to label, a name
                            or sequence of names of genes to label, or `None`
                            to not add labels
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `textalloc.allocate()` when adding gene labels to
                          control the text properties, such as:
                           - `textcolor`/`textsize`: the text color and size
                           - `x_scatter`/`y_scatter`: the x/y coordinates of
                             points in the scatter plot, to repel labels away
                             from. Defaults to all points in the plot.
                           - `min_distance`/`max_distance`: the minimum and
                             maximum distances from each point to its label, as
                             a proportion of the width of the x-axis. Defaults
                             to 0 and 0.02, instead of textalloc's defaults of
                             0.015 and 0.2
                           - `draw_lines`: whether to draw lines between each
                              label and its corresponding point. Defaults to
                              `False`, instead of textalloc's default of
                              `True`.
                          See github.com/ckjellson/textalloc#parameters for the
                          full list of possible arguments. Can only be
                          specified when `genes_to_label` is non-zero.
            upregulated_size: the size of each upregulated gene's point
            downregulated_size: the size of each downregulated gene's point
            non_significant_size: the size of each non-significant gene's point
            upregulated_color: the color of each upregulated gene's point. Can
                               be any valid Matplotlib color, like a hex string
                               (e.g. `'#FF0000'`), a named color (e.g. 'red'),
                               a 3- or 4-element RGB/RGBA tuple of integers
                               0-255 or floats 0-1, or a single float 0-1 for
                               grayscale.
            downregulated_color: the color of each downregulated gene's point.
                                 Can be any valid Matplotlib color, like a hex
                                 string (e.g. `'#FF0000'`), a named color (e.g.
                                 'red'), a 3- or 4-element RGB/RGBA tuple of
                                 integers 0-255 or floats 0-1, or a single
                                 float 0-1 for grayscale.
            non_significant_color: the color of each non-significant gene's
                                   point. Can be any valid Matplotlib color,
                                   like a hex string (e.g. `'#FF0000'`), a
                                   named color (e.g. 'red'), a 3- or 4-element
                                   RGB/RGBA tuple of integers 0-255 or floats
                                   0-1, or a single float 0-1 for grayscale.
            upregulated_scatter_kwargs: a dictionary of keyword arguments to be
                                        passed to `ax.scatter()` for
                                        upregulated genes, such as:
                                        - `rasterized`: whether to convert the
                                          scatter plot points to a raster
                                          (bitmap) image when saving to a
                                          vector format like PDF. Defaults to
                                          `True`, instead of Matplotlib's
                                          default of `False`.
                                        - `marker`: the shape to use for
                                          plotting each gene
                                        - `alpha`: the transparency of each
                                          point
                                        - `linewidths` and `edgecolors`: the
                                          width and color of the borders around
                                          each marker. These are absent by
                                          default (`linewidths=0`), unlike
                                          Matplotlib's default. Both arguments
                                          can be either single values or
                                          sequences.
                                        - `zorder`: the order in which the
                                          genes are plotted, with higher values
                                          appearing on top of lower ones.
                                        Specifying `s` or `c`/`color`/`norm`/
                                        `vmin`/`vmax` will raise an error,
                                        since these arguments conflict with the
                                        `upregulated_size` and
                                        `upregulated_color` arguments,
                                        respectively.
            downregulated_scatter_kwargs: a dictionary of keyword arguments to
                                          be passed to `ax.scatter()` for
                                          downregulated genes; see the
                                          documentation of the
                                          `upregulated_scatter_kwargs` argument
                                          for details
            non_significant_scatter_kwargs: a dictionary of keyword arguments
                                            to be passed to `ax.scatter()` for
                                            non-significant genes; see the
                                            documentation of the
                                            `upregulated_scatter_kwargs`
                                            argument for details
            legend: whether to add a legend showing the marker style for
                    upregulated, downregulated, and non-significant points
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border (`frameon` is `False` by default,
                             instead of Matplotlib's default of `True`)
                           - `title` to add a legend title
                           Can only be specified when `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the volcano plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.table['cell_type']:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        # If `filename` is not `None`, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        # Check that `significance_column` is the name of a floating-point
        # column in `self.table`
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        # Check that `threshold` is greater than 0 and less than or equal to 1
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        # Subset `self.table` to the selected cell type, and log-transform the 
        # significance column and the threshold
        table = self.table.filter(cell_type=cell_type)\
            .with_columns(-pl.col(significance_column).log10())
        threshold = -np.log10(threshold)
        # Check that `genes_to_label` is an integer, a sequence of strings, or
        # `None`. If an integer, take that many gene names.
        if isinstance(genes_to_label, (int, np.integer)):
            label = genes_to_label != 0
            if label:
                x_to_label = table['logFC'][:genes_to_label]
                y_to_label = table[significance_column][:genes_to_label]
                genes_to_label = table['gene'][:genes_to_label]
            elif label_kwargs is not None:
                error_message = (
                    'label_kwargs cannot be specified when genes_to_label=0, '
                    'since no genes are being labeled')
                raise ValueError(error_message)
        else:
            label = True
            if genes_to_label is not None:
                genes_to_label = \
                    to_tuple_checked(genes_to_label, 'genes_to_label', str,
                                     'strings')
                genes_to_label = pl.DataFrame({'gene': genes_to_label})\
                    .join(table.select('gene', 'logFC', significance_column),
                          how='left', on='gene')
                num_missing = genes_to_label['logFC'].null_count()
                if num_missing == len(genes_to_label):
                    error_message = (
                        "none of the specified genes were found in "
                        "table['gene']")
                    raise ValueError(error_message)
                elif num_missing > 0:
                    gene = genes_to_label\
                        .filter(pl.col.logFC.is_null())['gene'][0]
                    error_message = (
                        f"one of the specified genes, {gene!r}, was not found "
                        f"in table['gene']")
                    raise ValueError(error_message)
                x_to_label = genes_to_label['logFC']
                y_to_label = genes_to_label[significance_column]
                genes_to_label = genes_to_label['gene']
        if label:
            # noinspection PyUnresolvedReferences
            import textalloc
        # Check that `upregulated_size`, `downregulated_size`, and
        # `non_significant_size` are positive numbers
        for size, size_name in (upregulated_size, 'upregulated_size'), \
                (downregulated_size, 'downregulated_size'), \
                (non_significant_size, 'non_significant_size'):
            check_type(size, size_name, (int, float), 'a positive number')
            check_bounds(size, size_name, 0, left_open=True)
        # Check that `upregulated_color`, `downregulated_color`, and
        # `non_significant_color` are valid Matplotlib colors, and convert them
        # to hex
        if not plt.matplotlib.colors.is_color_like(upregulated_color):
            error_message = 'upregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        upregulated_color = plt.matplotlib.colors.to_hex(upregulated_color)
        if not plt.matplotlib.colors.is_color_like(downregulated_color):
            error_message = \
                'downregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        downregulated_color = plt.matplotlib.colors.to_hex(downregulated_color)
        if not plt.matplotlib.colors.is_color_like(non_significant_color):
            error_message = \
                'non_significant_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        non_significant_color = \
            plt.matplotlib.colors.to_hex(non_significant_color)
        # Check that the three `scatter_kwargs` arguments do not contain
        # the `s` or `c`/`color`/`cmap`/`norm`/`vmin`/`vmax` keys
        for kwargs, kwargs_prefix in (
                (upregulated_scatter_kwargs, 'upregulated'),
                (downregulated_scatter_kwargs, 'downregulated'),
                (non_significant_scatter_kwargs, 'non_significant')):
            if kwargs is None:
                continue
            if 's' in kwargs:
                error_message = (
                    f"'s' cannot be specified as a key in "
                    f"{kwargs_prefix}_scatter_kwargs; specify the "
                    f"{kwargs_prefix}_size argument instead")
                raise ValueError(error_message)
            for key in 'c', 'color', 'cmap', 'norm', 'vmin', 'vmax':
                if key in kwargs:
                    error_message = (
                        f'{key!r} cannot be specified as a key in '
                        f'scatter_kwargs; specify the {kwargs_prefix}_color '
                        f'argument instead')
                    raise ValueError(error_message)
        # Override the defaults for certain values of the three
        # `scatter_kwargs` arguments
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        upregulated_scatter_kwargs = \
            default_scatter_kwargs | upregulated_scatter_kwargs \
            if upregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        downregulated_scatter_kwargs = \
            default_scatter_kwargs | downregulated_scatter_kwargs \
            if downregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        non_significant_scatter_kwargs = \
            default_scatter_kwargs | non_significant_scatter_kwargs \
            if non_significant_scatter_kwargs is not None else \
                default_scatter_kwargs
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        # For each of the kwargs arguments, if the argument is not `None`,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((label_kwargs, 'label_kwargs'),
                                    (upregulated_scatter_kwargs,
                                     'upregulated_scatter_kwargs'),
                                    (downregulated_scatter_kwargs,
                                     'downregulated_scatter_kwargs'),
                                    (non_significant_scatter_kwargs,
                                     'non_significant_scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        # Check that `legend` and `despine` are Boolean
        check_type(legend, 'legend', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                plt.figure(constrained_layout=True)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            # Make the volcano plot
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC > 0)
                       .to_numpy()
                       .T, s=upregulated_size, c=upregulated_color,
                       label='Upregulated', **upregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC < 0)
                       .to_numpy()
                       .T, s=downregulated_size, c=downregulated_color,
                       label='Downregulated', **downregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) < threshold)
                       .to_numpy()
                       .T, s=non_significant_size, c=non_significant_color,
                       label='Non-significant',
                       **non_significant_scatter_kwargs)
            ax.set_ylim(bottom=0)
            # Add labels, using textalloc to avoid overlap
            if label:
                # noinspection PyUnboundLocalVariable
                default_label_kwargs = dict(
                    ax=ax, x=x_to_label, y=y_to_label,
                    text_list=genes_to_label,
                    x_scatter=table['logFC'].to_numpy(), 
                    y_scatter=table[significance_column].to_numpy(),
                    min_distance=0, max_distance=0.02, draw_lines=False)
                label_kwargs = default_label_kwargs | label_kwargs \
                    if label_kwargs is not None else default_label_kwargs
                textalloc.allocate(**label_kwargs)
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if legend:
                default_legend_kwargs = dict(frameon=False)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                ax.legend(**legend_kwargs)
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
