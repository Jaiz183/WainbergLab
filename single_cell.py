from __future__ import annotations
import os
import operator
import re
import signal
import sys
import warnings
from collections.abc import Iterable
from contextlib import contextmanager
from decimal import Decimal
from datetime import date, datetime, time, timedelta
from functools import cache, reduce
from itertools import chain, islice, pairwise
from pathlib import Path
from subprocess import run
from textwrap import fill
from threadpoolctl import threadpool_limits
from timeit import default_timer
from typing import Any, Callable, Dict, ItemsView, KeysView, Literal, \
    Mapping, Sequence, ValuesView, Union

if 'logging' not in sys.modules and hasattr(os, 'register_at_fork'):
    # Python's `logging` module calls `os.register_at_fork()` to handle aspects
    # of its internal locking. Unfortunately, this can cause KeyboardInterrupts
    # to be repeatedly ignored during HDF5 loading with errors like:
    # Exception ignored in: <function _releaseLock at 0x7fb85797c5e0>
    # Traceback (most recent call last):
    #   File ".../logging/__init__.py", line 243, in _releaseLock
    #     def _releaseLock():
    # To get around this bug, temporarily monkeypatch `os.register_at_fork()`
    # to be a null-op, then `import logging`. Unfortunately, there's no way to
    # unregister a callback created with `os.register_at_fork()`, so this "fix"
    # only works when the `logging` module has not been imported yet.
    original_register_at_fork = os.register_at_fork
    os.register_at_fork = lambda *args, **kwargs: None
    try:
        import logging
    finally:
        original_register_at_fork = os.register_at_fork


class ignore_sigint:
    """
    Ignore Ctrl + C when importing certain modules, to avoid errors due to
    incomplete imports.
    """
    def __enter__(self):
        signal.signal(signal.SIGINT, signal.SIG_IGN)
    
    def __exit__(self, *_):
        signal.signal(signal.SIGINT, signal.default_int_handler)


with ignore_sigint():
    import h5py
    import numpy as np
    import polars as pl
    from scipy import sparse
    from scipy.sparse._compressed import _cs_matrix
    from scipy.sparse.linalg import LinearOperator, svds
    from scipy.special import stdtrit
    from scipy.stats import rankdata


Color = Union[str, float, np.floating,
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer]],
              tuple[Union[int, np.integer], Union[int, np.integer],
                    Union[int, np.integer], Union[int, np.integer]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating]],
              tuple[Union[float, np.floating], Union[float, np.floating],
                    Union[float, np.floating], Union[float, np.floating]]]
Indexer = Union[int, np.integer, str, slice,
                np.ndarray[1, Union[np.dtype[np.integer], np.dtype[np.bool_]]],
                pl.Series, list[Union[int, np.integer, str, bool, np.bool_]]]
Scalar = Union[str, int, float, Decimal, date, time, datetime, timedelta, bool,
               bytes]
UnsDict = \
    Dict[str, Union[str, int, np.integer, float, np.floating, bool, np.bool_,
                    np.ndarray[Any, Any], 'UnsDict']]
UnsItem = Union[str, int, np.integer, float, np.floating, bool, np.bool_,
                np.ndarray[Any, Any], UnsDict]
SingleCellColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['SingleCell'], Union[pl.Series, np.ndarray]]]
PseudobulkColumn = \
    Union[str, pl.Expr, pl.Series, np.ndarray,
          Callable[['Pseudobulk', str], Union[pl.Series, np.ndarray]]]


def array_equal(a1: np.ndarray[Any, Any], a2: np.ndarray[Any, Any]) -> bool:
    """
    Tests whether two NumPy arrays are equal. NaNs will always compare equal.
    
    Args:
        a1: the first input array
        a2: the second input array

    Returns:
        Whether the two arrays are equal.
    """
    return np.array_equal(a1, a2,
                          equal_nan=a1.dtype != object and a2.dtype != object)


def bincount(x: np.ndarray[1, np.dtype[np.uint32]],
             *,
             num_bins: int | np.integer,
             num_threads: int | np.integer,
             counts: np.ndarray[1, np.dtype[np.uint32]] | None = None) -> \
        np.ndarray[1, np.dtype[np.uint32]]:
    """
    A faster version of `numpy.bincount` for integer data that uses a fixed
    number of bins, lacks weights and supports multithreading.
    
    Args:
        x: a 1D `uint32` NumPy array
        num_bins: the number of bins
        num_threads: the number of threads to use when counting
        counts: an optional preallocated array to store the bin counts in;
                assumed to be the correct size

    Returns:
        A 1D `uint32` NumPy array of length `num_bins`, containing the bin
        counts.
    """
    if counts is None:
        counts = np.empty(num_bins, dtype=np.uint32)
    cython_inline(r'''
        from cython.parallel cimport parallel, threadid
        from libcpp.vector cimport vector
        
        ctypedef fused integer:
            unsigned
            int
            long
        
        def bincount(const integer[::1] arr,
                     unsigned[::1] counts,
                     const unsigned num_threads):
            cdef unsigned long start, end, i, num_bins, chunk_size, \
                num_elements = arr.shape[0]
            cdef unsigned thread_index
            cdef vector[vector[unsigned]] thread_counts
            cdef unsigned* counts_pointer
            
            if num_threads == 1:
                counts[:] = 0
                for i in range(num_elements):
                    counts[arr[i]] += 1
            else:
                # Store counts for each thread in a temporary buffer, then
                # aggregate at the end. As an optimization, put the counts for
                # the last thread (`thread_index == num_threads - 1`) directly
                # into the final `counts` array.
                thread_counts.resize(num_threads - 1)
                num_bins = counts.shape[0]
                chunk_size = (num_elements + num_threads - 1) / num_threads
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    start = thread_index * chunk_size
                    if thread_index == num_threads - 1:
                        end = num_elements
                        counts[:] = 0
                        for i in range(start, end):
                            counts[arr[i]] += 1
                    else:
                        thread_counts[thread_index].resize(num_bins)
                        counts_pointer = thread_counts[thread_index].data()
                        end = start + chunk_size
                        for i in range(start, end):
                            counts_pointer[arr[i]] += 1
                
                # Aggregate counts from all threads except the last
                for thread_index in range(num_threads - 1):
                    counts_pointer = thread_counts[thread_index].data()
                    for i in range(num_bins):
                        counts[i] += counts_pointer[i]
        ''', warn_undeclared=False)['bincount'](
            arr=x, counts=counts, num_threads=num_threads)
    return counts


def bonferroni(pvalues: pl.Expr) -> pl.Expr:
    """
    Performs Bonferroni correction on a polars expression of p-values.
    
    Args:
        pvalues: a polars expression; may contain missing data

    Returns:
        A polars expression of Bonferroni-corrected p-values.
    """
    return (pvalues * (pvalues.len() - pvalues.null_count()))\
        .clip(upper_bound=1)


def check_bounds(variable: Any,
                 variable_name: str,
                 lower_bound: int | np.integer | None = None,
                 upper_bound: int | np.integer | None = None,
                 *,
                 left_open: bool = False,
                 right_open: bool = False) -> None:
    """
    Check whether `variable` is between lower bound and upper bound, inclusive.
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        lower_bound: the smallest allowed value for variable, or None to have
                     no lower bound
        upper_bound: the largest allowed value for variable, or None to have no
                     upper bound
        left_open: if True, require variable to be strictly greater than
                   lower_bound, rather than >= lower_bound; has no effect if
                   lower_bound is None
        right_open: if True, require variable to be strictly less than
                    upper_bound, rather than <= upper_bound; has no effect if
                    upper_bound is None
    """
    if lower_bound is None and upper_bound is None:
        error_message = 'lower_bound and upper_bound cannot both be None'
        raise ValueError(error_message)
    if lower_bound is not None and (variable <= lower_bound if left_open
                                    else variable < lower_bound) or \
            upper_bound is not None and (variable >= upper_bound if right_open
                                         else variable > upper_bound):
        error_message = f'{variable_name} is {variable:,}, but must be'
        if lower_bound is not None:
            error_message += f' {">" if left_open else "≥"} {lower_bound:,}'
            if upper_bound is not None:
                error_message += ' and'
        if upper_bound is not None:
            error_message += f' {"<" if right_open else "≤"} {upper_bound:,}'
        raise ValueError(error_message)


def check_dtype(series: pl.Series,
                series_name: str,
                expected_dtypes: pl.datatypes.classes.DataTypeClass | str |
                                 tuple[pl.datatypes.classes.DataTypeClass |
                                       str, ...]) -> None:
    """
    Check whether `series` has the expected polars dtype.
    
    Args:
        series: the polars Series to be checked
        series_name: the name of the variable, used in the error message
        expected_dtypes: the expected dtype or dtypes. Specify the string
                        `'integer'` to include all integer dtypes, and
                        `'floating-point'` to include all floating-point
                        dtypes.
    """
    base_type = series.dtype.base_type()
    if not isinstance(expected_dtypes, tuple):
        expected_dtypes = expected_dtypes,
    for expected_type in expected_dtypes:
        if base_type == expected_type or expected_type == 'integer' and \
                base_type in pl.INTEGER_DTYPES or \
                expected_type == 'floating-point' and \
                base_type in pl.FLOAT_DTYPES:
            return
    if len(expected_dtypes) == 1:
        expected_dtypes = str(expected_dtypes[0])
    elif len(expected_dtypes) == 2:
        expected_dtypes = ' or '.join(map(str, expected_dtypes))
    else:
        expected_dtypes = ', '.join(map(str, expected_dtypes[:-1])) + \
                          ', or ' + str(expected_dtypes[-1])
    error_message = (
        f'{series_name} must be {expected_dtypes}, but has data type '
        f'{base_type!r}')
    raise TypeError(error_message)


def check_R_variable_name(
        R_variable_name: str,
        variable_name: str,
        R_keywords: set[str] = {
            'if', 'else', 'repeat', 'while', 'function', 'for', 'in', 'next',
            'break', 'TRUE', 'FALSE', 'NULL', 'Inf', 'NaN', 'NA',
            'NA_integer_', 'NA_real_', 'NA_complex_', 'NA_character_',
            '...'}) -> None:
    """
    Check whether `R_variable_name` is a valid variable name in R.
    
    Args:
        R_variable_name: the R variable name to be checked
        variable_name: the name of the Python variable the R variable name
                       `R_variable_name` is stored in
        R_keywords: the set of R reserved keywords to check against
    """
    import re
    if not R_variable_name:
        error_message = f'{variable_name} is an empty string'
        raise ValueError(error_message)
    if R_variable_name[0] == '.':
        if len(R_variable_name) > 1 and R_variable_name[1].isdigit():
            error_message = (
                f'{variable_name} {R_variable_name!r} starts with a period '
                f'followed by a digit, which is not a valid R variable name')
            raise ValueError(error_message)
    elif not R_variable_name[0].isidentifier():
        error_message = (
            f'{variable_name} {R_variable_name!r} must start with a letter, '
            f'number, period or underscore')
        raise ValueError(error_message)
    if not re.fullmatch(r'[\w.]*', R_variable_name[1:]):
        invalid_characters = \
            sorted(set(re.findall(r'[^\w.]',
                                  ''.join(dict.fromkeys(R_variable_name)))))
        if len(invalid_characters) == 1:
            description = f"the character '{invalid_characters[0]}'"
        else:
            description = f"the characters " + ", ".join(
                f"'{character}'" for character in invalid_characters) + \
                f" and '{invalid_characters[-1]}'"
        error_message = (
            f'{variable_name} {R_variable_name!r} contains {description}, but '
            f'must contain only letters, numbers, periods and underscores')
        raise ValueError(error_message)
    if R_variable_name in R_keywords or (R_variable_name.startswith('..') and
                                         R_variable_name[2:].isdigit()):
        error_message = (
            f'{variable_name} {R_variable_name!r} is a reserved keyword in R, '
            f'and cannot be used as a variable name')
        raise ValueError(error_message)


def check_type(variable: Any, variable_name: str,
               expected_types: type | tuple[type, ...],
               expected_type_name: str) -> None:
    """
    Check whether `variable` has the expected type.
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        expected_types: the expected type or types (specifying int, float, or
                        bool also implicitly includes their NumPy equivalents)
        expected_type_name: the name of the expected type, used in the error
                            message (e.g. 'a polars DataFrame')
    """
    if isinstance(variable, expected_types):
        return
    if not isinstance(expected_types, tuple):
        expected_types = expected_types,
    for t in expected_types:
        if t is int:
            if isinstance(variable, np.integer):
                return
        elif t is float:
            if isinstance(variable, np.floating):
                return
        elif t is bool:
            if isinstance(variable, np.bool_):
                return
    error_message = (
        f'{variable_name} must be {expected_type_name}, but has type '
        f'{type(variable).__name__!r}')
    raise TypeError(error_message)


def check_types(variable: Iterable[Any],
                variable_name: str,
                expected_types: type | tuple[type, ...],
                expected_type_name: str):
    """
    Check whether all elements of `variable` are of the expected type(s).
    
    Args:
        variable: the variable to be checked
        variable_name: the name of the variable, used in the error message
        expected_types: the expected type or types
        expected_type_name: the name of the expected type, used in the error
                            message (e.g. 'polars DataFrames')
    """
    if not isinstance(expected_types, tuple):
        expected_types = expected_types,
    for element in variable:
        if not isinstance(element, expected_types):
            for t in expected_types:
                if t is int:
                    if isinstance(variable, np.integer):
                        break
                elif t is float:
                    if isinstance(variable, np.floating):
                        break
                elif t is bool:
                    if isinstance(variable, np.bool_):
                        break
            else:
                error_message = (
                    f'all elements of {variable_name} must be '
                    f'{expected_type_name}, but it contains an element of '
                    f'type {type(element).__name__!r}')
                raise TypeError(error_message)


def concatenate(arrays: Sequence[np.ndarray[1, Any]] |
                        Sequence[np.ndarray[2, Any]],
                *,
                num_threads: int) -> np.ndarray[1, Any] | np.ndarray[2, Any]:
    """
    Concatenate 1D or 2D C-contiguous dense arrays.
    
    Equivalent to `np.concatenate(axis=0)`, but supports multithreading.
    
    Args:
        arrays: the arrays to concatenate
        num_threads: the number of threads to use when concatenating

    Returns:
        The concatenated array.
    """
    concatenate = cython_inline(_uninitialized_vector_import + r'''
        import numpy as np
        cimport numpy as np
        from cython.parallel cimport parallel, threadid
        from libc.string cimport memcpy
        from libcpp.vector cimport vector
        
        def concatenate(list arrays, const unsigned num_threads):
            cdef unsigned array_index, thread_index, chunk_size = 64, \
                itemsize = arrays[0].itemsize, num_arrays = len(arrays)
            cdef unsigned long num_bytes, num_chunks, num_rows, start_chunk, \
                end_chunk, current_chunk, array_offset, total_bytes = 0, \
                total_chunks = 0, total_rows = 0
            cdef uninitialized_vector[unsigned long] byte_count_buffer, \
                chunk_count_buffer, byte_offset_buffer, chunk_offset_buffer
            cdef uninitialized_vector[char*] array_pointers
            cdef unsigned long[::1] byte_count, chunk_count, byte_offset, chunk_offset
            cdef char* output_pointer
            cdef char* src
            cdef char* dest
            cdef np.ndarray array, output
            
            if num_threads == 1:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                byte_count_buffer.resize(num_arrays)
                byte_count = <unsigned long[:num_arrays]> byte_count_buffer.data()
                
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data, after the first 0
                    array_pointers[array_index] = <char*> array.data
                    
                    # 2) the number of bytes per array
                    byte_count[array_index] = array.size * itemsize
                    
                    # 3) the total number of rows across all arrays
                    total_rows += array.shape[0]
                
                # Create the output array and get a pointer to its data
                if array.ndim == 1:
                    output = np.empty(total_rows, dtype=array.dtype)
                else:
                    output = np.empty((total_rows, array.shape[1]),
                                      dtype=array.dtype)
                output_pointer = <char*> output.data
                
                # Copy the data from each input array to the output array
                for array_index in range(num_arrays):
                    memcpy(output_pointer, array_pointers[array_index],
                           byte_count[array_index])
                    output_pointer += byte_count[array_index]
            else:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                byte_count_buffer.resize(num_arrays)
                byte_count = <unsigned long[:num_arrays]> byte_count_buffer.data()
                chunk_count_buffer.resize(num_arrays)
                chunk_count = <unsigned long[:num_arrays]> chunk_count_buffer.data()
                byte_offset_buffer.resize(num_arrays)
                byte_offset = <unsigned long[:num_arrays]> byte_offset_buffer.data()
                chunk_offset_buffer.resize(num_arrays)
                chunk_offset = <unsigned long[:num_arrays]> chunk_offset_buffer.data()
            
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data
                    array_pointers[array_index] = <char*> array.data
                    
                    # 2) the number of bytes per array
                    num_bytes = array.size * itemsize
                    byte_count[array_index] = num_bytes
                    
                    # 3) the number of `chunk_size`-byte chunks per array, rounding up
                    num_chunks = (num_bytes - 1) / chunk_size + 1
                    chunk_count[array_index] = num_chunks
                    
                    # 4) the total number of bytes and chunks up to the start of each array
                    byte_offset[array_index] = total_bytes
                    chunk_offset[array_index] = total_chunks
                    
                    # 5) the total number of bytes, chunks, and rows across all arrays
                    total_bytes += num_bytes
                    total_chunks += num_chunks
                    total_rows += array.shape[0]
            
                # Ensure each thread has at least one chunk to work on
                if total_chunks < num_threads:
                    num_threads = total_chunks
            
                # Create the output array and get a pointer to its data
                if array.ndim == 1:
                    output = np.empty(total_rows, dtype=array.dtype)
                else:
                    output = np.empty((total_rows, array.shape[1]),
                                      dtype=array.dtype)
                output_pointer = <char*> output.data
            
                # Distribute threads across arrays, proportional to their number of chunks.
                # Each thread operates on a contiguous range of chunks, which may span
                # multiple arrays.
                with nogil, parallel(num_threads=num_threads):
                    # Get the thread index
                    thread_index = threadid()
            
                    # Calculate the range of chunks this thread is responsible for
                    start_chunk = (thread_index * total_chunks) / num_threads
                    end_chunk = ((thread_index + 1) * total_chunks) / num_threads
                    
                    # Find the (first) array that corresponds to this thread's chunk range
                    array_index = 0
                    while chunk_offset[array_index] + \
                            chunk_count[array_index] <= start_chunk:
                        array_index = array_index + 1
                        
                    # Copy data from the input arrays to the output array
                    current_chunk = start_chunk
                    while current_chunk < end_chunk:
                        # Calculate the chunk offset within the current array
                        array_offset = current_chunk - chunk_offset[array_index]
                        
                        # Calculate the source and destination pointers
                        src = array_pointers[array_index] + array_offset * chunk_size
                        dest = output_pointer + byte_offset[array_index] + \
                            array_offset * chunk_size
            
                        # Calculate the number of chunks to copy from the current array:
                        # min(remaining in array, remaining in thread)
                        num_chunks = min(chunk_count[array_index] - array_offset,
                                         end_chunk - current_chunk)
                        
                        # Get the number of bytes to copy from the current array:
                        # min(num_chunks * chunk_size, remaining bytes in array)
                        num_bytes = min(num_chunks * chunk_size,
                                        byte_count[array_index] - array_offset * chunk_size)
    
                        # Copy the data
                        memcpy(dest, src, num_bytes)
            
                        # Update the position and array index
                        current_chunk = current_chunk + num_chunks
                        array_index = array_index + 1
        
            return output
        ''', warn_undeclared=False)['concatenate']
    return concatenate(arrays, num_threads)


@cache
def cython_inline(code, debug=False, boundscheck=None, cdivision=True,
                  initializedcheck=None, wraparound=False,
                  warn_undeclared=True, include_dirs=None, libraries=None,
                  clang=False, extra_compiler_flags=None,
                  extra_linker_flags=None, verbose=False,
                  **other_cython_settings):
    """
    A drop-in replacement for `cython.inline()` that supports cimports. It
    turns on the major Cython optimizations (`boundscheck=False`,
    `cdivision=True`, `initializedcheck=False`, `wraparound=False`) and sets
    `language_level=3` for full Python 3 compatibility.
    
    Args:
        code: a string of Cython code to compile
        debug: whether to turn off most compiler optimizations (`-Ofast`,
               `-funroll-loops`) and turn on debug compilation (`-g -Og`) and
               Cython's boundscheck and initializedcheck
        boundscheck: whether to perform array bounds checking when indexing;
                     always affects array/memoryview indexing, but also affects
                     list, tuple, and string indexing when wraparound=False
        cdivision: whether to use C-style rather than Python-style division and
                   remainder operations; disabling leads to a ~35% speed
                   penalty for these operations
        initializedcheck: whether to check whether memoryviews and C++ classes
                          are initialized before using them
        wraparound: whether to support Python-style negative indexing
        warn_undeclared: whether to warn about undeclared variables (i.e. those
                         without a cdef type declaration)
        include_dirs: an optional tuple of include directories of libraries to
                      link against; `np.get_include()` will always be included
        libraries: an optional tuple of libraries to link against,
                   e.g. ('hdf5',)
        clang: whether to compile with clang instead of GCC
        extra_compiler_flags: a tuple of extra compiler flags to use on top
                              of the defaults
        extra_linker_flags: a tuple of extra linker flags to use on top of
                            the defaults
        verbose: if True, print Cython's compilation logs
        **other_cython_settings: other Cython settings, which will be written
                                 into the source code as #cython compiler
                                 directives
    
    Returns:
        The {function_name: function} dictionary of compiled functions that
        would be returned by cython.inline().
    """
    from hashlib import md5
    from inspect import getmembers
    from textwrap import dedent
    
    # ~ is read-only on Niagara compute nodes, so build in CYTHON_CACHE_DIR in
    # scratch instead
    cython_cache_dir = os.path.abspath(os.environ.get(
        'CYTHON_CACHE_DIR', os.path.expanduser('~/.cython')))
    os.makedirs(cython_cache_dir, exist_ok=True)
    
    # If `boundscheck` and/or `initializedcheck` are `None`, turn them on if
    # `debug` is `True`, and turn them off otherwise
    if boundscheck is None:
        boundscheck = debug
    if initializedcheck is None:
        initializedcheck = debug
        
    # Remove extra levels of indentation from the code string (since it's
    # usually defined inside a function, so there's at least one extra level of
    # indentation that would cause a syntax error if not removed) and remove
    # any leading newlines if present (since when users define code strings,
    # they usually use triple-quoted strings, and the code usually doesn't
    # start until the line after the three opening quotes, leading to a single
    # leading newline)
    settings = dict(language_level=3, boundscheck=boundscheck,
                    cdivision=cdivision, initializedcheck=initializedcheck,
                    wraparound=wraparound,
                    **{'warn.undeclared': warn_undeclared})
    settings.update(other_cython_settings)
    code = ''.join(f'#cython: {setting_name}={setting}\n'
                   for setting_name, setting in settings.items()) + \
                   f'#distutils: define_macros=NPY_NO_DEPRECATED_API=' \
                   f'NPY_1_7_API_VERSION\n' + dedent(code)
    
    # Define build options and environment variables
    if include_dirs is not None:
        include_dirs = (np.get_include(),) + include_dirs
    else:
        include_dirs = np.get_include(),
    include_dirs = \
        '[' + ', '.join(f'{include_dir!r}'
                        for include_dir in include_dirs) + ']'
    if libraries is not None:
        libraries = \
            f'[' + ', '.join(f'{library!r}' for library in libraries) + ']'
    narval = os.environ.get('CLUSTER') == 'narval'
    niagara = os.environ.get('CLUSTER') == 'niagara'
    march = 'znver2' if narval else 'skylake' if niagara else 'native'
    extra_compiler_flags = \
        ''.join(f', {flag!r}' for flag in extra_compiler_flags) \
        if extra_compiler_flags is not None else ''
    extra_linker_flags = \
        ''.join(f', {flag!r}' for flag in extra_linker_flags) \
        if extra_linker_flags is not None else ''
    build_options = f'''
                language='c++',
                include_dirs={include_dirs},
                libraries={libraries},
                extra_compile_args=['-g', '-Og', '-march={march}', '-fopenmp',
                                    '-Wall', '-Wextra', '-Wpedantic',
                                    '-Werror', '-Wno-maybe-uninitialized',
                                    '-Wno-ignored-qualifiers'
                                    {extra_compiler_flags}]
                    if {debug} else ['-Ofast', '-march={march}',
                                     '-funroll-loops', '-fopenmp', '-Wall',
                                     '-Wextra', '-Wpedantic', '-Werror',
                                     '-Wno-maybe-uninitialized',
                                     '-Wno-ignored-qualifiers'
                                     {extra_compiler_flags}],
                extra_link_args=['-fopenmp'{extra_linker_flags}] if {debug}
                                else
                                ['-Ofast', '-fopenmp'{extra_linker_flags}]'''
    build_environment_variables = \
        f'CXX={"clang++" if clang else "g++"}{" CFLAGS=-g " if debug else ""}'
    
    # Make a short alphabetic module name by taking the MD5 hash of the
    # concatenated code, build options, and build environment variables, and
    # converting the hexadecimal digits to letters (0 -> a, 1 -> b, ...,
    # 9 --> j, a --> k, ..., f --> p)
    module_name = ''.join(chr(ord(c) + (49 if c <= '9' else 10))
                          for c in md5((code + build_options +
                                        build_environment_variables)
                                       .encode('utf-8')).hexdigest())
    code_file = os.path.join(cython_cache_dir, f'{module_name}.pyx')
    
    # Try to import the module; build it if it does not exist
    sys.path.append(cython_cache_dir)
    try:
        module = __import__(module_name)
    except ModuleNotFoundError:
        # Create the code file
        with open(code_file, 'w') as f:
            # noinspection PyTypeChecker
            print(code, file=f)
        
        # Write the build script to a temp file based on the module name
        build_file = os.path.join(cython_cache_dir, f'{module_name}_build.py')
        with open(build_file, 'w') as f:
            build_script = dedent(f'''
                from setuptools import Extension, setup
                from Cython.Build import cythonize
                setup(name='{module_name}', ext_modules=cythonize([
                    Extension('{module_name}', ['{code_file}'],
                              {build_options})],
                    build_dir='{cython_cache_dir}'))''')
            # noinspection PyTypeChecker
            print(build_script, file=f)
        
        # Build the code (note: `sys.executable` is the location of Python)
        run(f'cd {cython_cache_dir} && {build_environment_variables} '
            f'{sys.executable} {build_file} build_ext --inplace'
            f'{"" if verbose else " > /dev/null"}', check=True, shell=True)
        
        # Remove the temp file
        os.unlink(build_file)
        
        # Try again
        module = __import__(module_name)
    finally:
        sys.path = sys.path[:-1]
    
    # Create a dict of all the Cython functions defined in the module
    function_dict = {function_name: function
                     for function_name, function in getmembers(module)
                     if repr(function).startswith('<cyfunction')}
   
    # Return the dict of Cython functions
    return function_dict


def fdr(pvalues: pl.Expr) -> pl.Expr:
    """
    Performs FDR correction on a polars expression of p-values.
    
    Args:
        pvalues: a polars expression; may contain missing data

    Returns:
        A polars expression of FDR q-values.
    """
    num_null = pvalues.null_count().cast(pl.Int64)
    num_non_null = pvalues.len() - pvalues.null_count()
    reverse_order = pvalues.arg_sort(descending=True, nulls_last=True)
    return (pvalues.gather(reverse_order) /
            (pl.int_range(num_non_null, -num_null, -1) / num_non_null))\
        .cum_min()\
        .gather(reverse_order.arg_sort())


def filter_columns(df: pl.DataFrame,
                   predicates: pl.Expr,
                   *more_predicates: pl.Expr) -> pl.DataFrame:
    """
    Selects columns from a polars DataFrame where all the Boolean expressions
    in `predicates` evaluate to `True`, like `filter()` but for columns instead
    of rows. Use it in method chains, e.g.
    `df.pipe(filter_columns, pl.all().n_unique() > 1)`.
    
    Args:
        df: a polars DataFrame
        predicates: the Boolean expressions to filter on
        *more_predicates: additional Boolean expressions, specified as
                          positional arguments

    Returns:
        `df`, filtered to the columns where all the Boolean expressions in
        `predicates` evaluate to `True`.
    """
    predicates = to_tuple(predicates) + more_predicates
    boolean_expression = reduce(lambda a, b: a & b, predicates)
    return df.pipe(lambda df: df.select(df.select(boolean_expression)
                                        .unpivot()
                                        .filter(pl.col.value)
                                        ['variable']
                                    .to_list()))


def generate_palette(num_colors: int | np.integer,
                     *,
                     lightness_range: tuple[
                         int | float | np.integer | np.floating,
                         int | float | np.integer | np.floating] =
                        (100 / 3, 200 / 3),
                     chroma_range: tuple[
                         int | float | np.integer | np.floating,
                         int | float | np.integer | np.floating] = (50, 100),
                     hue_range: tuple[
                        int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
                     first_color: str = '#008cb9',
                     stride: int | np.integer = 5):
    """
    Generate a maximally perceptually distinct color palette.
    
    The first color in the palette is `first_color`. The second color is the
    color that's most perceptually distinct from `first_color`, i.e. has the
    largest distance from it in the perceptually uniform CAM02-UCS color space.
    The third color is the color that has the largest distance from either of
    the first two colors, i.e. the color that maximizes the minimum distance
    to any of the colors currently in the palette. And so on.
    
    An optimized version of github.com/taketwo/glasbey that only generates R,
    G, and B values of (0, 5, 10, ..., 255) instead of (0, 1, 2, ..., 255).
    You can change this stride (by default 5) with the `stride` paramter.
    
    Args:
        num_colors: the number of colors to include in the palette
        lightness_range: a two-element tuple with the lightness range of colors
                         to generate, or None to take the full range:
                         `(0, 100)`
        chroma_range: a two-element tuple with the chroma range of colors to
                      generate, or None to take the full range: `(0, 100)`.
                      Grays have low chroma, and vivid colors have high chroma.
        hue_range: a two-element tuple with the hue range of colors to
                   generate, or None to take the full range: `(0, 360)`. Red is
                   at 0°, green at 120°, and blue at 240°. Because it wraps
                   around, the first element of the tuple can be greater than
                   the second, unlike for `lightness_range` and `chroma_range`.
        first_color: the first color of the palette. Can be any valid
                     Matplotlib color, like a hex string (e.g. `'#FF0000'`), a
                     named color (e.g. 'red'), a 3- or 4-element RGB/RGBA tuple
                     of integers 0-255 or floats 0-1, or a single float 0-1 for
                     grayscale.
        stride: as an optimization, consider only RGB colors where R, G, and B
                are all multiples of this value. Must be a small divisor of
                255: 1, 3, 5, 15, or 17. Set to 1 for the best possible
                solution, at orders of magnitude more computational cost.
    
    Returns:
        A list of hex codes like `'#A06B72'`, with `first_color` as the first
        hex code.
    """
    from colorspacious import cspace_convert
    from matplotlib.colors import is_color_like, to_hex, to_rgb
    
    # Check ranges
    for argument, argument_name, max_value in (
            (lightness_range, 'lightness_range', 100),
            (chroma_range, 'chroma_range', 100),
            (hue_range, 'hue_range', 360)):
        if argument is not None:
            check_type(argument, argument_name, tuple, 'a two-element tuple')
            if len(argument) != 2:
                error_message = (
                    f'{argument_name} must be a two-element tuple, but has '
                    f'{len(argument):,} elements')
                raise ValueError(error_message)
            for i in range(2):
                check_type(argument[i], f'{argument_name}[i]', (int, float),
                           f'a number between 0 and {max_value}, inclusive')
            if argument[0] < 0:
                error_message = f'{argument_name}[0] must be ≥ 0'
                raise ValueError(error_message)
            if argument[1] > max_value:
                error_message = f'{argument_name}[1] must be ≤ {max_value}'
                raise ValueError(error_message)
            if argument is not hue_range and argument[0] > argument[1]:
                error_message = \
                    f'{argument_name}[0] must be ≤ {argument_name}[1]'
                raise ValueError(error_message)
    
    # Check that `first_color` is a valid Matplotlib color, and convert it to
    # RGB and then to the perceptually uniform CAM02-UCS color space
    if not is_color_like(first_color):
        error_message = 'first_color is not a valid Matplotlib color'
        raise ValueError(error_message)
    first_color = to_rgb(first_color)
    first_color = cspace_convert(first_color, 'sRGB1', 'CAM02-UCS')
    if lightness_range is not None or chroma_range is not None or \
            hue_range is not None:
        lightness, chroma, hue = \
            cspace_convert(first_color, 'CAM02-UCS', 'JCh')
        if lightness_range is not None and \
                not lightness_range[0] <= lightness <= lightness_range[1]:
            error_message = (
                f'first_color has a lightness of {lightness}, outside the '
                f'specified lightness_range of {lightness_range}')
            raise ValueError(error_message)
        if chroma_range is not None and \
                not chroma_range[0] <= chroma <= chroma_range[1]:
            error_message = (
                f'first_color has a chroma of {chroma}, outside the specified '
                f'chroma_range of {chroma_range}')
            raise ValueError(error_message)
        if hue_range is not None and not (hue_range[0] <= hue <= hue_range[1]
                                          if hue_range[0] <= hue_range[1] else
                                          hue_range[0] <= hue or
                                          hue <= hue_range[1]):
            error_message = (
                f'first_color has a hue of {hue}, outside the specified '
                f'hue_range of {hue_range}')
            raise ValueError(error_message)
    
    # Check `stride`
    check_type(stride, 'stride', int, 'one of the integers 1, 3, 5, 15, or 17')
    if stride not in (1, 3, 5, 15, 17):
        error_message = 'stride must be 1, 3, 5, 15, or 17'
        raise ValueError(error_message)
    
    # Generate a lookup table with all possible RGB colors where R, G and B are
    # multiples of 5, encoded in CAM02-UCS space. Table rows correspond to
    # individual RGB colors; columns correspond to J', a', and b' components.
    rgb = np.arange(0, 256, stride)
    colors = np.empty([len(rgb)] * 3 + [3])
    colors[..., 0] = rgb[:, None, None]
    colors[..., 1] = rgb[None, :, None]
    colors[..., 2] = rgb[None, None, :]
    colors = colors.reshape(-1, 3)
    colors = cspace_convert(colors, 'sRGB255', 'CAM02-UCS')
    
    # Remove colors outside the specified lightness, chroma and/or hue ranges
    if lightness_range is not None or chroma_range is not None or \
            hue_range is not None:
        jch = cspace_convert(colors, 'CAM02-UCS', 'JCh')
        mask = np.ones(len(colors), dtype=bool)
        if lightness_range is not None:
            mask &= (jch[:, 0] >= lightness_range[0]) & \
                    (jch[:, 0] <= lightness_range[1])
        if chroma_range is not None:
            mask &= (jch[:, 1] >= chroma_range[0]) & \
                    (jch[:, 1] <= chroma_range[1])
        if hue_range is not None:
            if hue_range[0] <= hue_range[1]:
                mask &= (jch[:, 2] >= hue_range[0]) & \
                        (jch[:, 2] <= hue_range[1])
            else:
                mask &= (jch[:, 2] >= hue_range[0]) | \
                        (jch[:, 2] <= hue_range[1])
        colors = colors[mask]
    
    # Initialize the palette to `first_color`, then iteratively add the color
    # that's farthest away from all other colors (i.e. with the maximum min
    # distance to any color already in the palette)
    palette = [first_color]
    distances = np.full(len(colors), np.inf)
    while len(palette) < num_colors:
        # Update palette-colors distances to account for the color just added
        distance_to_newest_color = \
            np.linalg.norm((colors - palette[-1]), axis=1)
        np.minimum(distances, distance_to_newest_color, distances)
        
        # Add the color with the new maximum distance
        palette.append(colors[distances.argmax()])
    
    # Convert the generated palette to sRGB1 format
    palette = cspace_convert(palette, 'CAM02-UCS', 'sRGB1')
    
    # Clip palette to [0, 1], in case some colors are slightly out-of-range
    palette = palette.clip(0, 1)
    
    # Convert RGB to hex
    # noinspection PyTypeChecker
    palette = np.apply_along_axis(to_hex, 1, palette)
    return palette


def getnnz(sparse_matrix: csr_array | csc_array,
           axis: Literal[0] | Literal[1] | None,
           num_threads: int | np.integer,
           nnz: np.ndarray[1, np.dtype[np.integer]] | None = None) -> \
        np.ndarray[1, np.dtype[np.integer]]:
    """
    Count the number of stored values in a sparse array along an axis,
    including explicitly stored zeros. Matches the behavior of the
    now-deprecated `getnnz()` function for scipy sparse arrays, but differs
    from `sparse_matrix.count_nonzero()`, which excludes explicit zeros.
    
    Args:
        sparse_matrix: a CSR or CSC sparse array or matrix
        axis: whether to count the number of stored values within each column
              (`axis=0`), or within each row (`axis=1`)
        num_threads: the number of threads to use when counting; only used for
                     CSR when `axis=0` and for CSC when `axis=1`
        nnz: an optional preallocated array to store the number of non-zero
             values in; assumed to be the correct size
    
    Returns:
        The number of stored values as a 1D array.
    """
    is_csr = isinstance(sparse_matrix, csr_array)
    if axis == is_csr:
        # The code below is equivalent to `nnz = np.diff(sparse_matrix.indptr)`
        indptr = sparse_matrix.indptr
        if nnz is None:
            nnz = np.empty(len(indptr) - 1, dtype=np.uint32)
        cython_inline('''
            from cython.parallel cimport prange
            
            ctypedef fused integer:
                unsigned
                int
                long
            
            def getnnz_csr(const integer[::1] indptr,
                     unsigned[::1] nnz,
                     const unsigned num_threads):
                cdef unsigned long i, num_major = nnz.shape[0]
                
                if num_threads == 1:
                    for i in range(num_major):
                        nnz[i] = indptr[i + 1] - indptr[i]
                else:
                    for i in prange(num_major, nogil=True,
                                    num_threads=num_threads):
                        nnz[i] = indptr[i + 1] - indptr[i]
        ''', warn_undeclared=False)['getnnz_csr'](
            indptr=indptr, nnz=nnz, num_threads=num_threads)
        return nnz
    else:
        return bincount(sparse_matrix.indices,
                        num_bins=sparse_matrix.shape[is_csr],
                        num_threads=num_threads,
                        counts=nnz)


def plural(string: str, count: int | np.integer) -> str:
    """
    Adds an s to the end of string, unless `count` is 1 or -1.
    
    Args:
        string: a string
        count: a count

    Returns:
        `string`, with an s at the end if `count` is 1 or -1
    """
    return string if abs(count) == 1 else f'{string}s'


def sparse_equal(a1: csr_array | csc_array, a2: csr_array | csc_array) -> bool:
    """
    Tests whether two SciPy sparse arrays or matrices OF THE SAME FORMAT (e.g.
    CSR) are equal. NaNs will always compare equal.
    
    Args:
        a1: the first input array or matrix
        a2: the second input array or matrix

    Returns:
        Whether the two arrays or matrices are equal.
    """
    return a1.nnz == a2.nnz and array_equal(a1.indptr, a2.indptr) and \
        array_equal(a1.data, a2.data) and array_equal(a1.indices, a2.indices)


def sparse_major_stack(arrays: Sequence[csr_array] | Sequence[csc_array],
                       *,
                       num_threads: int) -> csr_array | csc_array:
    """
    Concatenate sparse arrays along their major axis. Equivalent to
    `scipy.sparse.vstack()` for CSR arrays and `scipy.sparse.hstack()` for CSC
    arrays, but supports multithreading.
    
    Args:
        arrays: the sparse arrays to concatenate
        num_threads: the number of threads to use when concatenating. Does not
                     affect the concatenated array's `num_threads`; this will
                     always be the same as the first array's `num_threads`.

    Returns:
        The concatenated sparse array.
    """
    cython_functions = cython_inline(_uninitialized_vector_import + r'''
        cimport numpy as np
        from cython.parallel cimport parallel, threadid
        from libc.string cimport memcpy
        
        def concatenate_indptrs_int64(list arrays, const unsigned num_threads):
            # Similar to `concatenate()`, but instead of a straight copy, skip the
            # first element and add the cumulative nnz while copying, for every indptr
            # but the first. This means we can't just use bytes and generic char*
            # pointers, we need to use elements and long* pointers. So our chunk size
            # is now 8 elements, not 64 bytes.
            
            cdef unsigned array_index, thread_index, chunk_size = 8, \
                num_arrays = len(arrays)
            cdef unsigned long num_elements, offset, num_chunks, start_chunk, \
                end_chunk, current_chunk, array_offset, i, total_elements = 0, \
                total_chunks = 0, total_nnz = 0
            cdef uninitialized_vector[unsigned long] element_count_buffer, \
                chunk_count_buffer, element_offset_buffer, chunk_offset_buffer, \
                nnz_offset_buffer
            cdef uninitialized_vector[long*] array_pointers
            cdef unsigned long[::1] element_count, chunk_count, element_offset, \
                chunk_offset, nnz_offset
            cdef long* output_pointer
            cdef long* array_pointer
            cdef long* src
            cdef long* dest
            cdef np.ndarray array, output
            cdef np.npy_intp total_elements_plus_one
         
            if num_threads == 1:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                element_count_buffer.resize(num_arrays)
                element_count = \
                    <unsigned long[:num_arrays]> element_count_buffer.data()
                element_offset_buffer.resize(num_arrays)
                element_offset = \
                    <unsigned long[:num_arrays]> element_offset_buffer.data()
                nnz_offset_buffer.resize(num_arrays)
                nnz_offset = <unsigned long[:num_arrays]> nnz_offset_buffer.data()
                
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data, after the first 0
                    array_pointers[array_index] = (<long*> array.data) + 1
                    
                    # 2) the number of elements per array (for the entirety of this
                    # function, by "elements", we mean elements after the first 0)
                    num_elements = array.shape[0] - 1
                    element_count[array_index] = num_elements

                    # 3) the total number of nnz up to the start of each array
                    nnz_offset[array_index] = total_nnz
                    
                    # 4) the total number of elements and nnz across all arrays
                    total_elements += num_elements
                    total_nnz += array[num_elements]
                
                # Create the output indptr and get a pointer to its data after the first
                # element, which we set to 0
                np.import_array()
                # output = np.empty(total_elements + 1, dtype=np.int64)
                total_elements_plus_one = total_elements + 1
                output = np.PyArray_EMPTY(1, &total_elements_plus_one, np.NPY_INT64, 0)
                output_pointer = <long*> output.data
                output_pointer[0] = 0
                output_pointer += 1
                
                # Copy the first indptr's data
                num_elements = element_count[0]
                memcpy(output_pointer, array_pointers[0],
                       num_elements * sizeof(int))
                output_pointer += num_elements
                
                # Copy the remaining indptrs' data, adding the nnz offset
                for array_index in range(1, num_arrays):
                    num_elements = element_count[array_index]
                    offset = nnz_offset[array_index]
                    array_pointer = array_pointers[array_index]
                    for i in range(num_elements):
                        output_pointer[i] = array_pointer[i] + offset
                    output_pointer += num_elements
            else:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                element_count_buffer.resize(num_arrays)
                element_count = \
                    <unsigned long[:num_arrays]> element_count_buffer.data()
                chunk_count_buffer.resize(num_arrays)
                chunk_count = <unsigned long[:num_arrays]> chunk_count_buffer.data()
                element_offset_buffer.resize(num_arrays)
                element_offset = \
                    <unsigned long[:num_arrays]> element_offset_buffer.data()
                chunk_offset_buffer.resize(num_arrays)
                chunk_offset = <unsigned long[:num_arrays]> chunk_offset_buffer.data()
                nnz_offset_buffer.resize(num_arrays)
                nnz_offset = <unsigned long[:num_arrays]> nnz_offset_buffer.data()
                
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data, after the first 0
                    array_pointers[array_index] = (<long*> array.data) + 1
                    
                    # 2) the number of elements per array (for the entirety of this
                    # function, by "elements", we mean elements after the first 0)
                    num_elements = array.shape[0] - 1
                    element_count[array_index] = num_elements
                    
                    # 3) the number of `chunk_size`-element chunks per array, rounding up
                    num_chunks = (num_elements - 1) / chunk_size + 1
                    chunk_count[array_index] = num_chunks
                    
                    # 4) the total number of elements, chunks, and nnz up to the start of
                    # each array
                    element_offset[array_index] = total_elements
                    chunk_offset[array_index] = total_chunks
                    nnz_offset[array_index] = total_nnz
                    
                    # 5) the total number of elements, chunks, and nnz across all arrays
                    total_elements += num_elements
                    total_chunks += num_chunks
                    total_nnz += array[num_elements]
            
                # Ensure each thread has at least one chunk to work on
                if total_chunks < num_threads:
                    num_threads = total_chunks
            
                # Create the output indptr and get a pointer to its data after the first
                # element, which we set to 0
                np.import_array()
                # output = np.empty(total_elements + 1, dtype=np.int64)
                total_elements_plus_one = total_elements + 1
                output = np.PyArray_EMPTY(1, &total_elements_plus_one, np.NPY_INT64, 0)
                output_pointer = <long*> output.data
                output_pointer[0] = 0
                output_pointer += 1
            
                # Distribute threads across arrays, proportional to their number of chunks.
                # Each thread operates on a contiguous range of chunks, which may span
                # multiple arrays.
                with nogil, parallel(num_threads=num_threads):
                    # Get the thread index
                    thread_index = threadid()
            
                    # Calculate the range of chunks this thread is responsible for
                    start_chunk = (thread_index * total_chunks) / num_threads
                    end_chunk = ((thread_index + 1) * total_chunks) / num_threads
                    
                    # Find the (first) array that corresponds to this thread's chunk range
                    array_index = 0
                    while chunk_offset[array_index] + \
                            chunk_count[array_index] <= start_chunk:
                        array_index = array_index + 1
                        
                    # Copy data from the input indptrs to the output indptr
                    current_chunk = start_chunk
                    while current_chunk < end_chunk:
                        # Calculate the chunk offset within the current array
                        array_offset = current_chunk - chunk_offset[array_index]
                        
                        # Calculate the source and destination pointers
                        src = array_pointers[array_index] + array_offset * chunk_size
                        dest = output_pointer + element_offset[array_index] + \
                            array_offset * chunk_size
            
                        # Calculate the number of chunks to copy from the current array:
                        # min(remaining in array, remaining in thread)
                        num_chunks = min(chunk_count[array_index] - array_offset,
                                         end_chunk - current_chunk)
                        
                        # Get the number of elements to copy from the current array:
                        # min(num_chunks * chunk_size, remaining elements in array)
                        num_elements = min(num_chunks * chunk_size,
                                           element_count[array_index] -
                                           array_offset * chunk_size)
    
                        # Copy the data, adding the nnz offset for all indptrs but the first
                        if array_index == 0:
                            memcpy(dest, src, num_elements * sizeof(long))
                        else:
                            offset = nnz_offset[array_index]
                            for i in range(num_elements):
                                dest[i] = src[i] + offset
                                
                        # Update the position and array index
                        current_chunk = current_chunk + num_chunks
                        array_index = array_index + 1
            
                return output
        
        def concatenate_indptrs_int32(list arrays, const unsigned num_threads):
            # The int32 version of `concatenate_indptrs_int64()`. We can safely use
            # unsigned rather than unsigned long for everything.
            
            cdef unsigned array_index, num_elements, offset, num_chunks, \
                thread_index, start_chunk, end_chunk, current_chunk, array_offset, i, \
                chunk_size = 16, num_arrays = len(arrays), total_elements = 0, \
                total_chunks = 0, total_nnz = 0
            cdef uninitialized_vector[unsigned] element_count_buffer, \
                chunk_count_buffer, element_offset_buffer, chunk_offset_buffer, \
                nnz_offset_buffer
            cdef uninitialized_vector[int*] array_pointers
            cdef unsigned[::1] element_count, chunk_count, element_offset, \
                chunk_offset, nnz_offset
            cdef int* output_pointer
            cdef int* array_pointer
            cdef int* src
            cdef int* dest
            cdef np.ndarray array, output
            cdef np.npy_intp total_elements_plus_one
         
            if num_threads == 1:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                element_count_buffer.resize(num_arrays)
                element_count = \
                    <unsigned[:num_arrays]> element_count_buffer.data()
                element_offset_buffer.resize(num_arrays)
                element_offset = \
                    <unsigned[:num_arrays]> element_offset_buffer.data()
                nnz_offset_buffer.resize(num_arrays)
                nnz_offset = <unsigned[:num_arrays]> nnz_offset_buffer.data()
                
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data, after the first 0
                    array_pointers[array_index] = (<int*> array.data) + 1
                    
                    # 2) the number of elements per array (for the entirety of this
                    # function, by "elements", we mean elements after the first 0)
                    num_elements = array.shape[0] - 1
                    element_count[array_index] = num_elements

                    # 3) the total number of nnz up to the start of each array
                    nnz_offset[array_index] = total_nnz
                    
                    # 4) the total number of elements and nnz across all arrays
                    total_elements += num_elements
                    total_nnz += array[num_elements]
                
                # Create the output indptr and get a pointer to its data after the first
                # element, which we set to 0
                np.import_array()
                # output = np.empty(total_elements + 1, dtype=np.int32)
                total_elements_plus_one = total_elements + 1
                output = np.PyArray_EMPTY(1, &total_elements_plus_one, np.NPY_INT32, 0)
                output_pointer = <int*> output.data
                output_pointer[0] = 0
                output_pointer += 1
                
                # Copy the first indptr's data
                num_elements = element_count[0]
                memcpy(output_pointer, array_pointers[0],
                       num_elements * sizeof(int))
                output_pointer += num_elements
                
                # Copy the remaining indptrs' data, adding the nnz offset
                for array_index in range(1, num_arrays):
                    num_elements = element_count[array_index]
                    offset = nnz_offset[array_index]
                    array_pointer = array_pointers[array_index]
                    for i in range(num_elements):
                        output_pointer[i] = array_pointer[i] + offset
                    output_pointer += num_elements
            else:
                # Cache relevant information from each array:
                array_pointers.resize(num_arrays)
                element_count_buffer.resize(num_arrays)
                element_count = \
                    <unsigned[:num_arrays]> element_count_buffer.data()
                chunk_count_buffer.resize(num_arrays)
                chunk_count = <unsigned[:num_arrays]> chunk_count_buffer.data()
                element_offset_buffer.resize(num_arrays)
                element_offset = \
                    <unsigned[:num_arrays]> element_offset_buffer.data()
                chunk_offset_buffer.resize(num_arrays)
                chunk_offset = <unsigned[:num_arrays]> chunk_offset_buffer.data()
                nnz_offset_buffer.resize(num_arrays)
                nnz_offset = <unsigned[:num_arrays]> nnz_offset_buffer.data()
                
                for array_index in range(num_arrays):
                    array = arrays[array_index]
                    
                    # 1) the pointer to each array's data, after the first 0
                    array_pointers[array_index] = (<int*> array.data) + 1
                    
                    # 2) the number of elements per array (for the entirety of this
                    # function, by "elements", we mean elements after the first 0)
                    num_elements = array.shape[0] - 1
                    element_count[array_index] = num_elements
                    
                    # 3) the number of `chunk_size`-element chunks per array, rounding up
                    num_chunks = (num_elements - 1) / chunk_size + 1
                    chunk_count[array_index] = num_chunks
                    
                    # 4) the total number of elements, chunks, and nnz up to the start of
                    # each array
                    element_offset[array_index] = total_elements
                    chunk_offset[array_index] = total_chunks
                    nnz_offset[array_index] = total_nnz
                    
                    # 5) the total number of elements, chunks, and nnz across all arrays
                    total_elements += num_elements
                    total_chunks += num_chunks
                    total_nnz += array[num_elements]
            
                # Ensure each thread has at least one chunk to work on
                if total_chunks < num_threads:
                    num_threads = total_chunks
            
                # Create the output indptr and get a pointer to its data after the first
                # element, which we set to 0
                np.import_array()
                # output = np.empty(total_elements + 1, dtype=np.int32)
                total_elements_plus_one = total_elements + 1
                output = np.PyArray_EMPTY(1, &total_elements_plus_one, np.NPY_INT32, 0)
                output_pointer = <int*> output.data
                output_pointer[0] = 0
                output_pointer += 1
            
                # Distribute threads across arrays, proportional to their number of chunks.
                # Each thread operates on a contiguous range of chunks, which may span
                # multiple arrays.
                with nogil, parallel(num_threads=num_threads):
                    # Get the thread index
                    thread_index = threadid()
            
                    # Calculate the range of chunks this thread is responsible for
                    start_chunk = (thread_index * total_chunks) / num_threads
                    end_chunk = ((thread_index + 1) * total_chunks) / num_threads
                    
                    # Find the (first) array that corresponds to this thread's chunk range
                    array_index = 0
                    while chunk_offset[array_index] + \
                            chunk_count[array_index] <= start_chunk:
                        array_index = array_index + 1
                        
                    # Copy data from the input indptrs to the output indptr
                    current_chunk = start_chunk
                    while current_chunk < end_chunk:
                        # Calculate the chunk offset within the current array
                        array_offset = current_chunk - chunk_offset[array_index]
                        
                        # Calculate the source and destination pointers
                        src = array_pointers[array_index] + array_offset * chunk_size
                        dest = output_pointer + element_offset[array_index] + \
                            array_offset * chunk_size
            
                        # Calculate the number of chunks to copy from the current array:
                        # min(remaining in array, remaining in thread)
                        num_chunks = min(chunk_count[array_index] - array_offset,
                                         end_chunk - current_chunk)
                        
                        # Get the number of elements to copy from the current array:
                        # min(num_chunks * chunk_size, remaining elements in array)
                        num_elements = min(num_chunks * chunk_size,
                                           element_count[array_index] -
                                           array_offset * chunk_size)
    
                        # Copy the data, adding the nnz offset for all indptrs but the first
                        if array_index == 0:
                            memcpy(dest, src, num_elements * sizeof(int))
                        else:
                            offset = nnz_offset[array_index]
                            for i in range(num_elements):
                                dest[i] = src[i] + offset
                                
                        # Update the position and array index
                        current_chunk = current_chunk + num_chunks
                        array_index = array_index + 1
        
            return output
        ''')
    data = concatenate([array.data for array in arrays],
                       num_threads=num_threads)
    if sum(array.indptr[-1] for array in arrays) > 2_147_483_647:
        indices = concatenate([array.indices.astype(np.int64, copy=False)
                               for array in arrays], num_threads=num_threads)
        indptr = cython_functions['concatenate_indptrs_int64']([
            array.indptr.astype(np.int64, copy=False)
            for array in arrays], num_threads)
    else:
        indices = concatenate([array.indices.astype(np.int32, copy=False)
                               for array in arrays], num_threads=num_threads)
        indptr = cython_functions['concatenate_indptrs_int32']([
            array.indptr.astype(np.int32, copy=False)
            for array in arrays], num_threads)
    first_array = arrays[0]
    if isinstance(first_array, csr_array):
        shape = len(indptr) - 1, first_array.shape[1]
        result = csr_array((data, indices, indptr), shape=shape)
    else:
        shape = first_array.shape[0], len(indptr) - 1
        result = csc_array((data, indices, indptr), shape=shape)
    result._num_threads = first_array._num_threads
    return result


def sparse_minor_stack(arrays: Sequence[csr_array] | Sequence[csc_array],
                       *,
                       num_threads: int) -> csr_array | csc_array:
    """
    Concatenate sparse arrays along their minor axis. Equivalent to
    `scipy.sparse.hstack()` for CSR arrays and `scipy.sparse.vstack()` for CSC
    arrays, but supports multithreading.
    
    Args:
        arrays: the sparse arrays to concatenate
        num_threads: the number of threads to use when concatenating. Does not
                     affect the concatenated array's `num_threads`; this will
                     always be the same as the first array's `num_threads`.

    Returns:
        The concatenated sparse array.
    """
    sparse_minor_stack = cython_inline(_uninitialized_vector_import + r'''
        cimport numpy as np
        from cython.parallel cimport prange
        from libc.string cimport memcpy
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        ctypedef fused signed_integer:
            int
            long
        
        def sparse_minor_stack(list arrays,
                               const unsigned[::1] offsets,
                               numeric[::1] data,
                               signed_integer[::1] indices,
                               signed_integer[::1] indptr,
                               const unsigned num_major,
                               const unsigned num_threads):
            # Closely based on SciPy's `csr_hstack()` function from sparsetools/csr.h
            
            cdef unsigned array_index, i, total_nnz, nnz, offset, output_offset, \
                num_arrays = len(arrays)
            cdef np.ndarray array_data, array_indices, array_indptr
            cdef signed_integer start, end, j
            cdef uninitialized_vector[numeric*] data_pointers
            cdef uninitialized_vector[signed_integer*] indices_pointers, \
                indptr_pointers
            indptr_pointers.resize(num_arrays)
            indices_pointers.resize(num_arrays)
            data_pointers.resize(num_arrays)
            
            # Get pointers to the start of each sparse array's `data`,
            # `indices` and `indptr`
            for array_index in range(num_arrays):
                array_data = arrays[array_index].data
                array_indices = arrays[array_index].indices
                array_indptr = arrays[array_index].indptr
                data_pointers[array_index] = <numeric*> array_data.data
                indices_pointers[array_index] = <signed_integer*> array_indices.data
                indptr_pointers[array_index] = <signed_integer*> array_indptr.data
            
            if num_threads == 1:
                # Populate `data`, `indices`, and `indptr`
                indptr[0] = 0
                total_nnz = 0
                for i in range(num_major):
                    # First iteration (array_index == 0)
                    start = indptr_pointers[0][i]
                    end = indptr_pointers[0][i + 1]
                    nnz = end - start
                    
                    # Copy data
                    memcpy(&data[total_nnz], &data_pointers[0][start],
                           nnz * sizeof(numeric))
                    
                    # Copy indices (no offset needed)
                    memcpy(&indices[total_nnz], &indices_pointers[0][start],
                           nnz * sizeof(signed_integer))
                    
                    total_nnz += nnz
                    
                    # Remaining iterations (array_index > 0)
                    for array_index in range(1, num_arrays):
                        start = indptr_pointers[array_index][i]
                        end = indptr_pointers[array_index][i + 1]
                        nnz = end - start
                        
                        # Copy data
                        memcpy(&data[total_nnz],
                               &data_pointers[array_index][start],
                               nnz * sizeof(numeric))
                        
                        # Copy indices and apply offset
                        offset = offsets[array_index]
                        output_offset = total_nnz - start
                        for j in range(start, end):
                            indices[output_offset + j] = \
                                indices_pointers[array_index][j] + offset
                        
                        total_nnz += nnz
                    
                    indptr[i + 1] = total_nnz
            else:
                # Same as the single-threaded version, but cumsum `indptr` in a separate
                # single-threaded loop, then populate `data` and `indices` in a second
                # parallel loop once the offsets from `indptr` are known.
                
                with nogil:
                    # Populate `indptr`
                    indptr[0] = 0
                    for i in prange(num_major, num_threads=num_threads):
                        total_nnz = 0
                        for array_index in range(num_arrays):
                            start = indptr_pointers[array_index][i]
                            end = indptr_pointers[array_index][i + 1]
                            nnz = end - start
                            total_nnz = total_nnz + nnz
                        indptr[i + 1] = total_nnz
                    for i in range(1, num_major):
                        indptr[i + 1] += indptr[i]
                        
                    # Populate `data` and `indices`
                    for i in prange(num_major, num_threads=num_threads):
                        total_nnz = indptr[i]
                        
                        # First iteration (array_index == 0)
                        start = indptr_pointers[0][i]
                        end = indptr_pointers[0][i + 1]
                        nnz = end - start
                        
                        # Copy data
                        memcpy(&data[total_nnz], &data_pointers[0][start],
                               nnz * sizeof(numeric))
                        
                        # Copy indices (no offset needed)
                        memcpy(&indices[total_nnz], &indices_pointers[0][start],
                               nnz * sizeof(signed_integer))
                        
                        total_nnz = total_nnz + nnz
                        
                        # Remaining iterations (array_index > 0)
                        array_index = 1
                        while True:
                            start = indptr_pointers[array_index][i]
                            end = indptr_pointers[array_index][i + 1]
                            nnz = end - start
                            
                            # Copy data
                            memcpy(&data[total_nnz],
                                   &data_pointers[array_index][start],
                                   nnz * sizeof(numeric))
                            
                            # Copy indices and apply offset
                            offset = offsets[array_index]
                            for j in range(start, end):
                                output_offset = total_nnz - start
                                indices[output_offset + j] = \
                                    indices_pointers[array_index][j] + offset
                            
                            array_index = array_index + 1
                            if array_index == num_arrays:
                                break
                            
                            total_nnz = total_nnz + nnz
    ''', warn_undeclared=False)['sparse_minor_stack']
    
    # Get the offset of each array along the minor axis, and the total number
    # of non-zero elements across all arrays
    first_array = arrays[0]
    is_csr = isinstance(first_array, csr_array)
    offsets = np.cumsum([0] + [array.shape[is_csr] for array in arrays[:-1]],
                        dtype=np.uint32)
    total_nnz = sum(array.nnz for array in arrays)
    
    # Determine whether the output sparse array can get away with using int32
    # instead of int64 `indices` and `indptr`. The maximum possible value of
    # the concatenated `indices` is the sum of the shapes minus 1, which is why
    # we use 2_147_483_648 not 2_147_483_647 for the first test below.
    index_dtype = np.int64 if offsets[-1] > 2_147_483_648 or \
                              total_nnz > 2_147_483_647 else np.int32
    
    # Allocate output arrays
    num_major = first_array.shape[not is_csr]
    data = np.empty(total_nnz, dtype=first_array.dtype)
    indices = np.empty(total_nnz, dtype=index_dtype)
    indptr = np.empty(num_major + 1, dtype=index_dtype)
    
    # Perform the concatenation, filling `data`, `indices`, and `indptr`
    sparse_minor_stack(arrays, offsets, data, indices, indptr, num_major,
                       num_threads)
    
    # Construct the final sparse array
    if is_csr:
        shape = num_major, offsets[-1]
        result = csr_array((data, indices, indptr), shape=shape)
    else:
        shape = offsets[-1], num_major
        result = csc_array((data, indices, indptr), shape=shape)
    result._num_threads = first_array._num_threads
    return result


@contextmanager
def Timer(message: str | None = None, verbose: bool = True) -> None:
    """
    Use `with Timer(message):` to time the code inside the `with` block.
    
    Args:
        message: an optional message to print when starting the with `block`
                 (with "..." after) and ending the with block (with the time
                 after)
        verbose: if `False`, disables the Timer. This is useful to
                 conditionally run the Timer based on the value of a boolean
                 variable.
    """
    if verbose:
        if message is not None:
            print(f'{message}...')
        start = default_timer()
        aborted = False
        try:
            # noinspection PyTypeChecker
            yield
        except Exception as e:
            aborted = True
            raise e
        finally:
            end = default_timer()
            duration = end - start
            
            days = int(duration // 86400)
            hours = int((duration % 86400) // 3600)
            minutes = int((duration % 3600) // 60)
            seconds = int(duration % 60)
            milliseconds = int((duration * 1000) % 1000)
            microseconds = int((duration * 1000000) % 1000)
            nanoseconds = int((duration * 1000000000) % 1000)
            
            time_parts = []
            if days > 0:
                time_parts.append(f'{days}d')
            if hours > 0:
                time_parts.append(f'{hours}h')
            if minutes > 0:
                time_parts.append(f'{minutes}m')
            if seconds > 0:
                time_parts.append(f'{seconds}s')
            if milliseconds > 0:
                time_parts.append(f'{milliseconds}ms')
            if microseconds > 0:
                time_parts.append(f'{microseconds}µs')
            if nanoseconds > 0:
                time_parts.append(f'{nanoseconds}ns')
            
            time_str = \
                ' '.join(time_parts[:2]) if time_parts else 'less than 1ns'
            
            print(f'{message if message is not None else "Command"} '
                  f'{"aborted after" if aborted else "took"} '
                  f'{time_str}')
    else:
        # noinspection PyTypeChecker
        yield


def to_tuple(variable: Any) -> tuple[Any, ...]:
    """
    Cast Iterables (except str/bytes) to tuple, but box non-Iterables (and
    str/bytes) in a length-1 tuple.
    
    Args:
        variable: a variable

    Returns:
        `variable` as a tuple
    """
    return tuple(variable) if isinstance(variable, Iterable) and not \
        isinstance(variable, (str, bytes)) else (variable,)


def to_tuple_checked(variable: Any,
                     variable_name: str,
                     expected_types: type | tuple[type, ...],
                     expected_type_name: str) -> tuple[Any, ...]:
    """
    Like `to_tuple`, but check that `variable` or its elements are of the
    expected type(s) and that it is non-empty.
    
    Args:
        variable: the variable to be checked and expanded
        variable_name: the name of the variable, used in error messages
        expected_types: the expected type or types
        expected_type_name: the name of the expected type, used in error
                            messages (e.g. `'polars DataFrames'`)

    Returns:
        `variable` as a tuple.
    """
    if isinstance(variable, Iterable) and \
            not isinstance(variable, (str, bytes)):
        variable = tuple(variable)
        if len(variable) == 0:
            error_message = f'{variable_name} is empty'
            raise ValueError(error_message)
        check_types(variable, variable_name, expected_types,
                    expected_type_name)
    else:
        check_type(variable, variable_name, expected_types,
                   f'{expected_type_name} (or a sequence thereof)')
        variable = variable,
    return variable


# Functions for load-balancing parallel iteration over sparse arrays

_thread_offset_import = r'''
        from libcpp.algorithm cimport lower_bound
        from libcpp.pair cimport pair
        
        ctypedef unsigned unsigned_  # hack to get templates to work with unsigned
        
        ctypedef fused signed_integer:
            int
            long
        
        cdef inline pair[unsigned, unsigned] get_thread_offset(
                const signed_integer[::1] indptr,
                const unsigned thread_index,
                const unsigned num_threads) noexcept nogil:
            # Gets the start and end row (for CSR) or column (for CSC) index
            # that each thread will work on, to ensure each thread works on
            # about the same number of sparse matrix elements as a form of
            # load-balancing.

            cdef unsigned start, end, num_major = indptr.shape[0] - 1
            cdef unsigned long num_elements_per_thread = \
                indptr[num_major] / num_threads, \
                num_elements_start = thread_index * num_elements_per_thread, \
                num_elements_end = (thread_index + 1) * num_elements_per_thread
            
            start = 0 if thread_index == 0 else min(
                <unsigned> (lower_bound(&indptr[0], &indptr[0] + indptr.shape[0],
                                        num_elements_start) - &indptr[0]),
                num_major)
            end = num_major if thread_index == num_threads - 1 else min(
                <unsigned> (lower_bound(&indptr[0], &indptr[0] + indptr.shape[0],
                                        num_elements_end) - &indptr[0]),
                num_major)
            return pair[unsigned_, unsigned_](start, end)

        cdef inline void get_thread_offsets(const signed_integer[::1] indptr,
                                            unsigned* thread_offsets,
                                            const unsigned num_threads) noexcept nogil:
            # A variant of `get_thread_offset()` that gets the start and end row
            # (for CSR) or column (for CSC) index that each thread will work on,
            # for all threads simultaneously. For each thread,
            # `thread_offsets[thread_index]` will contain the start index, and
            # `thread_offsets[thread_index + 1]` will contain the end index.

            cdef unsigned thread_index, num_major = indptr.shape[0] - 1
            cdef unsigned long num_elements_start, \
                num_elements_per_thread = indptr[num_major] / num_threads

            thread_offsets[0] = 0
            for thread_index in range(1, num_threads):
                num_elements_start = thread_index * num_elements_per_thread
                thread_offsets[thread_index] = min(
                    <unsigned> (lower_bound(&indptr[0], &indptr[0] + indptr.shape[0],
                                            num_elements_start) - &indptr[0]),
                    num_major)
            thread_offsets[num_threads] = num_major
'''

# Custom sparse array classes that support parallel operations

cython_functions = cython_inline(_thread_offset_import + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.vector cimport vector
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        cdef extern from * nogil:
            """
            #define atomic_and(x, y) _Pragma("omp atomic") x &= y
            """
            void atomic_and(bint &x, bint y)
        
        cpdef bint csr_has_canonical_format(const unsigned n_row,
                                            const signed_integer[::1] Ap,
                                            const signed_integer[::1] Aj,
                                            const unsigned num_threads):
            cdef unsigned i
            cdef signed_integer jj
            cdef bint has_canonical_format = True
            
            if num_threads == 1:
                for i in range(n_row):
                    if Ap[i] > Ap[i + 1]:
                        return False
                    for jj in range(Ap[i] + 1, Ap[i + 1]):
                        if not Aj[jj - 1] < Aj[jj]:
                            return False
            else:
                # Use hardware-level atomics to thread-safely flag non-canonical
                # rows
                for i in prange(n_row, nogil=True, num_threads=num_threads):
                    if Ap[i] > Ap[i + 1]:
                        atomic_and(has_canonical_format, False)
                        with gil:
                            return False
                    for jj in range(Ap[i] + 1, Ap[i + 1]):
                        if not Aj[jj - 1] < Aj[jj]:
                            atomic_and(has_canonical_format, False)
                            with gil:
                                return False
            return has_canonical_format
        
        def csr_has_sorted_indices(const unsigned n_row,
                                          const signed_integer[::1] Ap,
                                          const signed_integer[::1] Aj,
                                          const unsigned num_threads):
            cdef unsigned i
            cdef signed_integer jj
            cdef bint has_sorted_indices = True
            
            if num_threads == 1:
                for i in range(n_row):
                    for jj in range(Ap[i], Ap[i + 1] - 1):
                        if Aj[jj] > Aj[jj + 1]:
                            return False
            else:
                # Use hardware-level atomics to thread-safely flag unsorted rows
                for i in prange(n_row, nogil=True, num_threads=num_threads):
                    for jj in range(Ap[i], Ap[i + 1] - 1):
                        if Aj[jj] > Aj[jj + 1]:
                            atomic_and(has_sorted_indices, False)
                            with gil:
                                return False
                        
            return has_sorted_indices
        
        cdef extern from * nogil:
            """
            #include <algorithm>
            #include <iterator>
            
            template<typename I, typename T>
            struct RefTuple;
    
            template<typename I, typename T>
            struct Tuple {
                I i;
                T x;
                Tuple(const RefTuple<I, T>& t) noexcept : i(t.i), x(t.x) {}
                bool operator<(const Tuple<I, T>& t) const noexcept { return i < t.i; };
                bool operator<(const RefTuple<I, T>& t) const noexcept { return i < t.i; };
            };
    
            template<typename I, typename T>
            struct RefTuple {
                I& i;
                T& x;
                RefTuple(I& i, T& x) noexcept : i(i), x(x) {}
                void operator=(const Tuple<I, T>& t) noexcept { i = t.i; x = t.x; };
                void operator=(const RefTuple<I, T>& t) noexcept { i = t.i; x = t.x; };
                bool operator<(const Tuple<I, T>& t) const noexcept { return i < t.i; };
            };
    
            template<typename I, typename T>
            inline void swap(RefTuple<I, T>&& t1, RefTuple<I, T>&& t2) noexcept {
                std::swap(t1.i, t2.i);
                std::swap(t1.x, t2.x);
            }
    
            template<typename I, typename T>
            class IterTuple {
                I* i;
                T* x;
            public:
                using iterator_category = std::random_access_iterator_tag;
                using value_type = Tuple<I, T>;
                using difference_type = std::ptrdiff_t;
                using pointer = Tuple<I, T>*;
                using reference = Tuple<I, T>&;
                IterTuple(I* i, T* t) noexcept : i(i), x(t) {}
                IterTuple(const IterTuple& e) noexcept : i(e.i), x(e.x) {}
                RefTuple<I, T> operator*() const noexcept {
                    return RefTuple<I, T>(*i, *x); }
                IterTuple& operator++() noexcept { i++; x++; return *this; }
                IterTuple& operator--() noexcept { i--; x--; return *this; }
                IterTuple operator++(int) noexcept {
                    IterTuple tmp(*this); i++; x++; return tmp; }
                IterTuple operator--(int) noexcept {
                    IterTuple tmp(*this); i--; x--; return tmp; }
                difference_type operator-(IterTuple& rhs) const noexcept {
                    return i - rhs.i; }
                IterTuple operator+(difference_type n) const noexcept {
                    IterTuple tmp(*this); tmp.i += n; tmp.x += n; return tmp; }
                IterTuple operator-(difference_type n) const noexcept {
                    IterTuple tmp(*this); tmp.i -= n; tmp.x -= n; return tmp; }
                bool operator==(const IterTuple& rhs) const noexcept { return i == rhs.i; }
                bool operator!=(const IterTuple& rhs) const noexcept { return i != rhs.i; }
                bool operator<(IterTuple& rhs) const noexcept { return i < rhs.i; }
            };
    
            template<typename I, typename T>
            void sort_both_by_indices(I* Aj_start, I* Aj_end,
                                      T* Ax_start, T* Ax_end) noexcept {
                IterTuple<I, T> begin(Aj_start, Ax_start);
                IterTuple<I, T> end(Aj_end, Ax_end);
                std::sort(begin, end);
            }
            """
            cdef void sort_both_by_indices[I, T](I* Aj_start, I* Aj_end,
                                                 T* Ax_start, T* Ax_end) noexcept
    
        def csr_sort_indices(const unsigned n_row,
                             const signed_integer[::1] Ap,
                             signed_integer[::1] Aj,
                             numeric[::1] Ax,
                             unsigned num_threads):
            cdef unsigned i, thread_index
            cdef signed_integer row_start, row_end
            cdef pair[unsigned, unsigned] row_range
            
            num_threads = min(num_threads, n_row)
            if num_threads == 1:
                for i in range(n_row):
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
                    sort_both_by_indices(&Aj[row_start], &Aj[row_end],
                                         &Ax[row_start], &Ax[row_end])
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(Ap, thread_index, num_threads)
                    for i in range(row_range.first, row_range.second):
                        row_start = Ap[i]
                        row_end = Ap[i + 1]
                        sort_both_by_indices(&Aj[row_start], &Aj[row_end],
                                             &Ax[row_start], &Ax[row_end])
        
        def get_csr_submatrix1(const unsigned n_row,
                               const unsigned n_col,
                               const signed_integer[::1] Ap,
                               const signed_integer[::1] Aj,
                               const numeric[::1] Ax,
                               const signed_integer ir0,
                               const signed_integer ir1,
                               const signed_integer ic0,
                               const signed_integer ic1,
                               signed_integer[::1] Bp,
                               unsigned num_threads):
            cdef unsigned i, thread_index, new_n_row = ir1 - ir0
            cdef signed_integer new_nnz, jj
            cdef pair[unsigned, unsigned] row_range
            
            Bp[0] = 0
            num_threads = min(num_threads, new_n_row)
            if num_threads == 1:
                new_nnz = 0
                for i in range(new_n_row):
                    for jj in range(Ap[ir0 + i], Ap[ir0 + i + 1]):
                        if Aj[jj] >= ic0 and Aj[jj] < ic1:
                            new_nnz += 1
                    Bp[i + 1] = new_nnz
                    
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        PyErr_CheckSignals()
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(Ap[ir0:ir1 + 1], thread_index,
                                                  num_threads)
                    for i in range(row_range.first, row_range.second):
                        new_nnz = 0
                        for jj in range(Ap[ir0 + i], Ap[ir0 + i + 1]):
                            if Aj[jj] >= ic0 and Aj[jj] < ic1:
                                new_nnz = new_nnz + 1
                        Bp[i + 1] = new_nnz
                        
                        # Check for KeyboardInterrupts
                        if i % 10000 == 9999:
                            with gil:
                                PyErr_CheckSignals()
                    
                for i in range(1, new_n_row):
                    Bp[i + 1] += Bp[i]  # cumsum
        
        def get_csr_submatrix2(const unsigned n_row,
                               const unsigned n_col,
                               const signed_integer[::1] Ap,
                               const signed_integer[::1] Aj,
                               const numeric[::1] Ax,
                               const signed_integer ir0,
                               const signed_integer ir1,
                               const signed_integer ic0,
                               const signed_integer ic1,
                               signed_integer[::1] Bp,
                               signed_integer[::1] Bj,
                               numeric[::1] Bx,
                               unsigned num_threads):
            cdef unsigned i, thread_index, new_n_row = ir1 - ir0
            cdef signed_integer jj, kk
            cdef pair[unsigned, unsigned] row_range
            
            num_threads = min(num_threads, new_n_row)
            if num_threads == 1:
                kk = 0
                for i in range(new_n_row):
                    for jj in range(Ap[ir0 + i], Ap[ir0 + i + 1]):
                        if Aj[jj] >= ic0 and Aj[jj] < ic1:
                            Bj[kk] = Aj[jj] - ic0
                            Bx[kk] = Ax[jj]
                            kk = kk + 1
                            
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        PyErr_CheckSignals()
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(Ap[ir0:ir1 + 1], thread_index,
                                                  num_threads)
                    for i in range(row_range.first, row_range.second):
                        kk = Bp[i]
                        for jj in range(Ap[ir0 + i], Ap[ir0 + i + 1]):
                            if Aj[jj] >= ic0 and Aj[jj] < ic1:
                                Bj[kk] = Aj[jj] - ic0
                                Bx[kk] = Ax[jj]
                                kk = kk + 1
                                
                        # Check for KeyboardInterrupts
                        if i % 10000 == 9999:
                            with gil:
                                PyErr_CheckSignals()
        
        def csr_row_index(const unsigned n_row_idx,
                          const signed_integer[::1] rows,
                          const signed_integer[::1] Ap,
                          const signed_integer[::1] Aj,
                          const numeric[::1] Ax,
                          signed_integer[::1] Bp,
                          signed_integer[::1] Bj,
                          numeric[::1] Bx,
                          const unsigned num_threads):
            
            cdef unsigned i
            cdef signed_integer row, row_start, row_end, dest_row_start, dest_row_end
            
            if num_threads == 1:
                dest_row_start = 0
                for i in range(n_row_idx):
                    row = rows[i]
                    row_start = Ap[row]
                    row_end = Ap[row + 1]
                    dest_row_end = dest_row_start + row_end - row_start
                    Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                    Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                    dest_row_start = dest_row_end
                    
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        PyErr_CheckSignals()
            else:
                for i in prange(n_row_idx, nogil=True, num_threads=num_threads):
                    row = rows[i]
                    row_start = Ap[row]
                    row_end = Ap[row + 1]
                    dest_row_start = Bp[i]
                    dest_row_end = Bp[i + 1]
                    Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                    Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                    
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        with gil:
                            PyErr_CheckSignals()
        
        def csr_row_slice(const unsigned start,
                          const unsigned stop,
                          const int step,
                          const signed_integer[::1] Ap,
                          const signed_integer[::1] Aj,
                          const numeric[::1] Ax,
                          signed_integer[::1] Bp,
                          signed_integer[::1] Bj,
                          numeric[::1] Bx,
                          const unsigned num_threads):
                    
            cdef unsigned i, row, num_iterations
            cdef signed_integer row_start, row_end, dest_row_start, dest_row_end
            
            if num_threads == 1:
                dest_row_start = 0
                for i in range(start, stop, step):
                    row_start = Ap[i]
                    row_end = Ap[i + 1]
                    dest_row_end = dest_row_start + row_end - row_start
                    Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                    Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                    dest_row_start = dest_row_end
                    
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        PyErr_CheckSignals()
            else:
                if step > 0 and start < stop:
                    num_iterations = (stop - start + step - 1) // step
                elif step < 0 and start > stop:
                    num_iterations = (start - stop - step - 1) // -step
                else:
                    num_iterations = 0
                for i in prange(num_iterations, nogil=True,
                                num_threads=num_threads):
                    row = start + i * step
                    row_start = Ap[row]
                    row_end = Ap[row + 1]
                    dest_row_start = Bp[i]
                    dest_row_end = Bp[i + 1]
                    Bj[dest_row_start:dest_row_end] = Aj[row_start:row_end]
                    Bx[dest_row_start:dest_row_end] = Ax[row_start:row_end]
                    
                    # Check for KeyboardInterrupts
                    if i % 10000 == 9999:
                        with gil:
                            PyErr_CheckSignals()
            
        def csr_column_index1(const unsigned n_idx,
                              const signed_integer[::1] col_idxs,
                              const unsigned n_row,
                              const unsigned n_col,
                              const signed_integer[::1] Ap,
                              const signed_integer[::1] Aj,
                              signed_integer[::1] col_offsets,
                              signed_integer[::1] Bp,
                              unsigned num_threads):
            cdef unsigned i, j, thread_index
            cdef signed_integer new_nnz, jj
            cdef pair[unsigned, unsigned] row_range
            
            # bincount(col_idxs)
            col_offsets[:] = 0
            for i in range(n_idx):
                j = col_idxs[i]
                col_offsets[j] += 1
        
            # Compute new indptr
            Bp[0] = 0
            num_threads = min(num_threads, n_row)
            if num_threads == 1:
                new_nnz = 0
                for i in range(n_row):
                    for jj in range(Ap[i], Ap[i + 1]):
                        new_nnz += col_offsets[Aj[jj]]
                    Bp[i + 1] = new_nnz
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(Ap, thread_index, num_threads)
                    for i in range(row_range.first, row_range.second):
                        new_nnz = 0
                        for jj in range(Ap[i], Ap[i + 1]):
                            new_nnz = new_nnz + col_offsets[Aj[jj]]
                        Bp[i + 1] = new_nnz
                for i in range(1, n_row):
                    Bp[i + 1] += Bp[i]
            
            # cumsum in-place
            for j in range(1, n_col):
                col_offsets[j] += col_offsets[j - 1]
        
        def csr_column_index2(const signed_integer[::1] col_order,
                              const signed_integer[::1] col_offsets,
                              const signed_integer nnz,
                              const signed_integer[::1] Ap,
                              const signed_integer[::1] Aj,
                              const numeric[::1] Ax,
                              signed_integer[::1] Bp,
                              signed_integer[::1] Bj,
                              numeric[::1] Bx,
                              unsigned num_threads):
            
            cdef unsigned row, thread_index
            cdef signed_integer n, jj, j, offset, prev_offset, k
            cdef numeric v
            cdef pair[unsigned, unsigned] row_range
            
            num_threads = min(num_threads, Ap.shape[0] - 1)
            if num_threads == 1:
                n = 0
                for jj in range(nnz):
                    j = Aj[jj]
                    offset = col_offsets[j]
                    prev_offset = 0 if j == 0 else col_offsets[j - 1]
                    if offset != prev_offset:
                        v = Ax[jj]
                        for k in range(prev_offset, offset):
                            Bj[n] = col_order[k]
                            Bx[n] = v
                            n += 1
                
                # Check for KeyboardInterrupts
                if jj % 1000000 == 999999:
                    PyErr_CheckSignals()
                
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(Ap, thread_index, num_threads)
                    for row in range(row_range.first, row_range.second):
                        n = Bp[row]
                        for jj in range(Ap[row], Ap[row + 1]):
                            j = Aj[jj]
                            offset = col_offsets[j]
                            prev_offset = 0 if j == 0 else col_offsets[j - 1]
                            if offset != prev_offset:
                                v = Ax[jj]
                                for k in range(prev_offset, offset):
                                    Bj[n] = col_order[k]
                                    Bx[n] = v
                                    n = n + 1
                        
                        # Check for KeyboardInterrupts
                        if row % 10000 == 9999:
                            with gil:
                                PyErr_CheckSignals()
        
        def csr_sample_values(const unsigned n_row,
                              const unsigned n_col,
                              const signed_integer[::1] Ap,
                              const signed_integer[::1] Aj,
                              const numeric[::1] Ax,
                              const signed_integer n_samples,
                              const signed_integer[::1] Bi,
                              const signed_integer[::1] Bj,
                              numeric[::1] Bx,
                              const unsigned num_threads):
            cdef signed_integer n, i, j, row_start, row_end, offset, jj, \
                nnz = Ap[n_row], threshold = nnz / 10  # constant is arbitrary
            cdef numeric x
        
            if n_samples > threshold and csr_has_canonical_format(n_row, Ap, Aj,
                                                                  num_threads):
                if num_threads == 1:
                    for n in range(n_samples):
                        i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                        j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
            
                        row_start = Ap[i]
                        row_end = Ap[i + 1]
            
                        if row_start < row_end:
                            offset = lower_bound(&Aj[0] + row_start,
                                                 &Aj[0] + row_end, j) - &Aj[0]
                            if offset < row_end and Aj[offset] == j:
                                Bx[n] = Ax[offset]
                            else:
                                Bx[n] = 0
                        else:
                            Bx[n] = 0
                    
                    # Check for KeyboardInterrupts
                    if n % 10000 == 9999:
                        PyErr_CheckSignals()
                    
                else:
                    for n in prange(n_samples, nogil=True,
                                    num_threads=num_threads):
                        i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                        j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
            
                        row_start = Ap[i]
                        row_end = Ap[i + 1]
            
                        if row_start < row_end:
                            offset = lower_bound(&Aj[0] + row_start,
                                                 &Aj[0] + row_end, j) - &Aj[0]
                            if offset < row_end and Aj[offset] == j:
                                Bx[n] = Ax[offset]
                            else:
                                Bx[n] = 0
                        else:
                            Bx[n] = 0
                        
                        # Check for KeyboardInterrupts
                        if n % 10000 == 9999:
                            with gil:
                                PyErr_CheckSignals()
            else:
                if num_threads == 1:
                    for n in range(n_samples):
                        i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                        j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
            
                        row_start = Ap[i]
                        row_end = Ap[i + 1]
            
                        x = 0
                        for jj in range(row_start, row_end):
                            if Aj[jj] == j:
                                x += Ax[jj]
                        Bx[n] = x
                        
                        # Check for KeyboardInterrupts
                        if n % 10000 == 9999:
                            PyErr_CheckSignals()
                else:
                    for n in prange(n_samples, nogil=True,
                                    num_threads=num_threads):
                        i = Bi[n] + n_row if Bi[n] < 0 else Bi[n]  # sample row
                        j = Bj[n] + n_col if Bj[n] < 0 else Bj[n]  # sample column
            
                        row_start = Ap[i]
                        row_end = Ap[i + 1]
            
                        x = 0
                        for jj in range(row_start, row_end):
                            if Aj[jj] == j:
                                x = x + Ax[jj]
                        Bx[n] = x
                        
                        # Check for KeyboardInterrupts
                        if n % 10000 == 9999:
                            with gil:
                                PyErr_CheckSignals()
        
        def csr_tocsc(const unsigned n_row,
                      const unsigned n_col,
                      const signed_integer[::1] Ap,
                      const signed_integer[::1] Aj,
                      const numeric[::1] Ax,
                      signed_integer[::1] Bp,
                      signed_integer[::1] Bi,
                      numeric[::1] Bx,
                      unsigned num_threads):
            cdef unsigned col, row, thread_index, preceding_thread_index
            cdef signed_integer n, cumsum, temp, jj, dest, preceding_count, i, \
                nnz = Ap[n_row]
            cdef pair[unsigned, unsigned] row_range
            cdef vector[vector[unsigned]] thread_col_counts
            cdef vector[vector[signed_integer]] thread_col_offsets
            cdef unsigned* col_counts
            cdef signed_integer* col_offsets
            
            num_threads = min(num_threads, n_row)
            if num_threads == 1:
                # Count the number of non-zero entries per column of `A`
                Bp[:n_col] = 0
                for n in range(nnz):
                    Bp[Aj[n]] += 1
        
                # Cumsum the nnz per column, shifting right by 1, to get `Bp`
                cumsum = 0
                for col in range(n_col):
                    temp = Bp[col]
                    Bp[col] = cumsum
                    cumsum += temp
                Bp[n_col] = nnz
        
                # Fill in `Bi` and `Bx`, using `Bp[col]` to keep track of the current
                # insertion index for each column
                for row in range(n_row):
                    for jj in range(Ap[row], Ap[row + 1]):
                        col = Aj[jj]
                        dest = Bp[col]
                        Bi[dest] = row
                        Bx[dest] = Ax[jj]
                        Bp[col] += 1
                    
                    # Check for KeyboardInterrupts
                    if row % 10000 == 9999:
                        PyErr_CheckSignals()
                
                # After the previous step, `Bp[col]` now points to the start of
                # `col + 1`'s column. Reset it to point to the start of `col`'s column.
                col = n_col
                while col > 0:
                    Bp[col] = Bp[col - 1]
                    col = col - 1
                Bp[0] = 0
            else:
                thread_col_counts.resize(num_threads)
                thread_col_offsets.resize(num_threads)
                
                with nogil:
                    # Count the number of non-zero entries per column of `A`
                    # per thread
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_col_counts[thread_index].resize(n_col)
                        thread_col_offsets[thread_index].resize(n_col)
                        row_range = get_thread_offset(Ap, thread_index, num_threads)
                        col_counts = thread_col_counts[thread_index].data()
                        for row in range(row_range.first, row_range.second):
                            for jj in range(Ap[row], Ap[row + 1]):
                                col = Aj[jj]
                                col_counts[col] += 1
        
                    # Sum these numbers of non-zero entries across threads
                    Bp[:] = 0
                    for col in range(n_col):
                        for thread_index in range(num_threads):
                            Bp[col] += thread_col_counts[thread_index][col]
        
                    # Cumsum the nnz per column, shifting right by 1, to get `Bp`
                    cumsum = 0
                    for col in range(n_col):
                        temp = Bp[col]
                        Bp[col] = cumsum
                        cumsum += temp
                    Bp[n_col] = nnz
        
                    # Fill in `Bi` and `Bx`, using `thread_col_offsets[col]` to keep
                    # track of the current insertion index for each column. Initialize
                    # each thread's `thread_col_offsets[col]` by summing
                    # `thread_col_counts` for that column across all threads preceding
                    # `thread_index`, and adding `Bp[col]`, the start index of the
                    # column.
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_offsets = thread_col_offsets[thread_index].data()
                        for col in range(n_col):
                            preceding_count = Bp[col]
                            for preceding_thread_index in range(thread_index):
                                preceding_count = preceding_count + \
                                    thread_col_counts[preceding_thread_index][col]
                            col_offsets[col] = preceding_count
                        
                        row_range = get_thread_offset(Ap, thread_index, num_threads)
                        for row in range(row_range.first, row_range.second):
                            for jj in range(Ap[row], Ap[row + 1]):
                                col = Aj[jj]
                                dest = col_offsets[col]
                                Bi[dest] = row
                                Bx[dest] = Ax[jj]
                                col_offsets[col] += 1
                            
                            # Check for KeyboardInterrupts
                            if row % 10000 == 9999:
                                with gil:
                                    PyErr_CheckSignals()
        
        def csc_tocsr(const unsigned n_row,
                      const unsigned n_col,
                      const signed_integer[::1] Ap,
                      const signed_integer[::1] Ai,
                      const numeric[::1] Ax,
                      signed_integer[::1] Bp,
                      signed_integer[::1] Bj,
                      numeric[::1] Bx,
                      const unsigned num_threads):
            return csr_tocsc(n_col, n_row, Ap, Ai, Ax, Bp, Bj, Bx, num_threads)
        
        def csr_eliminate_zeros(const unsigned n_row,
                                const unsigned n_col,
                                signed_integer[::1] Ap,
                                signed_integer[::1] Aj,
                                numeric[::1] Ax):
            cdef unsigned i
            cdef signed_integer row_end = 0, nnz = 0, jj, j
            cdef numeric x
    
            for i in range(n_row):
                jj = row_end
                row_end = Ap[i+1]
                while jj < row_end:
                    j = Aj[jj]
                    x = Ax[jj]
                    if x != 0:
                        Aj[nnz] = j
                        Ax[nnz] = x
                        nnz += 1
                    jj += 1
                Ap[i + 1] = nnz
''', warn_undeclared=False)
get_csr_submatrix1 = cython_functions['get_csr_submatrix1']
get_csr_submatrix2 = cython_functions['get_csr_submatrix2']
csr_row_index = cython_functions['csr_row_index']
csr_row_slice = cython_functions['csr_row_slice']
csr_column_index1 = cython_functions['csr_column_index1']
csr_column_index2 = cython_functions['csr_column_index2']
csr_sample_values = cython_functions['csr_sample_values']
csr_has_canonical_format = cython_functions['csr_has_canonical_format']
csr_has_sorted_indices = cython_functions['csr_has_sorted_indices']
csr_sort_indices = cython_functions['csr_sort_indices']
csr_tocsc = cython_functions['csr_tocsc']
csc_tocsr = cython_functions['csc_tocsr']
csr_eliminate_zeros = cython_functions['csr_eliminate_zeros']


def get_csr_submatrix(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1,
                      num_threads):
    # Allocate indptr
    Bp = np.empty(ir1 - ir0 + 1, dtype=Ap.dtype)
    
    # Count nonzeros and populate indptr
    get_csr_submatrix1(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1, Bp,
                       num_threads)
    
    # Allocate indices and data
    new_nnz = Bp[-1]
    Bj = np.empty(new_nnz, dtype=Aj.dtype)
    Bx = np.empty(new_nnz, dtype=Ax.dtype)
    
    # Populate indices and data
    get_csr_submatrix2(n_row, n_col, Ap, Aj, Ax, ir0, ir1, ic0, ic1,
                       Bp, Bj, Bx, num_threads)
    return Bp, Bj, Bx
    

def isintlike(x) -> bool:
    """Is x appropriate as an index into a sparse matrix? Returns True
    if it can be cast safely to a machine int.
    """
    # Fast-path check to eliminate non-scalar values. operator.index would
    # catch this case too, but the exception catching is slow.
    if np.ndim(x) != 0:
        return False
    try:
        operator.index(x)
    except (TypeError, ValueError):
        try:
            loose_int = bool(int(x) == x)
        except (TypeError, ValueError):
            return False
        if loose_int:
            error_message = \
                'inexact indices into sparse matrices are not allowed'
            raise ValueError(error_message)
        return loose_int
    return True


def _process_slice(sl, num):
    if sl is None:
        i0, i1 = 0, num
    elif isinstance(sl, slice):
        i0, i1, stride = sl.indices(num)
        if stride != 1:
            error_message = 'slicing with step != 1 not supported'
            raise ValueError(error_message)
        i0 = min(i0, i1)  # give an empty slice when i0 > i1
    elif isintlike(sl):
        if sl < 0:
            sl += num
        i0, i1 = sl, sl + 1
        if i0 < 0 or i1 > num:
            error_message = f'index out of bounds: 0 <= {i0} < {i1} <= {num}'
            raise IndexError(error_message)
    else:
        error_message = 'expected slice or scalar'
        raise TypeError(error_message)

    return i0, i1


class cs_matrix(_cs_matrix):
    _num_threads = os.cpu_count()
    _has_sorted_indices = None
    _has_canonical_format = None
    
    def __getitem__(self, key):
        index, new_shape = self._validate_indices(key)
        INT_TYPES = int, np.integer
        
        # 1D array
        if len(index) == 1:
            idx = index[0]
            if isinstance(idx, np.ndarray):
                if idx.shape == ():
                    idx = idx.item()
            if isinstance(idx, INT_TYPES):
                res = self._get_int(idx)
            elif isinstance(idx, slice):
                res = self._get_slice(idx)
            else:  # assume array idx
                res = self._get_array(idx)
            
            # handle np.newaxis in idx when result would otherwise be a scalar
            if res.shape == () and new_shape != ():
                if len(new_shape) == 1:
                    return self.__class__([res], shape=new_shape,
                                          dtype=self.dtype)
                if len(new_shape) == 2:
                    return self.__class__([[res]], shape=new_shape,
                                          dtype=self.dtype)
            return res.reshape(new_shape)

        # 2D array
        row, col = index

        # Dispatch to specialized methods.
        if isinstance(row, INT_TYPES):
            if isinstance(col, INT_TYPES):
                return self._get_intXint(row, col)
            elif isinstance(col, slice):
                return self._get_intXslice(row, col)
            elif col.ndim == 1:
                return self._get_intXarray(row, col)
            elif col.ndim == 2:
                return self._get_intXarray(row, col)
            else:
                raise IndexError('index results in >2 dimensions')
        elif isinstance(row, slice):
            if isinstance(col, INT_TYPES):
                return self._get_sliceXint(row, col)
            elif isinstance(col, slice):
                if row == slice(None) and row == col:
                    return self.copy()
                else:
                    return self._get_sliceXslice(row, col)
            elif col.ndim == 1:
                return self._get_sliceXarray(row, col)
            else:
                raise IndexError('index results in >2 dimensions')
        else:
            if isinstance(col, INT_TYPES):
                return self._get_arrayXint(row, col)
            elif isinstance(col, slice):
                return self._get_arrayXslice(row, col)
            # arrayXarray preprocess
            elif row.ndim == 2 and row.shape[1] == 1 and \
                    (col.ndim == 1 or col.shape[0] == 1):
                # outer indexing
                return self._get_columnXarray(row[:, 0], col.ravel())
            else:
                # inner indexing
                row, col = self._broadcast_arrays(row, col)
                if row.shape != col.shape:
                    raise IndexError('number of row and column indices differ')
                if row.size == 0:
                    return self.__class__(np.atleast_2d(row).shape,
                                         dtype=self.dtype)
                else:
                    return self._get_arrayXarray(row, col)
    
    @staticmethod
    def _broadcast_arrays(a, b):
        """
        Same as np.broadcast_arrays(a, b) but old writeability rules.

        NumPy >= 1.17.0 transitions broadcast_arrays to return
        read-only arrays. Set writeability explicitly to avoid warnings.
        Retain the old writeability rules, as our Cython code assumes
        the old behavior.
        """
        x, y = np.broadcast_arrays(a, b)
        x.flags.writeable = a.flags.writeable
        y.flags.writeable = b.flags.writeable
        return x, y
    
    @staticmethod
    def _compatible_boolean_index(idx, desired_ndim):
        """Check for boolean array or array-like. peek before asarray for
        array-like"""
        # use attribute ndim to indicate a compatible array and check dtype
        # if not, look at 1st element as quick rejection of bool, else slower
        # asanyarray
        if not hasattr(idx, 'ndim'):
            # is first element boolean?
            try:
                ix = next(iter(idx), None)
                for _ in range(desired_ndim):
                    if isinstance(ix, bool):
                        break
                    ix = next(iter(ix), None)
                else:
                    return None
            except TypeError:
                return None
            # since first is boolean, construct array and check all elements
            idx = np.asanyarray(idx)
    
        if idx.dtype.kind == 'b':
            return idx
        return None
    
    def _validate_indices(self, key):
        """Returns two tuples: (index tuple, requested shape tuple)"""
        # single ellipsis
        if key is Ellipsis:
            return (slice(None),) * self.ndim, self.shape

        if not isinstance(key, tuple):
            key = [key]

        ellps_pos = None
        index_1st = []
        prelim_ndim = 0
        for i, idx in enumerate(key):
            if idx is ...:
                if ellps_pos is not None:
                    error_message = \
                        'an index can only have a single ellipsis'
                    raise IndexError(error_message)
                ellps_pos = i
            elif idx is None:
                index_1st.append(idx)
            elif isinstance(idx, slice) or isintlike(idx):
                index_1st.append(idx)
                prelim_ndim += 1
            else:
                ix = self._compatible_boolean_index(idx, self.ndim)
                if ix is not None:
                    index_1st.append(ix)
                    prelim_ndim += ix.ndim
                elif sparse.issparse(idx):
                    raise IndexError(
                        'Indexing with sparse matrices is not supported '
                        'except boolean indexing where matrix and index '
                        'are equal shapes.')
                else:  # dense array
                    index_1st.append(np.asarray(idx))
                    prelim_ndim += 1
        ellip_slices = (self.ndim - prelim_ndim) * [slice(None)]
        if ellip_slices:
            if ellps_pos is None:
                index_1st.extend(ellip_slices)
            else:
                index_1st = index_1st[:ellps_pos] + ellip_slices + \
                            index_1st[ellps_pos:]

        # second pass (have processed ellipsis and preprocessed arrays)
        idx_shape = []
        index_ndim = 0
        index = []
        array_indices = []
        for i, idx in enumerate(index_1st):
            if idx is None:
                idx_shape.append(1)
            elif isinstance(idx, slice):
                index.append(idx)
                Ms = self._shape[index_ndim]
                len_slice = len(range(*idx.indices(Ms)))
                idx_shape.append(len_slice)
                index_ndim += 1
            elif isintlike(idx):
                N = self._shape[index_ndim]
                if not -N <= idx < N:
                    error_message = f'index ({idx}) out of range'
                    raise IndexError(error_message)
                idx = int(idx + N if idx < 0 else idx)
                index.append(idx)
                index_ndim += 1
            # bool array (checked in first pass)
            elif idx.dtype.kind == 'b':
                ix = idx
                tmp_ndim = index_ndim + ix.ndim
                mid_shape = self._shape[index_ndim:tmp_ndim]
                if ix.shape != mid_shape:
                    error_message = (
                        f'bool index {i} has shape {mid_shape} instead of '
                        f'{ix.shape}')
                    raise IndexError(error_message)
                index.extend(ix.nonzero())
                array_indices.extend(range(index_ndim, tmp_ndim))
                index_ndim = tmp_ndim
            else:  # dense array
                N = self._shape[index_ndim]
                idx = self._asindices(idx, N)
                index.append(idx)
                array_indices.append(index_ndim)
                index_ndim += 1
        if index_ndim > self.ndim:
            error_message = (
                f'invalid index ndim. Array is {self.ndim}D. Index needs '
                f'{index_ndim}D')
            raise IndexError(error_message)
        if len(array_indices) > 1:
            idx_arrays = \
                self._broadcast_arrays(*(index[i] for i in array_indices))
            if any(idx_arrays[0].shape != ix.shape
                   for ix in idx_arrays[1:]):
                shapes = " ".join(str(ix.shape) for ix in idx_arrays)
                error_message = (
                    f'shape mismatch: indexing arrays could not be '
                    f'broadcast together with shapes {shapes}')
                raise IndexError(error_message)
            idx_shape = list(idx_arrays[0].shape) + idx_shape
        elif len(array_indices) == 1:
            arr_index = array_indices[0]
            arr_shape = list(index[arr_index].shape)
            idx_shape = idx_shape[:arr_index] + arr_shape + \
                        idx_shape[arr_index:]
        ndim = len(idx_shape)
        if ndim > 2:
            error_message = \
                f'Only 1D or 2D arrays allowed. Index makes {ndim}D'
            raise IndexError(error_message)
        return tuple(index), tuple(idx_shape)
    
    @property
    def num_threads(self) -> int:
        """
        Get the number of threads used for sparse array operations.
        
        Returns:
            The number of threads used for sparse array operations.
        """
        return self._num_threads
     
    @num_threads.setter
    def num_threads(self, num_threads: int | np.integer) -> None:
        """
        Set the number of threads used for sparse array operations.
        
        Args:
            num_threads: the new number of threads to use for sparse array
                         operations. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`.
        """
        check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
        if num_threads == -1:
            num_threads = os.cpu_count()
        else:
            num_threads = int(num_threads)
            if num_threads <= 0:
                error_message = (
                    f'num_threads is {num_threads:,}, but must be a positive '
                    f'integer or -1')
                raise ValueError(error_message)
        self._num_threads = num_threads
        
    def _get_intXint(self, row, col):
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self.shape)
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data,
            major, major + 1, minor, minor + 1, self._num_threads)
        return data.sum(dtype=self.dtype)

    def _get_sliceXslice(self, row, col):
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        if major.step in (1, None) and minor.step in (1, None):
            return self._get_submatrix(major, minor, copy=True)
        return self._major_slice(major)._minor_slice(minor)

    def _get_arrayXarray(self, row, col):
        # inner indexing
        idx_dtype = self.indices.dtype
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self.shape)
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        major = np.asarray(major, dtype=idx_dtype)
        minor = np.asarray(minor, dtype=idx_dtype)

        val = np.empty(major.size, dtype=self.dtype)
        csr_sample_values(M, N, self.indptr, self.indices, self.data,
                          major.size, major.ravel(), minor.ravel(), val,
                          self._num_threads)
        if major.ndim == 1:
            return val
        result = self.__class__(val.reshape(major.shape))
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _get_columnXarray(self, row, col):
        # outer indexing
        # noinspection PyUnresolvedReferences
        major, minor = self._swap((row, col))
        return self._major_index_fancy(major)._minor_index_fancy(minor)

    def _major_index_fancy(self, idx):
        """Index along the major axis where idx is an array of ints.
        """
        idx_dtype = self._get_index_dtype((self.indptr, self.indices))
        indices = np.asarray(idx, dtype=idx_dtype).ravel()

        # noinspection PyUnresolvedReferences
        N = self._swap(self._shape_as_2d)[1]
        M = len(indices)
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, N)) if self.ndim == 2 else (M,)
        if M == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            result._has_sorted_indices = self._has_sorted_indices
            result._has_canonical_format = self._has_canonical_format
            return result

        row_nnz = (self.indptr[indices + 1] - self.indptr[indices])\
            .astype(idx_dtype, copy=False)
        res_indptr = np.empty(M + 1, dtype=idx_dtype)
        res_indptr[0] = 0
        np.cumsum(row_nnz, out=res_indptr[1:])

        nnz = res_indptr[-1]
        res_indices = np.empty(nnz, dtype=idx_dtype)
        res_data = np.empty(nnz, dtype=self.dtype)
        csr_row_index(
            M,
            indices,
            self.indptr.astype(idx_dtype, copy=False),
            self.indices.astype(idx_dtype, copy=False),
            self.data,
            res_indptr,
            res_indices,
            res_data,
            self._num_threads
        )

        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _major_slice(self, idx, copy=False):
        """Index along the major axis where idx is a slice object.
        """
        if idx == slice(None):
            return self.copy() if copy else self

        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        start, stop, step = idx.indices(M)
        M = len(range(start, stop, step))
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, N)) if self.ndim == 2 else (M,)
        if M == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            result._has_sorted_indices = self._has_sorted_indices
            result._has_canonical_format = self._has_canonical_format
            return result

        # Work out what slices are needed for `row_nnz`
        # start,stop can be -1, only if step is negative
        start0, stop0 = start, stop
        if stop == -1 and start >= 0:
            stop0 = None
        start1, stop1 = start + 1, stop + 1

        row_nnz = self.indptr[start1:stop1:step] - \
            self.indptr[start0:stop0:step]
        idx_dtype = self.indices.dtype
        res_indptr = np.empty(M + 1, dtype=idx_dtype)
        res_indptr[0] = 0
        np.cumsum(row_nnz, out=res_indptr[1:])

        if step == 1:
            all_idx = slice(self.indptr[start], self.indptr[stop])
            res_indices = np.array(self.indices[all_idx], copy=copy)
            res_data = np.array(self.data[all_idx], copy=copy)
        else:
            nnz = res_indptr[-1]
            res_indices = np.empty(nnz, dtype=idx_dtype)
            res_data = np.empty(nnz, dtype=self.dtype)
            csr_row_slice(start, stop, step, self.indptr, self.indices,
                          self.data, res_indptr, res_indices, res_data,
                          self._num_threads)

        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _minor_index_fancy(self, idx):
        """Index along the minor axis where idx is an array of ints.
        """
        idx_dtype = self._get_index_dtype((self.indices, self.indptr))
        indices = self.indices.astype(idx_dtype, copy=False)
        indptr = self.indptr.astype(idx_dtype, copy=False)

        idx = np.asarray(idx, dtype=idx_dtype).ravel()
        
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        k = len(idx)
        # noinspection PyUnresolvedReferences
        new_shape = self._swap((M, k)) if self.ndim == 2 else (k,)
        if k == 0:
            result = self.__class__(new_shape, dtype=self.dtype)
            result._num_threads = self._num_threads
            result._has_sorted_indices = self._has_sorted_indices
            result._has_canonical_format = self._has_canonical_format
            return result

        # pass 1: count idx entries and compute new indptr
        col_offsets = np.empty(N, dtype=idx_dtype)
        res_indptr = np.empty_like(self.indptr, dtype=idx_dtype)
        csr_column_index1(
            k,
            idx,
            M,
            N,
            indptr,
            indices,
            col_offsets,
            res_indptr,
            self._num_threads
        )

        # pass 2: copy indices/data for selected idxs
        col_order = np.argsort(idx).astype(idx_dtype, copy=False)
        nnz = res_indptr[-1]
        res_indices = np.empty(nnz, dtype=idx_dtype)
        res_data = np.empty(nnz, dtype=self.dtype)
        csr_column_index2(col_order, col_offsets, len(self.indices),
                          indptr, indices, self.data, res_indptr, res_indices,
                          res_data, self._num_threads)
        result = self.__class__((res_data, res_indices, res_indptr),
                                shape=new_shape, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _minor_slice(self, idx, copy=False):
        """Index along the minor axis where idx is a slice object.
        """
        if idx == slice(None):
            return self.copy() if copy else self
        
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        start, stop, step = idx.indices(N)
        N = len(range(start, stop, step))
        if N == 0:
            # noinspection PyUnresolvedReferences
            result = self.__class__(self._swap((M, N)), dtype=self.dtype)
            result._num_threads = self._num_threads
            result._has_sorted_indices = self._has_sorted_indices
            result._has_canonical_format = self._has_canonical_format
            return result
        if step == 1:
            return self._get_submatrix(minor=idx, copy=copy)
        return self._minor_index_fancy(np.arange(start, stop, step))

    def _get_submatrix(self, major=None, minor=None, copy=False):
        """Return a submatrix of this matrix.

        major, minor: None, int, or slice with step 1
        """
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        i0, i1 = _process_slice(major, M)
        j0, j1 = _process_slice(minor, N)

        if i0 == 0 and j0 == 0 and i1 == M and j1 == N:
            return self.copy() if copy else self

        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, i0, i1, j0, j1,
            self._num_threads)

        # noinspection PyUnresolvedReferences
        shape = self._swap((i1 - i0, j1 - j0))
        if self.ndim == 1:
            shape = (shape[1],)
        result = self.__class__((data, indices, indptr), shape=shape,
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result
    
    def eliminate_zeros(self):
        """Remove zero entries from the array/matrix

        This is an *in place* operation.
        """
        # noinspection PyUnresolvedReferences
        M, N = self._swap(self._shape_as_2d)
        csr_eliminate_zeros(M, N, self.indptr, self.indices, self.data)
        self.indices = self.indices[:self.nnz]
        self.data = self.data[:self.nnz]
    
    @property
    def has_canonical_format(self) -> bool:
        """Whether the array/matrix has sorted indices and no duplicates

        Returns
            - `True`: if the above applies
            - `False`: otherwise

        `has_canonical_format` implies `has_sorted_indices`, so if the latter
        flag is `False`, so will the former be; if the former is found `True`,
        the latter flag is also set.
        """
        # first check to see if result was cached
        if self._has_sorted_indices is False:
            # not sorted => not canonical
            # noinspection PyAttributeOutsideInit
            self._has_canonical_format = False
        elif self._has_canonical_format is None:
            # noinspection PyAttributeOutsideInit
            self._has_canonical_format = bool(
                csr_has_canonical_format(
                    len(self.indptr) - 1, self.indptr, self.indices,
                    self._num_threads))
            if self._has_canonical_format:
                # noinspection PyAttributeOutsideInit
                self._has_sorted_indices = True
        return self._has_canonical_format

    @has_canonical_format.setter
    def has_canonical_format(self, val: bool):
        # noinspection PyAttributeOutsideInit
        self._has_canonical_format = bool(val)
        if val:
            # noinspection PyAttributeOutsideInit
            self._has_sorted_indices = True
            
    @property
    def has_sorted_indices(self) -> bool:
        """Whether the indices are sorted

        Returns
            - True: if the indices of the array/matrix are in sorted order
            - False: otherwise
        """
        # first check to see if result was cached
        if self._has_sorted_indices is None:
            # noinspection PyAttributeOutsideInit
            self._has_sorted_indices = bool(
                csr_has_sorted_indices(
                    len(self.indptr) - 1, self.indptr, self.indices,
                    self._num_threads))
        return self._has_sorted_indices

    @has_sorted_indices.setter
    def has_sorted_indices(self, val: bool):
        # noinspection PyAttributeOutsideInit
        self._has_sorted_indices = bool(val)
    
    def sort_indices(self):
        """Sort the indices of this array/matrix *in place*
        """
        if not self.has_sorted_indices:
            csr_sort_indices(len(self.indptr) - 1, self.indptr, self.indices,
                             self.data, self._num_threads)
            self._has_sorted_indices = True
    
    def astype(self, dtype, casting='unsafe', copy=True):
        dtype = np.dtype(dtype)
        if self.dtype != dtype:
            matrix = self._with_data(
                self.data.astype(dtype, casting=casting, copy=True),
                copy=True
            )
            result = matrix._with_data(matrix._deduped_data(), copy=False)
            result._num_threads = self._num_threads
            result._has_sorted_indices = self._has_sorted_indices
            result._has_canonical_format = self._has_canonical_format
            return result
        elif copy:
            return self.copy()
        else:
            return self
    
    def copy(self):
        result = self._with_data(self.data.copy(), copy=True)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result


class csr_array(cs_matrix, sparse.csr_array):
    def tocsc(self, copy=False):
        M, N = self.shape
        idx_dtype = self._get_index_dtype((self.indptr, self.indices),
                                          maxval=max(self.nnz, M))
        indptr = np.empty(N + 1, dtype=idx_dtype)
        indices = np.empty(self.nnz, dtype=idx_dtype)
        data = np.empty(self.nnz, dtype=sparse._sputils.upcast(self.dtype))
        
        csr_tocsc(M, N,
                  self.indptr.astype(idx_dtype, copy=False),
                  self.indices.astype(idx_dtype, copy=False),
                  self.data,
                  indptr,
                  indices,
                  data,
                  self._num_threads)
        
        result = csc_array((data, indices, indptr), shape=self.shape)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result
        
    def equals(self, other: Any) -> bool:
        """
        Determine whether this `csr_array` equals another object.
        
        Args:
            other: another object

        Returns:
            `True` if `other` is a `csr_array` and has `data`, `indices` and 
            `indptr` all equal to this one's, `False` otherwise. NaNs will 
            always compare equal.  
        """
        return isinstance(other, csr_array) and sparse_equal(self, other)
    
    def _getrow(self, i):
        """Returns a copy of row i of the matrix, as a (1 x n)
        CSR matrix (row vector).
        """
        if self.ndim == 1:
            if i not in (0, -1):
                error_message = f'index ({i}) out of range'
                raise IndexError(error_message)
            return self.reshape((1, self.shape[0]), copy=True)

        M, N = self.shape
        i = int(i)
        if i < 0:
            i += M
        if i < 0 or i >= M:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, i, i + 1, 0, N,
            self._num_threads)
        result = self.__class__((data, indices, indptr), shape=(1, N),
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _getcol(self, i):
        """Returns a copy of column i. A (m x 1) sparse array (column vector).
        """
        if self.ndim == 1:
            error_message = \
                'getcol not provided for 1d arrays. Use indexing A[j]'
            raise ValueError(error_message)
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += N
        if i < 0 or i >= N:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        indptr, indices, data = get_csr_submatrix(
            M, N, self.indptr, self.indices, self.data, 0, M, i, i + 1,
            self._num_threads)
        result = self.__class__((data, indices, indptr), shape=(M, 1),
                                dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _get_int(self, idx):
        spot = np.flatnonzero(self.indices == idx)
        if spot.size:
            return self.data[spot[0]]
        return self.data.dtype.type(0)

    def _get_slice(self, idx):
        if idx == slice(None):
            return self.copy()
        if idx.step in (1, None):
            ret = self._get_submatrix(0, idx, copy=True)
            return ret.reshape(ret.shape[-1])
        return self._minor_slice(idx)
    
    def _get_intXarray(self, row, col):
        return self._getrow(row)._minor_index_fancy(col)

    def _get_intXslice(self, row, col):
        if col.step in (1, None):
            return self._get_submatrix(row, col, copy=True)
        
        M, N = self.shape
        start, stop, stride = col.indices(N)

        ii, jj = self.indptr[row:row+2]
        row_indices = self.indices[ii:jj]
        row_data = self.data[ii:jj]

        if stride > 0:
            ind = (row_indices >= start) & (row_indices < stop)
        else:
            ind = (row_indices <= start) & (row_indices > stop)

        if abs(stride) > 1:
            ind &= (row_indices - start) % stride == 0

        row_indices = (row_indices[ind] - start) // stride
        row_data = row_data[ind]
        row_indptr = np.array([0, len(row_indices)])

        if stride < 0:
            row_data = row_data[::-1]
            row_indices = abs(row_indices[::-1])

        shape = (1, max(0, int(np.ceil(float(stop - start) / stride))))
        result = self.__class__((row_data, row_indices, row_indptr),
                                shape=shape, dtype=self.dtype, copy=False)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result

    def _get_sliceXint(self, row, col):
        if row.step in (1, None):
            return self._get_submatrix(row, col, copy=True)
        return self._major_slice(row)._get_submatrix(minor=col)

    def _get_sliceXarray(self, row, col):
        return self._major_slice(row)._minor_index_fancy(col)

    def _get_arrayXint(self, row, col):
        return self._major_index_fancy(row)._get_submatrix(minor=col)

    def _get_arrayXslice(self, row, col):
        if col.step not in (1, None):
            col = np.arange(*col.indices(self.shape[1]))
            return self._get_arrayXarray(row, col)
        return self._major_index_fancy(row)._get_submatrix(minor=col)


class csc_array(cs_matrix, sparse.csc_array):
    def tocsr(self, copy=False):
        M, N = self.shape
        idx_dtype = self._get_index_dtype((self.indptr, self.indices),
                                          maxval=max(self.nnz, N))
        indptr = np.empty(M + 1, dtype=idx_dtype)
        indices = np.empty(self.nnz, dtype=idx_dtype)
        data = np.empty(self.nnz, dtype=sparse._sputils.upcast(self.dtype))

        csc_tocsr(M, N,
                  self.indptr.astype(idx_dtype, copy=False),
                  self.indices.astype(idx_dtype, copy=False),
                  self.data,
                  indptr,
                  indices,
                  data,
                  self._num_threads)

        result = csr_array((data, indices, indptr), shape=self.shape)
        result._num_threads = self._num_threads
        result._has_sorted_indices = self._has_sorted_indices
        result._has_canonical_format = self._has_canonical_format
        return result
    
    def equals(self, other: Any) -> bool:
        """
        Determine whether this `csc_array` equals another object.
        
        Args:
            other: another object

        Returns:
            `True` if `other` is a `csc_array` and has `data`, `indices` and 
            `indptr` all equal to this one's, `False` otherwise. NaNs will 
            always compare equal.  
        """
        return isinstance(other, csc_array) and sparse_equal(self, other)
    
    def _getrow(self, i):
        """Returns a copy of row i of the matrix, as a (1 x n)
        CSR matrix (row vector).
        """
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += M
        if i < 0 or i >= M:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        return self._get_submatrix(minor=i).tocsr()

    def _getcol(self, i):
        """Returns a copy of column i of the matrix, as a (m x 1)
        CSC matrix (column vector).
        """
        M, N = self.shape
        i = int(i)
        if i < 0:
            i += N
        if i < 0 or i >= N:
            error_message = f'index ({i}) out of range'
            raise IndexError(error_message)
        return self._get_submatrix(major=i, copy=True)

    def _get_intXarray(self, row, col):
        return self._major_index_fancy(col)._get_submatrix(minor=row)

    def _get_intXslice(self, row, col):
        if col.step in (1, None):
            return self._get_submatrix(major=col, minor=row, copy=True)
        return self._major_slice(col)._get_submatrix(minor=row)

    def _get_sliceXint(self, row, col):
        if row.step in (1, None):
            return self._get_submatrix(major=col, minor=row, copy=True)
        return self._get_submatrix(major=col)._minor_slice(row)

    def _get_sliceXarray(self, row, col):
        return self._major_index_fancy(col)._minor_slice(row)

    def _get_arrayXint(self, row, col):
        return self._get_submatrix(major=col)._minor_index_fancy(row)

    def _get_arrayXslice(self, row, col):
        return self._major_slice(col)._minor_index_fancy(row)


# A drop-in replacement for Cython's `vector` that does not automatically
# zero-initialize

_uninitialized_vector_import = '''
        cdef extern from * nogil:
            """
            // A simplified standalone version of boost::noinit_adaptor
        
            template<class A>
            struct noinit_adaptor : A {
                template<class U>
                struct rebind {
                    typedef noinit_adaptor<typename std::allocator_traits<A>::template
                                           rebind_alloc<U>> other;
                };
        
                template<class U>
                void construct(U* p) {
                    ::new(p) U;
                }
        
                template<class U, class V>
                void construct(U* p, const V& v) {
                    ::new(p) U(v);
                }
        
                template<class U>
                void destroy(U* p) {
                    p->~U();
                }
            };
        
            #include <vector>
            template<class T>
            using uninitialized_vector = std::vector<T, noinit_adaptor<std::allocator<T>>>;
            """
            cdef cppclass uninitialized_vector[T]:
                ctypedef T value_type
                ctypedef size_t size_type
                ctypedef ptrdiff_t difference_type
        
                cppclass const_iterator
                cppclass iterator:
                    iterator() except +
                    iterator(iterator&) except +
                    T& operator*()
                    iterator operator++()
                    iterator operator--()
                    iterator operator++(int)
                    iterator operator--(int)
                    iterator operator+(size_type)
                    iterator operator-(size_type)
                    difference_type operator-(iterator)
                    difference_type operator-(const_iterator)
                    bint operator==(iterator)
                    bint operator==(const_iterator)
                    bint operator!=(iterator)
                    bint operator!=(const_iterator)
                    bint operator<(iterator)
                    bint operator<(const_iterator)
                    bint operator>(iterator)
                    bint operator>(const_iterator)
                    bint operator<=(iterator)
                    bint operator<=(const_iterator)
                    bint operator>=(iterator)
                    bint operator>=(const_iterator)
                cppclass const_iterator:
                    const_iterator() except +
                    const_iterator(iterator&) except +
                    const_iterator(const_iterator&) except +
                    operator=(iterator&) except +
                    const T& operator*()
                    const_iterator operator++()
                    const_iterator operator--()
                    const_iterator operator++(int)
                    const_iterator operator--(int)
                    const_iterator operator+(size_type)
                    const_iterator operator-(size_type)
                    difference_type operator-(iterator)
                    difference_type operator-(const_iterator)
                    bint operator==(iterator)
                    bint operator==(const_iterator)
                    bint operator!=(iterator)
                    bint operator!=(const_iterator)
                    bint operator<(iterator)
                    bint operator<(const_iterator)
                    bint operator>(iterator)
                    bint operator>(const_iterator)
                    bint operator<=(iterator)
                    bint operator<=(const_iterator)
                    bint operator>=(iterator)
                    bint operator>=(const_iterator)
        
                cppclass const_reverse_iterator
                cppclass reverse_iterator:
                    reverse_iterator() except +
                    reverse_iterator(reverse_iterator&) except +
                    T& operator*()
                    reverse_iterator operator++()
                    reverse_iterator operator--()
                    reverse_iterator operator++(int)
                    reverse_iterator operator--(int)
                    reverse_iterator operator+(size_type)
                    reverse_iterator operator-(size_type)
                    difference_type operator-(iterator)
                    difference_type operator-(const_iterator)
                    bint operator==(reverse_iterator)
                    bint operator==(const_reverse_iterator)
                    bint operator!=(reverse_iterator)
                    bint operator!=(const_reverse_iterator)
                    bint operator<(reverse_iterator)
                    bint operator<(const_reverse_iterator)
                    bint operator>(reverse_iterator)
                    bint operator>(const_reverse_iterator)
                    bint operator<=(reverse_iterator)
                    bint operator<=(const_reverse_iterator)
                    bint operator>=(reverse_iterator)
                    bint operator>=(const_reverse_iterator)
                cppclass const_reverse_iterator:
                    const_reverse_iterator() except +
                    const_reverse_iterator(reverse_iterator&) except +
                    operator=(reverse_iterator&) except +
                    const T& operator*()
                    const_reverse_iterator operator++()
                    const_reverse_iterator operator--()
                    const_reverse_iterator operator++(int)
                    const_reverse_iterator operator--(int)
                    const_reverse_iterator operator+(size_type)
                    const_reverse_iterator operator-(size_type)
                    difference_type operator-(iterator)
                    difference_type operator-(const_iterator)
                    bint operator==(reverse_iterator)
                    bint operator==(const_reverse_iterator)
                    bint operator!=(reverse_iterator)
                    bint operator!=(const_reverse_iterator)
                    bint operator<(reverse_iterator)
                    bint operator<(const_reverse_iterator)
                    bint operator>(reverse_iterator)
                    bint operator>(const_reverse_iterator)
                    bint operator<=(reverse_iterator)
                    bint operator<=(const_reverse_iterator)
                    bint operator>=(reverse_iterator)
                    bint operator>=(const_reverse_iterator)
        
                uninitialized_vector() except +
                uninitialized_vector(uninitialized_vector&) except +
                uninitialized_vector(size_type) except +
                uninitialized_vector(size_type, T&) except +
                T& operator[](size_type)
                bint operator==(uninitialized_vector&, uninitialized_vector&)
                bint operator!=(uninitialized_vector&, uninitialized_vector&)
                bint operator<(uninitialized_vector&, uninitialized_vector&)
                bint operator>(uninitialized_vector&, uninitialized_vector&)
                bint operator<=(uninitialized_vector&, uninitialized_vector&)
                bint operator>=(uninitialized_vector&, uninitialized_vector&)
                void assign(size_type, const T&)
                void assign[InputIt](InputIt, InputIt) except +
                T& at(size_type) except +
                T& back()
                iterator begin()
                const_iterator const_begin "begin"()
                const_iterator cbegin()
                size_type capacity()
                void clear()
                bint empty()
                iterator end()
                const_iterator const_end "end"()
                const_iterator cend()
                iterator erase(iterator)
                iterator erase(iterator, iterator)
                T& front()
                iterator insert(iterator, const T&) except +
                iterator insert(iterator, size_type, const T&) except +
                iterator insert[InputIt](iterator, InputIt, InputIt) except +
                size_type max_size()
                void pop_back()
                void push_back(T&) except +
                reverse_iterator rbegin()
                const_reverse_iterator const_rbegin "rbegin"()
                const_reverse_iterator crbegin()
                reverse_iterator rend()
                const_reverse_iterator const_rend "rend"()
                const_reverse_iterator crend()
                void reserve(size_type) except +
                void resize(size_type) except +
                void resize(size_type, T&) except +
                size_type size()
                void swap(uninitialized_vector&)
                T* data()
                const T* const_data "data"()
                void shrink_to_fit() except +
                iterator emplace(const_iterator, ...) except +
                T& emplace_back(...) except +
'''

# Functions for min- and max-heaps

_heap_functions = '''
        cdef inline void max_heap_replace_top(unsigned* labels_i,
                                              float* distances_i,
                                              const unsigned label,
                                              const float distance,
                                              const unsigned k) noexcept nogil:
            # Replaces the top element from the max-heap defined by `distances_i[0..k-1]`
            # and `labels_i[0..k-1]`. Equivalent to `std::pop_heap` followed by
            # `std::push_heap`, but done more efficiently as a single operation.

            cdef unsigned j = 1, child
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            labels_i -= 1
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] <= distances_i[child + 1]
                if distance > distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                labels_i[j] = labels_i[child]
                j = child
            distances_i[j] = distance
            labels_i[j] = label

        cdef inline void max_heap_pop(unsigned* labels_i,
                                      float* distances_i,
                                      const unsigned k) noexcept nogil:
            # Pops the top element from the max-heap defined by `distances_i[0..k-1]` and
            # `labels_i[0..k-1]`. On output the `k-1`th element is undefined.

            cdef unsigned label, j = 1, child
            cdef float distance
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            labels_i -= 1
            distance = distances_i[k]
            label = labels_i[k]
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] <= distances_i[child + 1]
                if distance > distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                labels_i[j] = labels_i[child]
                j = child
            distances_i[j] = distance
            labels_i[j] = label

        cdef inline void max_heap_sort(unsigned* labels_i,
                                       float* distances_i,
                                       const unsigned k) noexcept nogil:
            cdef unsigned j, label
            cdef float distance
            for j in range(k):
                # Save the root (maximum element)
                distance = distances_i[0]
                label = labels_i[0]
                # Restore the heap property with reduced size `k - i`
                max_heap_pop(labels_i, distances_i, k - j)
                # Place the maximum element after the end of the heap
                distances_i[k - j - 1] = distance
                labels_i[k - j - 1] = label

        cdef inline void min_heap_replace_top(unsigned* labels_i,
                                              float* distances_i,
                                              const unsigned label,
                                              const float distance,
                                              const unsigned k) noexcept nogil:
            # Replaces the top element from the min-heap defined by `distances_i[0..k-1]`
            # and `labels_i[0..k-1]`. Equivalent to `std::pop_heap` followed by
            # `std::push_heap`, but done more efficiently as a single operation.
            
            cdef unsigned j = 1, child
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            labels_i -= 1
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] >= distances_i[child + 1]
                if distance < distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                labels_i[j] = labels_i[child]
                j = child
            distances_i[j] = distance
            labels_i[j] = label
        
        cdef inline void min_heap_pop(unsigned* labels_i,
                                      float* distances_i,
                                      const unsigned k) noexcept nogil:
            # Pops the top element from the min-heap defined by `distances_i[0..k-1]` and
            # `labels_i[0..k-1]`. On output the `k-1`th element is undefined.
        
            cdef unsigned label, j = 1, child
            cdef float distance
            distances_i -= 1  # use 1-based indexing for easier node->child translation
            labels_i -= 1
            distance = distances_i[k]
            label = labels_i[k]
            while True:
                child = j << 1
                if child > k:
                    break
                child += child < k and distances_i[child] >= distances_i[child + 1]
                if distance < distances_i[child]:
                    break
                distances_i[j] = distances_i[child]
                labels_i[j] = labels_i[child]
                j = child
            distances_i[j] = distance
            labels_i[j] = label
        
        cdef inline void min_heap_sort(unsigned* labels_i,
                                       float* distances_i,
                                       const unsigned k) noexcept nogil:
            cdef unsigned j, label
            cdef float distance
            for j in range(k):
                # Save the root (minimum element)
                distance = distances_i[0]
                label = labels_i[0]
                # Restore the heap property with reduced size `k - i`
                min_heap_pop(labels_i, distances_i, k - j)
                # Place the minimum element after the end of the heap
                distances_i[k - j - 1] = distance
                labels_i[k - j - 1] = label
'''

# Functions for k-means clustering and nearest-neighbors search

_kmeans_and_knn_functions = cython_inline(_uninitialized_vector_import + _heap_functions + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libc.float cimport FLT_MAX
        from libc.limits cimport UINT_MAX
        from libc.string cimport memcpy
        from libcpp.algorithm cimport fill
        from libcpp.cmath cimport sqrt
        from libcpp.vector cimport vector
        from scipy.linalg.cython_blas cimport sgemm as sgemm_

        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))

        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state

        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef inline float random_uniform(unsigned long* state) noexcept nogil:
            # Returns a random number in U(0, 1)
            return <float> rand(state) / UINT_MAX
        
        cdef inline void sgemm(const char transa,
                               const char transb,
                               const int m,
                               const int n,
                               const int k,
                               const float alpha,
                               const float* a,
                               const int lda,
                               const float* b,
                               const int ldb,
                               const float beta,
                               float* c,
                               const int ldc) noexcept nogil:
            # Matrix multiplication wrapper that accepts scalars by value and
            # provides const-correctness
            
            sgemm_(<char*> &transa, <char*> &transb, <int*> &m, <int*> &n, <int*> &k,
                   <float*> &alpha, <float*> a, <int*> &lda, <float*> b, <int*> &ldb,
                   <float*> &beta, c, <int*> &ldc)

        cdef inline void kmeans_random_init(const float[:, ::1] X,
                                            float[:, ::1] centroids,
                                            const unsigned num_cells,
                                            const unsigned num_clusters,
                                            const unsigned long seed):
            # Initialize centroids with random cells. Use a bitmap to efficiently keep
            # track of which cells have been sampled already, to avoid sampling the
            # same cell twice.
            
            cdef unsigned i, j
            cdef unsigned long word_index, bit_index, state = srand(seed)
            cdef vector[unsigned long] bitmap_buffer
            bitmap_buffer.resize((num_cells + 63) / 64)
            cdef unsigned long[::1] bitmap = \
                <unsigned long[:(num_cells + 63) / 64]> bitmap_buffer.data()
            
            for i in range(num_clusters):
                while True:
                    j = randint(num_cells, &state)
                    word_index = j >> 6
                    bit_index = j & 63
                    if not bitmap[word_index] & (1 << bit_index):
                        bitmap[word_index] |= 1 << bit_index
                        centroids[i] = X[j]
                        break
        
        cdef inline void kmeans_barbar_init(
                const float[:, ::1] X,
                float[:, ::1] centroids,
                const unsigned num_init_iterations,
                const float oversampling_factor,
                const unsigned num_cells,
                const unsigned num_clusters,
                const unsigned num_dimensions,
                const unsigned chunk_size,
                const unsigned long seed):
            cdef unsigned i, j, k, random_cell, c0, c1, iteration, \
                num_newly_selected_cells, num_previously_selected_cells, \
                selected_cell, chunk_index, start, chunk_num_cells, \
                num_selected_cells, end, cluster_index, selected_centroid, \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size
            cdef int best_selected_cell
            cdef unsigned long state
            cdef float cost, difference, distance, norm, l_over_cost, chunk_cost, \
                min_distance, inverse_cost, probability, alpha = -2, beta = 1, \
                l = oversampling_factor * num_clusters
            cdef char transA = b'T', transB = b'N'
            cdef uninitialized_vector[float] min_distances_buffer, X_norms_buffer, \
                newly_selected_X_buffer, distances_buffer, chunk_costs_buffer
            cdef vector[unsigned] best_selected_cells_buffer, selected_cells, \
                selected_cell_weights_buffer
            cdef uninitialized_vector[unsigned] centroid_indices_buffer
            cdef float[::1] min_distances, X_norms, chunk_costs
            cdef float[:, ::1] newly_selected_X, distances
            cdef unsigned[::1] best_selected_cells, selected_cell_weights, \
                centroid_indices
            
            # Reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
            
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # This will constitute a shortlist from which we will select the final
            # centroids to initialize k-means with.
            state = srand(seed - 1)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
            
            # Calculate the (squared Euclidean) distance from each cell to the
            # random cell, storing it in `min_distances`. In the same loop, also
            # calculate the squared L2 norm of each cell, ||X||².
            min_distances_buffer.resize(num_cells)
            min_distances = <float[:num_cells]> min_distances_buffer.data()
            X_norms_buffer.resize(num_cells)
            X_norms = <float[:num_cells]> X_norms_buffer.data()
            for i in range(num_cells):
                difference = X[i, 0] - X[random_cell, 0]
                distance = difference * difference
                norm = X[i, 0] * X[i, 0]
                for j in range(1, num_dimensions):
                    difference = X[i, j] - X[random_cell, j]
                    distance += difference * difference
                    norm += X[i, j] * X[i, j]
                min_distances[i] = distance
                X_norms[i] = norm
            
            # Sum the `min_distances` separately at the end, to ensure
            # deterministic parallelism
            cost = 0
            for i in range(num_cells):
                cost += min_distances[i]
            
            # Sample each cell with probability `l * min_distances[i] / cost`.
            # Set `min_distances` to zero for sampled cells to reflect that
            # each sampled cell's nearest centroid candidate is now itself.
            l_over_cost = l / cost
            for i in range(num_cells):
                state = srand(seed + i)
                if l_over_cost * min_distances[i] >= random_uniform(&state):
                    selected_cells.push_back(i)
                    min_distances[i] = 0
            
            # Keep track of how many cells were selected, newly selected on this past
            # iteration (for this first iteration, all cells except the first random
            # cell), and selected on a previous iteration (just the first random cell).
            num_selected_cells = selected_cells.size()
            num_newly_selected_cells = num_selected_cells - 1
            num_previously_selected_cells = 1
            
            # `best_selected_cells` maps each cell to the index in `selected_cells`
            # with the cell's nearest candidate centroid selected so far. For each
            # newly selected cell, set `best_selected_cells[selected_cell]` to the
            # cell's index, to reflect that the cell's nearest centroid candidate is
            # now itself. `best_selected_cells` starts off at 0 for non-newly
            # selected cells, i.e. they still have the initial random cell
            # (`selected_cells[0]`) listed as their closest centroid candidate.
            best_selected_cells_buffer.resize(num_cells)
            best_selected_cells = <unsigned[:num_cells]> \
                best_selected_cells_buffer.data()
            for i in range(num_previously_selected_cells, num_selected_cells):
                selected_cell = selected_cells[i]
                best_selected_cells[selected_cell] = i
            
            # For each remaining iteration...
            chunk_costs_buffer.resize(num_chunks)
            chunk_costs = <float[:num_chunks]> chunk_costs_buffer.data()
            iteration = 1
            while True:
                # For very small datasets, it's possible no cells are selected
                # on a given iteration. In that case, skip the distance and cost
                # calculations and go straight to sampling more cells.
                if num_newly_selected_cells > 0:
                    # Copy newly selected cells into a temporary buffer
                    newly_selected_X_buffer.resize(num_newly_selected_cells * num_dimensions)
                    newly_selected_X = \
                        <float[:num_newly_selected_cells, :num_dimensions]> \
                        newly_selected_X_buffer.data()
                    for i in range(num_newly_selected_cells):
                        selected_cell = selected_cells[num_previously_selected_cells + i]
                        newly_selected_X[i, :] = X[selected_cell, :]
                    
                    # Update each cell's nearest candidate centroid selected so far
                    # (`best_selected_cells`) and distance to this candidate centroid
                    # (`min_distances`) to account for the newly selected cells. We
                    # only have to compute distances from each cell to the newly
                    # selected cells, rather than to all selected cells. Use the
                    # identity:
                    # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                    # but add the ||X||² at the end since the minimum distance for
                    # a given cell doesn't depend on ||X||².
                    distances_buffer.resize(chunk_size * num_newly_selected_cells)
                    distances = <float[:chunk_size, :num_newly_selected_cells]> \
                        distances_buffer.data()
                    for chunk_index in range(num_chunks):
                        chunk_cost = 0
                        start = chunk_index * chunk_size
                        chunk_num_cells = num_cells - start \
                            if chunk_index == num_chunks - 1 else chunk_size
                        for i in range(chunk_num_cells):
                            for j in range(num_newly_selected_cells):
                                # distances = ||C||²
                                selected_cell = \
                                    selected_cells[num_previously_selected_cells + j]
                                distances[i, j] = X_norms[selected_cell]
                        
                        # distances -= 2 * X.dot(C.T)
                        sgemm(transA, transB, num_newly_selected_cells,
                              chunk_num_cells, num_dimensions, alpha,
                              &newly_selected_X[0, 0], num_dimensions, &X[start, 0],
                              num_dimensions, beta, &distances[0, 0],
                              num_newly_selected_cells)
                        
                        # distances += ||X||², and find the best distance. As an
                        # optimization, to avoid having to add `X_norms[start + i]`
                        # (i.e. ||X||²) to `min_distance` and
                        # `num_previously_selected_cells` to `best_selected_cell` to
                        # every candidate inside the inner loop (which would be
                        # necessary to make them comparable to the previous minimum
                        # distance and best selected cell), subtract these terms at the
                        # start and add them back at the end. (`best_selected_cell` is
                        # declared as a signed integer to allow for underflow here.)
                        for i in range(chunk_num_cells):
                            min_distance = min_distances[start + i] - X_norms[start + i]
                            best_selected_cell = best_selected_cells[start + i] - num_previously_selected_cells
                            for j in range(num_newly_selected_cells):
                                distance = distances[i, j]
                                if distance < min_distance:
                                    min_distance = distance
                                    best_selected_cell = j
                            min_distance += X_norms[start + i]
                            min_distances[start + i] = min_distance
                            best_selected_cells[start + i] = best_selected_cell + num_previously_selected_cells
                            chunk_cost += min_distance
                        chunk_costs[chunk_index] = chunk_cost
                    
                # Sum costs for each chunk separately at the end, to ensure
                # deterministic parallelism
                cost = 0
                for chunk_index in range(num_chunks):
                    cost += chunk_costs[chunk_index]
                    
                # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                # Note that since we set `min_distances` to 0 for cells that were
                # sampled, we will avoid sampling them twice. As before, set
                # `min_distances` to zero for newly sampled cells, to reflect that
                # each sampled cell's nearest centroid candidate is now itself.
                l_over_cost = l / cost
                for i in range(num_cells):
                    state = srand(seed + i * iteration)
                    probability = random_uniform(&state)
                    if l_over_cost * min_distances[i] >= probability:
                        selected_cells.push_back(i)
                        min_distances[i] = 0
                        best_selected_cells[i] = i
                
                # Update the number of selected, previously selected and newly
                # selected cells
                num_previously_selected_cells = num_selected_cells
                num_selected_cells = selected_cells.size()
                num_newly_selected_cells = num_selected_cells - num_previously_selected_cells
                
                # As before, set `best_selected_cells` for each newly selected cell to the
                # cell's index, to reflect that the cell's nearest centroid candidate is
                # now itself.
                for i in range(num_previously_selected_cells, num_selected_cells):
                    selected_cell = selected_cells[i]
                    best_selected_cells[selected_cell] = i
                
                # Check for KeyboardInterrupts after each iteration
                PyErr_CheckSignals()
                
                # Stop after `num_init_iterations` iterations, unless we have found
                # fewer than `num_clusters` centroid candidates so far. Also stop
                # if we have already selected every cell as a centroid candidate,
                # which can happen if the dataset is very small.
                iteration += 1
                if iteration == num_init_iterations and \
                        num_selected_cells >= num_clusters or \
                        num_selected_cells == num_cells:
                    break
            
            # Now we are done selecting cells as candidate centroids and need to
            # whittle down to the final centroids. Get the weight for each selected
            # cell: the number of cells that are closer to the selected cell than
            # to any other selected cell.
            selected_cell_weights_buffer.resize(num_selected_cells)
            selected_cell_weights = <unsigned[:num_selected_cells]> \
                selected_cell_weights_buffer.data()
            for i in range(num_cells):
                best_selected_cell = best_selected_cells[i]
                selected_cell_weights[best_selected_cell] += 1
            
            # Run k-means++ to select `num_clusters` of the selected cells as the
            # centroids, using `selected_cell_weights` as weights. Start by selecting a
            # random cell from our selected cells as the first centroid.
            state = srand(seed - 2)
            random_cell = selected_cells[randint(num_selected_cells, &state)]
            centroid_indices_buffer.resize(num_clusters)
            centroid_indices = <unsigned[:num_clusters]> centroid_indices_buffer.data()
            centroid_indices[0] = random_cell
            centroids[0] = X[random_cell]
            
            # Return if only one cluster was requested
            if num_clusters == 1:
                return
        
            # Find each selected cell's distance to this centroid, weighted
            # by the cell's weight in `selected_cell_weights`. Store these
            # distances in the first `num_selected_cells` entries of the
            # `min_distances` vector.
            for i in range(num_selected_cells):
                selected_cell = selected_cells[i]
                difference = X[selected_cell, 0] - X[random_cell, 0]
                distance = difference * difference
                for j in range(1, num_dimensions):
                    difference = X[selected_cell, j] - X[random_cell, j]
                    distance += difference * difference
                distance *= selected_cell_weights[i]
                min_distances[i] = distance
            
            # Sum the weighted distances separately at the end, to ensure
            # deterministic parallelism
            cost = 0
            for i in range(num_selected_cells):
                cost += min_distances[i]
            
            # Iteratively select the remaining centroids
            cluster_index = 1
            while True:
                # Sample a single cell `i` with probability
                # `min_distances[i] / cost`. Set `min_distances` to 0 for the
                # sampled cell, to avoid sampling it twice.
                inverse_cost = 1 / cost
                probability = random_uniform(&state)
                i = 0
                while True:
                    probability -= min_distances[i] * inverse_cost
                    if probability < 0:
                        break
                    i += 1
                min_distances[i] = 0
                centroid_indices[cluster_index] = selected_cells[i]
                centroids[cluster_index] = X[selected_cells[i]]
                
                # Stop once all centroids have been selected
                if cluster_index == num_clusters - 1:
                    break
                
                # Update each selected cell's weighted distance to its nearest
                # centroid, if it is closer to this new centroid than to any we
                # have selected so far.
                for i in range(num_selected_cells):
                    selected_cell = selected_cells[i]
                    difference = X[selected_cell, 0] - centroids[cluster_index, 0]
                    distance = difference * difference
                    for j in range(1, num_dimensions):
                        difference = X[selected_cell, j] - centroids[cluster_index, j]
                        distance += difference * difference
                    distance *= selected_cell_weights[i]
                    if distance < min_distances[i]:
                        min_distances[i] = distance
                
                # Sum the weighted distances separately at the end, to ensure
                # deterministic parallelism
                cost = 0
                for i in range(num_selected_cells):
                    cost += min_distances[i]

                # Increment the centroid counter
                cluster_index += 1
        
        cdef inline void kmeans_barbar_init_parallel(
                const float[:, ::1] X,
                float[:, ::1] centroids,
                const unsigned num_init_iterations,
                const float oversampling_factor,
                const unsigned num_cells,
                const unsigned num_clusters,
                const unsigned num_dimensions,
                const unsigned long seed,
                const unsigned chunk_size,
                const unsigned num_threads):
            cdef unsigned i, j, k, random_cell, thread_index, c0, c1, iteration, \
                num_newly_selected_cells, num_previously_selected_cells, \
                selected_cell, chunk_index, start, chunk_num_cells, chunk_size_2, \
                num_selected_cells, end, cluster_index, selected_centroid, \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size
            cdef int best_selected_cell
            cdef unsigned long state
            cdef float cost, difference, distance, norm, l_over_cost, chunk_cost, \
                min_distance, inverse_cost, probability, alpha = -2, beta = 1, \
                l = oversampling_factor * num_clusters
            cdef char transA = b'T', transB = b'N'
            cdef uninitialized_vector[float] min_distances_buffer, X_norms_buffer, \
                newly_selected_X, chunk_costs_buffer
            cdef vector[unsigned] best_selected_cells_buffer, selected_cells, \
                selected_cell_weights
            cdef uninitialized_vector[unsigned] centroid_indices_buffer
            cdef vector[uninitialized_vector[float]] thread_distances
            cdef vector[vector[unsigned]] thread_selected_cells, \
                thread_selected_cell_weights
            cdef float[::1] min_distances, X_norms, chunk_costs
            cdef unsigned[::1] best_selected_cells, centroid_indices
            cdef unsigned* cell_weights
            cdef float* distance_pointer

            # Reserve 25% more than the expected number to be safe
            selected_cells.reserve(<unsigned> (1.25 * num_init_iterations * l))
            
            # Sample a random cell from `X`, and add it to our list of selected cells.
            # This will constitute a shortlist from which we will select the final
            # centroids to initialize k-means with.
            state = srand(seed - 1)
            random_cell = randint(num_cells, &state)
            selected_cells.push_back(random_cell)
            
            # Calculate the (squared Euclidean) distance from each cell to the random
            # cell, storing it in `min_distances`. In the same loop, also calculate the
            # squared L2 norm of each cell, ||X||².
            min_distances_buffer.resize(num_cells)
            min_distances = <float[:num_cells]> min_distances_buffer.data()
            X_norms_buffer.resize(num_cells)
            X_norms = <float[:num_cells]> X_norms_buffer.data()
            with nogil:
                for i in prange(num_cells, num_threads=num_threads):
                    difference = X[i, 0] - X[random_cell, 0]
                    distance = difference * difference
                    norm = X[i, 0] * X[i, 0]
                    for j in range(1, num_dimensions):
                        difference = X[i, j] - X[random_cell, j]
                        distance = distance + difference * difference
                        norm = norm + X[i, j] * X[i, j]
                    min_distances[i] = distance
                    X_norms[i] = norm
                
                # Sum the `min_distances` single-threaded at the end, to ensure
                # deterministic parallelism
                cost = 0
                for i in range(num_cells):
                    cost += min_distances[i]
                
                # Sample each cell with probability `l * min_distances[i] / cost`.
                # Set `min_distances` to zero for sampled cells to reflect that
                # each sampled cell's nearest centroid candidate is now itself.
                l_over_cost = l / cost
                thread_selected_cells.resize(num_threads)
                with parallel(num_threads=num_threads):
                    thread_index = threadid()
                    thread_selected_cells[thread_index].reserve(
                        <unsigned> (1.25 * l / num_threads))
                    c0 = num_cells * thread_index / num_threads
                    c1 = num_cells * (thread_index + 1) / num_threads
                    for i in range(c0, c1):
                        state = srand(seed + i)
                        if l_over_cost * min_distances[i] >= random_uniform(&state):
                            thread_selected_cells[thread_index].push_back(i)
                            min_distances[i] = 0
            
            # Aggregate each thread's selected cells into a single vector
            for thread_index in range(num_threads):
                selected_cells.insert(
                    selected_cells.end(),
                    thread_selected_cells[thread_index].begin(),
                    thread_selected_cells[thread_index].end())
            
            # Keep track of how many cells were selected, newly selected on this past
            # iteration (for this first iteration, all cells except the first random
            # cell), and selected on a previous iteration (just the first random cell).
            num_selected_cells = selected_cells.size()
            num_newly_selected_cells = num_selected_cells - 1
            num_previously_selected_cells = 1
            
            # `best_selected_cells` maps each cell to the index in `selected_cells`
            # with the cell's nearest candidate centroid selected so far. For each
            # newly selected cell, set `best_selected_cells[selected_cell]` to the
            # cell's index, to reflect that the cell's nearest centroid candidate is
            # now itself. `best_selected_cells` starts off at 0 for non-newly
            # selected cells, i.e. they still have the initial random cell
            # (`selected_cells[0]`) listed as their closest centroid candidate.
            best_selected_cells_buffer.resize(num_cells)
            best_selected_cells = <unsigned[:num_cells]> \
                best_selected_cells_buffer.data()
            for i in range(num_previously_selected_cells, num_selected_cells):
                selected_cell = selected_cells[i]
                best_selected_cells[selected_cell] = i
            
            # For each remaining iteration...
            chunk_costs_buffer.resize(num_chunks)
            chunk_costs = <float[:num_chunks]> chunk_costs_buffer.data()
            thread_distances.resize(num_threads)
            iteration = 1
            while True:
                with nogil:
                    # For very small datasets, it's possible no cells are selected
                    # on a given iteration. In that case, skip the distance and cost
                    # calculations and go straight to sampling more cells.
                    if num_newly_selected_cells > 0:
                        # Copy newly selected cells into a temporary buffer
                        newly_selected_X.resize(
                            num_newly_selected_cells * num_dimensions)
                        for i in prange(num_newly_selected_cells,
                                        num_threads=num_threads):
                            selected_cell = selected_cells[
                                num_previously_selected_cells + i]
                            memcpy(newly_selected_X.data() + i * num_dimensions,
                                   &X[selected_cell, 0],
                                   num_dimensions * sizeof(float))
                        
                        # Update each cell's nearest candidate centroid selected so far
                        # (`best_selected_cells`) and distance to this candidate centroid
                        # (`min_distances`) to account for the newly selected cells. We
                        # only have to compute distances from each cell to the newly
                        # selected cells, rather than to all selected cells. Use the
                        # identity:
                        # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                        # but add the ||X||² at the end since the minimum distance for
                        # a given cell doesn't depend on ||X||².
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_distances[thread_index].resize(
                                chunk_size * num_newly_selected_cells)
                            distance_pointer = thread_distances[thread_index].data()
                            for chunk_index in prange(num_chunks):
                                chunk_cost = 0
                                start = chunk_index * chunk_size
                                chunk_num_cells = num_cells - start \
                                    if chunk_index == num_chunks - 1 else chunk_size
                                for i in range(chunk_num_cells):
                                    for j in range(num_newly_selected_cells):
                                        # distances = ||C||²
                                        selected_cell = \
                                            selected_cells[num_previously_selected_cells + j]
                                        distance_pointer[i * num_newly_selected_cells + j] = \
                                            X_norms[selected_cell]
                                
                                # distances -= 2 * X.dot(C.T)
                                sgemm(transA, transB, num_newly_selected_cells,
                                      chunk_num_cells, num_dimensions, alpha,
                                      newly_selected_X.data(), num_dimensions,
                                      &X[start, 0], num_dimensions, beta,
                                      distance_pointer, num_newly_selected_cells)
                                
                                # distances += ||X||², and find the best distance. As an
                                # optimization, to avoid having to add `X_norms[start + i]`
                                # (i.e. ||X||²) to `min_distance` and
                                # `num_previously_selected_cells` to `best_selected_cell` to
                                # every candidate inside the inner loop (which would be
                                # necessary to make them comparable to the previous minimum
                                # distance and best selected cell), subtract these terms at the
                                # start and add them back at the end. (`best_selected_cell` is
                                # declared as a signed integer to allow for underflow here.)
                                for i in range(chunk_num_cells):
                                    min_distance = min_distances[start + i] - X_norms[start + i]
                                    best_selected_cell = best_selected_cells[start + i] - num_previously_selected_cells
                                    for j in range(num_newly_selected_cells):
                                        distance = distance_pointer[i * num_newly_selected_cells + j]
                                        if distance < min_distance:
                                            min_distance = distance
                                            best_selected_cell = j
                                    min_distance = min_distance + X_norms[start + i]
                                    min_distances[start + i] = min_distance
                                    best_selected_cells[start + i] = best_selected_cell + num_previously_selected_cells
                                    chunk_cost = chunk_cost + min_distance
                                chunk_costs[chunk_index] = chunk_cost
                        
                        # Sum costs for each chunk single-threaded at the end, to ensure
                        # deterministic parallelism
                        cost = 0
                        for chunk_index in range(num_chunks):
                            cost += chunk_costs[chunk_index]
        
                    # Sample each cell `i` with probability `l * min_distances[i] / cost`.
                    # Note that since we set `min_distances` to 0 for cells that were
                    # sampled, we will avoid sampling them twice. As before, set
                    # `min_distances` to zero for newly sampled cells, to reflect that
                    # each sampled cell's nearest centroid candidate is now itself.
                    l_over_cost = l / cost
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_selected_cells[thread_index].clear()  # reset
                        c0 = num_cells * thread_index / num_threads
                        c1 = num_cells * (thread_index + 1) / num_threads
                        for i in range(c0, c1):
                            state = srand(seed + i * iteration)
                            probability = random_uniform(&state)
                            if l_over_cost * min_distances[i] >= probability:
                                thread_selected_cells[thread_index].push_back(i)
                                min_distances[i] = 0
                                best_selected_cells[i] = i
                
                # Aggregate each thread's selected cells into a single vector.
                # Update the number of selected, previously selected and newly
                # selected cells.
                num_previously_selected_cells = num_selected_cells
                for thread_index in range(num_threads):
                    selected_cells.insert(
                        selected_cells.end(),
                        thread_selected_cells[thread_index].begin(),
                        thread_selected_cells[thread_index].end())
                num_selected_cells = selected_cells.size()
                num_newly_selected_cells = num_selected_cells - num_previously_selected_cells
                
                # As before, set `best_selected_cells` for each newly selected cell to the
                # cell's index, to reflect that the cell's nearest centroid candidate is
                # now itself.
                for i in range(num_previously_selected_cells, num_selected_cells):
                    selected_cell = selected_cells[i]
                    best_selected_cells[selected_cell] = i
                
                # Check for KeyboardInterrupts after each iteration
                PyErr_CheckSignals()
                
                # Stop after `num_init_iterations` iterations, unless we have found
                # fewer than `num_clusters` centroid candidates so far. Also stop
                # if we have already selected every cell as a centroid candidate,
                # which can happen if the dataset is very small.
                iteration += 1
                if iteration == num_init_iterations and \
                        num_selected_cells >= num_clusters or \
                        num_selected_cells == num_cells:
                    break
            
            centroid_indices_buffer.resize(num_clusters)
            centroid_indices = <unsigned[:num_clusters]> centroid_indices_buffer.data()
            with nogil:
                # Now we are done selecting cells as candidate centroids and need to
                # whittle down to the final centroids. Get the weight for each selected
                # cell: the number of cells that are closer to the selected cell than
                # to any other selected cell. Store weights for each thread in a
                # temporary buffer, then aggregate at the end. As an optimization, put
                # the row sums for the last thread (`thread_index == num_threads - 1`)
                # directly into the final `selected_cell_weights` vector.
                thread_selected_cell_weights.resize(num_threads - 1)
                chunk_size_2 = num_cells / num_threads
                with parallel(num_threads=num_threads):
                    thread_index = threadid()
                    start = thread_index * chunk_size_2
                    if thread_index == num_threads - 1:
                        end = num_cells
                        selected_cell_weights.resize(num_selected_cells)
                        for i in range(start, end):
                            best_selected_cell = best_selected_cells[i]
                            selected_cell_weights[best_selected_cell] += 1
                    else:
                        thread_selected_cell_weights[thread_index].resize(num_selected_cells)
                        cell_weights = thread_selected_cell_weights[thread_index].data()
                        end = start + chunk_size_2
                        for i in range(start, end):
                            best_selected_cell = best_selected_cells[i]
                            cell_weights[best_selected_cell] += 1
                for thread_index in range(num_threads - 1):
                    cell_weights = thread_selected_cell_weights[thread_index].data()
                    for i in range(num_selected_cells):
                        selected_cell_weights[i] += cell_weights[i]
                
                # Run k-means++ to select `num_clusters` of the selected cells as the
                # centroids, using `selected_cell_weights` as weights. Start by selecting a
                # random cell from our selected cells as the first centroid.
                state = srand(seed - 2)
                random_cell = selected_cells[randint(num_selected_cells, &state)]
                centroid_indices[0] = random_cell
                centroids[0] = X[random_cell]
                
                # Return if only one cluster was requested
                if num_clusters == 1:
                    return
            
                # Find each selected cell's distance to this centroid, weighted
                # by the cell's weight in `selected_cell_weights`. Store these
                # distances in the first `num_selected_cells` entries of the
                # `min_distances` vector.
                for i in prange(num_selected_cells, num_threads=num_threads):
                    selected_cell = selected_cells[i]
                    difference = X[selected_cell, 0] - X[random_cell, 0]
                    distance = difference * difference
                    for j in range(1, num_dimensions):
                        difference = X[selected_cell, j] - X[random_cell, j]
                        distance = distance + difference * difference
                    min_distances[i] = distance * selected_cell_weights[i]
                
                # Sum the weighted distances single-threaded at the end, to ensure
                # deterministic parallelism
                cost = 0
                for i in range(num_selected_cells):
                    cost += min_distances[i]
                
                # Iteratively select the remaining centroids
                cluster_index = 1
                while True:
                    # Sample a single cell `i` with probability
                    # `min_distances[i] / cost`. Set `min_distances` to 0 for the
                    # sampled cell, to avoid sampling it twice.
                    inverse_cost = 1 / cost
                    probability = random_uniform(&state)
                    i = 0
                    while True:
                        probability -= min_distances[i] * inverse_cost
                        if probability < 0:
                            break
                        i += 1
                    min_distances[i] = 0
                    centroid_indices[cluster_index] = selected_cells[i]
                    centroids[cluster_index] = X[selected_cells[i]]
                    
                    # Stop once all centroids have been selected
                    if cluster_index == num_clusters - 1:
                        break
                    
                    # Update each selected cell's weighted distance to its nearest
                    # centroid, if it is closer to this new centroid than to any
                    # we have selected so far.
                    cost = 0
                    for i in prange(num_selected_cells, num_threads=num_threads):
                        selected_cell = selected_cells[i]
                        difference = X[selected_cell, 0] - centroids[cluster_index, 0]
                        distance = difference * difference
                        for j in range(1, num_dimensions):
                            difference = X[selected_cell, j] - centroids[cluster_index, j]
                            distance = distance + difference * difference
                        distance = distance * selected_cell_weights[i]
                        if distance < min_distances[i]:
                            min_distances[i] = distance
                    
                    # Sum the weighted distances single-threaded at the end, to ensure
                    # deterministic parallelism
                    cost = 0
                    for i in range(num_selected_cells):
                        cost += min_distances[i]

                    # Increment the centroid counter
                    cluster_index += 1
        
        cdef inline void relocate_empty_clusters(
                const float[:, ::1] X,
                const unsigned[::1] cluster_labels,
                const float[:, ::1] centroids,
                float[:, ::1] centroids_new,
                const unsigned[::1] num_cells_per_cluster):
            # Relocate centroids with no cells assigned to them
        
            cdef unsigned i, j, k, num_empty, new_cluster_label, old_cluster_label, \
                num_cells = X.shape[0], num_dimensions = X.shape[1], \
                num_clusters = centroids.shape[0]
            cdef float distance, difference
            cdef vector[unsigned] empty_cluster_indices
            cdef uninitialized_vector[unsigned] farthest_cells_buffer
            cdef uninitialized_vector[float] farthest_distances_buffer
            cdef unsigned[::1] farthest_cells
            cdef float[::1] farthest_distances
            cdef str error_message
            
            # Collect indices of empty clusters
            for i in range(num_clusters):
                if num_cells_per_cluster[i] == 0:
                    empty_cluster_indices.push_back(i)
            
            # Return if no clusters are empty
            num_empty = empty_cluster_indices.size()
            if num_empty == 0:
                return
            
            # Find the `num_empty` farthest points from their assigned centroids,
            # using a min-heap to keep track of the `num_empty` largest distances
            farthest_cells_buffer.resize(num_empty)
            farthest_cells = <unsigned[:num_empty]> farthest_cells_buffer.data()
            farthest_distances_buffer.resize(num_empty)
            farthest_distances = <float[:num_empty]> farthest_distances_buffer.data()
            for i in range(num_empty):
                farthest_distances[i] = -FLT_MAX
            for i in range(num_cells):
                j = cluster_labels[i]
                difference = X[i, 0] - centroids[j, 0]
                distance = difference * difference
                for k in range(1, num_dimensions):
                    difference = X[i, k] - centroids[j, k]
                    distance = distance + difference * difference
                if distance > farthest_distances[0]:
                    min_heap_replace_top(&farthest_cells[0], &farthest_distances[0], i,
                                         distance, num_empty)
                                       
            # Sort the heap to get distances in descending order
            min_heap_sort(&farthest_cells[0], &farthest_distances[0], num_empty)
            
            # Check if any of the farthest distances are 0
            if farthest_distances[0] == 0:
                error_message = (
                    f'num_clusters ({num_clusters:,}) is greater than the number '
                    f'of cells with distinct principal component loadings '
                    f'({num_cells - num_empty:,}); decrease num_clusters')
                raise ValueError(error_message)
                
            # Relocate empty clusters to points
            for i in range(num_empty):
                new_cluster_label = empty_cluster_indices[i]
                j = farthest_cells[i]
                old_cluster_label = cluster_labels[j]
                
                # Move the cell from the old cluster to the new cluster
                for k in range(num_dimensions):
                    centroids_new[old_cluster_label, k] -= X[j, k]
                    centroids_new[new_cluster_label, k] = X[j, k]
        
        cdef inline unsigned relocate_empty_clusters_parallel(
                const float[:, ::1] X,
                const unsigned[::1] cluster_labels,
                const float[:, ::1] centroids,
                float[:, ::1] centroids_new,
                const unsigned[::1] num_cells_per_cluster,
                const unsigned num_threads) noexcept nogil:
            # Relocate centroids with no cells assigned to them
        
            cdef unsigned i, j, k, num_empty, new_cluster_label, old_cluster_label, \
                num_cells = X.shape[0], num_dimensions = X.shape[1], \
                num_clusters = centroids.shape[0]
            cdef float distance, difference
            cdef vector[unsigned] empty_cluster_indices
            cdef uninitialized_vector[unsigned] farthest_cells
            cdef uninitialized_vector[float] farthest_distances
            
            # Collect indices of empty clusters
            for i in range(num_clusters):
                if num_cells_per_cluster[i] == 0:
                    empty_cluster_indices.push_back(i)
                
            # Return if no clusters are empty
            num_empty = empty_cluster_indices.size()
            if num_empty == 0:
                return 0
            
            # Find the `num_empty` farthest points from their assigned centroids,
            # using a min-heap to keep track of the `num_empty` largest distances
            farthest_cells.resize(num_empty)
            farthest_distances.resize(num_empty)
            for i in range(num_empty):
                farthest_distances[i] = -FLT_MAX
            for i in prange(num_cells, num_threads=num_threads):
                j = cluster_labels[i]
                difference = X[i, 0] - centroids[j, 0]
                distance = difference * difference
                for k in range(1, num_dimensions):
                    difference = X[i, k] - centroids[j, k]
                    distance = distance + difference * difference
                if distance > farthest_distances[0]:
                    min_heap_replace_top(&farthest_cells[0], &farthest_distances[0], i,
                                         distance, num_empty)
                                       
            # Sort the heap to get distances in descending order
            min_heap_sort(&farthest_cells[0], &farthest_distances[0], num_empty)
            
            # Check if any of the farthest distances are 0
            if farthest_distances[0] == 0:
                return num_empty  # error code
                
            # Relocate empty clusters to points
            for i in range(num_empty):
                new_cluster_label = empty_cluster_indices[i]
                j = farthest_cells[i]
                old_cluster_label = cluster_labels[j]
                
                # Move the cell from the old cluster to the new cluster
                for k in range(num_dimensions):
                    centroids_new[old_cluster_label, k] -= X[j, k]
                    centroids_new[new_cluster_label, k] = X[j, k]
        
        cdef inline void partial_distances(
                const float* A,
                const float* B,
                const float* B_norms,
                float* distances,
                const unsigned num_A,
                const unsigned num_B,
                const unsigned num_dimensions) noexcept nogil:
            # Calculate the "partial" distance from each row of A
            # (of shape `num_A × num_dimensions`) to each row of B (of shape
            # `num_B × num_dimensions`). Use the identity:
            # ||A - B||² = ||A||² - 2 * A.dot(B.T) + ||B||²,
            # but skip calculating the ||A||² term since the closest row of `B`
            # for a given row of `A` does not depend on ||A||². This is why we
            # call it a "partial" distance.
            
            cdef char transA = b'T', transB = b'N'
            cdef float alpha = -2, beta = 1
            cdef unsigned i, j

            for i in range(num_A):
                for j in range(num_B):
                    # distances = ||B||²
                    distances[i * num_B + j] = B_norms[j]
            
            # distances -= 2 * A.dot(B.T)
            sgemm(transA, transB, num_B, num_A, num_dimensions, alpha, B,
                  num_dimensions, A, num_dimensions, beta, distances, num_B)
        
        def kmeans(const float[:, ::1] X,
                   unsigned[::1] cluster_labels,
                   float[:, ::1] centroids,
                   float[:, ::1] centroids_new,
                   unsigned[::1] num_cells_per_cluster,
                   const bint kmeans_barbar,
                   const unsigned num_init_iterations,
                   const float oversampling_factor,
                   const unsigned num_kmeans_iterations,
                   const unsigned chunk_size,
                   const unsigned long seed,
                   unsigned num_threads):
            cdef unsigned i, j, k, l, iteration, best_cluster, thread_index, \
                chunk_index, start, chunk_num_cells, num_empty, \
                num_cells = X.shape[0], num_clusters = centroids.shape[0], \
                num_dimensions = centroids.shape[1], \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size
            cdef unsigned long state = srand(seed)
            cdef float difference, distance, min_distance, norm, \
                inv_cells_minus_clusters = 1.0 / (num_cells - num_clusters), \
                one_plus_eps = 1025.0 / 1024, one_minus_eps = 1023.0 / 1024
            cdef uninitialized_vector[float] centroid_norms_buffer, \
                chunk_centroids_new_buffer, distances_buffer
            cdef vector[vector[unsigned]] thread_num_cells_per_cluster
            cdef vector[uninitialized_vector[float]] thread_distances
            cdef float[::1] centroid_norms
            cdef float[:, ::1] distances, temp
            cdef float[:, :, ::1] chunk_centroids_new
            cdef float* distance_pointer
            cdef unsigned* num_cells_per_cluster_pointer
            cdef str error_message
            
            num_threads = min(num_threads, num_chunks)
            if num_threads == 1:
                if kmeans_barbar:
                    # Initialize centroids with k-means||
                    kmeans_barbar_init(X, centroids, num_init_iterations,
                                       oversampling_factor, num_cells, num_clusters,
                                       num_dimensions, chunk_size, seed)
                else:
                    # Initialize centroids with random points
                    kmeans_random_init(X, centroids, num_cells, num_clusters, seed)
                
                # 1. Run the E and M steps of k-means for `num_kmeans_iterations`
                # iterations
                
                centroid_norms_buffer.resize(num_clusters)
                centroid_norms = <float[:num_clusters]> centroid_norms_buffer.data()
                chunk_centroids_new_buffer.resize(num_chunks * num_clusters * num_dimensions)
                chunk_centroids_new = <float[:num_chunks, :num_clusters, :num_dimensions]> \
                    chunk_centroids_new_buffer.data()
                distances_buffer.resize(chunk_size * num_clusters)
                distances = <float[:chunk_size, :num_clusters]> distances_buffer.data()
                
                for iteration in range(num_kmeans_iterations):
                    centroids_new[:] = 0
                    num_cells_per_cluster[:] = 0
                    chunk_centroids_new[:] = 0
                    
                    # Calculate the squared L2 norm of each centroid, ||C||²
                    for i in range(num_clusters):
                        norm = centroids[i, 0] * centroids[i, 0]
                        for j in range(1, num_dimensions):
                            norm += centroids[i, j] * centroids[i, j]
                        centroid_norms[i] = norm
                    
                    # Run the E and M steps of Lloyd's algorithm in chunks
                    for chunk_index in range(num_chunks):
                        start = chunk_index * chunk_size
                        chunk_num_cells = num_cells - start \
                            if chunk_index == num_chunks - 1 else chunk_size

                        # Calculate the distance from each cell in the chunk to
                        # each centroid. Use the identity:
                        # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                        # but skip calculating the ||X||² term since the best
                        # cluster for a given cell does not depend on ||X||².
                        partial_distances(&X[start, 0], &centroids[0, 0],
                                          &centroid_norms[0], &distances[0, 0],
                                          chunk_num_cells, num_clusters,
                                          num_dimensions)
                        
                        # Find the closest centroid to each cell in the chunk,
                        # i.e. the cell's cluster assignment. Keep track of how
                        # many cells were assigned to each cluster, and
                        # calculate the total contribution of the cells in the
                        # chunk to the new centroids (i.e. the sum of the cells
                        # that were assigned to a centroid's cluster; we will
                        # normalize to get the mean later). Aggregate centroids
                        # by chunk, to allow deterministic parallelism.
                        for i in range(chunk_num_cells):
                            min_distance = distances[i, 0]
                            best_cluster = 0
                            for j in range(1, num_clusters):
                                distance = distances[i, j]
                                if distance < min_distance:
                                    min_distance = distance
                                    best_cluster = j
                            cluster_labels[start + i] = best_cluster
                            num_cells_per_cluster[best_cluster] += 1
                            for j in range(num_dimensions):
                                chunk_centroids_new[chunk_index, best_cluster, j] += \
                                    X[start + i, j]
                        
                    # Aggregate the contributions of each chunk to the new centroids.
                    # Normalize the new centroids by the number of cells in the cluster
                    # to get the mean instead of the sum.
                    for i in range(num_clusters):
                        for chunk_index in range(num_chunks):
                            for j in range(num_dimensions):
                                centroids_new[i, j] += \
                                    chunk_centroids_new[chunk_index, i, j]
                        if num_cells_per_cluster[i] > 0:
                            norm = 1.0 / num_cells_per_cluster[i]
                            for j in range(num_dimensions):
                                centroids_new[i, j] *= norm
                
                    # Handle empty clusters
                    relocate_empty_clusters(X, cluster_labels, centroids,
                                            centroids_new, num_cells_per_cluster)
                    
                    # Swap `centroids` and `centroids_new` after each k-means iteration
                    temp = centroids
                    centroids = centroids_new
                    centroids_new = temp
                    
                    # Check for KeyboardInterrupts
                    PyErr_CheckSignals()
                
                # 2. Run one last iteration of the E step, to get the right
                # cluster assignments
                
                num_cells_per_cluster[:] = 0
                
                # Calculate the squared L2 norm of each centroid, ||C||²
                for i in range(num_clusters):
                    norm = centroids[i, 0] * centroids[i, 0]
                    for j in range(1, num_dimensions):
                        norm += centroids[i, j] * centroids[i, j]
                    centroid_norms[i] = norm
                    
                # Run the E step of Lloyd's algorithm in chunks
                for chunk_index in range(num_chunks):
                    start = chunk_index * chunk_size
                    chunk_num_cells = num_cells - start \
                        if chunk_index == num_chunks - 1 else chunk_size

                    # Calculate the distance from each cell in the chunk to
                    # each centroid. Use the identity:
                    # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                    # but skip calculating the ||X||² term since the best
                    # cluster for a given cell does not depend on ||X||².
                    partial_distances(&X[start, 0], &centroids[0, 0],
                                      &centroid_norms[0], &distances[0, 0],
                                      chunk_num_cells, num_clusters,
                                      num_dimensions)
                    
                    # Find the closest centroid to each cell in the chunk,
                    # i.e. the cell's cluster assignment. Keep track of how
                    # many cells were assigned to each cluster.
                    for i in range(chunk_num_cells):
                        min_distance = distances[i, 0]
                        best_cluster = 0
                        for j in range(1, num_clusters):
                            distance = distances[i, j]
                            if distance < min_distance:
                                min_distance = distance
                                best_cluster = j
                        cluster_labels[start + i] = best_cluster
                        num_cells_per_cluster[best_cluster] += 1
            else:
                # Same as the single-threaded version, but the centroid-finding step needs
                # each thread to scan through every cell and only process the cells
                # that match a particular cluster, to avoid expensive synchronization.
                # The k-means|| has similar additional complexity.

                if kmeans_barbar:
                    # Initialize centroids with k-means||
                    kmeans_barbar_init_parallel(X, centroids, num_init_iterations,
                                                oversampling_factor, num_cells,
                                                num_clusters, num_dimensions, seed,
                                                chunk_size, num_threads)
                else:
                    # Initialize centroids with random points
                    kmeans_random_init(X, centroids, num_cells, num_clusters, seed)
                
                # 1. Run the E and M steps of k-means for `num_kmeans_iterations`
                # iterations
                
                centroid_norms_buffer.resize(num_clusters)
                centroid_norms = <float[:num_clusters]> centroid_norms_buffer.data()
                chunk_centroids_new_buffer.resize(num_chunks * num_clusters * num_dimensions)
                chunk_centroids_new = <float[:num_chunks, :num_clusters, :num_dimensions]> \
                    chunk_centroids_new_buffer.data()
                thread_num_cells_per_cluster.resize(num_threads)
                thread_distances.resize(num_threads)
                
                for iteration in range(num_kmeans_iterations):
                    with nogil:
                        # Calculate the squared L2 norm of each centroid, ||C||²
                        for i in prange(num_clusters, num_threads=num_threads):
                            norm = centroids[i, 0] * centroids[i, 0]
                            for j in range(1, num_dimensions):
                                norm = norm + centroids[i, j] * centroids[i, j]
                            centroid_norms[i] = norm
                        
                        # Run the E and M steps of Lloyd's algorithm in chunks
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()

                            # Allocate thread-local buffers to store temporary data for
                            # each chunk
                            if iteration == 0:
                                thread_num_cells_per_cluster[thread_index].resize(
                                    num_clusters)
                                thread_distances[thread_index].resize(chunk_size * num_clusters)
                            else:
                                fill(thread_num_cells_per_cluster[thread_index].begin(),
                                     thread_num_cells_per_cluster[thread_index].end(), 0)
                            distance_pointer = thread_distances[thread_index].data()
                            num_cells_per_cluster_pointer = \
                                thread_num_cells_per_cluster[thread_index].data()
                            
                            for chunk_index in prange(num_chunks):
                                start = chunk_index * chunk_size
                                chunk_num_cells = num_cells - start \
                                    if chunk_index == num_chunks - 1 else chunk_size

                                # Calculate the distance from each cell in the chunk to
                                # each centroid. Use the identity:
                                # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                                # but skip calculating the ||X||² term since the best
                                # cluster for a given cell does not depend on ||X||².
                                partial_distances(&X[start, 0], &centroids[0, 0],
                                                  &centroid_norms[0], distance_pointer,
                                                  chunk_num_cells, num_clusters,
                                                  num_dimensions)
                                
                                # Find the closest centroid to each cell in the chunk,
                                # i.e. the cell's cluster assignment. Keep track of how
                                # many cells were assigned to each cluster, and
                                # calculate the total contribution of the cells in the
                                # chunk to the new centroids (i.e. the sum of the cells
                                # that were assigned to a centroid's cluster; we will
                                # normalize to get the mean later). Aggregate centroids
                                # by chunk, to allow deterministic parallelism, but just
                                # aggregate the number of cells in each cluster by thread
                                # since they are not floating-point.
                                chunk_centroids_new[chunk_index, :] = 0
                                for i in range(chunk_num_cells):
                                    min_distance = distance_pointer[i * num_clusters]
                                    best_cluster = 0
                                    for j in range(1, num_clusters):
                                        distance = distance_pointer[i * num_clusters + j]
                                        if distance < min_distance:
                                            min_distance = distance
                                            best_cluster = j
                                    cluster_labels[start + i] = best_cluster
                                    num_cells_per_cluster_pointer[best_cluster] += 1
                                    for j in range(num_dimensions):
                                        chunk_centroids_new[chunk_index, best_cluster, j] += \
                                            X[start + i, j]
                        
                        # Aggregate the contributions of each thread/chunk to
                        # a) the number of cells assigned to each cluster, and
                        # b) the new centroids. Normalize the new centroids by
                        # the number of cells in the cluster to get the mean
                        # instead of the sum.
                        for i in prange(num_clusters, num_threads=num_threads):
                            num_cells_per_cluster[i] = 0
                            for thread_index in range(num_threads):
                                num_cells_per_cluster[i] += \
                                    thread_num_cells_per_cluster[thread_index][i]
                            centroids_new[i, :] = 0
                            for chunk_index in range(num_chunks):
                                for j in range(num_dimensions):
                                    centroids_new[i, j] += \
                                        chunk_centroids_new[chunk_index, i, j]
                            if num_cells_per_cluster[i] > 0:
                                norm = 1.0 / num_cells_per_cluster[i]
                                for j in range(num_dimensions):
                                    centroids_new[i, j] *= norm
                    
                        # Handle empty clusters
                        num_empty = relocate_empty_clusters_parallel(
                            X, cluster_labels, centroids, centroids_new,
                            num_cells_per_cluster, num_threads)
                    
                    if num_empty:
                        error_message = (
                            f'num_clusters ({num_clusters:,}) is greater than the number '
                            f'of cells with distinct principal component loadings '
                            f'({num_cells - num_empty:,}); decrease num_clusters')
                        raise ValueError(error_message)
                    
                    # Swap `centroids` and `centroids_new` after each k-means
                    # iteration (can't use `std::swap()` on memoryviews so do
                    # it the old-fashioned way)
                    temp = centroids
                    centroids = centroids_new
                    centroids_new = temp
                    
                    # Check for KeyboardInterrupts
                    PyErr_CheckSignals()
                
                # 2. Run one last iteration of the E step, to get the right
                # cluster assignments
                
                with nogil:
                    # Calculate the squared L2 norm of each centroid, ||C||²
                    for i in prange(num_clusters, num_threads=num_threads):
                        norm = centroids[i, 0] * centroids[i, 0]
                        for j in range(1, num_dimensions):
                            norm = norm + centroids[i, j] * centroids[i, j]
                        centroid_norms[i] = norm
                    
                    # Run the E step of Lloyd's algorithm in chunks
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        fill(thread_num_cells_per_cluster[thread_index].begin(),
                             thread_num_cells_per_cluster[thread_index].end(), 0)
                        distance_pointer = thread_distances[thread_index].data()
                        num_cells_per_cluster_pointer = \
                            thread_num_cells_per_cluster[thread_index].data()
                        
                        for chunk_index in prange(num_chunks):
                            start = chunk_index * chunk_size
                            chunk_num_cells = num_cells - start \
                                if chunk_index == num_chunks - 1 else chunk_size

                            # Calculate the distance from each cell in the chunk to
                            # each centroid. Use the identity:
                            # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                            # but skip calculating the ||X||² term since the best
                            # cluster for a given cell does not depend on ||X||².
                            partial_distances(&X[start, 0], &centroids[0, 0],
                                              &centroid_norms[0], distance_pointer,
                                              chunk_num_cells, num_clusters,
                                              num_dimensions)
                            
                            # Find the closest centroid to each cell in the chunk,
                            # i.e. the cell's cluster assignment. Keep track of how
                            # many cells were assigned to each cluster.
                            for i in range(chunk_num_cells):
                                min_distance = distance_pointer[i * num_clusters]
                                best_cluster = 0
                                for j in range(1, num_clusters):
                                    distance = distance_pointer[i * num_clusters + j]
                                    if distance < min_distance:
                                        min_distance = distance
                                        best_cluster = j
                                cluster_labels[start + i] = best_cluster
                                num_cells_per_cluster_pointer[best_cluster] += 1
                    
                    # Aggregate the contributions of each thread/chunk to
                    # the number of cells assigned to each cluster.
                    for i in prange(num_clusters, num_threads=num_threads):
                        num_cells_per_cluster[i] = 0
                        for thread_index in range(num_threads):
                            num_cells_per_cluster[i] += \
                                thread_num_cells_per_cluster[thread_index][i]
        
        def knn_self(const float[:, ::1] X,
                     const unsigned[::1] cluster_labels,
                     const float[:, ::1] centroids,
                     const unsigned[::1] num_cells_per_cluster,
                     unsigned[:, ::1] neighbors,
                     float[:, ::1] distances,
                     const unsigned num_neighbors,
                     const unsigned min_clusters_searched,
                     const unsigned max_clusters_searched,
                     const unsigned num_candidates_per_neighbor,
                     const unsigned chunk_size,
                     unsigned num_threads):
            # Find the `num_neighbors` nearest neighbors of each cell in `X` among the
            # cells in `X`, which have k-means cluster labels `cluster_labels`. Store
            # the nearest-neighbor indices in `neighbors` and distances in `distances`.

            cdef unsigned i, j, k, cluster_label, thread_index, chunk_index, start, \
                chunk_num_X, num_searched, cluster_index, cluster_num_cells, \
                neighbor, num_X = X.shape[0], num_dimensions = X.shape[1], \
                num_clusters = centroids.shape[0], \
                num_chunks = (num_X + chunk_size - 1) / chunk_size, \
                num_candidates = num_neighbors * num_candidates_per_neighbor
            cdef float norm, difference, distance
            cdef vector[vector[unsigned]] cluster_members
            cdef uninitialized_vector[float] centroid_norms_buffer, \
                temp_distances_buffer, centroid_distances_buffer
            cdef uninitialized_vector[unsigned] nearest_clusters_buffer
            cdef vector[uninitialized_vector[float]] thread_distances
            centroid_norms_buffer.resize(num_clusters)
            centroid_distances_buffer.resize(num_X * max_clusters_searched)
            nearest_clusters_buffer.resize(num_X * max_clusters_searched)
            cdef float[::1] centroid_norms = \
                <float[:num_clusters]> centroid_norms_buffer.data()
            cdef float[:, ::1] temp_distances, centroid_distances = \
                <float[:num_X, :max_clusters_searched]> \
                    centroid_distances_buffer.data()
            cdef unsigned[:, ::1] nearest_clusters = \
                <unsigned[:num_X, :max_clusters_searched]> \
                    nearest_clusters_buffer.data()
            
            num_threads = min(num_threads, num_chunks)
            if num_threads == 1:
                # Create the inverted file index: a mapping from cluster labels to the
                # cells from `X` in the cluster. This is just an inversion of
                # `cluster_labels`.
                cluster_members.resize(num_clusters)
                for cluster_label in range(num_clusters):
                    cluster_members[cluster_label].reserve(num_cells_per_cluster[cluster_label])
                for i in range(num_X):
                    cluster_members[cluster_labels[i]].push_back(i)
                
                PyErr_CheckSignals()

                # Calculate the squared L2 norm of each centroid, ||C||²
                for i in range(num_clusters):
                    norm = centroids[i, 0] * centroids[i, 0]
                    for j in range(1, num_dimensions):
                        norm += centroids[i, j] * centroids[i, j]
                    centroid_norms[i] = norm

                # Find the `max_clusters_searched` nearest centroids of each cell in `X`,
                # storing their indices in `nearest_clusters`. Use a max-heap to keep
                # track of the `max_clusters_searched` smallest distances.
                temp_distances_buffer.resize(chunk_size * num_clusters)
                temp_distances = <float[:chunk_size, :num_clusters]> \
                    temp_distances_buffer.data()
                for chunk_index in range(num_chunks):
                    start = chunk_index * chunk_size
                    chunk_num_X = num_X - start \
                        if chunk_index == num_chunks - 1 else chunk_size
                    
                    # Calculate the distance from each cell in the chunk to
                    # each centroid. Use the identity:
                    # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                    # but skip calculating the ||X||² term since the ranking
                    # of centroid distances for a given cell does not depend on ||X||².
                    partial_distances(&X[start, 0], &centroids[0, 0],
                                      &centroid_norms[0], &temp_distances[0, 0],
                                      chunk_num_X, num_clusters,
                                      num_dimensions)
                    for i in range(chunk_num_X):
                        for j in range(max_clusters_searched):
                            centroid_distances[start + i, j] = FLT_MAX
                        for cluster_index in range(num_clusters):
                            distance = temp_distances[i, cluster_index]
                            
                            # If this centroid is one of the `max_clusters_searched` nearest
                            # centroids found so far, add it to the heap, and remove the
                            # formerly `max_clusters_searched`th-nearest centroid (which is now
                            # not in the top `max_clusters_searched` centroids anymore)
                            if distance < centroid_distances[start + i, 0]:
                                max_heap_replace_top(&nearest_clusters[start + i, 0],
                                                     &centroid_distances[start + i, 0],
                                                     cluster_index, distance,
                                                     max_clusters_searched)
                        
                        # Sort the heap to get nearest clusters in ascending order of distance
                        max_heap_sort(&nearest_clusters[start + i, 0],
                                      &centroid_distances[start + i, 0],
                                      max_clusters_searched)
                
                PyErr_CheckSignals()

                # Search each cell's `max_clusters_searched` nearest clusters for
                # nearest-neighbor candidates, stopping early (at the end of fully
                # searching a cluster) if `min_clusters_searched` clusters have
                # been searched and more than `num_candidates` cells have been
                # considered. As an optimization, unroll the first iteration of
                # the loop across clusters so we can check for self-neighbors
                # only for the cluster the cell itself is in.
                for i in range(num_X):
                    for j in range(num_neighbors):
                        distances[i, j] = FLT_MAX
                    
                    # First iteration (the cluster `i` is in), checking for self-neighbors
                    cluster_label = nearest_clusters[i, 0]
                    cluster_num_cells = num_cells_per_cluster[cluster_label]
                    for j in range(cluster_num_cells):
                        neighbor = cluster_members[cluster_label][j]
                        if i == neighbor:
                            continue  # skip self-neighbors
                        difference = X[neighbor, 0] - X[i, 0]
                        distance = difference * difference
                        for k in range(1, num_dimensions):
                            difference = X[neighbor, k] - X[i, k]
                            distance = distance + difference * difference
                        if distance < distances[i, 0]:
                            max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                 neighbor, distance, num_neighbors)
                    num_searched = cluster_num_cells
                    if min_clusters_searched > 1 or num_searched < num_candidates:
                        # Remaining iterations (the other clusters)
                        for cluster_index in range(1, max_clusters_searched):
                            cluster_label = nearest_clusters[i, cluster_index]
                            cluster_num_cells = num_cells_per_cluster[cluster_label]
                            for j in range(cluster_num_cells):
                                neighbor = cluster_members[cluster_label][j]
                                difference = X[neighbor, 0] - X[i, 0]
                                distance = difference * difference
                                for k in range(1, num_dimensions):
                                    difference = X[neighbor, k] - X[i, k]
                                    distance = distance + difference * difference
                                if distance < distances[i, 0]:
                                    max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                         neighbor, distance, num_neighbors)
                            num_searched = num_searched + cluster_num_cells
                            if cluster_index >= min_clusters_searched - 1 and \
                                    num_searched >= num_candidates:
                                break
                    
                    # Sort the heap to get neighbors in ascending order of distance
                    max_heap_sort(&neighbors[i, 0], &distances[i, 0], num_neighbors)
                    
                    # Check for KeyboardInterrupts
                    if i % 1000 == 999:
                        PyErr_CheckSignals()
            else:
                with nogil:
                    # Create the inverted file index: a mapping from cluster labels to the
                    # cells from `X` in the cluster. This is just an inversion of
                    # `cluster_labels`.
                    cluster_members.resize(num_clusters)
                    for cluster_label in range(num_clusters):
                        cluster_members[cluster_label].reserve(num_cells_per_cluster[cluster_label])
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        for i in range(num_X):  # not prange! every thread looks at every cell
                            cluster_label = cluster_labels[i]
                            if cluster_label % num_threads == thread_index:
                                cluster_members[cluster_label].push_back(i)
                        
                    # Calculate the squared L2 norm of each centroid, ||C||²
                    for i in prange(num_clusters, num_threads=num_threads):
                        norm = centroids[i, 0] * centroids[i, 0]
                        for j in range(1, num_dimensions):
                            norm = norm + centroids[i, j] * centroids[i, j]
                        centroid_norms[i] = norm

                    # Find the `max_clusters_searched` nearest centroids of each cell in `X`,
                    # storing their indices in `nearest_clusters`. Use a max-heap to keep
                    # track of the `max_clusters_searched` smallest distances.
                    thread_distances.resize(num_threads)
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_distances[thread_index].resize(chunk_size * num_clusters)
                        for chunk_index in prange(num_chunks):
                            start = chunk_index * chunk_size
                            chunk_num_X = num_X - start \
                                if chunk_index == num_chunks - 1 else chunk_size
                            
                            # Calculate the distance from each cell in the chunk to
                            # each centroid. Use the identity:
                            # ||X - C||² = ||X||² - 2 * X.dot(C.T) + ||C||²
                            # but skip calculating the ||X||² term since the ranking
                            # of centroid distances for a given cell does not depend on ||X||².
                            partial_distances(&X[start, 0], &centroids[0, 0],
                                              &centroid_norms[0],
                                              thread_distances[thread_index].data(),
                                              chunk_num_X, num_clusters,
                                              num_dimensions)
                            for i in range(chunk_num_X):
                                for j in range(max_clusters_searched):
                                    centroid_distances[start + i, j] = FLT_MAX
                                for cluster_index in range(num_clusters):
                                    distance = thread_distances[thread_index][
                                        i * num_clusters + cluster_index]
                                    
                                    # If this centroid is one of the `max_clusters_searched` nearest
                                    # centroids found so far, add it to the heap, and remove the
                                    # formerly `max_clusters_searched`th-nearest centroid (which is now
                                    # not in the top `max_clusters_searched` centroids anymore)
                                    if distance < centroid_distances[start + i, 0]:
                                        max_heap_replace_top(&nearest_clusters[start + i, 0],
                                                             &centroid_distances[start + i, 0],
                                                             cluster_index, distance,
                                                             max_clusters_searched)
                            
                                # Sort the heap to get nearest clusters in ascending order of distance
                                max_heap_sort(&nearest_clusters[start + i, 0],
                                              &centroid_distances[start + i, 0],
                                              max_clusters_searched)
                            
                            # Check for KeyboardInterrupts
                            if chunk_index % 10 == 9:
                                with gil:
                                    PyErr_CheckSignals()
    
                    # Search each cell's `max_clusters_searched` nearest clusters for
                    # nearest-neighbor candidates, stopping early (at the end of fully
                    # searching a cluster) if `min_clusters_searched` clusters have
                    # been searched and more than `num_candidates` cells have been
                    # considered. As an optimization, unroll the first iteration of
                    # the loop across clusters so we can check for self-neighbors
                    # only for the cluster the cell itself is in.
                    for i in prange(num_X, num_threads=num_threads):
                        for j in range(num_neighbors):
                            distances[i, j] = FLT_MAX
                        
                        # First iteration (the cluster `i` is in), checking for self-neighbors
                        cluster_label = nearest_clusters[i, 0]
                        cluster_num_cells = num_cells_per_cluster[cluster_label]
                        for j in range(cluster_num_cells):
                            neighbor = cluster_members[cluster_label][j]
                            if i == neighbor:
                                continue  # skip self-neighbors
                            difference = X[neighbor, 0] - X[i, 0]
                            distance = difference * difference
                            for k in range(1, num_dimensions):
                                difference = X[neighbor, k] - X[i, k]
                                distance = distance + difference * difference
                            if distance < distances[i, 0]:
                                max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                     neighbor, distance, num_neighbors)
                        num_searched = cluster_num_cells
                        if min_clusters_searched > 1 or num_searched < num_candidates:
                            # Remaining iterations (the other clusters)
                            for cluster_index in range(1, max_clusters_searched):
                                cluster_label = nearest_clusters[i, cluster_index]
                                cluster_num_cells = num_cells_per_cluster[cluster_label]
                                for j in range(cluster_num_cells):
                                    neighbor = cluster_members[cluster_label][j]
                                    difference = X[neighbor, 0] - X[i, 0]
                                    distance = difference * difference
                                    for k in range(1, num_dimensions):
                                        difference = X[neighbor, k] - X[i, k]
                                        distance = distance + difference * difference
                                    if distance < distances[i, 0]:
                                        max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                             neighbor, distance, num_neighbors)
                                num_searched = num_searched + cluster_num_cells
                                if cluster_index >= min_clusters_searched - 1 and \
                                        num_searched >= num_candidates:
                                    break
                        
                        # Sort the heap to get neighbors in ascending order of distance
                        max_heap_sort(&neighbors[i, 0], &distances[i, 0], num_neighbors)
                        
                        # Check for KeyboardInterrupts
                        if i % 1000 == 999:
                            with gil:
                                PyErr_CheckSignals()
        
        def knn_cross(const float[:, ::1] Y,
                      const float[:, ::1] X,
                      const unsigned[::1] cluster_labels,
                      const float[:, ::1] centroids,
                      const unsigned[::1] num_cells_per_cluster,
                      unsigned[:, ::1] neighbors,
                      float[:, ::1] distances,
                      const unsigned num_neighbors,
                      const unsigned min_clusters_searched,
                      const unsigned max_clusters_searched,
                      const unsigned num_candidates_per_neighbor,
                      const unsigned chunk_size,
                      unsigned num_threads):
            # Find the `num_neighbors` nearest neighbors of each cell in `Y` among the
            # cells in `X`, which have k-means cluster labels `cluster_labels`. Store
            # the nearest-neighbor indices in `neighbors` and distances in `distances`.

            cdef unsigned i, j, k, cluster_label, thread_index, chunk_index, start, \
                chunk_num_Y, num_searched, cluster_index, cluster_num_cells, \
                neighbor, num_Y = Y.shape[0], num_X = X.shape[0], \
                num_dimensions = X.shape[1], num_clusters = centroids.shape[0], \
                num_chunks = (num_Y + chunk_size - 1) / chunk_size, \
                num_candidates = num_neighbors * num_candidates_per_neighbor
            cdef float norm, difference, distance
            cdef vector[vector[unsigned]] cluster_members
            cdef uninitialized_vector[float] centroid_norms_buffer, \
                temp_distances_buffer, centroid_distances_buffer
            cdef uninitialized_vector[unsigned] nearest_clusters_buffer
            cdef vector[uninitialized_vector[float]] thread_distances
            centroid_norms_buffer.resize(num_clusters)
            centroid_distances_buffer.resize(num_Y * max_clusters_searched)
            nearest_clusters_buffer.resize(num_Y * max_clusters_searched)
            cdef float[::1] centroid_norms = \
                <float[:num_clusters]> centroid_norms_buffer.data()
            cdef float[:, ::1] temp_distances, centroid_distances = \
                <float[:num_Y, :max_clusters_searched]> \
                    centroid_distances_buffer.data()
            cdef unsigned[:, ::1] nearest_clusters = \
                <unsigned[:num_Y, :max_clusters_searched]> \
                    nearest_clusters_buffer.data()

            num_threads = min(num_threads, num_chunks)
            if num_threads == 1:
                # Create the inverted file index: a mapping from cluster labels to the
                # cells from `X` in the cluster. This is just an inversion of
                # `cluster_labels`.
                cluster_members.resize(num_clusters)
                for cluster_label in range(num_clusters):
                    cluster_members[cluster_label].reserve(num_cells_per_cluster[cluster_label])
                for i in range(num_X):
                    cluster_members[cluster_labels[i]].push_back(i)
                
                PyErr_CheckSignals()

                # Calculate the squared L2 norm of each centroid, ||C||²
                for i in range(num_clusters):
                    norm = centroids[i, 0] * centroids[i, 0]
                    for j in range(1, num_dimensions):
                        norm += centroids[i, j] * centroids[i, j]
                    centroid_norms[i] = norm

                # Find the `max_clusters_searched` nearest centroids of each cell in `Y`,
                # storing their indices in `nearest_clusters`. Use a max-heap to keep
                # track of the `max_clusters_searched` smallest distances.

                temp_distances_buffer.resize(chunk_size * num_clusters)
                temp_distances = <float[:chunk_size, :num_clusters]> \
                    temp_distances_buffer.data()
                for chunk_index in range(num_chunks):
                    start = chunk_index * chunk_size
                    chunk_num_Y = num_Y - start \
                        if chunk_index == num_chunks - 1 else chunk_size
                    
                    # Calculate the distance from each cell in the chunk to
                    # each centroid. Use the identity:
                    # ||Y - C||² = ||Y||² - 2 * Y.dot(C.T) + ||C||²
                    # but skip calculating the ||Y||² term since the ranking
                    # of centroid distances for a given cell does not depend on ||Y||².
                    partial_distances(&Y[start, 0], &centroids[0, 0],
                                      &centroid_norms[0], &temp_distances[0, 0],
                                      chunk_num_Y, num_clusters,
                                      num_dimensions)
                    for i in range(chunk_num_Y):
                        for j in range(max_clusters_searched):
                            centroid_distances[start + i, j] = FLT_MAX
                        for cluster_index in range(num_clusters):
                            distance = temp_distances[i, cluster_index]
                            
                            # If this centroid is one of the `max_clusters_searched`
                            # nearest centroids found so far, add it to the heap, and
                            # remove the formerly `max_clusters_searched`th-nearest
                            # centroid (which is now not in the top
                            # `max_clusters_searched` centroids anymore)
                            if distance < centroid_distances[start + i, 0]:
                                max_heap_replace_top(&nearest_clusters[start + i, 0],
                                                     &centroid_distances[start + i, 0],
                                                     cluster_index, distance,
                                                     max_clusters_searched)
                        
                        # Sort the heap to get nearest clusters in ascending order of distance
                        max_heap_sort(&nearest_clusters[start + i, 0],
                                      &centroid_distances[start + i, 0],
                                      max_clusters_searched)
                
                PyErr_CheckSignals()

                # Search each cell's `max_clusters_searched` nearest clusters for
                # nearest-neighbor candidates, stopping early (at the end of fully
                # searching a cluster) if `min_clusters_searched` clusters have
                # been searched and more than `num_candidates` cells have been
                # considered. Unlike for `knn_self()`, do not check for self-neighbors.
                for i in range(num_Y):
                    for j in range(num_neighbors):
                        distances[i, j] = FLT_MAX
                    num_searched = 0
                    for cluster_index in range(max_clusters_searched):
                        cluster_label = nearest_clusters[i, cluster_index]
                        cluster_num_cells = num_cells_per_cluster[cluster_label]
                        for j in range(cluster_num_cells):
                            neighbor = cluster_members[cluster_label][j]
                            difference = X[neighbor, 0] - Y[i, 0]
                            distance = difference * difference
                            for k in range(1, num_dimensions):
                                difference = X[neighbor, k] - Y[i, k]
                                distance = distance + difference * difference
                            if distance < distances[i, 0]:
                                max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                     neighbor, distance, num_neighbors)
                        num_searched = num_searched + cluster_num_cells
                        if cluster_index >= min_clusters_searched - 1 and \
                                num_searched >= num_candidates:
                            break
                    
                    # Sort the heap to get neighbors in ascending order of distance
                    max_heap_sort(&neighbors[i, 0], &distances[i, 0], num_neighbors)
                    
                    # Check for KeyboardInterrupts
                    if i % 1000 == 999:
                        PyErr_CheckSignals()
            else:
                with nogil:
                    # Create the inverted file index: a mapping from cluster labels to the
                    # cells from `X` in the cluster. This is just an inversion of
                    # `cluster_labels`.
                    cluster_members.resize(num_clusters)
                    for cluster_label in range(num_clusters):
                        cluster_members[cluster_label].reserve(num_cells_per_cluster[cluster_label])
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        for i in range(num_X):  # not prange! every thread looks at every cell
                            cluster_label = cluster_labels[i]
                            if cluster_label % num_threads == thread_index:
                                cluster_members[cluster_label].push_back(i)
                        
                    # Calculate the squared L2 norm of each centroid, ||C||²
                    for i in prange(num_clusters, num_threads=num_threads):
                        norm = centroids[i, 0] * centroids[i, 0]
                        for j in range(1, num_dimensions):
                            norm = norm + centroids[i, j] * centroids[i, j]
                        centroid_norms[i] = norm

                    # Find the `max_clusters_searched` nearest centroids of each cell in `Y`,
                    # storing their indices in `nearest_clusters`. Use a max-heap to keep
                    # track of the `max_clusters_searched` smallest distances.
                    thread_distances.resize(num_threads)
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_distances[thread_index].resize(chunk_size * num_clusters)
                        for chunk_index in prange(num_chunks):
                            start = chunk_index * chunk_size
                            chunk_num_Y = num_Y - start \
                                if chunk_index == num_chunks - 1 else chunk_size
                            
                            # Calculate the distance from each cell in the chunk to
                            # each centroid. Use the identity:
                            # ||Y - C||² = ||Y||² - 2 * Y.dot(C.T) + ||C||²
                            # but skip calculating the ||Y||² term since the ranking
                            # of centroid distances for a given cell does not depend on ||Y||².
                            partial_distances(&Y[start, 0], &centroids[0, 0],
                                              &centroid_norms[0],
                                              thread_distances[thread_index].data(),
                                              chunk_num_Y, num_clusters,
                                              num_dimensions)
                            for i in range(chunk_num_Y):
                                for j in range(max_clusters_searched):
                                    centroid_distances[start + i, j] = FLT_MAX
                                for cluster_index in range(num_clusters):
                                    distance = thread_distances[thread_index][
                                        i * num_clusters + cluster_index]
                                    
                                    # If this centroid is one of the
                                    # `max_clusters_searched` nearest centroids found
                                    # so far, add it to the heap, and remove the
                                    # formerly `max_clusters_searched`th-nearest
                                    # centroid (which is now not in the top
                                    # `max_clusters_searched` centroids anymore)
                                    if distance < centroid_distances[start + i, 0]:
                                        max_heap_replace_top(
                                            &nearest_clusters[start + i, 0],
                                            &centroid_distances[start + i, 0],
                                            cluster_index, distance,
                                            max_clusters_searched)
                                
                                # Sort the heap to get nearest clusters in ascending
                                # order of distance
                                max_heap_sort(&nearest_clusters[start + i, 0],
                                              &centroid_distances[start + i, 0],
                                              max_clusters_searched)
                            
                            # Check for KeyboardInterrupts
                            if chunk_index % 10 == 9:
                                with gil:
                                    PyErr_CheckSignals()
    
                    # Search each cell's `max_clusters_searched` nearest clusters for
                    # nearest-neighbor candidates, stopping early (at the end of fully
                    # searching a cluster) if `min_clusters_searched` clusters have
                    # been searched and more than `num_candidates` cells have been
                    # considered. Unlike for `knn_self()`, do not check for
                    # self-neighbors.
                    for i in prange(num_Y, num_threads=num_threads):
                        for j in range(num_neighbors):
                            distances[i, j] = FLT_MAX
                        num_searched = 0
                        for cluster_index in range(max_clusters_searched):
                            cluster_label = nearest_clusters[i, cluster_index]
                            cluster_num_cells = num_cells_per_cluster[cluster_label]
                            for j in range(cluster_num_cells):
                                neighbor = cluster_members[cluster_label][j]
                                difference = X[neighbor, 0] - Y[i, 0]
                                distance = difference * difference
                                for k in range(1, num_dimensions):
                                    difference = X[neighbor, k] - Y[i, k]
                                    distance = distance + difference * difference
                                if distance < distances[i, 0]:
                                    max_heap_replace_top(&neighbors[i, 0], &distances[i, 0],
                                                         neighbor, distance, num_neighbors)
                            num_searched = num_searched + cluster_num_cells
                            if cluster_index >= min_clusters_searched - 1 and \
                                    num_searched >= num_candidates:
                                break
                        
                        # Sort the heap to get neighbors in ascending order of distance
                        max_heap_sort(&neighbors[i, 0], &distances[i, 0], num_neighbors)
                        
                        # Check for KeyboardInterrupts
                        if i % 1000 == 999:
                            with gil:
                                PyErr_CheckSignals()
        ''')


class SingleCell:
    """
    A single-cell dataset.
    
    Has slots for:
    - `X`: a scipy sparse array of counts per cell and gene
    - `obs`: a polars DataFrame of cell metadata
    - `var`: a polars DataFrame of gene metadata
    - `obsm`: a dictionary of NumPy arrays and polars DataFrames of cell
      metadata
    - `varm`: a dictionary of NumPy arrays and polars DataFrames of gene
      metadata
    - `uns`: a dictionary of scalars (strings, numbers or Booleans) or NumPy
      arrays, or nested dictionaries thereof
    as well as `obs_names` and `var_names`, aliases for `obs[:, 0]` and
    `var[:, 0]`.
    
    Why is `X` a sparse array rather than matrix? Aside from being more modern
    and consistent with `np.array`, it's also faster, as explained at
    github.com/scipy/scipy/blob/2aee5efcbe3720f41fe55f336f492ae0acbecdee/scipy/
    sparse/_base.py#L1333-L1337.
    """
    # noinspection PyUnresolvedReferences
    def __init__(self,
                 source: str | Path | 'AnnData' | None = None,
                 *,
                 X: sparse.csr_array | sparse.csc_array | sparse.csr_matrix |
                    sparse.csc_matrix | Literal[False] | None = None,
                 obs: pl.DataFrame | None = None,
                 var: pl.DataFrame | None = None,
                 obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       Literal[False] | None = None,
                 varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] |
                       Literal[False] | None = None,
                 obsp: dict[str, sparse.csr_array | sparse.csc_array |
                                 sparse.csr_matrix | sparse.csc_matrix] |
                       Literal[False] | None = None,
                 varp: dict[str, sparse.csr_array | sparse.csc_array |
                                 sparse.csr_matrix | sparse.csc_matrix] |
                       Literal[False] | None = None,
                 uns: UnsDict | Literal[False] | None = None,
                 X_key: str | None = None,
                 assay: str | None = None,
                 obs_columns: str | Iterable[str] = None,
                 var_columns: str | Iterable[str] = None,
                 num_threads: int | np.integer = -1) -> None:
        """
        Load a SingleCell dataset from a file, or create one from an in-memory
        AnnData object or count matrix + metadata.
        
        SingleCell supports reading and writing files from each of the three
        major single-cell ecosystems:
        - scverse/Scanpy AnnData (`.h5ad`)
        - Seurat (`.rds` and `.h5Seurat`)
        - Bioconductor SingleCellExperiment (`.rds`)
        as well as raw 10x data files (`.h5` or `.mtx`).
        
        By default, when an AnnData object, `.h5ad` file, `.h5Seurat` file, or
        `.rds` file contains both raw and normalized counts, only the raw
        counts will be loaded. To load normalized counts instead, use the `X`
        argument (for AnnData objects) or `X_key` argument (for files).
        
        Reading and writing `.rds` files requires the ryp Python-R bridge.
        To create a SingleCell dataset from an in-memory Seurat or
        SingleCellExperiment object in the ryp R workspace, use
        `SingleCell.from_seurat()` or `SingleCell.from_sce()`.
        
        Reading and writing loom files is not supported because SingleCell only
        supports sparse count matrices, and loom only supports dense matrices.
        Using loom files for SingleCell data is not recommended due to this
        wastefulness. If you must, load them with
        `SingleCell(scanpy.read_loom(loom_filename))`; `read_loom()` implicitly
        converts the counts to a sparse matrix by default.
        
        Args:
            source: a filename or AnnData object, or `None` if specifying `X`,
                    `obs`, and `var` instead. Supported file formats are
                    scverse/Scanpy AnnData (`.h5ad`), Seurat (`.rds` and
                    `.h5Seurat`), Bioconductor SingleCellExperiment (`.rds`),
                    and raw 10x data files (`.h5` or `.mtx`). If `source` is a
                    10x `.mtx.gz` filename, `barcodes.tsv.gz` and
                    `features.tsv.gz` are assumed to be in the same directory,
                    unless custom paths to these files are specified via the
                    `obs` and/or `var` arguments.
            X: If `source` is `None`, the data as a sparse array or matrix
               (with rows = cells, columns = genes).
               If `source` is an AnnData object, an optional sparse array or
               matrix to use as `X`. By default, `X` will be loaded from
               `source.layers['UMIs']` or `source.raw.X` if present and
               `source.X` otherwise.
               If `X` is `None` when `source` is `None`, or `False` when
               `source` is a filename, do not store any data in `X` and set it
               to `None`. This helps save memory, but the resulting dataset
               cannot be saved, converted to another format, or used to run
               analyses that require `X`.
            obs: a polars DataFrame of metadata for each cell (row of `X`), or
                 `None` if specifying `source` instead. Or, if `source` is a
                 10x `.mtx.gz` filename, an optional filename for cell-level
                 metadata, which is otherwise assumed to be at
                 `barcodes.tsv.gz` in the same directory as the `.mtx.gz` file.
            var: a polars DataFrame of metadata for each gene (column of `X`),
                 or `None` if specifying `source` instead. Or, if `source` is a
                 10x `.mtx.gz` filename, an optional filename for gene-level
                 metadata, which is otherwise assumed to be at
                 `features.tsv.gz` in the same directory as the `.mtx.gz` file.
            obsm: an optional dictionary mapping string names to NumPy arrays
                  and polars DataFrames of metadata for each cell, or `False`
                  to skip loading `obsm` when reading `.h5ad` and `.h5Seurat`
                  files
            varm: an optional dictionary mapping string names to NumPy arrays
                  and polars DataFrames of metadata for each gene, or `False`
                  to skip loading `varm` when reading `.h5ad` files
            obsp: an optional dictionary mapping string names to sparse arrays
                  or matrices containing pairwise cell-cell information like
                  nearest-neighbors graphs, or `False` to skip loading `obsp`
                  when reading `.h5ad` and `.h5Seurat` files
            varp: an optional dictionary mapping string names to sparse arrays
                  or matrices containing pairwise gene-gene information, or
                  `False` to skip loading `varp` when reading `.h5ad` files
            uns: an optional dictionary mapping string names to unstructured
                 metadata - scalars (strings, numbers or Booleans), NumPy
                 arrays, or nested dictionaries thereof - or `False` to skip
                 loading `uns` when reading `.h5ad` or `.h5Seurat` files
            X_key: if `source` is an AnnData `.h5ad`, Seurat `.rds` or
                   `.h5Seurat` filename, or SingleCellExperiment `.rds`
                   filename, the location within `source` to use as `X`:
                   - If `source` is an `.h5ad` filename, the name of the key in
                     the `.h5ad` file to use as `X`. If `None`, defaults to
                     `'layers/UMIs'` (i.e. `self.layers['UMIs']` in Scanpy) or
                     `'raw/X'` (i.e. `self.raw.X` in Scanpy) if present,
                     otherwise `'X'`.
                     Tip: `SingleCell.ls(h5ad_file)` shows the structure of an
                     `.h5ad` file without loading it, allowing you to figure
                     out which key to use as `X`.
                   - If `source` is a Seurat `.rds` or `.h5Seurat` filename,
                     the slot within the active assay (or the assay specified
                     by the `assay` argument, if not `None`) to use as `X`. Set
                     to `'data'` to load the normalized counts, or
                     `'scale.data'` to load the normalized and scaled counts,
                     if available. If `None`, defaults to `'counts'`.
                   - If `source` is a SingleCellExperiment `.rds` filename, the
                     element within `@assays@data` to use as `X`. Set to
                     `'logcounts'` to load the normalized counts, if available.
                     If `None`, defaults to `'counts'`.
            assay: if `source` is a Seurat `.rds` or `.h5Seurat` filename, the
                   name of the assay within the Seurat object to load data
                   from. Defaults to the Seurat object's `active.assay`
                   attribute (usually `'RNA'`).
            obs_columns: if `source` is an `.h5ad` or `.h5Seurat` filename, the
                         columns of `obs` to load. If not specified, load all
                         columns. Specifying only a subset of columns can speed
                         up reading. Not supported for `.h5` files, since they
                         only have a single `obs` column (`'barcodes'`), nor
                         for Seurat and SingleCellExperiment `.rds` files,
                         since `.rds` files do not support partial loading.
            var_columns: if `source` is an `.h5ad`, `.h5`, or `.h5Seurat`
                         filename, the columns of `var` to load. If not
                         specified, load all columns. Specifying only a subset
                         of columns can speed up reading. Not supported for
                         Seurat and SingleCellExperiment `.rds` files, since
                         the `.rds` file format does not support partial
                         loading.
            num_threads: the number of threads to use when reading `.h5ad` and
                         `.h5` files, and the default number of threads to use
                         for all subsequent operations on this SingleCell
                         dataset. Also sets the number of threads for this
                         SingleCell dataset's count matrix, if present. By
                         default (`num_threads=-1`), use all available cores,
                         as determined by `os.cpu_count()`.
        
        Note:
            Both ordered and unordered categorical columns of `obs` and `var`
            will be loaded as polars Enums rather than polars Categoricals.
            This is because polars Categoricals use a shared numerical encoding
            across columns, so their codes are not `[0, 1, 2, ...]` like pandas
            categoricals and polars Enums are. Using Categoricals leads to a
            large overhead (~25%) when loading `obs` from an `.h5ad` file, for
            example.
        
        Note:
            SingleCell does not support dense matrices, which are highly
            memory-inefficient for single-cell data. Passing a NumPy array as
            the `X` argument will give an error; if for some reason your data
            has been improperly stored as a dense matrix, convert it to a
            sparse array first with `csr_array(numpy_array)`). However, when
            loading from disk or converting from other formats, dense matrices
            will be automatically converted to sparse matrices, to avoid giving
            an error when loading or converting.
        """
        # Initialize this SingleCell dataset's `num_threads`
        num_threads = SingleCell._process_num_threads_static(num_threads)
        self._num_threads = num_threads
        
        # Initialize the SingleCell dataset depending on which arguments were
        # specified
        if source is not None:
            if str(type(source)).startswith("<class 'anndata"):
                # AnnData object
                with ignore_sigint():
                    from anndata import AnnData
                check_type(source, 'source', AnnData,
                           'a filename, Path, or Anndata object')
                for prop, prop_name in (
                        (obs, 'obs'), (var, 'var'), (obsm, 'obsm'),
                        (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                        (uns, 'uns'), (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns')):
                    if prop is not None:
                        error_message = (
                            f'when initializing a SingleCell dataset from an '
                            f'AnnData object, {prop_name} must be None')
                        raise ValueError(error_message)
                
                # Get `X`
                if X is False:
                    self._X = None
                else:
                    if X is None:
                        has_layers_UMIs = 'UMIs' in source._layers
                        has_raw_X = hasattr(source._raw, '_X')
                        if has_layers_UMIs and has_raw_X:
                            error_message = (
                                "both layers['UMIs'] and raw.X are present in "
                                "this AnnData object; this should never "
                                "happen in well-formed AnnData objects")
                            raise ValueError(error_message)
                        X = source._layers['UMIs'] if has_layers_UMIs else \
                            source._raw._X if has_raw_X else source._X
                        if not isinstance(X, (
                                sparse.csr_array, sparse.csc_array,
                                sparse.csr_matrix, sparse.csc_matrix)):
                            error_message = (
                                f'to initialize a SingleCell dataset from an '
                                f'AnnData object, its X must be a csr_array, '
                                f'csc_array, csr_matrix, or csc_matrix, but '
                                f'it has type {type(X).__name__!r}. Either '
                                f'convert X to a csr_array or csc_array, or '
                                f'specify a sparse array or matrix to use as '
                                f'X via the X argument')
                            raise TypeError(error_message)
                    else:
                        check_type(X, 'X', (
                                sparse.csr_array, sparse.csc_array,
                                sparse.csr_matrix, sparse.csc_matrix),
                                   'a csr_array, csc_array, csr_matrix, or '
                                   'csc_matrix')
                    if isinstance(X, (csr_array, csc_array)):
                        pass
                    elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
                        X = csr_array(X)
                    else:
                        X = csc_array(X)
                    self._X = X
                
                # Get `obs` cnd `vcr`
                for attr in '_obs', '_var':
                    df = getattr(source, attr)
                    if df.index.name is None:
                        # Make the index name consistent with what you'd get if
                        # you had loaded the same AnnData object directly from
                        # an `.h5ad` file
                        df = df.rename_axis('_index')
                    
                    # Convert Categoricals with string categories to polars
                    # Enums, and Categoricals with other types of categories
                    # (e.g. integers) to non-categorical columns of the
                    # corresponding polars dtype (e.g. pl.Int64) since polars
                    # only supports string categories
                    schema_overrides = {}
                    cast_dict = {}
                    for column, dtype in \
                            df.dtypes[df.dtypes == 'category'].items():
                        categories_dtype = dtype.categories.dtype
                        if categories_dtype == object:
                            schema_overrides[column] = \
                                pl.Enum(dtype.categories)
                        else:
                            cast_dict[column] = categories_dtype
                    setattr(self, attr, pl.from_pandas(
                        df.astype(cast_dict),
                        schema_overrides=schema_overrides, include_index=True))
                
                # Get the other fields
                self._obsm: dict[str, np.ndarray] = dict(source._obsm)
                self._varm: dict[str, np.ndarray] = dict(source._varm)
                self._obsp: dict[str, csr_array | csc_array] = \
                    dict(source._obsp)
                self._varp: dict[str, csr_array | csc_array] = \
                    dict(source._varp)
                self._uns = dict(source._uns)
            else:
                # Filename
                check_type(source, 'source', (str, Path),
                           'a filename, Path, or AnnData object')
                source = os.path.expanduser(source)
                if source.endswith('.h5ad'):
                    if not os.path.exists(source):
                        error_message = f'.h5ad file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in \
                            (obs, 'obs'), (var, 'var'), (assay, 'assay'):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5ad file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    for prop, prop_name in (
                            (obsm, 'obsm'), (varm, 'varm'), (obsp, 'obsp'),
                            (varp, 'varp'), (uns, 'uns')):
                        if prop is not None and prop is not False:
                            error_message = (
                                f'when loading an .h5ad file, {prop_name} '
                                f'must be None or False')
                            raise ValueError(error_message)
                    if obs_columns is not None:
                        obs_columns = to_tuple_checked(
                            obs_columns, 'obs_columns', str, 'strings')
                    if var_columns is not None:
                        var_columns = to_tuple_checked(
                            var_columns, 'var_columns', str, 'strings')
                    
                    # For the AnnData on-disk format specification, see
                    # anndata.readthedocs.io/en/latest/fileformat-prose.html
                    with h5py.File(source) as h5ad_file:
                        # Load `obs` and `var`
                        self._obs = SingleCell._read_h5ad_dataframe(
                            h5ad_file, 'obs', columns=obs_columns,
                            num_threads=num_threads)
                        self._var = SingleCell._read_h5ad_dataframe(
                            h5ad_file, 'var', columns=var_columns,
                            num_threads=num_threads)
                        
                        # Load `obsm`
                        if obsm is None and 'obsm' in h5ad_file:
                            obsm = h5ad_file['obsm']
                            self._obsm = {
                                key: value[:]
                                     if isinstance(value, h5py.Dataset) else
                                     SingleCell._read_h5ad_dataframe(
                                         h5ad_file, f'obsm/{key}',
                                         num_threads=num_threads)
                                for key, value in obsm.items()}
                        else:
                            self._obsm = {}
                        
                        # Load `varm`
                        if varm is None and 'varm' in h5ad_file:
                            varm = h5ad_file['varm']
                            self._varm = {
                                key: value[:]
                                     if isinstance(value, h5py.Dataset) else
                                     SingleCell._read_h5ad_dataframe(
                                         h5ad_file, f'varm/{key}',
                                         num_threads=num_threads)
                                for key, value in varm.items()}
                        else:
                            self._varm = {}
                        
                        # Load `obsp`
                        if obsp is None and 'obsp' in h5ad_file:
                            obsp = h5ad_file['obsp']
                            self._obsp = {
                                key: (csr_array
                                      if value.attrs['encoding-type'] ==
                                         'csr_matrix' else csc_array)(
                                    (value['data'][:], value['indices'][:],
                                     value['indptr'][:]),
                                    shape=value.attrs['shape'])
                                for key, value in obsp.items()}
                        else:
                            self._obsp = {}
                        
                        # Load `varp`
                        if varp is None and 'varp' in h5ad_file:
                            varp = h5ad_file['varp']
                            self._varp = {
                                key: (csr_array
                                      if value.attrs['encoding-type'] ==
                                         'csr_matrix' else csc_array)(
                                    (value['data'][:], value['indices'][:],
                                     value['indptr'][:]),
                                    shape=value.attrs['shape'])
                                for key, value in varp.items()}
                        else:
                            self._varp = {}
                        
                        # Load `uns`
                        if uns is None and 'uns' in h5ad_file:
                            self._uns = SingleCell._read_uns(h5ad_file['uns'])
                        else:
                            self._uns = {}
                        
                        # Load `X`
                        if X is False:
                            if X_key is not None:
                                error_message = (
                                    'when loading an .h5ad file with X=False, '
                                    'X_key must be None')
                                raise ValueError(error_message)
                            self._X = None
                        else:
                            if X_key is None:
                                has_layers_UMIs = 'layers/UMIs' in h5ad_file
                                has_raw_X = 'raw/X' in h5ad_file
                                if has_layers_UMIs and has_raw_X:
                                    error_message = (
                                        "both layers['UMIs'] and raw.X are "
                                        "present; this should never happen in "
                                        "well-formed .h5ad files")
                                    raise ValueError(error_message)
                                X_key = 'layers/UMIs' if has_layers_UMIs else \
                                    'raw/X' if has_raw_X else 'X'
                            else:
                                check_type(X_key, 'X_key', str, 'a string')
                                if X_key not in h5ad_file:
                                    error_message = (
                                        f'X_key {X_key!r} is not present in '
                                        f'the .h5ad file')
                                    raise ValueError(error_message)
                            X = h5ad_file[X_key]
                            matrix_class = X.attrs['encoding-type'] \
                                if 'encoding-type' in X.attrs else \
                                X.attrs['h5sparse_format'] + '_matrix'
                            if matrix_class == 'csr_matrix':
                                array_class = csr_array
                            elif matrix_class == 'csc_matrix':
                                array_class = csc_array
                            else:
                                error_message = (
                                    f"X has unsupported encoding-type "
                                    f"{matrix_class!r}, but should be "
                                    f"'csr_matrix' or 'csc_matrix'")
                                raise ValueError(error_message)
                            self._X = array_class((
                                SingleCell._read_dataset(
                                    X['data'], num_threads),
                                SingleCell._read_dataset(
                                    X['indices'], num_threads),
                                SingleCell._read_dataset(
                                    X['indptr'], num_threads)),
                                shape=X.attrs['shape'] if 'shape' in X.attrs
                                      else X.attrs['h5sparse_shape'])
                elif source.endswith('.h5Seurat'):
                    if not os.path.exists(source):
                        error_message = \
                            f'.h5Seurat file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (obs, 'obs'), (var, 'var'), (varm, 'varm'),
                            (varp, 'varp')):
                        if prop is not None:
                            error_message = (
                                f'when loading an .h5Seurat file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    for prop, prop_name in \
                            (obsm, 'obsm'), (obsp, 'obsp'), (uns, 'uns'):
                        if prop is not None and prop is not False:
                            error_message = (
                                f'when loading an .h5Seurat file, {prop_name} '
                                f'must be None or False')
                            raise ValueError(error_message)
                    if obs_columns is not None:
                        obs_columns = to_tuple_checked(
                            obs_columns, 'obs_columns', str, 'strings')
                    if var_columns is not None:
                        var_columns = to_tuple_checked(
                            var_columns, 'var_columns', str, 'strings')
                    with h5py.File(source) as h5Seurat_file:
                        
                        # Check that `assay` is an assay in the `.h5Seurat`
                        # file, or set it to `active.assay` if `None`
                        if assay is None:
                            assay = h5Seurat_file.attrs['active.assay'].item()
                        elif assay not in h5Seurat_file['assays']:
                            error_message = (
                                f'assay {assay!r} does not exist in the '
                                f'.h5Seurat file; specify a different assay '
                                f'than {assay!r}')
                            raise ValueError(error_message)
                        
                        # Load `obs`
                        self._obs = SingleCell._read_h5Seurat_dataframe(
                            h5Seurat_file['meta.data'], columns=obs_columns,
                            num_threads=num_threads)
                        
                        # Load `var`
                        assay_group = h5Seurat_file['assays'][assay]
                        self._var = SingleCell._read_h5Seurat_dataframe(
                            assay_group['meta.features'], columns=var_columns,
                            num_threads=num_threads) \
                            if 'meta.features' in assay_group else \
                            pl.Series('feature.names',
                                      assay_group['features'][:])\
                                .cast(pl.String)\
                                .to_frame()
                        
                        # Load `obsm`
                        self._obsm = {
                            key: value['cell.embeddings'][:].T
                            for key, value in
                                h5Seurat_file['reductions'].items()
                            if value.attrs['active.assay'] == assay}
                        
                        # Load `obsp`
                        self._obsp = {
                            key: csr_array((
                                SingleCell._read_dataset(
                                    value['data'], num_threads),
                                SingleCell._read_dataset(
                                    value['indices'], num_threads),
                                SingleCell._read_dataset(
                                    value['indptr'], num_threads)),
                                shape=value.attrs['dims'][::-1])
                            for key, value in h5Seurat_file['graphs'].items()
                            if value.attrs['assay.used'] == assay}
                        
                        # Load `uns`
                        self._uns = SingleCell._read_h5Seurat_uns(
                            h5Seurat_file['misc'])
                        
                        # Load `X`
                        if X is False:
                            if X_key is not None:
                                error_message = (
                                    'when loading an .h5Seurat file with '
                                    'X=False, X_key must be None')
                                raise ValueError(error_message)
                            self._X = None
                        else:
                            if X_key is None:
                                X_key = 'counts'
                                if X_key not in assay_group:
                                    error_message = (
                                        f"the 'counts' key is not present in "
                                        f"the .h5Seurat file as part of assay "
                                        f"{assay!r}; specify a different "
                                        f"assay than {assay!r} or specify "
                                        f"X_key as something other than "
                                        f"'counts'")
                                    raise ValueError(error_message)
                            else:
                                check_type(X_key, 'X_key', str,
                                           'a string or False')
                                if X_key not in assay_group:
                                    error_message = (
                                        f'X_key {X_key!r} is not present in '
                                        f'the .h5Seurat file as part of assay '
                                        f'{assay!r}; specify a different '
                                        f'assay than {assay!r} or a different '
                                        f'X_key than {X_key!r}')
                                    raise ValueError(error_message)
                            X = assay_group[X_key]
                            self._X = csr_array((
                                SingleCell._read_dataset(
                                    X['data'], num_threads),
                                SingleCell._read_dataset(
                                    X['indices'], num_threads),
                                SingleCell._read_dataset(
                                    X['indptr'], num_threads)),
                                shape=X.attrs['dims'][::-1])
                        self._varm = {}
                        self._varp = {}
                elif source.endswith('.h5'):
                    if not os.path.exists(source):
                        error_message = f'10x .h5 file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (obs, 'obs'), (var, 'var'), (obsm, 'obsm'),
                            (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                            (uns, 'uns'), (X_key, 'X_key'), (assay, 'assay'),
                            (obs_columns, 'obs_columns')):
                        if prop is not None:
                            error_message = (
                                f'when loading a 10x .h5 file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    with h5py.File(source) as h5_file:
                        matrix = h5_file['matrix']
                        features = matrix['features']
                        
                        # Load `obs` and `var`
                        self._obs = pl.Series('barcodes',
                                              matrix['barcodes'][:])\
                            .cast(pl.String)\
                            .to_frame()
                        if var_columns is not None:
                            var_columns = to_tuple_checked(
                                var_columns, 'var_columns', str, 'strings')
                            for column in var_columns:
                                if column not in features:
                                    error_message = (
                                        f'var_columns contains the column '
                                        f'{column!r}, which is not present in '
                                        f'the .h5 file')
                                    raise ValueError(error_message)
                        else:
                            var_columns = \
                                ['name', 'id', 'feature_type', 'genome'] + \
                                [column for column in
                                 ('pattern', 'read', 'sequence')
                                 if column in features]
                        self._var = pl.DataFrame([
                            pl.Series(column, features[column][:])
                            .cast(pl.String) for column in var_columns])
                       
                        # Load `X`
                        if X is None:
                            self._X = csr_array((
                                SingleCell._read_dataset(
                                    matrix['data'], num_threads),
                                SingleCell._read_dataset(
                                    matrix['indices'], num_threads),
                                SingleCell._read_dataset(
                                    matrix['indptr'], num_threads)),
                                shape=matrix['shape'][:][::-1])
                        elif X is False:
                            self._X = None
                        else:
                            error_message = (
                                'when loading a 10x .h5 file, X must be None '
                                'or False')
                            raise ValueError(error_message)
                        self._obsm = {}
                        self._varm = {}
                        self._obsp = {}
                        self._varp = {}
                        self._uns = {}
                elif source.endswith('.mtx.gz'):
                    if not os.path.exists(source):
                        error_message = f'10x file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name, prop_description in (
                            (obs, 'obs', 'a barcodes.tsv.gz file of '
                                         'cell-level metadata'),
                            (var, 'var', 'a features.tsv.gz file of '
                                         'gene-level metadata')):
                        if prop is not None and not \
                                isinstance(prop, (str, Path)):
                            error_message = (
                                f'when loading a 10x .mtx.gz file, '
                                f'{prop_name} must be None or the path to '
                                f'{prop_description}')
                            raise TypeError(error_message)
                    for prop, prop_name in (
                            (obsm, 'obsm'), (varm, 'varm'), (obsp, 'obsp'),
                            (varp, 'varp'), (uns, 'uns'), (X_key, 'X_key'),
                            (assay, 'assay'), (obs_columns, 'obs_columns'),
                            (var_columns, 'var_columns')):
                        if prop is not None:
                            error_message = (
                                f'when loading a 10 .mtx.gz file, {prop_name} '
                                f'must be None')
                            raise ValueError(error_message)
                    from scipy.io import mmread
                    
                    # Load `obs` and `var`
                    self._obs = pl.read_csv(
                        f'{os.path.dirname(source)}/barcodes.tsv.gz'
                        if obs is None else obs,
                        has_header=False, new_columns=['cell'])
                    self._var = pl.read_csv(
                        f'{os.path.dirname(source)}/features.tsv.gz'
                        if var is None else var,
                        has_header=False, new_columns=['gene'])
                    
                    # Load `X`
                    if X is None:
                        self._X = csr_array(mmread(source).T.tocsr())
                    elif X is False:
                        self._X = None
                    else:
                        error_message = (
                            'when loading a 10x .mtx.gz file, X must be None '
                            'or False')
                        raise ValueError(error_message)
                    self._obsm = {}
                    self._varm = {}
                    self._obsp = {}
                    self._varp = {}
                    self._uns = {}
                elif source.endswith('.rds'):
                    if not os.path.exists(source):
                        error_message = f'.rds file {source} does not exist'
                        raise FileNotFoundError(error_message)
                    for prop, prop_name in (
                            (X, 'X'), (obs, 'obs'), (var, 'var'),
                            (varm, 'varm'), (obsp, 'obsp'), (varp, 'varp'),
                            (uns, 'uns'), (obs_columns, 'obs_columns'),
                            (var_columns, 'var_columns')):
                        if prop is not None:
                            error_message = (
                                f'when loading a .rds file, {prop_name} must '
                                f'be None')
                            raise ValueError(error_message)
                    from ryp import r, to_py, to_r
                    r(f'.SingleCell.object = readRDS({source!r})')
                    try:
                        if X_key is None:
                            X_key = 'counts'
                        else:
                            check_type(X_key, 'X_key', str, 'a string')
                        classes = to_py('class(.SingleCell.object)',
                                        squeeze=False)
                        if len(classes) == 1:
                            if classes[0] == 'Seurat':
                                r('suppressPackageStartupMessages('
                                  'library(SeuratObject))')
                                # noinspection PyTypeChecker
                                self._X, self._obs, self._var, self._obsm, \
                                    self._obsp, self._uns = \
                                    SingleCell._from_seurat(
                                        '.SingleCell.object', assay=assay,
                                        slot=X_key, slot_name='X_key')
                                self._varm = {}
                                self._varp = {}
                            elif classes[0] == 'SingleCellExperiment':
                                if assay is not None:
                                    error_message = (
                                        f'when loading a SingleCellExperiment '
                                        f'.rds file, assay must be None')
                                    raise ValueError(error_message)
                                r('suppressPackageStartupMessages('
                                  'library(SingleCellExperiment))')
                                # noinspection PyTypeChecker
                                self._X, self._obs, self._var, self._obsm, \
                                    self._uns = SingleCell._from_sce(
                                        '.SingleCell.object', slot=X_key,
                                        slot_name='X_key')
                                self._obsp = {}
                                self._varm = {}
                                self._varp = {}
                            else:
                                error_message = (
                                    f'the R object loaded from {source} must '
                                    f'be a Seurat or SingleCellExperiment '
                                    f'object, but has class {classes[0]!r}')
                                raise TypeError(error_message)
                        elif len(classes) == 0:
                            error_message = (
                                f'the R object loaded from {source} must be a '
                                f'Seurat or SingleCellExperiment object, but '
                                f'has no classes')
                            raise TypeError(error_message)
                        else:
                            classes_string = \
                                ', '.join(f'{c!r}' for c in classes[:-1])
                            error_message = (
                                f'the R object loaded from {source} must be a '
                                f'Seurat object, but has classes '
                                f'{classes_string} and {classes[-1]!r}')
                            raise TypeError(error_message)
                    finally:
                        r('rm(.SingleCell.object)')
                else:
                    error_message = (
                        f'source is a filename with unsupported extension '
                        f'{".".join(source.split(".")[1:])}; it must be .h5ad '
                        f'(AnnData), .h5 or .mtx.gz (10x), .rds (Seurat or '
                        f'SingleCellExperiment), or .h5Seurat (Seurat)')
                    raise ValueError(error_message)
        else:
            # Sparse array or matrix
            if X is not None:
                check_type(X, 'X', (
                    sparse.csr_array, sparse.csc_array, sparse.csr_matrix,
                    sparse.csc_matrix),
                           'a csr_array, csc_array, csr_matrix, or csc_matrix')
                for prop, prop_name in (
                        (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns')):
                    if prop is not None:
                        error_message = (
                            f'when X is a sparse array or matrix, {prop_name} '
                            f'must be None')
                        raise ValueError(error_message)
            else:
                for prop, prop_name in (
                        (X_key, 'X_key'), (assay, 'assay'),
                        (obs_columns, 'obs_columns'),
                        (var_columns, 'var_columns')):
                    if prop is not None:
                        error_message = (
                            f'when X and source are both None, {prop_name} '
                            f'must be None')
                        raise ValueError(error_message)
            check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
            check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
            if obsm is None:
                obsm = {}
            else:
                check_type(obsm, 'obsm', dict, 'a dictionary')
                obsm = obsm.copy()
            if varm is None:
                varm = {}
            else:
                check_type(varm, 'varm', dict, 'a dictionary')
                varm = varm.copy()
            if obsp is None:
                obsp = {}
            else:
                check_type(obsp, 'obsp', dict, 'a dictionary')
                obsp = obsp.copy()
            if varp is None:
                varp = {}
            else:
                check_type(varp, 'varp', dict, 'a dictionary')
                varp = varp.copy()
            if uns is None:
                uns = {}
            else:
                check_type(uns, 'uns', dict, 'a dictionary')
                valid_uns_types = str, int, np.integer, float, np.floating, \
                    bool, np.bool_, np.ndarray
                # noinspection PyTypeChecker
                for description, value in SingleCell._iter_uns(uns):
                    if not isinstance(value, valid_uns_types):
                        error_message = (
                            f'all values of uns must be scalars (strings, '
                            f'numbers or Booleans) or NumPy arrays, or nested '
                            f'dictionaries thereof, but {description} has '
                            f'type {type(value).__name__!r}')
                        raise TypeError(error_message)
                # noinspection PyTypeChecker
                uns = SingleCell._copy_uns(uns)
            for field, field_name in (obsm, 'obsm'), (varm, 'varm'):
                for key, value in field.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {field_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
                    if isinstance(value, np.ndarray):
                        if value.ndim != 2:
                            error_message = (
                                f'all values of {field_name} must be 2D NumPy '
                                f'arrays or polars DataFrames, but '
                                f'{field_name}[{key!r}] is a {value.ndim:,}D '
                                f'NumPy array')
                            raise ValueError(error_message)
                    elif not isinstance(value, pl.DataFrame):
                        error_message = (
                            f'all values of {field_name} must be NumPy '
                            f'arrays or polars DataFrames, but {field_name}'
                            f'[{key!r}] has type {type(value).__name__!r}')
                        raise TypeError(error_message)
            for field, field_name in (obsp, 'obsp'), (varp, 'varp'):
                new = {}
                for key, value in field.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {field_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
                    if isinstance(value, (csr_array, csc_array)):
                        pass
                    elif isinstance(value,
                                    (sparse.csr_array, sparse.csr_matrix)):
                        value = csr_array(value)
                    elif isinstance(value,
                                    (sparse.csc_array, sparse.csc_matrix)):
                        value = csc_array(value)
                    else:
                        error_message = (
                            f'every value of {field_name} must be a '
                            f'csr_array, csc_array, csr_matrix, or '
                            f'csc_matrix, but {field_name}[{key!r}] has type '
                            f'{type(value).__name__!r}')
                        raise TypeError(error_message)
                    new[key] = value
                setattr(self, f'_{field_name}', new)
            if isinstance(X, (csr_array, csc_array)) or \
                    X is None:
                pass
            elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
                X = csr_array(X)
            else:
                X = csc_array(X)
            self._X = X
            self._obs = obs
            self._var = var
            self._obsm = obsm
            self._varm = varm
            self._uns = uns
        
        # Propagate this SingleCell dataset's `num_threads` to its count matrix
        if self._X is not None:
            self._X._num_threads = num_threads
        
        # Sanity-check dimensions and dtypes
        num_cells = self._obs.shape[0]
        num_genes = self._var.shape[0]
        if num_cells == 0:
            error_message = 'len(obs) is 0: no cells remain'
            raise ValueError(error_message)
        if num_genes == 0:
            error_message = 'len(var) is 0: no genes remain'
            raise ValueError(error_message)
        if num_cells > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) cells, which is '
                'not currently supported')
            raise ValueError(error_message)
        if num_genes > 2_147_483_647:
            error_message = (
                'X has more than INT32_MAX (2,147,483,647) genes, which is '
                'not currently supported')
            raise ValueError(error_message)
        if self._obs[:, 0].dtype not in (pl.String, pl.Categorical, pl.Enum):
            error_message = (
                f'the first column of obs ({self._obs.columns[0]!r}) must be '
                f'String, Enum, or Categorical, but has data type '
                f'{self._obs[:, 0].dtype.base_type()!r}')
            raise ValueError(error_message)
        if self._var[:, 0].dtype not in (pl.String, pl.Categorical, pl.Enum):
            error_message = (
                f'the first column of var ({self._var.columns[0]!r}) must be '
                f'String, Enum, or Categorical, but has data type '
                f'{self._var[:, 0].dtype.base_type()!r}')
            raise ValueError(error_message)
        if self._X is not None:
            dtype = self._X.dtype
            if dtype != np.int32 and dtype != np.int64 and \
                    dtype != np.float32 and dtype != np.float64 and \
                    dtype != np.uint32 and dtype != np.uint64:
                error_message = (
                    f'X must be (u)int32/64 or float32/64, but has data type '
                    f'{str(dtype)}')
                raise TypeError(error_message)
            if self._X.shape[0] != num_cells:
                error_message = (
                    f'len(obs) is {num_cells:,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
            if self._X.shape[1] != num_genes:
                error_message = (
                    f'len(var) is {num_genes:,}, but X.shape[1] is '
                    f'{self._X.shape[1]:,}')
                raise ValueError(error_message)
            if self._X.nnz == 0:
                error_message = 'X has no non-zero entries'
                raise ValueError(error_message)
        for key, value in self._obsm.items():
            if len(value) != num_cells:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_cells:,}')
                raise ValueError(error_message)
        for key, value in self._varm.items():
            if len(value) != num_genes:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{num_genes:,}')
                raise ValueError(error_message)
        for key, value in self._obsp.items():
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'obsp[{key!r}].shape[{dim}] is {value.shape[dim]:,}, '
                        f'but X.shape[0] is {num_cells:,}')
                    raise ValueError(error_message)
        for key, value in self._varp.items():
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'varp[{key!r}].shape[{dim}] is {value.shape[dim]:,}, '
                        f'but X.shape[1] is {num_genes:,}')
                    raise ValueError(error_message)
        
        # Set `uns['normalized']` and `uns['QCed']` to `False` if not set yet;
        # if already set but not a Boolean, back it up to
        # `uns['_normalized']`/`uns['_QCed']`
        for key in 'normalized', 'QCed':
            if key in self._uns:
                if not isinstance(self._uns[key], bool):
                    new_key = f'_{key}'
                    while new_key in self._uns:
                        new_key = f'_{new_key}'
                    warning_message = (
                        f'uns[{key!r}] already exists and is not Boolean; '
                        f'moving it to uns[{new_key!r}]')
                    warnings.warn(warning_message)
                    self._uns[new_key] = self._uns[key]
                    self._uns[key] = False
            else:
                self._uns[key] = False
        
        # As a stopgap for now, convert float64 obsm/varm/obsp/varp to float32 
        # with a warning
        for attr_name, attr in ('obsm', self._obsm), ('varm', self._varm):
            for key, value in attr.items():
                if isinstance(value, np.ndarray) and value.dtype == np.float64:
                    warning_message = (
                        f'{attr_name}[{key!r}] is float64; converting to '
                        f'float32')
                    warnings.warn(warning_message)
                    attr[key] = value.astype(np.float32)
        for attr_name, attr in ('obsp', self._obsp), ('varp', self.varp):
            for key, value in attr.items():
                if value.dtype == np.float64:
                    warning_message = (
                        f'{attr_name}[{key!r}] is float64; converting to '
                        f'float32')
                    warnings.warn(warning_message)
                    value.data = value.data.astype(np.float32)
        
    @property
    def X(self) -> csr_array | csc_array | None:
        return self._X
    
    @X.setter
    def X(self, X: sparse.csr_array | sparse.csc_array | sparse.csr_matrix |
                   sparse.csc_matrix) -> None:
        if isinstance(X, (csr_array, csc_array)):
            pass
        elif isinstance(X, (sparse.csr_array, sparse.csr_matrix)):
            X = csr_array(X)
        elif isinstance(X, (sparse.csc_array, sparse.csc_matrix)):
            X = csc_array(X)
        elif X is None:
            error_message = (
                'attempting to set X to None; if you want to remove X to save '
                'memory, use drop_X() instead')
            raise ValueError(error_message)
        else:
            error_message = (
                f'new X must be a csr_array, csc_array, csr_matrix, or '
                f'csc_matrix, but has type {type(X).__name__!r}')
            raise TypeError(error_message)
        if X.shape != self._X.shape:
            error_message = (
                f'new X is {X.shape[0]:,} × {X.shape[1]:,}, but old X is '
                f'{self._X.shape[0]:,} × {self._X.shape[1]:,}')
            raise ValueError(error_message)
        dtype = self._X.dtype
        if dtype != np.int32 and dtype != np.int64 and \
                dtype != np.float32 and dtype != np.float64 and \
                dtype != np.uint32 and dtype != np.uint64:
            error_message = (
                f'new X must be (u)int32/64 or float32/64, but has data type '
                f'{str(dtype)}')
            raise TypeError(error_message)
        self._X = X
    
    @property
    def obs(self) -> pl.DataFrame:
        return self._obs
    
    @obs.setter
    def obs(self, obs: pl.DataFrame) -> None:
        check_type(obs, 'obs', pl.DataFrame, 'a polars DataFrame')
        if len(obs) != len(self._obs):
            error_message = (
                f'new obs has length {len(obs):,}, but old obs has length '
                f'{len(self._obs):,}')
            raise ValueError(error_message)
        self._obs = obs

    @property
    def var(self) -> pl.DataFrame:
        return self._var
    
    @var.setter
    def var(self, var: pl.DataFrame) -> None:
        check_type(var, 'var', pl.DataFrame, 'a polars DataFrame')
        if len(var) != len(self._var):
            error_message = (
                f'new var has length {len(var):,}, but old var has length '
                f'{len(self._var):,}')
            raise ValueError(error_message)
        self._var = var
    
    @property
    def obsm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._obsm
    
    @obsm.setter
    def obsm(self, obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_cells = self._X.shape[0]
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but new obsm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(obsm) != num_cells:
                error_message = (
                    f'the length of new obsm[{key!r}] is {len(value):,}, but '
                    f'X.shape[0] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._obsm = obsm.copy()
    
    @property
    def varm(self) -> dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        return self._varm
    
    @varm.setter
    def varm(self, varm: dict[str, np.ndarray[2, Any] | pl.DataFrame]) -> None:
        num_genes = self._X.shape[1]
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but new varm '
                    f'contains a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(varm) != num_genes:
                error_message = (
                    f'the length of new varm[{key!r}] is {len(value):,}, but '
                    f'X.shape[1] is {self._X.shape[0]:,}')
                raise ValueError(error_message)
        self._varm = varm.copy()
    
    @property
    def obsp(self) -> dict[str, csr_array | csc_array]:
        return self._obsp
    
    @obsp.setter
    def obsp(self,
             obsp: dict[str, sparse.csr_array | sparse.csc_array |
                             sparse.csr_matrix | sparse.csc_matrix]) -> None:
        num_cells = self._X.shape[0]
        new_obsp = {}
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of obsp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new obsp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'new obsp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[0] is '
                        f'{num_cells:,}')
                    raise ValueError(error_message)
            new_obsp[key] = value
        self._obsp = new_obsp
    
    @property
    def varp(self) -> dict[str, csr_array | csc_array]:
        return self._varp
    
    @varp.setter
    def varp(self,
             varp: dict[str, sparse.csr_array | sparse.csc_array |
                             sparse.csr_matrix | sparse.csc_matrix]) -> None:
        num_genes = self._X.shape[1]
        new_varp = {}
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of varp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new varp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[1] is '
                        f'{num_genes:,}')
                    raise ValueError(error_message)
            new_varp[key] = value
        self._varp = new_varp
    
    @property
    def uns(self) -> UnsDict:
        return self._uns
    
    @uns.setter
    def uns(self, uns: UnsDict) -> None:
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        self._uns = SingleCell._copy_uns(uns)
    
    @staticmethod
    def _iter_uns(uns: UnsDict, *, prefix: str = 'uns') -> \
            Iterable[tuple[str, str | int | float | np.integer | np.floating |
                                bool | np.bool_ | np.ndarray[Any, Any]]]:
        """
        Recurse through `uns`, yielding tuples of a string describing each key
        (e.g. `"uns['a']['b']"`) and the corresponding value.
        
        Args:
            uns: an `uns` dictionary
            prefix: the prefix to prepend to each key; applied recursively

        Yields:
            Length-2 tuples where the first element is a string describing each
            key, and the second element is the corresponding value.
        """
        for key, value in uns.items():
            key = f'{prefix}[{key!r}]'
            if isinstance(value, dict):
                SingleCell._iter_uns(value, prefix=key)
            else:
                yield key, value
    
    @staticmethod
    def _copy_uns(uns: UnsDict, *, deep: bool = False) -> UnsDict:
        """
        Make a copy of `uns`.
        
        Args:
            uns: an `uns` dictionary
            deep: whether to make a deep or shallow copy of `uns`; if
                  `deep=True`, copy the underlying NumPy arrays if any are
                  present

        Returns:
            A copy of `uns`.
        """
        copied_uns = {}
        if deep:
            for key, value in uns.items():
                if isinstance(value, dict):
                    copied_uns[key] = SingleCell._copy_uns(value, deep=deep)
                elif isinstance(value, np.ndarray):
                    copied_uns[key] = value.copy()
                else:
                    copied_uns[key] = value
        else:
            for key, value in uns.items():
                if isinstance(value, dict):
                    copied_uns[key] = SingleCell._copy_uns(value, deep=deep)
                else:
                    copied_uns[key] = value
        return copied_uns
    
    @staticmethod
    def _read_uns(uns_group: h5py.Group) -> UnsDict:
        """
        Recursively load `uns` from an `.h5ad` file.
        
        Args:
            uns_group: `uns` as an `h5py.Group`

        Returns:
            The loaded `uns`.
        """
        return {key: SingleCell._read_uns(value)
                     if isinstance(value, h5py.Group) else
                     (pl.Series(value[:]).cast(pl.String).to_numpy()
                      if value.shape else value[()].decode('utf-8'))
                     if value.dtype == object else
                     (value[:] if value.shape else value[()].item())
                for key, value in uns_group.items()}
    
    @staticmethod
    def _save_uns(uns: UnsDict,
                  uns_group: h5py.Group,
                  h5ad_file: h5py.File) -> None:
        """
        Recursively save `uns` to an `.h5ad` file.
        
        Args:
            uns: an `uns` dictionary
            uns_group: `uns` as an `h5py.Group`
            h5ad_file: an `h5py.File` open in write mode
        """
        uns_group.attrs['encoding-type'] = 'dict'
        uns_group.attrs['encoding-version'] = '0.1.0'
        for key, value in uns.items():
            if isinstance(value, dict):
                SingleCell._save_uns(value, uns_group.create_group(key),
                                     h5ad_file)
            else:
                dataset = uns_group.create_dataset(key, data=value)
                dataset.attrs['encoding-type'] = \
                    ('string-array' if value.dtype == object else 'array') \
                    if isinstance(value, np.ndarray) else \
                    'string' if isinstance(value, str) else 'numeric-scalar'
                dataset.attrs['encoding-version'] = '0.2.0'
    
    @staticmethod
    def _read_h5Seurat_uns(uns_group: h5py.Group) -> UnsDict:
        """
        Recursively load `uns` (i.e. `misc`) from an `.h5Seurat` file.
        
        Args:
            uns_group: `uns` as an `h5py.Group`

        Returns:
            The loaded `uns`.
        """
        return {key: SingleCell._read_uns(value)
                     if isinstance(value, h5py.Group) else
                     (pl.Series(value[:]).cast(pl.String).to_numpy()
                      if len(value) > 1 else value[:].item().decode('utf-8'))
                     if value.dtype == object else
                     (value[:] if len(value) > 1 else value[:].item())
                for key, value in uns_group.items()}
    
    @staticmethod
    def _save_h5Seurat_uns(uns: UnsDict,
                           misc_group: h5py.Group,
                           h5Seurat_file: h5py.File) -> None:
        """
        Recursively save `uns` (i.e. `misc`) to an `.h5Seurat` file. Only
        string values will be saved.
        
        Args:
            uns: an `uns` dictionary
            misc_group: `uns` as an `h5py.Group`
            h5Seurat_file: an `h5py.File` open in write mode
        """
        for key, value in uns.items():
            if isinstance(value, dict):
                SingleCell._save_h5Seurat_uns(
                    value, misc_group.create_group(key), h5Seurat_file)
            elif isinstance(value, str):
                misc_group.create_dataset(key, data=value)
    
    @property
    def obs_names(self) -> pl.Series:
        return self._obs[:, 0]
    
    @property
    def var_names(self) -> pl.Series:
        return self._var[:, 0]
    
    def set_obs_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of `obs`, i.e. the `obs_names`.
        
        Args:
            column: the column name in `obs`; must be String, Enum, or
                    Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of
            `obs`. If `column` is already the first column, return this dataset
            unchanged.
        """
        obs = self._obs
        check_type(column, 'column', str, 'a string')
        if column == obs.columns[0]:
            return self
        if column not in obs:
            error_message = f'{column!r} is not a column of obs'
            raise ValueError(error_message)
        check_dtype(obs[column], f'obs[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=obs.select(column, pl.exclude(column)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def set_var_names(self, column: str) -> SingleCell:
        """
        Sets a column as the new first column of `var`, i.e. the `var_names`.
        
        Args:
            column: the column name in `var`; must be String, Enum, or
                    Categorical

        Returns:
            A new SingleCell dataset with `column` as the first column of
            `var`. If `column` is already the first column, return this dataset
            unchanged.
        """
        var = self._var
        check_type(column, 'column', str, 'a string')
        if column == var.columns[0]:
            return self
        if column not in var:
            error_message = f'{column!r} is not a column of var'
            raise ValueError(error_message)
        check_dtype(self._var[column], f'var[{column!r}]',
                    (pl.String, pl.Categorical, pl.Enum))
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=var.select(column, pl.exclude(column)),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns, 
                          num_threads=self._num_threads)
    
    @property
    def num_threads(self) -> int:
        """
        Get the default number of threads used for this SingleCell dataset's
        operations.
        
        Returns:
            The default number of threads.
        """
        return self._num_threads
    
    @num_threads.setter
    def num_threads(self, num_threads: int | np.integer) -> None:
        """
        Set the default number of threads used for this SingleCell dataset's
        operations. Also sets the number of threads for this SingleCell
        object's count matrix, if present.
        
        Args:
            num_threads: the new default number of threads. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`.
        """
        check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
        if num_threads == -1:
            num_threads = os.cpu_count()
        else:
            num_threads = int(num_threads)
            if num_threads <= 0:
                error_message = (
                    f'num_threads is {num_threads:,}, but must be a positive '
                    f'integer or -1')
                raise ValueError(error_message)
        self._num_threads = num_threads
        if self._X is not None:
            self._X.num_threads = num_threads
    
    def set_num_threads(self, num_threads: int | np.integer) -> SingleCell:
        """
        Return a new SingleCell dataset with a different default number of
        threads. Also sets the number of threads for the SingleCell dataset's
        count matrix, if present.
        
        Args:
            num_threads: the new default number of threads. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`.
        """
        if self._X is not None:
            self._X.num_threads = num_threads
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=num_threads)
    
    def _process_num_threads(self,
                             num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        SingleCell function.
        
        Check that `num_threads` is a positive integer, -1 or `None`; if
        `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use.
        """
        if num_threads is None:
            return self._num_threads
        check_type(num_threads, 'num_threads', int,
                   'a positive integer, -1, or None')
        if num_threads == -1:
            return os.cpu_count()
        else:
            num_threads = int(num_threads)
            if num_threads <= 0:
                error_message = (
                    f'num_threads is {num_threads:,}, but must be a positive '
                    f'integer, -1, or None')
                raise ValueError(error_message)
            return num_threads
    
    @staticmethod
    def _process_num_threads_static(num_threads: int | np.integer) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        SingleCell function.
        
        Check that `num_threads` is a positive integer or -1; if -1, set to
        `os.cpu_count()`. Unlike `_process_num_threads()`, there is no
        SingleCell object, so `num_threads` cannot be `None`.
        
        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use.
        """
        check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
        if num_threads == -1:
            return os.cpu_count()
        else:
            num_threads = int(num_threads)
            if num_threads <= 0:
                error_message = (
                    f'num_threads is {num_threads:,}, but must be a positive '
                    f'integer or -1')
                raise ValueError(error_message)
            return num_threads
    
    @staticmethod
    def _read_datasets(datasets: Sequence[h5py.Dataset],
                       *,
                       num_threads: int | np.integer) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Read a sequence of HDF5 datasets into a dictionary of 1D NumPy arrays.
        Assume all are from the same file (this is not checked).
        
        Args:
            datasets: a sequence of `h5py.Dataset` objects to read
            num_threads: the number of threads to use when reading; if >1,
                         spawn multiple processes and read into shared memory

        Returns:
            A dictionary of NumPy arrays with the contents of the datasets; the
            keys are taken from each dataset's `name` attribute.
        """
        if len(datasets) == 0:
            return {}
        import multiprocessing
        # noinspection PyUnresolvedReferences
        from multiprocessing.sharedctypes import _new_value
        import resource
        
        # Increase the maximum number of possible file descriptors this process
        # can use, since dataframes with hundreds of columns can easily exhaust
        # the common default limit of 1024 file descriptors (`ulimit -n`)
        soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft_limit < hard_limit:
            resource.setrlimit(resource.RLIMIT_NOFILE,
                               (hard_limit, hard_limit))
        
        # Allocate shared memory for each dataset. Use `_new_value()` instead
        # of `multiprocessing.Array()` to avoid the memset at github.com/
        # python/cpython/blob/main/Lib/multiprocessing/sharedctypes.py#L62.
        # noinspection PyTypeChecker
        buffers = {dataset.name: _new_value(
            len(dataset) * np.ctypeslib.as_ctypes_type(dataset.dtype))
            for dataset in datasets}
        
        # Spawn `num_threads` processes: the first loads the first
        # `len(dataset) / num_threads` elements of each dataset, the second
        # loads the next `len(dataset) / num_threads`, etc. Because the chunks
        # loaded by each process are non-overlapping, there's no need to lock.
        filename = datasets[0].file.filename
        
        def read_dataset_chunks(thread_index: int) -> None:
            try:
                with h5py.File(filename) as h5ad_file:
                    for dataset_name, buffer in buffers.items():
                        chunk_size = \
                            (len(buffer) + num_threads - 1) // num_threads
                        start = thread_index * chunk_size
                        end = min(start + chunk_size, len(buffer))
                        chunk = np.s_[start:end]
                        dataset = h5ad_file[dataset_name]
                        dataset.read_direct(np.frombuffer(
                            buffer, dtype=dataset.dtype),
                            source_sel=chunk, dest_sel=chunk)
            except KeyboardInterrupt:
                pass
        
        # Ignore polars warning about using `os.fork()`
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning,
                                    module='multiprocessing')
            processes = []
            for thread_index in range(num_threads):
                process = multiprocessing.Process(
                    target=read_dataset_chunks, args=(thread_index,))
                processes.append(process)
                process.start()
            for process in processes:
                process.join()
        
        # Wrap the shared memory in NumPy arrays
        arrays = {
            dataset_name: np.frombuffer(buffer, dtype=dataset.dtype)
            for (dataset_name, buffer), dataset in
            zip(buffers.items(), datasets)}
        return arrays
    
    @staticmethod
    def _read_dataset(dataset: h5py.Dataset,
                      num_threads: int | np.integer,
                      preloaded_datasets: dict[str, np.ndarray[1, Any]] |
                                          None = None) -> np.ndarray[1, Any]:
        """
        Read an HDF5 dataset into a 1D NumPy array.
        
        Args:
            dataset: the `h5py.Dataset` to read
            num_threads: the number of threads to use for reading; if >1, spawn
                         multiple processes and read into shared memory, unless
                         - the array is small enough (heuristically, under 10k
                           elements) that it's probably faster to read
                           single-threaded
                         - the array is so small that there's less than one
                           element to read per thread
                         - the array has `dtype=object` (not compatible with
                           shared memory)
            preloaded_datasets: a dictionary of preloaded datasets, or `None`
                                to always load from scratch. If specified and
                                `dataset.name` is in `preloaded_datasets`,
                                just return `preloaded_datasets[dataset.name]`.
        
        Returns:
            A 1D NumPy array with the contents of the dataset.
        """
        if preloaded_datasets is not None and \
                dataset.name in preloaded_datasets:
            return preloaded_datasets[dataset.name]
        dtype = dataset.dtype
        min_size = max(10_000, num_threads)
        if num_threads == 1 or dtype == object or dataset.size < min_size:
            return dataset[:]
        else:
            return SingleCell._read_datasets(
                [dataset], num_threads=num_threads)[dataset.name]
    
    @staticmethod
    def _preload_datasets(group: h5py.Group,
                          num_threads: int | np.integer = 1) -> \
            dict[str, np.ndarray[1, Any]]:
        """
        Given a group from an `.h5ad` file, preload all datasets inside it,
        except for those where:
        - the array is small enough (heuristically, under 10k elements) that
          it's probably faster to read single-threaded
        - the array is so small that there's less than one element to read per
          thread the array has `dtype=object` (not compatible with shared
          memory)
        
        Args:
            group: an `h5py.Group` to preload
            num_threads: the number of threads to use when preloading; if
                         `num_threads == 1`, do not preload

        Returns:
            A (possibly empty) dictionary of preloaded datasets.
        """
        if num_threads == 1:
            return {}
        datasets = []
        min_size = max(10_000, num_threads)
        group.visititems(
            lambda name, node: datasets.append(node)
            if isinstance(node, h5py.Dataset) and node.dtype != object
            and node.size >= min_size else None)
        return SingleCell._read_datasets(datasets, num_threads=num_threads)
    
    @staticmethod
    def _read_h5ad_dataframe(h5ad_file: h5py.File,
                             key: str,
                             *,
                             columns: str | Sequence[str] | None = None,
                             num_threads: int | np.integer) -> \
            pl.DataFrame:
        """
        Load `obs` or `var` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `h5py.File` open in read mode
            key: the key to load as a DataFrame, e.g. `'obs'` or `'var'`
            columns: the column(s) of the DataFrame to load; the index column
                     is always loaded as the first column, regardless of
                     whether it is specified here, and then the remaining
                     columns are loaded in the order specified
            num_threads: the number of threads to use when reading
        
        Returns:
            A polars DataFrame of the data in `h5ad_file[key]`.
        """
        # Get the group corresponding to `obs` or `var`
        group = h5ad_file[key]
        
        # Special case: the entire `obs` or `var` may rarely be a single NumPy
        # structured array (`dtype=void`)
        if isinstance(group, h5py.Dataset) and \
                np.issubdtype(group.dtype, np.void):
            data = pl.from_numpy(group[:])
            data = data.with_columns(pl.col(pl.Binary).cast(pl.String))
            return data
        
        # Preload the datasets in this group
        preloaded_datasets = SingleCell._preload_datasets(group, num_threads)
        
        # Get the list of which columns to load, in which order
        data = {}
        if columns is None:
            columns = group.attrs['column-order']
        else:
            columns = [column for column in to_tuple(columns)
                       if column != group.attrs['_index']]
            for column in columns:
                if column not in group.attrs['column-order']:
                    error_message = f'{column!r} is not a column of {key}'
                    raise ValueError(error_message)
        
        # Create the DataFrame
        for column in chain((group.attrs['_index'],), columns):
            value = group[column]
            encoding_type = value.attrs.get('encoding-type')
            if encoding_type == 'categorical' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'categories' or key == 'codes'
                    for key in value.keys())) or 'categories' in value.attrs:
                
                # Sometimes, the categories are stored in a different place
                # which is pointed to by value.attrs['categories']
                if 'categories' in value.attrs:
                    category_object = h5ad_file[value.attrs['categories']]
                    category_encoding_type = None
                    # noinspection PyTypeChecker
                    codes = SingleCell._read_dataset(
                        value, num_threads, preloaded_datasets)
                else:
                    category_object = value['categories']
                    category_encoding_type = \
                        category_object.attrs.get('encoding-type')
                    codes = SingleCell._read_dataset(
                        value['codes'], num_threads, preloaded_datasets)
                
                # Sometimes, the categories are themselves nullable
                # integer or Boolean arrays
                if category_encoding_type == 'nullable-integer' or \
                        category_encoding_type == 'nullable-boolean' or (
                        isinstance(category_object, h5py.Group) and all(
                        key == 'values' or key == 'mask'
                        for key in category_object.keys())):
                    data[column] = pl.Series(SingleCell._read_dataset(
                        category_object['values'], num_threads,
                        preloaded_datasets)[codes])
                    mask = pl.Series(SingleCell._read_dataset(
                        category_object['mask'], num_threads,
                        preloaded_datasets)[codes] | (codes == -1))
                    has_missing = mask.any()
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                    continue
                # noinspection PyTypeChecker
                categories = SingleCell._read_dataset(
                    category_object, num_threads, preloaded_datasets)
                mask = pl.Series(codes == -1)
                has_missing = mask.any()
                
                # polars does not (as of version 1.0) support Categoricals or
                # Enums with non-string categories, so if the categories are
                # not strings, just map the codes to the categories.
                if category_encoding_type == 'array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype != object):
                    data[column] = pl.Series(categories[codes],
                                             nan_to_null=True)
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                elif category_encoding_type == 'string-array' or (
                        isinstance(category_object, h5py.Dataset) and
                        category_object.dtype == object):
                    if has_missing:
                        codes[mask] = 0
                    data[column] = pl.Series(codes, dtype=pl.UInt32)
                    if has_missing:
                        data[column] = data[column].set(mask, None)
                    data[column] = data[column].cast(
                        pl.Enum(pl.Series(categories).cast(pl.String)))
                else:
                    encoding = \
                        f'encoding-type {category_encoding_type!r}' \
                        if category_encoding_type is not None else \
                            'encoding'
                    error_message = (
                        f'{column!r} column of {key!r} is a categorical '
                        f'with unsupported {encoding}')
                    raise ValueError(error_message)
            elif encoding_type == 'nullable-integer' or \
                    encoding_type == 'nullable-boolean' or (
                    isinstance(value, h5py.Group) and all(
                    key == 'values' or key == 'mask' for key in value.keys())):
                values = SingleCell._read_dataset(
                    value['values'], num_threads, preloaded_datasets)
                mask = SingleCell._read_dataset(
                    value['mask'], num_threads, preloaded_datasets)
                data[column] = pl.Series(values).set(pl.Series(mask), None)
            elif encoding_type == 'array' or (
                    isinstance(value, h5py.Dataset) and value.dtype != object):
                data[column] = pl.Series(SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets), nan_to_null=True)
            elif encoding_type == 'string-array' or (
                    isinstance(value, h5py.Dataset) and value.dtype == object):
                data[column] = SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets)
            else:
                encoding = f'encoding-type {encoding_type!r}' \
                    if encoding_type is not None else 'encoding'
                error_message = \
                    f'{column!r} column of {key!r} has unsupported {encoding}'
                raise ValueError(error_message)
        data = pl.DataFrame(data)
        
        # NumPy doesn't support encoding object-dtyped string arrays as UTF-8,
        # so do the conversion in polars instead
        data = data.with_columns(pl.col(pl.Binary).cast(pl.String))
        return data
    
    @staticmethod
    def _read_h5Seurat_dataframe(group: h5py.Group,
                                 *,
                                 columns: str | Sequence[str] | None = None,
                                 num_threads: int | np.integer) -> \
            pl.DataFrame:
        """
        Load `obs` (i.e. `meta.data`) or `var (i.e. the active assay's
        `meta.features`) from an `.h5Seurat` file as a polars DataFrame.
        
        Args:
            group: the group (`meta.data` or `meta.features`) to read
            columns: the column(s) of the DataFrame to load; the index column
                     is always loaded as the first column, regardless of
                     whether it is specified here, and then the remaining
                     columns are loaded in the order specified
            num_threads: the number of threads to use when reading
        
        Returns:
            A polars DataFrame of the cell- or gene-level metadata.
        """
        # Preload the datasets in `group`
        preloaded_datasets = SingleCell._preload_datasets(group, num_threads)
        
        # Get the list of which columns to load, in which order
        if columns is None:
            columns = group.attrs['colnames']
        else:
            columns = [column for column in to_tuple(columns)
                       if column != group.attrs['_index']]
            for column in columns:
                if column not in group.attrs['colnames']:
                    error_message = f'{column!r} is not a column of meta.data'
                    raise ValueError(error_message)
        
        # Get the list of which columns are Boolean
        Boolean_columns = group.attrs.get('logicals', ())
        
        # Create the DataFrame
        data = {}
        for column in chain(group.attrs['_index'], columns):
            value = group[column]
            if isinstance(value, h5py.Group):
                # Factor
                if len(value) != 2 or 'levels' not in value or \
                        'values' not in value:
                    error_message = (
                        f"the h5Seurat file's meta.data contains a group "
                        f"of unknown format, ")
                    if len(value) <= 5:
                        error_message += (
                            f'with the following keys: '
                            f'{", ".join(map(repr, value))}')
                    else:
                        error_message += (
                            f'with {len(value):,} keys, including the '
                            f'following: '
                            f'{", ".join(map(repr, islice(value, 5)))}')
                    raise ValueError(error_message)
                values = SingleCell._read_dataset(
                    value['values'], num_threads, preloaded_datasets)
                levels = value['levels'][:]
                # noinspection PyUnresolvedReferences
                data[column] = (pl.Series(values) - 1)\
                    .cast(pl.Enum(pl.Series(levels).cast(pl.String)))
            else:
                data[column] = pl.Series(SingleCell._read_dataset(
                    value, num_threads, preloaded_datasets), nan_to_null=True)
                if column in Boolean_columns:
                    data[column] = data[column]\
                        .replace({2: None})\
                        .cast(pl.Boolean)
                elif data[column].dtype == pl.Int32:
                    data[column] = data[column].replace({-2147483648: None})
        data = pl.DataFrame(data)
        
        # NumPy doesn't support encoding object-dtyped string arrays as UTF-8,
        # so do the conversion in polars instead. Also convert `b'NA'`
        # (h5Seurat's missing value indicator) to `null`.
        data = data.with_columns(pl.col(pl.Binary).replace({b'NA': None})
                                 .cast(pl.String))
        return data
    
    @staticmethod
    def read_obs(h5ad_file: h5py.File | str | Path,
                 *,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer = -1) -> pl.DataFrame:
        """
        Load just `obs` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            columns: the column(s) of `obs` to load; if `None`, load all
                     columns
            num_threads: the number of threads to use when reading. By
                         default (`num_threads=-1`), use all available cores,
                         as determined by `os.cpu_count()`.
    
        Returns:
            A polars DataFrame of the data in `obs`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        
        # Check that `num_threads` is a positive integer or -1; if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads_static(num_threads)
        
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'obs', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_var(h5ad_file: str | Path,
                 *,
                 columns: str | Iterable[str] | None = None,
                 num_threads: int | np.integer = -1) -> pl.DataFrame:
        """
        Load just `var` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            columns: the column(s) of `var` to load; if `None`, load all
                     columns
            num_threads: the number of threads to use when reading. By
                         default (`num_threads=-1`), use all available cores,
                         as determined by `os.cpu_count()`.
    
        Returns:
            A polars DataFrame of the data in `var`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if columns is not None:
            columns = to_tuple_checked(columns, 'columns', str, 'strings')
        
        # Check that `num_threads` is a positive integer or -1; if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads_static(num_threads)
        
        with h5py.File(filename) as f:
            return SingleCell._read_h5ad_dataframe(
                f, 'var', columns=columns, num_threads=num_threads)
    
    @staticmethod
    def read_obsm(h5ad_file: str | Path,
                  *,
                  keys: str | Iterable[str] | None = None,
                  num_threads: int | np.integer = -1) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just `obsm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `obsm` to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of `obsm`. By default (`num_threads=-1`), use all
                         available cores, as determined by `os.cpu_count()`.
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            obsm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        
        # Check that `num_threads` is a positive integer or -1; if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads_static(num_threads)
        
        with h5py.File(filename) as f:
            if 'obsm' in f:
                obsm = f['obsm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'obsm/{key}', num_threads=num_threads)
                            for key, value in obsm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in obsm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of obsm')
                            raise ValueError(error_message)
                    return {key: obsm[key][:]
                                 if isinstance(obsm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'obsm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but obsm is empty'
                    raise ValueError(error_message)
                return {}
         
    @staticmethod
    def read_varm(h5ad_file: str | Path,
                  *,
                  keys: str | Iterable[str] | None = None,
                  num_threads: int | np.integer = -1) -> \
            dict[str, np.ndarray[2, Any] | pl.DataFrame]:
        """
        Load just `varm` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `varm` to load; if `None`, load all keys
            num_threads: the number of threads to use when reading DataFrame
                         keys of `varm`. By default (`num_threads=-1`), use all
                         available cores, as determined by `os.cpu_count()`.
        
        Returns:
            A dictionary of NumPy arrays and polars DataFrames of the data in
            varm.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        
        # Check that `num_threads` is a positive integer or -1; if -1, set to
        # `os.cpu_count()`
        num_threads = SingleCell._process_num_threads_static(num_threads)
        
        with h5py.File(filename) as f:
            if 'varm' in f:
                varm = f['varm']
                if keys is None:
                    return {key: value[:]
                                 if isinstance(value, h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                     f, f'varm/{key}', num_threads=num_threads)
                            for key, value in varm.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in varm:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of varm')
                            raise ValueError(error_message)
                    return {key: varm[key][:]
                                 if isinstance(varm[key], h5py.Dataset) else
                                 SingleCell._read_h5ad_dataframe(
                                    f, f'varm/{key}', num_threads=num_threads)
                            for key in keys}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but varm is empty'
                    raise ValueError(error_message)
                return {}
    
    @staticmethod
    def read_obsp(h5ad_file: str | Path,
                  *,
                  keys: str | Iterable[str] | None = None) -> \
            dict[str, csr_array | csc_array]:
        """
        Load just `obsp` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `obsp` to load; if `None`, load all keys
        
        Returns:
            A dictionary of sparse arrays of the data in `obsp`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        
        with h5py.File(filename) as f:
            if 'obsp' in f:
                obsp = f['obsp']
                if keys is None:
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in obsp.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in obsp:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of obsp')
                            raise ValueError(error_message)
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in
                            ((key, obsp[key]) for key in keys)}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but obsp is empty'
                    raise ValueError(error_message)
                return {}

    @staticmethod
    def read_varp(h5ad_file: str | Path,
                  *,
                  keys: str | Iterable[str] | None = None) -> \
            dict[str, csr_array | csc_array]:
        """
        Load just `varp` from an `.h5ad` file as a polars DataFrame.
        
        Args:
            h5ad_file: an `.h5ad` filename
            keys: the keys(s) of `varp` to load; if `None`, load all keys
        
        Returns:
            A dictionary of sparse arrays of the data in `varp`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        if keys is not None:
            keys = to_tuple_checked(keys, 'keys', str, 'strings')
        
        with h5py.File(filename) as f:
            if 'varp' in f:
                varp = f['varp']
                if keys is None:
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in varp.items()}
                else:
                    for key_index, key in enumerate(keys):
                        if key not in varp:
                            error_message = (
                                f'keys[{key_index}] is {key!r}, which is not '
                                f'a key of varp')
                            raise ValueError(error_message)
                    return {key: (csr_array if value.attrs['encoding-type'] ==
                                               'csr_matrix' else csc_array)(
                                (value['data'][:], value['indices'][:],
                                 value['indptr'][:]),
                                shape=value.attrs['shape'])
                            for key, value in
                            ((key, varp[key]) for key in keys)}
            else:
                if keys is not None:
                    error_message = 'keys was specified, but varp is empty'
                    raise ValueError(error_message)
                return {}
    
    @staticmethod
    def read_uns(h5ad_file: str | Path) -> UnsDict:
        """
        Load just `uns` from an `.h5ad` file as a dictionary.
        
        Args:
            h5ad_file: an .`h5ad` filename
        
        Returns:
            A dictionary of the data in `uns`.
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        with h5py.File(filename) as f:
            if 'uns' in f:
                return SingleCell._read_uns(f['uns'])
            else:
                return {}
    
    @staticmethod
    def _print_matrix_info(X: h5py.Group | h5py.Dataset, X_name: str) -> None:
        """
        Given a key of an `.h5ad` file representing a sparse or dense matrix,
        print its shape, data type and (if sparse) number of non-zero elements.
        
        Args:
            X: the key in the `.h5ad` file representing the matrix, as a
               `Group` or `Dataset` object
            X_name: the name of the key
        """
        is_sparse = isinstance(X, h5py.Group)
        if is_sparse:
            data = X['data']
            shape = X.attrs['shape'] if 'shape' in X.attrs else \
                X.attrs['h5sparse_shape']
            dtype = str(data.dtype)
            nnz = data.shape[0]
            print(f'{X_name}: {shape[0]:,} × {shape[1]:,} sparse array with '
                  f'{nnz:,} non-zero elements, data type {dtype!r}, and '
                  f'first non-zero element = {data[0]:.6g}')
        else:
            shape = X.shape
            dtype = str(X.dtype)
            print(f'{X_name}: {shape[0]:,} × {shape[1]:,} dense matrix with '
                  f'data type {dtype!r} and first element = {X[0, 0]:.6g}')
    
    @staticmethod
    def ls(h5ad_file: str | Path) -> None:
        """
        Print the fields in an `.h5ad` file. This can be useful e.g. when
        deciding which count matrix to load via the `X_key` argument to
        `SingleCell()`.
        
        Args:
            h5ad_file: an `.h5ad` filename
        """
        check_type(h5ad_file, 'h5ad_file', (str, Path),
                   'a string or pathlib.Path')
        h5ad_file = str(h5ad_file)
        if not h5ad_file.endswith('.h5ad'):
            error_message = f".h5ad file {h5ad_file!r} must end with '.h5ad'"
            raise ValueError(error_message)
        filename = os.path.expanduser(h5ad_file)
        if not os.path.exists(filename):
            error_message = f'.h5ad file {h5ad_file} does not exist'
            raise FileNotFoundError(error_message)
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        attrs = 'obs', 'var', 'obsm', 'varm', 'obsp', 'varp', 'uns'
        with h5py.File(filename) as f:
            # `X`
            SingleCell._print_matrix_info(f['X'], 'X')
            
            # layers
            if 'layers' in f:
                layers = f['layers']
                if len(layers) > 0:
                    for layer_name, layer in layers.items():
                        SingleCell._print_matrix_info(
                            layer, f'layers[{layer_name!r}]')
            
            # `obs`, `var`, `obsm`, `varm`, `obsp`, `varp`, `uns`
            for attr in attrs:
                if attr in f:
                    entries = f[attr]
                    if (attr == 'obs' or attr == 'var') and \
                            isinstance(entries, h5py.Dataset) and \
                            np.issubdtype(entries.dtype, np.void):
                        entries = entries.dtype.fields
                    if len(entries) > 0:
                        print(fill(f'{attr}: {", ".join(entries)}',
                                   width=terminal_width,
                                   subsequent_indent=' ' * (len(attr) + 2)))
            
            # raw
            if 'raw' in f:
                raw = f['raw']
                if len(raw) > 0:
                    print('raw:')
                    if 'X' in raw:
                        SingleCell._print_matrix_info(raw['X'], '    X')
                    if 'layers' in raw:
                        layers = raw['layers']
                        if len(layers) > 0:
                            for layer_name, layer in layers.items():
                                SingleCell._print_matrix_info(
                                    layer, f'    layers[{layer_name!r}]')
                    for attr in attrs:
                        if attr in raw:
                            entries = raw[attr]
                            if (attr == 'obs' or attr == 'var') and \
                                    isinstance(entries, h5py.Dataset) and \
                                    np.issubdtype(entries.dtype, np.void):
                                entries = entries.dtype.fields
                            if len(entries) > 0:
                                print(fill(f'    {attr}: {", ".join(entries)}',
                                           width=terminal_width,
                                           subsequent_indent=' ' * (
                                                   len(attr) + 6)))
    
    def __eq__(self, other: SingleCell) -> bool:
        """
        Test for equality with another SingleCell dataset.
        
        Args:
            other: the other SingleCell dataset to test for equality with

        Returns:
            Whether the two SingleCell datasets are identical.
        """
        if not isinstance(other, SingleCell):
            error_message = (
                f'the left-hand operand of `==` is a SingleCell dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return (other._X is None if self._X is None else
                other._X is not None) and \
               self._obs.equals(other._obs) and \
               self._var.equals(other._var) and \
               self._obsm.keys() == other._obsm.keys() and \
               self._varm.keys() == other._varm.keys() and \
            all(type(other._obsm[key]) is type(value) and
                (array_equal(other._obsm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._obsm[key].equals(value))
                for key, value in self._obsm.items()) and \
            all(type(other._varm[key]) is type(value) and
                (array_equal(other._varm[key], value)
                 if isinstance(value, np.ndarray) else
                 other._varm[key].equals(value))
                for key, value in self._varm.items()) and \
            SingleCell._eq_uns(self._uns, other._uns) and \
            self._X.equals(other._X)
    
    @staticmethod
    def _eq_uns(uns: UnsDict,
                other_uns: UnsDict,
                different_order_ok: bool = False) -> bool:
        """
        Test whether two `uns` are equal.
        
        Args:
            uns: an `uns`
            other_uns: another `uns`
            different_order_ok: whether to consider `uns` and `other_uns` equal
                                when they have the same keys and values, but in
                                a different order

        Returns:
            Whether `uns` and `other_uns` are equal.
        """
        return set(uns.keys()) == set(other_uns.keys()) \
            if different_order_ok else uns.keys() == other_uns.keys() and all(
            isinstance(value, dict) and isinstance(other_value, dict) and
            SingleCell._eq_uns(value, other_value, different_order_ok) or
            isinstance(value, np.ndarray) and
            isinstance(other_value, np.ndarray) and
            array_equal(value, other_value) or
            not isinstance(other_value, (dict, np.ndarray)) and
            value == other_value
            for key, value, other_value in
            ((key, value, other_uns[key]) for key, value in uns.items()))
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'SingleCell indices must be cells, a length-1 tuple of (cells,), '
            f'or a length-2 tuple of (cells, genes). Cells and genes must '
            f'each be a string or integer; a slice of strings or integers; or '
            f'a list, NumPy array, or polars Series of strings, integers, or '
            f'Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (`obs` or `var`)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('_SingleCell_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    # noinspection PyTypeChecker
    @staticmethod
    def _getitem_process(item: Indexer | tuple[Indexer, Indexer], index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to `__getitem__()`.
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (`obs` or `var`) to process the element with
                respect to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [SingleCell._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = SingleCell._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                SingleCell._getitem_error(item)
            if isinstance(stop, str):
                stop = SingleCell._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                SingleCell._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                SingleCell._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            subitem = pl.Series(subitem)
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            dtype = subitem.dtype
            if dtype in (pl.String, pl.Categorical, pl.Enum):
                names_dtype = df[:, 0].dtype
                if dtype != names_dtype:
                    subitem = subitem.cast(names_dtype)
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_SingleCell_index=pl.int_range(
                              pl.len(), dtype=pl.UInt32)),
                          on=df.columns[0], how='left')\
                    ['_SingleCell_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif dtype.is_integer() or dtype == pl.Boolean:
                return subitem
            else:
                SingleCell._getitem_error(item)
        else:
            SingleCell._getitem_error(item)
            
    def __getitem__(self, item: Indexer | tuple[Indexer, Indexer]) -> \
            SingleCell:
        """
        Subset to specific cell(s) and/or gene(s).
        
        Index with a tuple of `(cells, genes)`. If `cells` and `genes` are
        integers, arrays/lists/slices of integers, or arrays/lists of Booleans,
        the result will be a SingleCell dataset subset to `X[cells, genes]`,
        `obs[cells]`, `var[genes]`, `obsm[cells]`, `varm[genes]`,
        `obsp[cells][:, cells]`, and `varp[genes][:, genes]`. However, `cells`
        and/or `genes` can instead be strings (or arrays or slices of strings),
        in which case they refer to the first column of `obs` (`obs_names`)
        and/or `var` (`var_names`), respectively.
        
        Examples:
        - Subset to one cell, for all genes:
          ```
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416']
          sc[2]
          ```
        - Subset to one gene, for all cells:
          ```
          sc[:, 'APOE']
          sc[:, 13196]
          ```
        - Subset to one cell and one gene:
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416', 'APOE']
          sc[2, 13196]
          ```
        - Subset to a range of cells and genes:
          ```
          sc['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416':
             'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522',
             'APOE':'TREM2']
          sc[2:6, 13196:34268]
          ```
        - Subset to specific cells and genes:
          ```
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522']]
          sc[:, pl.Series(['APOE', 'TREM2'])]
          sc[['CGAATTGGTGACAGGT-L8TX_210916_01_B05-1131590416',
              'CCCTCTCAGCAGCCTC-L8TX_211007_01_A09-1135034522'],
              np.array(['APOE', 'TREM2'])]
          ```
        
        Args:
            item: the item to index with
        
        Returns:
            A new SingleCell dataset subset to the specified cells and/or
            genes.
        """
        if not isinstance(item, (int, str, slice, tuple, list,
                                 np.ndarray, pl.Series)):
            error_message = (
                f'SingleCell datasets must be indexed with an integer, '
                f'string, slice, tuple, list, NumPy array, or polars Series, '
                f'but you tried to index with an object of type '
                f'{type(item).__name__!r}')
            raise TypeError(error_message)
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 2:
                self._getitem_error(item)
        else:
            item = item,
        rows = self._getitem_process(item, 0, self._obs)
        rows_is_Series = isinstance(rows, pl.Series)
        if rows_is_Series:
            boolean_Series = rows.dtype == pl.Boolean
            obs = self._obs.filter(rows) if boolean_Series else self._obs[rows]
            rows_NumPy = rows.to_numpy()
        else:
            boolean_Series = False
            obs = self._obs[rows]
            rows_NumPy = rows
        obsm = {key: (value.filter(rows) if boolean_Series else
                      value[rows]) if isinstance(value, pl.DataFrame) else
                     value[rows_NumPy]
                for key, value in self._obsm.items()} if self._obsm else {}
        rows_is_slice = isinstance(rows, slice)
        obsp = ({key: value[rows_NumPy, rows_NumPy]
                 for key, value in self._obsp.items()} if rows_is_slice else
                {key: value[np.ix_(rows_NumPy, rows_NumPy)]
                 for key, value in self._obsp.items()}) if self._obsp else {}
        if len(item) == 1:
            return SingleCell(X=self._X[rows] if self._X is not None else None,
                              obs=obs, var=self._var, obsm=obsm,
                              varm=self._varm, obsp=obsp, varp=self._varp,
                              uns=self._uns, num_threads=self._num_threads)
        cols = self._getitem_process(item, 1, self._var)
        cols_is_Series = isinstance(cols, pl.Series)
        if cols_is_Series:
            boolean_Series = cols.dtype == pl.Boolean
            var = self._var.filter(cols) if boolean_Series else self._var[cols]
            cols_NumPy = cols.to_numpy()
        else:
            boolean_Series = False
            var = self._var[cols]
            cols_NumPy = cols
        varm = {key: (value.filter(cols) if boolean_Series else
                      value[cols]) if isinstance(value, pl.DataFrame) else
                     value[cols_NumPy]
                for key, value in self._varm.items()} if self._varm else {}
        cols_is_slice = isinstance(cols, slice)
        varp = ({key: value[cols_NumPy, cols_NumPy]
                 for key, value in self._varp.items()} if cols_is_slice else
                {key: value[np.ix_(cols_NumPy, cols_NumPy)]
                 for key, value in self._varp.items()}) if self._varp else {}
        X = None if self._X is None else self._X[rows_NumPy, cols_NumPy] \
            if rows_is_slice or cols_is_slice else \
            self._X[np.ix_(rows_NumPy, cols_NumPy)]
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, varm=varm,
                          obsp=obsp, varp=varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    # noinspection PyTypeChecker
    def cell(self,
             cell: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the row of `X` corresponding to a single cell, based on the cell's
        name in `obs_names`.
        
        Args:
            cell: the name of the cell in `obs_names`
            num_threads: the number of threads to use when retrieving the
                         row of `X`. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores
                         when `X` is a CSC array and 1 thread when `X` is a CSR
                         array. Cannot be specified when `X` is a CSR array,
                         since there is no benefit to parallelism in that case.
        
        Returns:
            The corresponding row of `X`, as a dense 1D NumPy array with zeros
            included.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so getting a row of X is not possible'
            raise ValueError(error_message)
        
        # Check that `num_threads` is not specified when `X` is a CSR array
        if num_threads is not None and isinstance(self._X, csr_array):
            error_message = (
                'num_threads cannot be specified when X is a CSR array, since '
                'cell() will always run single-threaded')
            raise ValueError(error_message)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        row_index = SingleCell._getitem_by_string(self._obs, cell)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[[row_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    # noinspection PyTypeChecker
    def gene(self,
             gene: str,
             *,
             num_threads: int | np.integer | None = None) -> \
            np.ndarray[1, Any]:
        """
        Get the column of `X` corresponding to a single gene, based on the
        gene's name in `var_names`.
        
        Args:
            gene: the name of the gene in `var_names`
            num_threads: the number of threads to use when retrieving the
                         row of `X`. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores
                         when `X` is a CSR array and 1 thread when `X` is a CSC
                         array. Cannot be specified when `X` is a CSC array,
                         since there is no benefit to parallelism in that case.
        
        Returns:
            The corresponding column of `X`, as a dense 1D NumPy array with
            zeros included.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = \
                'X is None, so getting a column of X is not possible'
            raise ValueError(error_message)
        
        # Check that `num_threads` is not specified when `X` is a CSC array
        if num_threads is not None and isinstance(self._X, csc_array):
            error_message = (
                'num_threads cannot be specified when X is a CSC array, since '
                'cell() will always run single-threaded')
            raise ValueError(error_message)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        column_index = SingleCell._getitem_by_string(self._var, gene)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return self._X[:, [column_index]].toarray().squeeze()
        finally:
            self._X._num_threads = original_num_threads
    
    def __len__(self) -> int:
        """
        Get the number of cells in this SingleCell dataset.
        
        Returns:
            The number of cells.
        """
        return self._obs.shape[0]
       
    def __repr__(self) -> str:
        """
        Get a string representation of this SingleCell dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        descr = (
            f'SingleCell dataset with {len(self._obs):,} '
            f'{plural("cell", len(self._obs))} (obs), {len(self._var):,} '
            f'{plural("gene", len(self._var))} (var), and ')
        if self._X is not None:
            descr += (f'{self._X.nnz:,} non-zero '
                      f'{"entries" if self._X.nnz != 1 else "entry"} (X)')
        else:
            descr += 'no X'
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        for attr in 'obs', 'var', 'obsm', 'varm', 'obsp', 'varp', 'uns':
            entries = getattr(self, attr).columns \
                if attr == 'obs' or attr == 'var' else getattr(self, attr)
            if len(entries) > 0:
                descr += '\n' + fill(
                    f'    {attr}: {", ".join(entries)}',
                    width=terminal_width,
                    subsequent_indent=' ' * (len(attr) + 6))
        return descr
    
    @property
    def shape(self) -> tuple[int, int]:
        """
        Get the shape of this SingleCell dataset.
        
        Returns:
            A length-2 tuple where the first element is the number of cells,
            and the second is the number of genes.
        """
        return self._obs.shape[0], self._var.shape[0]
    
    @staticmethod
    def _save_h5ad_dataframe(h5ad_file: h5py.File,
                             df: pl.DataFrame,
                             key: str,
                             preserve_strings: bool) -> None:
        """
        Save `obs` or `var` to an `.h5ad` file.
        
        Args:
            h5ad_file: an `h5py.File` open in write mode
            df: the DataFrame to write, e.g. `obs` or `var`
            key: the key to create in `h5ad_file`, e.g. `'obs'` or `'var'`
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space; if `True`,
                              preserve these columns as string columns
        """
        # Create a group for the data frame and add top-level metadata
        group = h5ad_file.create_group(key)
        group.attrs['_index'] = df.columns[0]
        group.attrs['column-order'] = df.columns[1:]
        group.attrs['encoding-type'] = 'dataframe'
        group.attrs['encoding-version'] = '0.2.0'
        for column in df:
            dtype = column.dtype
            if dtype == pl.String:
                if column.null_count() or not preserve_strings and \
                        column.is_duplicated().any():
                    column = column\
                        .cast(pl.Enum(column.unique(maintain_order=True)
                                      .drop_nulls()))
                    dtype = column.dtype
                else:
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'string-array'
                    dataset.attrs['encoding-version'] = '0.2.0'
                    continue
            if dtype == pl.Enum or dtype == pl.Categorical:
                is_Enum = dtype == pl.Enum
                subgroup = group.create_group(column.name)
                subgroup.attrs['encoding-type'] = 'categorical'
                subgroup.attrs['encoding-version'] = '0.2.0'
                subgroup.attrs['ordered'] = is_Enum
                categories = column.cat.get_categories()
                if not is_Enum:
                    column = column.cast(pl.Enum(categories))
                codes = column.to_physical().fill_null(-1)
                subgroup.create_dataset('codes', data=codes.to_numpy())
                if len(categories) == 0:
                    subgroup.create_dataset('categories', shape=(0,),
                                            dtype=h5py.special_dtype(vlen=str))
                else:
                    subgroup.create_dataset('categories',
                                            data=categories.to_numpy())
            elif dtype.is_float():
                # Nullable floats are not supported, so convert `null` to `NaN`
                dataset = group.create_dataset(
                    column.name, data=column.fill_null(np.nan).to_numpy())
                dataset.attrs['encoding-type'] = 'array'
                dataset.attrs['encoding-version'] = '0.2.0'
            else:  # Boolean or integer
                is_Boolean = dtype == pl.Boolean
                if column.null_count():
                    # Store as nullable integer/Boolean
                    subgroup = group.create_group(column.name)
                    subgroup.attrs['encoding-type'] = \
                        f'nullable-{"boolean" if is_Boolean else "integer"}'
                    subgroup.attrs['encoding-version'] = '0.1.0'
                    subgroup.create_dataset(
                        'values',
                        data=column.fill_null(False if is_Boolean else 1)
                        .to_numpy())
                    subgroup.create_dataset(
                        'mask', data=column.is_null().to_numpy())
                else:
                    # Store as regular integer/Boolean
                    dataset = group.create_dataset(column.name,
                                                   data=column.to_numpy())
                    dataset.attrs['encoding-type'] = 'array'
                    dataset.attrs['encoding-version'] = '0.2.0'
    
    @staticmethod
    def _save_h5Seurat_dataframe(h5ad_file: h5py.File,
                                 df: pl.DataFrame,
                                 key: str,
                                 preserve_strings: bool) -> None:
        """
        Save `obs` or `var` to an `.h5Seurat` file.
        
        Args:
            h5ad_file: an `h5py.File` open in write mode
            df: the DataFrame to write, e.g. `obs` or `var`
            key: the key to create in `h5ad_file`, e.g. `'meta.data'` or
                 `'RNA/meta.features'`
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space; if `True`,
                              preserve these columns as string columns
        """
        # Create a group for the data frame and add top-level metadata
        group = h5ad_file.create_group(key)
        group.attrs['_index'] = df.columns[0]
        group.attrs['colnames'] = df.columns[1:]
        group.attrs['logicals'] = pl.selectors.expand_selector(
            df, pl.selectors.by_dtype(pl.Boolean))
        for column in df:
            dtype = column.dtype
            if dtype == pl.String:
                if column.null_count() or not preserve_strings and \
                        column.is_duplicated().any():
                    column = column\
                        .cast(pl.Enum(column.unique(maintain_order=True)
                                      .drop_nulls()))
                    dtype = column.dtype
                else:
                    group.create_dataset(column.name, data=column.to_numpy())
                    continue
            if dtype == pl.Enum or dtype == pl.Categorical:
                subgroup = group.create_group(column.name)
                levels = column.cat.get_categories()
                if dtype != pl.Enum:
                    column = column.cast(pl.Enum(levels))
                # noinspection PyUnresolvedReferences
                values = (column.to_physical() + 1).fill_null(-2147483648)
                subgroup.create_dataset('values', data=values.to_numpy())
                if len(levels) == 0:
                    subgroup.create_dataset('levels', shape=(0,),
                                            dtype=h5py.special_dtype(vlen=str))
                else:
                    subgroup.create_dataset('levels', data=levels.to_numpy())
            else:
                if dtype.is_float():
                    column = column.fill_null(np.nan)
                elif dtype == pl.Boolean:
                    column = column.cast(pl.Int32).fill_null(2)
                else:  # integer
                    column = column.fill_null(-2147483648)
                group.create_dataset(column.name, data=column.to_numpy())
    
    def save(self,
             filename: str | Path,
             *,
             assay: str = 'RNA',
             X_key: str = 'counts',
             overwrite: bool = False,
             preserve_strings: bool = False,
             sce: bool = False) -> None:
        """
        Save this SingleCell dataset to a file. File format will be inferred
        from the file extension (e.g. `.h5ad`).
        
        Args:
            filename: an AnnData `.h5ad` file, Seurat `.rds` or `.h5Seurat`
                      file, SingleCellExperiment `.rds` file, or 10x `.h5` or
                      `.mtx.gz` file to save to. If the extension is `.rds`,
                      the `sce` argument will determine whether to save to a
                      Seurat or a SingleCellExperiment object.
                      - When saving to a Seurat `.rds` file, to match the
                        requirements of Seurat objects, the `'X_'` prefix
                        (often used by Scanpy) will be removed from each key of
                        obsm where it is present (e.g. `'X_umap'` will become
                        `'umap'`).
                      - When swaving to a Seurat `.rds` file, Seurat will add
                       `'orig.ident'`, `'nCount_RNA'` and `'nFeature_RNA'` as
                        gene-level metadata by default; you can disable the
                        calculation of the latter two columns with:
                        
                        ```python
                        from ryp import r
                        r('options(Seurat.object.assay.calcn = FALSE)')
                        ```
                        
                      - When saving to a Seurat`.rds` or `.h5Seurat` file or a
                        SingleCellExperiment `.rds` file, `varm` will not be
                        saved.
                      - When saving to a 10x `.h5` file, `obs['barcodes']`,
                        `var['feature_type']`, `var['genome']`, `var['id']`,
                        and `var['name']` must all exist. Only `X` and these
                        columns will be saved, along with whichever of
                        `var['pattern']`, `var['read']`, and `var['sequence']`
                        exist. All of these columns (if they exist) must be
                        String, Enum, or Categorical.
                      - When saving to a 10x `.mtx.gz` file, `barcodes.tsv.gz`
                        and `features.tsv.gz` will be created in the same
                        directory. Only `X`, `obs` and `var` will be saved.
            assay: when saving to a Seurat `.rds` or `.h5Seurat` file, the name
                   to use for the active assay
            X_key: when saving to a Seurat `.rds` or `.h5Seurat` file, the name
                   of the slot within the active assay to save `X` to; must be
                   `'counts'` or `'data'`. When saving to a
                   SingleCellExperiment `.rds` file, the name of the slot
                   within `@assays@data` to save `X` to.
            overwrite: if `False`, raises an error if (any of) the file(s)
                       exist; if `True`, overwrites them
            preserve_strings: if `False`, encode string columns with duplicate
                              values as Enums to save space, when saving to
                              AnnData `.h5ad` or Seurat or SingleCellExperiment
                              `.rds`; if `True`, preserve these columns as
                              string columns. (Regardless of the value of
                              `preserve_strings`, String columns with `null`
                              values will be encoded as Enums when saving to
                              `.h5ad`, since the `.h5ad` format cannot
                              represent them otherwise.)
            sce: if `True` and the extension of filename is `.rds`, save to a
                 SingleCellExperiment object instead of a Seurat object
        """
        # Check that `filename` is a string or `Path`; convert it to a string
        # and expand `~` into home directories
        check_type(filename, 'filename', (str, Path),
                   'a string or pathlib.Path')
        filename = str(filename)
        filename_expanduser = os.path.expanduser(filename)
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Raise an error if the filename already exists, unless
        # `overwrite=True`
        if not overwrite and os.path.exists(filename_expanduser):
            error_message = (
                f'filename {filename!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so saving is not possible'
            raise ValueError(error_message)
        
        # Get the file type from the extension, raising an error if invalid
        is_h5ad = filename.endswith('.h5ad')
        is_rds = filename.endswith('.rds')
        is_h5Seurat = filename.endswith('.h5Seurat')
        is_h5 = filename.endswith('.h5')
        is_hdf5 = is_h5ad or is_h5 or is_h5Seurat
        is_mtx = filename.endswith('.mtx.gz')
        is_Seurat = is_h5Seurat or is_rds and not sce
        if not (is_hdf5 or is_mtx or is_rds):
            error_message = (
                f"filename {filename!r} does not end with '.h5ad', '.rds', "
                f"'.h5Seurat', '.h5', or '.mtx.gz'")
            raise ValueError(error_message)
        
        # Check that `assay` is a string, and that `assay` is not specified
        # (i.e. retains its default value) unless saving to a Seurat object
        check_type(assay, 'assay', str, 'a string')
        if not is_Seurat and assay != 'RNA':
            error_message = \
                'assay cannot be specified unless saving to a Seurat object'
            raise ValueError(error_message)
        
        # Check that `X_key` is `'counts'` or `'data'` when saving to a Seurat
        # object, any string when saving to a SingleCellExperiment object, and
        # not specified (i.e. retains its default value) otherwise
        check_type(X_key, 'X_key', str, 'a string')
        if is_Seurat:
            if X_key != 'counts' and X_key != 'data':
                error_message = (
                    f"when saving to a Seurat object, X_key must be 'counts' "
                    f"or 'data', not {X_key!r}")
                raise ValueError(error_message)
        elif not sce and X_key != 'counts':
            error_message = (
                'X_key cannot be specified unless saving to a Seurat or '
                'SingleCellExperiment object')
            raise ValueError(error_message)
        
        # Check that `preserve_strings` and `sce` are Boolean, and that `sce`
        # is only `True` when saving to an `.rds` file
        check_type(preserve_strings, 'preserve_strings', bool, 'Boolean')
        check_type(sce, 'sce', bool, 'Boolean')
        if sce and not is_rds:
            error_message = 'sce can only be True when saving to an .rds file'
            raise ValueError(error_message)
        
        # Raise an error if `obs` or `var` (or, if saving to `.h5ad`, DataFrame
        # keys of `obsm` or `varm`) contain columns with unsupported data types
        # (anything but float, int, String, Categorical, Enum, Boolean)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'saving')
                    raise TypeError(error_message)
        if is_h5ad:
            for field, field_name in (self._obsm, 'obsm'), \
                    (self._varm, 'varm'):
                for key, value in field.items():
                    if not isinstance(value, pl.DataFrame):
                        continue
                    for column, dtype in value.schema.items():
                        if dtype.base_type() not in valid_dtypes:
                            error_message = (
                                f'{field}[{key!r}][{column!r}] has the data '
                                f'type {dtype.base_type()!r}, which is not '
                                f'supported when saving')
                            raise TypeError(error_message)
        
        # Raise an error if `obsm`, `varm` or `uns` contain NumPy arrays with
        # unsupported data types (`datetime64`, `timedelta64`, unstructured
        # `void`). Do not specifically check `dtype=object` to avoid extra
        # overhead.
        for field, field_name in \
                (self._obsm, 'obsm'), (self._varm, 'varm'), (self._uns, 'uns'):
            for key, value in field.items():
                if not isinstance(value, np.ndarray):
                    continue
                if value.dtype.type == np.void and value.dtype.names is None:
                    error_message = (
                        f'{field_name}[{key!r}] is an unstructured void '
                        f'array, which is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.datetime64:
                    error_message = (
                        f'{field_name}[{key!r}] is a datetime64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
                elif value.dtype == np.timedelta64:
                    error_message = (
                        f'{field_name}[{key!r}] is a timedelta64 array, which '
                        f'is not supported when saving')
                    raise TypeError(error_message)
        
        # Save, depending on the file extension
        if is_hdf5:
            try:
                with h5py.File(filename_expanduser, 'w') as hdf5_file:
                    if is_h5ad:
                        # Add top-level metadata
                        hdf5_file.attrs['encoding-type'] = 'anndata'
                        hdf5_file.attrs['encoding-version'] = '0.1.0'
                        
                        # Save `obs` and `var`
                        SingleCell._save_h5ad_dataframe(
                            hdf5_file, self._obs, 'obs', preserve_strings)
                        SingleCell._save_h5ad_dataframe(
                            hdf5_file, self._var, 'var', preserve_strings)
                        
                        # Save `obsm`
                        if self._obsm:
                            obsm = hdf5_file.create_group('obsm')
                            obsm.attrs['encoding-type'] = 'dict'
                            obsm.attrs['encoding-version'] = '0.1.0'
                            for key, value in self._obsm.items():
                                if isinstance(value, pl.DataFrame):
                                    SingleCell._save_h5ad_dataframe(
                                        hdf5_file, value, f'obsm/{key}',
                                        preserve_strings)
                                else:
                                    obsm.create_dataset(key, data=value)
                        
                        # Save `varm`
                        if self._varm:
                            varm = hdf5_file.create_group('varm')
                            varm.attrs['encoding-type'] = 'dict'
                            varm.attrs['encoding-version'] = '0.1.0'
                            for key, value in self._varm.items():
                                if isinstance(value, pl.DataFrame):
                                    SingleCell._save_h5ad_dataframe(
                                        hdf5_file, value, f'varm/{key}',
                                        preserve_strings)
                                else:
                                    varm.create_dataset(key, data=value)
                        
                        # Save `obsp`
                        obsp = hdf5_file.create_group('obsp')
                        obsp.attrs['encoding-type'] = 'dict'
                        obsp.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._obsp.items():
                            group = obsp.create_group(key)
                            group.attrs['encoding-type'] = 'csr_matrix' \
                                if isinstance(value, csr_array) else \
                                'csc_matrix'
                            group.attrs['encoding-version'] = '0.1.0'
                            group.attrs['shape'] = value.shape
                            group.create_dataset('data', data=value.data)
                            group.create_dataset('indices', data=value.indices)
                            group.create_dataset('indptr', data=value.indptr)
                        
                        # Save `varp`
                        varp = hdf5_file.create_group('varp')
                        varp.attrs['encoding-type'] = 'dict'
                        varp.attrs['encoding-version'] = '0.1.0'
                        for key, value in self._varp.items():
                            group = varp.create_group(key)
                            group.attrs['encoding-type'] = 'csr_matrix' \
                                if isinstance(value, csr_array) else \
                                'csc_matrix'
                            group.attrs['encoding-version'] = '0.1.0'
                            group.attrs['shape'] = value.shape
                            group.create_dataset('data', data=value.data)
                            group.create_dataset('indices', data=value.indices)
                            group.create_dataset('indptr', data=value.indptr)
                        
                        # Save `uns`
                        if self._uns:
                            SingleCell._save_uns(self._uns,
                                                 hdf5_file.create_group('uns'),
                                                 hdf5_file)
                        
                        # Save `X`
                        X = hdf5_file.create_group('X')
                        X.attrs['encoding-type'] = 'csr_matrix' \
                            if isinstance(self._X, csr_array) else 'csc_matrix'
                        X.attrs['encoding-version'] = '0.1.0'
                        X.attrs['shape'] = self._X.shape
                        X.create_dataset('data', data=self._X.data)
                        X.create_dataset('indices', data=self._X.indices)
                        X.create_dataset('indptr', data=self._X.indptr)
                    elif is_h5Seurat:
                        obs_names = self.obs_names
                        var_names = self.var_names
                        for names, names_name in (obs_names, 'obs_names'), \
                                (var_names, 'var_names'):
                            null_count = names.null_count()
                            if null_count:
                                error_message = (
                                    f'{names_name} contains {null_count:,} '
                                    f'null values, but must not contain any '
                                    f'when saving to an .h5Seurat file')
                                raise ValueError(error_message)
                        
                        # Add top-level metadata and required groups/datasets
                        hdf5_file.attrs['active.assay'] = assay
                        hdf5_file.attrs['project'] = 'SeuratProject'
                        hdf5_file.attrs['version'] = '5.0.0'
                        for required_group in 'commands', 'images', 'tools':
                            hdf5_file.create_group(required_group)
                        active_ident = hdf5_file.create_group('active.ident')
                        active_ident.create_dataset('levels', data=[b'local'])
                        active_ident.create_dataset(
                            'values', data=np.ones(len(self._obs),
                                                   dtype=np.int32))
                        hdf5_file.create_dataset('cell.names',
                                                 data=obs_names.to_numpy())
                        active_assay = \
                            hdf5_file.create_group(f'assays/{assay}')
                        active_assay.attrs['key'] = f'{assay.lower()}_'
                        active_assay.create_dataset(
                            'features', data=var_names.to_numpy())
                        active_assay.create_group('misc')
                        
                        # Save `obs` and `var`
                        SingleCell._save_h5Seurat_dataframe(
                            hdf5_file, self._obs, 'meta.data',
                            preserve_strings)
                        SingleCell._save_h5Seurat_dataframe(
                            hdf5_file, self._var,
                            f'assays/{assay}/meta.features', preserve_strings)
                        
                        # Save `obsm`
                        reductions = hdf5_file.create_group('reductions')
                        if self._obsm:
                            for key, value in self._obsm.items():
                                if isinstance(value, pl.DataFrame):
                                    continue
                                group = reductions.create_group(key)
                                group.attrs['active.assay'] = assay
                                group.attrs['global'] = np.int32(0)
                                group.attrs['key'] = f'{key.upper()}_'
                                # noinspection PyUnresolvedReferences
                                group.create_dataset('cell.embeddings',
                                                     data=value.to_numpy().T)
                        
                        # Save `obsp`
                        graphs = hdf5_file.create_group('graphs')
                        if self._obsp:
                            for key, value in self._obsp.items():
                                if isinstance(value, csc_array):
                                    value = value.tocsr()
                                group = graphs.greate_group(key)
                                group.attrs['dims'] = value.shape[::-1]
                                group.attrs['active.assay'] = assay
                                group.create_dataset('data',
                                                     data=value.data)
                                group.create_dataset('indices',
                                                     data=value.indices)
                                group.create_dataset('indptr',
                                                     data=value.indptr)
                        
                        # Save `uns`
                        misc = hdf5_file.create_group('misc')
                        if self._uns:
                            SingleCell._save_h5Seurat_uns(
                                self._uns, misc, hdf5_file)
                        
                        # Save `X`
                        X = active_assay.create_group(X_key)
                        if isinstance(X, csc_array):
                            X = value.tocsr()
                        X.attrs['dims'] = self._X.shape[::-1]
                        X.create_dataset('data', data=self._X.data)
                        X.create_dataset('indices', data=self._X.indices)
                        X.create_dataset('indptr', data=self._X.indptr)
                    else:  # `.h5`
                        obs_columns = ['barcodes']
                        var_columns = ['feature_type', 'genome', 'id', 'name']
                        for columns, df, df_name in \
                                (obs_columns, self._obs, 'obs'), \
                                (var_columns, self._var, 'var'):
                            for column in columns:
                                if column not in df:
                                    error_message = (
                                        f'{column!r} was not found in '
                                        f'{df_name}, but is a required column '
                                        f'when saving to a 10x .h5 file')
                                    raise ValueError(error_message)
                                check_dtype(df[column],
                                            f'{df_name}[{column!r}]',
                                            (pl.String, pl.Categorical,
                                             pl.Enum))
                        all_tag_keys = ['genome']
                        for column in 'pattern', 'read', 'sequence':
                            if column in self._var:
                                check_dtype(self._var[column],
                                            f'var[{column!r}]',
                                            (pl.String, pl.Categorical,
                                             pl.Enum))
                                var_columns.append(column)
                                all_tag_keys.append(column)
                        matrix = hdf5_file.create_group('matrix')
                        matrix.create_dataset('barcodes',
                                              data=self._obs[:, 0].to_numpy())
                        matrix.create_dataset('data', data=self._X.data)
                        features = matrix.create_group('features')
                        matrix.create_dataset('indices', data=self._X.indices)
                        matrix.create_dataset('indptr', data=self._X.indptr)
                        matrix.create_dataset('shape',
                                              data=self._X.shape[::-1])
                        features.create_dataset('_all_tag_keys',
                                                data=all_tag_keys)
                        for column in var_columns:
                            features.create_dataset(
                                column, data=self._var[column].to_numpy())
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
        elif is_mtx:
            barcode_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'barcodes.tsv.gz')
            feature_filename = os.path.join(
                os.path.dirname(filename_expanduser), 'features.tsv.gz')
            if not overwrite:
                for ancillary_filename in barcode_filename, feature_filename:
                    if os.path.exists(ancillary_filename):
                        error_message = (
                            f'{ancillary_filename!r} already exists; set '
                            f'overwrite=True to overwrite')
                        raise FileExistsError(error_message)
            from scipy.io import mmwrite
            try:
                mmwrite(filename_expanduser, self._X.T)
                self._obs.write_csv(barcode_filename, include_header=False)
                self._var.write_csv(feature_filename, include_header=False)
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                if os.path.exists(barcode_filename):
                    os.unlink(barcode_filename)
                if os.path.exists(feature_filename):
                    os.unlink(feature_filename)
                raise
        else:
            from ryp import r
            if preserve_strings:
                sc = self
            else:
                # Convert string columns with duplicate values to Enum
                enumify = lambda df: df.cast({
                    row[0]: pl.Enum(row[1]) for row in df
                    .select(pl.selectors.string()
                            .unique(maintain_order=True)
                            .implode()
                            .list.drop_nulls())
                    .unpivot()
                    .filter(pl.col.value.list.len() == len(df))
                    .rows()})
                sc = SingleCell(X=self._X, obs=enumify(self._obs),
                                var=enumify(self._var), obsm=self._obsm,
                                uns=self._uns, num_threads=self._num_threads)
            if sce:
                sc.to_sce('.SingleCell.object')
            else:
                sc.to_seurat('.SingleCell.object')
            try:
                r(f'saveRDS(.SingleCell.object, {filename_expanduser!r})')
            except:
                if os.path.exists(filename_expanduser):
                    os.unlink(filename_expanduser)
                raise
            finally:
                r('rm(.SingleCell.object)')
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],
                    column: SingleCellColumn,
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                            ...],
                    *,
                    QC_column: pl.Series | None = None,
                    allow_missing: bool = False,
                    allow_null: bool = False,
                    custom_error: str | None = None) -> pl.Series | None:
        """
        Get a column of the same length as `obs`/`var`, or `None` if the column
        is missing from `obs`/`var` and `allow_missing=True`.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of `obs`/`var`, a polars
                    expression that evaluates to a single column when applied
                    to `obs`/`var`, a polars Series or 1D NumPy array of the
                    same length as `obs`/`var`, or a function that takes in
                    `self` and returns a polars Series or 1D NumPy array of the
                    same length as `obs`/`var`
            variable_name: the name of the variable corresponding to `column`
            dtypes: the required dtype(s) of the column
            QC_column: an optional column of cells passing QC. If specified,
                       the presence of `null` values will only raise an error
                       for cells passing QC. Has no effect when
                       `allow_null=True`.
            allow_missing: whether to allow `column` to be a string missing
                           from `obs`/`var`, returning `None` in this case
            allow_null: whether to allow `column` to contain `null` values
            custom_error: a custom error message for when `column` is a string
                          and is not found in `obs`/`var`, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A polars Series of the same length as `obs`/`var`, or `None` if the
            column is missing from `obs`/`var` and `allow_missing=True`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if isinstance(column, str):
            variable_name = f'{variable_name} {column!r}'
            if column in obs_or_var:
                column = obs_or_var[column]
            elif allow_missing:
                return None
            else:
                error_message = \
                    f'{variable_name} is not a column of {obs_or_var_name}' \
                    if custom_error is None else \
                    custom_error.format(f'{column!r}')
                raise ValueError(error_message)
        elif isinstance(column, pl.Expr):
            column = obs_or_var.select(column)
            if column.width > 1:
                error_message = (
                    f'{variable_name} is a polars expression that expands to '
                    f'{column.width:,} columns rather than 1')
                raise ValueError(error_message)
            column = column.to_series()
        elif isinstance(column, pl.Series):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a polars Series of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        elif isinstance(column, np.ndarray):
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a NumPy array of length '
                    f'{len(column):,}, which differs from the length of '
                    f'{obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
            column = pl.Series(variable_name, column)
        elif callable(column):
            column = column(self)
            if isinstance(column, np.ndarray):
                if column.ndim != 1:
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'{column.ndim:,}D NumPy array, but must return a '
                        f'polars Series or 1D NumPy array')
                    raise ValueError(error_message)
                column = pl.Series(variable_name, column)
            elif not isinstance(column, pl.Series):
                error_message = (
                    f'{variable_name} is a function that returns a variable '
                    f'of type {type(column).__name__}, but must return a '
                    f'polars Series or 1D NumPy array')
                raise TypeError(error_message)
            if len(column) != len(obs_or_var):
                error_message = (
                    f'{variable_name} is a function that returns a column of '
                    f'length {len(column):,}, which differs from the length '
                    f'of {obs_or_var_name} ({len(obs_or_var):,})')
                raise ValueError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression, a polars Series, a 1D NumPy array, or a '
                f'function that returns a polars Series or 1D NumPy array '
                f'when applied to this SingleCell dataset, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        check_dtype(column, variable_name, dtypes)
        if not allow_null:
            if QC_column is None:
                null_count = column.null_count()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)}, but must not '
                        f'contain any')
                    raise ValueError(error_message)
            else:
                null_count = (column.is_null() & QC_column).sum()
                if null_count > 0:
                    error_message = (
                        f'{variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cells '
                        f'passing QC, but must not contain any')
                    raise ValueError(error_message)
        return column
    
    @staticmethod
    def _get_columns(obs_or_var_name: Literal['obs', 'var'],
                     datasets: Sequence[SingleCell],
                     columns: SingleCellColumn | None |
                              Sequence[SingleCellColumn | None],
                     variable_name: str,
                     dtypes: pl.datatypes.classes.DataTypeClass | str |
                             tuple[pl.datatypes.classes.DataTypeClass | str,
                                   ...],
                     *,
                     QC_columns: list[pl.Series | None] = None,
                     allow_None: bool = True,
                     allow_missing: bool = False,
                     allow_null: bool = False,
                     custom_error: str | None = None) -> \
            list[pl.Series | None]:
        """
        Get a column of the same length as `obs`/`var` from each dataset.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            datasets: a sequence of SingleCell datasets
            columns: a string naming a column of `obs`/`var`, a polars
                     expression that evaluates to a single column when applied
                     to `obs`/`var`, a polars Series or 1D NumPy array of the
                     same length as `obs`/`var`, or a function that takes in
                     `self` and returns a polars Series or 1D NumPy array of
                     the same length as `obs`/`var`. Or, a Sequence of these,
                     one per dataset in `datasets`. May also be `None` (or a
                     Sequence containing `None`) if `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the columns
            QC_columns: an optional column of cells passing QC for each
                        dataset. If not `None` for a given dataset, the
                        presence of `null` values for that dataset will only
                        raise an error for cells passing QC. Has no effect when
                        `allow_null=True`.
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_missing: whether to allow `columns` to be a string (or
                           contain strings) missing from certain datasets'
                           `obs`/`var`, returning `None` for these datasets
            allow_null: whether to allow `columns` to contain `null` values
            custom_error: a custom error message for when `column` is a string
                          and is not found in `obs`/`var`, and
                          `allow_missing=False`; use `{}` as a placeholder for
                          the name of the column
        
        Returns:
            A list of polars Series of the same length as `datasets`, where
            each Series has the same length as the corresponding dataset's
            `obs`/`var`. Or, if `columns` is `None` (or if some elements are
            `None`) or missing from `obs`/`var` (when `allow_missing=True`), a
            list of `None` (or where the corresponding elements are `None`).
        """
        if columns is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return [None] * len(datasets)
        if isinstance(columns, Sequence) and not isinstance(columns, str):
            if len(columns) != len(datasets):
                error_message = (
                    f'{variable_name} has length {len(columns):,}, but you '
                    f'specified {len(datasets):,} datasets')
                raise ValueError(error_message)
            if not allow_None and any(column is None for column in columns):
                error_message = \
                    f'{variable_name} contains an element that is None'
                raise TypeError(error_message)
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error)
                    if column is not None else None
                    for dataset, column in zip(datasets, columns)]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=column,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    if column is not None else None
                    for dataset, column, QC_column in
                    zip(datasets, columns, QC_columns)]
        else:
            if QC_columns is None:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    allow_null=allow_null, allow_missing=allow_missing,
                    custom_error=custom_error) for dataset in datasets]
            else:
                return [dataset._get_column(
                    obs_or_var_name=obs_or_var_name, column=columns,
                    variable_name=variable_name, dtypes=dtypes,
                    QC_column=QC_column, allow_null=allow_null,
                    allow_missing=allow_missing, custom_error=custom_error)
                    for dataset, QC_column in zip(datasets, QC_columns)]
    
    @staticmethod
    def _describe_column(column_name: str, column: SingleCellColumn):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
    
        Returns:
            The column's description: just the argument's name unless the value
            is a string (i.e. the column's name in `obs` or `var`), in which
            case also include the value.
        """
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
    
    # noinspection PyUnresolvedReferences
    def to_anndata(self, *, QC_column: str | None = 'passed_QC') -> 'AnnData':
        """
        Converts this SingleCell dataset to an AnnData object.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the AnnData object, set `QC_column` to `None`.
        
        Note that there is no `from_anndata()`; simply do
        `SingleCell(anndata_object)` to initialize a SingleCell dataset from an
        in-memory AnnData object.
        
        Args:
            QC_column: if not `None`, raise an error if this column is present
                       in `obs` and not all cells pass QC
        
        Returns:
            An AnnData object. For AnnData versions older than 0.11.0, which
            do not support `csr_array`/`csc_array`, counts will be converted to
            `csr_matrix`/`csc_matrix`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = \
                'X is None, so converting to an AnnData object is not possible'
            raise ValueError(error_message)
        with ignore_sigint():
            import anndata
            import pandas as pd
            import pyarrow as pa
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported by '
                        f'AnnData')
                    raise TypeError(error_message)
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the AnnData object')
                    raise ValueError(error_message)
        type_mapping = {
            pa.int8(): pd.Int8Dtype(), pa.int16(): pd.Int16Dtype(),
            pa.int32(): pd.Int32Dtype(), pa.int64(): pd.Int64Dtype(),
            pa.uint8(): pd.UInt8Dtype(), pa.uint16(): pd.UInt16Dtype(),
            pa.uint32(): pd.UInt32Dtype(), pa.uint64(): pd.UInt64Dtype(),
            pa.string(): pd.StringDtype(storage='pyarrow'),
            pa.bool_(): pd.BooleanDtype()}
        to_pandas = lambda df: df\
            .to_pandas(split_blocks=True, types_mapper=type_mapping.get)\
            .set_index(df.columns[0])
        return anndata.AnnData(
            X=sparse.csr_matrix(self._X) if isinstance(self._X, csr_array)
              else sparse.csc_matrix(self._X),
            obs=to_pandas(self._obs), var=to_pandas(self._var),
            obsm=self._obsm, varm=self._varm, uns=self._uns)
    
    # noinspection PyTypeChecker
    @staticmethod
    def _from_seurat(seurat_object_name: str,
                     *,
                     assay: str | None,
                     slot: str,
                     slot_name: str) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], dict[str, csc_array],
                  UnsDict]:
        """
        Create a SingleCell dataset from an in-memory Seurat object loaded with
        the ryp Python-R bridge. Used by `__init__()` and `from_seurat()`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to the Seurat object's
                   `active.assay` attribute (usually `'RNA'`)
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as `X`. Set to
                  `'data'` to load the normalized counts, or `'scale.data'` to
                  load the normalized and scaled counts, if available. If
                  dense, will be automatically converted to a sparse array.
            slot_name: the name of the variable passed via the `slot` argument
        
        Returns:
            A length-5 tuple of (`X`, `obs`, `var`, `obsm`, `obsp`, `uns`).
        """
        from ryp import r, to_py
        if assay is None:
            assay = to_py(f'{seurat_object_name}@active.assay')
        elif to_py(f'{seurat_object_name}@{assay}') is None:
            error_message = (
                f'assay {assay!r} does not exist in '
                f'{seurat_object_name}@assays; specify a different assay than '
                f'{assay!r}')
            raise ValueError(error_message)
        assay_slot = f'{seurat_object_name}@assays${assay}'
        
        # If Seurat v5, merge layers if necessary, and use `$slot` instead of
        # `@slot` for `X` and `meta.data` instead of `meta.features` for `var`
        v5 = to_py(f'inherits({assay_slot}, "Assay5")')
        if v5:
            r(f'.SingleCell.layers = names({assay_slot}@layers)')
            try:
                r(f'.SingleCell.layers = '
                  f'.SingleCell.layers[grep("^{slot}", .SingleCell.layers)]')
                num_matching_layers = to_py('length(.SingleCell.layers)')
                if num_matching_layers == 0:
                    error_message = (
                        f'none of the layers in {assay_slot}@layers has a '
                        f'name starting with {slot_name} {slot!r}; specify a '
                        f'different assay than {assay!r} or a different '
                        f'{slot_name} than {slot!r}')
                    raise ValueError(error_message)
                elif num_matching_layers > 1:
                    r(f'{assay_slot} = JoinLayers('
                      f'{assay_slot}, layers=.SingleCell.layers)')
            finally:
                r('rm(".SingleCell.layers")')
            gene_names = to_py(f'{assay_slot}@features[[{slot!r}]]')\
                .rename('gene')
            var = to_py(f'{assay_slot}@meta.data', index=False)\
                .select(gene_names, pl.all())
            X_slot = f'{assay_slot}@layers${slot}'
        else:
            # unlike v5 objects, v3 objects indicate the absence of a slot with
            # a 0 x 0 matrix
            if not (to_py(f'"{slot}" %in% slotNames({assay_slot})') and
                    to_py(f'prod(dim({assay_slot}@{slot}))') > 0):
                error_message = (
                    f'{slot_name} {slot!r} does not exist in {assay_slot}; '
                    f'specify a different assay than {assay!r} or a different '
                    f'{slot_name} than {slot!r}')
                raise ValueError(error_message)
            X_slot = f'{assay_slot}@{slot}'
            var = to_py(f'{assay_slot}@meta.features')
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{slot_name} {slot!r} exists in {assay_slot} but is not a '
                f'dgCMatrix (column-oriented sparse matrix) or matrix, '
                f'instead having ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; specify a different assay than {assay!r} or a different '
                f'{slot_name} than {slot!r}')
            raise TypeError(error_message)
        obs_key = f'{seurat_object_name}@meta.data'
        obs = to_py(obs_key, index='_index' if to_py(
            f'"cell" %in% {obs_key}') else 'cell')
        if var is None:
            var = to_py(f'rownames({assay_slot}@{slot})').to_frame('gene')
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        reduction_names = to_py(f'names({seurat_object_name}@reductions)')
        obsm = {reduction_name: to_py(f'{seurat_object_name}@reductions$'
                                      f'{reduction_name}@cell.embeddings',
                                      format='numpy')
                for reduction_name in reduction_names
                if not to_py(f'is.null({seurat_object_name}@reductions$'
                             f'{reduction_name})') and
                to_py(f'{seurat_object_name}@reductions${reduction_name}'
                      f'@assay.used') == assay} \
            if reduction_names is not None else {}
        graph_names = to_py(f'names({seurat_object_name}@graphs)')
        obsp = {graph_name: csc_array((
            to_py(f'{seurat_object_name}${graph_name}@x', format='numpy'),
            to_py(f'{seurat_object_name}${graph_name}@i', format='numpy'),
            to_py(f'{seurat_object_name}${graph_name}@p', format='numpy')),
            shape=to_py(f'{seurat_object_name}${graph_name}@Dim',
                        format='numpy'))
            for graph_name in graph_names
            if to_py(f'length({seurat_object_name}${graph_name}@'
                     f'assay.used)') == 0 or
               to_py(f'{seurat_object_name}${graph_name}@assay.used') == assay
        } if graph_names is not None else {}
        uns = to_py(f'{seurat_object_name}@misc')
        if not uns:
            # uns may be an empty unnamed list in R, which ryp converts to a
            # Python list rather than a dictionary
            uns = {}
        return X, obs, var, obsm, obsp, uns
    
    @staticmethod
    def from_seurat(seurat_object_name: str,
                    *,
                    assay: str | None = None,
                    slot: str = 'counts',
                    num_threads: int | np.integer | None) -> SingleCell:
        """
        Create a SingleCell dataset from a Seurat object that has already been
        loaded into memory via the ryp Python-R bridge. To load a Seurat object
        from disk, use e.g. `SingleCell('filename.rds')`.
        
        Args:
            seurat_object_name: the name of the Seurat object in the ryp R
                                workspace
            assay: the name of the assay within the Seurat object to load data
                   from; if `None`, defaults to the Seurat object's
                   `active.assay` attribute (usually `'RNA'`)
            slot: the slot within the active assay (or the assay specified by
                  the `assay` argument, if not `None`) to use as `X`. Defaults
                  to `'counts'`. Set to `'data'` to load the normalized counts,
                  or `'scale.data'` to load the normalized and scaled counts,
                  if available. If dense, will be automatically converted to a
                  sparse array.
            num_threads: the default number of threads to use for all
                         subsequent operations on this SingleCell dataset. Also
                         sets the number of threads for this SingleCell
                         dataset's count matrix, if present. Does not affect
                         the number of threads used for data loading; this will
                         always be single-threaded for Seurat objects.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import to_py
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if assay is not None:
            check_type(assay, 'assay', str, 'a string')
        check_type(slot, 'slot', str, 'a string')
        num_threads = SingleCell._process_num_threads_static(num_threads)
        if not to_py(f'inherits({seurat_object_name}, "Seurat")'):
            classes = to_py(f'class({seurat_object_name})', squeeze=False)
            error_message = (
                f'the R object named by seurat_object_name, '
                f'{seurat_object_name}, must be a Seurat object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, obsp, uns = \
            SingleCell._from_seurat(seurat_object_name, assay=assay, slot=slot,
                                    slot_name='slot')
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, obsp=obsp, uns=uns,
                          num_threads=num_threads)
    
    def to_seurat(self,
                  seurat_object_name: str,
                  *,
                  QC_column: str | None = 'passed_QC',
                  assay: str = 'RNA',
                  X_key: str = 'counts') -> None:
        """
        Convert this SingleCell dataset to a Seurat object (version 3, not
        version 5) in the R workspace of the ryp Python-R bridge.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the Seurat object, set `QC_column` to `None`.
        
        When converting to Seurat, to match the requirements of Seurat objects,
        the `'X_'` prefix (often used by Scanpy) will be removed from each key
        of `obsm` where it is present (e.g. `'X_umap'` will become `'umap'`).
        Seurat will also add `'orig.ident'`, `'nCount_RNA'` and
        `'nFeature_RNA'` as gene-level metadata by default; you can disable
        the calculation of the latter two columns with:
        
        ```python
        from ryp import r
        r('options(Seurat.object.assay.calcn = FALSE)')
        ```
        
        `varm` and DataFrame keys of `obsm` will not be converted.
        
        Args:
            seurat_object_name: the name of the R variable to assign the Seurat
                                object to
            QC_column: if not `None`, raise an error if this column is present
                       in `obs` and not all cells pass QC
            assay: the name to use for the active assay
            X_key: the name of the slot within the active assay to save `X` to;
                   must be `'counts'` or `'data'`
        """
        # Check that `X` is present
        if self._X is None:
            error_message = \
                'X is None, so converting to a Seurat object is not possible'
            raise ValueError(error_message)
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SeuratObject))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(seurat_object_name, 'seurat_object_name', str, 'a string')
        check_R_variable_name(seurat_object_name, 'seurat_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the Seurat object')
                    raise ValueError(error_message)
        check_type(assay, 'assay', str, 'a string')
        check_type(X_key, 'X_key', str, 'a string')
        if X_key != 'counts' and X_key != 'data':
            error_message = (
                f"when converting to a Seurat object, X_key must be 'counts' "
                f"or 'data', not {X_key!r}")
            raise ValueError(error_message)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a Seurat object')
                    raise TypeError(error_message)
        obs_names = self.obs_names
        var_names = self.var_names
        for names, names_name in (obs_names, 'obs_names'), \
                (var_names, 'var_names'):
            null_count = names.null_count()
            if null_count:
                error_message = (
                    f'{names_name} contains {null_count:,} null values, but '
                    f'must not contain any when converting to a Seurat object')
                raise ValueError(error_message)
        is_string = var_names.dtype == pl.String
        num_with_underscores = var_names.str.contains('_').sum() \
            if is_string else \
            var_names.cat.get_categories().str.contains('_').sum()
        if num_with_underscores:
            var_names_expression = f'pl.col.{var_names.name}' \
                if var_names.name.isidentifier() else \
                f'pl.col({var_names.name!r})'
            error_message = (
                f"var_names contains {num_with_underscores:,}"
                f"{'' if is_string else ' unique'} gene "
                f"{plural('name', num_with_underscores)} with "
                f"underscores, which are not supported by Seurat; Seurat "
                f"recommends changing the underscores to dashes, which you "
                f"can do with .with_columns_var({var_names_expression}"
                f"{'' if is_string else '.cast(pl.String)'}"
                f".str.replace_all('_', '-'))")
            raise ValueError(error_message)
        try:
            to_r(self._X.T, '.SingleCell.X.T', rownames=var_names,
                 colnames=obs_names)
            try:
                to_r(self._obs.drop(obs_names.name), '.SingleCell.obs',
                     rownames=obs_names)
                try:
                    r(f'{seurat_object_name} = CreateSeuratObject('
                      f'CreateAssayObject({X_key}=.SingleCell.X.T), '
                      f'meta.data=.SingleCell.obs, assay={assay!r})')
                    
                    # Reverse the column name-mangling introduced by Seurat
                    r(f'.SingleCell.cols_to_ignore = '
                      f'c("orig.ident", "nCount_{assay}", "nFeature_{assay}")')
                    try:
                        r(f'names({seurat_object_name}@meta.data)['
                          f'!names({seurat_object_name}@meta.data) %in% '
                          f'.SingleCell.cols_to_ignore] = '
                          f'names(.SingleCell.obs)[!names(.SingleCell.obs) '
                          f'%in% .SingleCell.cols_to_ignore]')
                    finally:
                        r('rm(.SingleCell.cols_to_ignore)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            to_r(self._var.drop(var_names.name), '.SingleCell.var',
                 rownames=var_names)
            try:
                r(f'{seurat_object_name}@assays${assay}@meta.features = '
                  f'.SingleCell.var')
            finally:
                r('rm(.SingleCell.var)')
            if self._obsm:
                for original_key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    
                    # Remove the initial X_ from the reduction name and suffix
                    # with `'_'` when creating the key, like
                    # mojaveazure.github.io/seurat-disk/reference/Convert.html
                    key = original_key.removeprefix('X_')
                    to_r(value, '.SingleCell.value', rownames=obs_names,
                         colnames=[f'{key}_{i}'
                                   for i in range(1, value.shape[1] + 1)])
                    try:
                        if value.dtype == np.int64 and \
                                to_py('class(.SingleCell.value)') == \
                                'integer64':
                            error_message = (
                                f'obsm[{original_key}] contains integers '
                                f'greater than INT32_MAX (2,147,483,647) '
                                f'or less than INT32_MIN - 1 '
                                f'(-2,147,483,648), which are not allowed '
                                f'when converting to a Seurat object')
                            raise ValueError(error_message)
                        r(f'{seurat_object_name}@reductions${key} = '
                          f'CreateDimReducObject(.SingleCell.value, '
                          f'key="{key}_", assay="{assay}")')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._obsp:
                to_r(self._obsp, '.SingleCell.obsp')
                try:
                    r(f'{seurat_object_name}@graphs = '
                      f'lapply(as.Graph, .SingleCell.obsp)')
                finally:
                    r('rm(.SingleCell.obsp)')
            if self._uns:
                to_r(self._uns, '.SingleCell.uns')
                try:
                    r(f'{seurat_object_name}@misc = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({seurat_object_name!r})'):
                r(f'rm({seurat_object_name})')
            raise
    
    @staticmethod
    def _get_DFrame(dframe: str, *, index: str) -> pl.DataFrame:
        """
        Convert a DFrame in the ryp R workspace to a Python DataFrame, raising
        an error if the DFrame contains any nested data stuctures.
        
        Args:
            dframe: the name of the DFrame object in the ryp R workspace
            index: the name of the column containing the rownames in the output
                   DataFrame; if this column name is already present in the
                   DFrame, fall back to calling the rownames column `'_index'`

        Returns:
            A polars DataFrame containing the data in `dframe`.
        """
        from ryp import to_py
        df = to_py(f'{dframe}@listData', index=False)
        if not all(isinstance(value, pl.Series) for value in df.values()):
            error_message = (
                f'{dframe} contains nested data; unnest before converting to '
                f'a SingleCell dataset')
            raise ValueError(error_message)
        if index in df.keys():
            index = '_index'
        df = pl.DataFrame({index: to_py(f'{dframe}@rownames')} | df)
        return df
    
    @staticmethod
    def _from_sce(sce_object_name: str,
                  *,
                  slot: str,
                  slot_name: str) -> \
            tuple[csr_array | csc_array, pl.DataFrame, pl.DataFrame,
                  dict[str, np.ndarray[2, Any]], UnsDict]:
        """
        Create a SingleCell dataset from an in-memory SingleCellExperiment
        object loaded with the ryp Python-R bridge. Used by `__init__()` and
        `from_sce()`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `sce_object@assays@data` to use as `X`.
                  Set to `'counts'` to load raw counts, or `'logcounts'` to
                  load the normalized counts if available. If dense, will be
                  automatically converted to a sparse array.
            slot_name: the name of the variable passed via the `slot` argument
        
        Returns:
            A length-5 tuple of (`X`, `obs`, `var`, `obsm`, `uns`).
        """
        from ryp import to_py
        X_slot = f'{sce_object_name}@assays@data${slot}'
        if not to_py(f'"{slot}" %in% names({sce_object_name}@assays@data)'):
            error_message = (
                f'{slot_name} {slot!r} does not exist in '
                f'{sce_object_name}@assays@data${slot}; specify a different '
                f'{slot_name} than {slot!r}')
            raise ValueError(error_message)
        X_classes = tuple(to_py(f'class({X_slot})', squeeze=False))
        if X_classes == ('dgCMatrix',):
            X = to_py(X_slot).T
        elif X_classes == ('matrix', 'array'):
            X = csr_array(to_py(X_slot, format='numpy').T)
        else:
            error_message = (
                f'{slot_name} {slot!r} exists in '
                f'{sce_object_name}@assays@data${slot} but is not a dgCMatrix '
                f'(column-oriented sparse matrix) or matrix, instead having ')
            if len(X_classes) == 0:
                error_message += 'no classes'
            elif len(X_classes) == 1:
                error_message += f'the class {X_classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in X_classes[:-1])} and '
                    f'{X_classes[-1]}')
            error_message += (
                f'; specify a different {slot_name} than {slot!r}')
            raise TypeError(error_message)
        obs = SingleCell._get_DFrame(f'colData({sce_object_name})',
                                     index='cell')
        var = SingleCell._get_DFrame(f'rowData({sce_object_name})',
                                     index='gene')
        obs = obs.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in obs.select(pl.col(pl.Categorical))})
        var = var.cast({column.name: pl.Enum(column.cat.get_categories())
                        for column in var.select(pl.col(pl.Categorical))})
        obsm = to_py(f'reducedDims({sce_object_name})@listData',
                     format='numpy')
        uns = to_py(f'{sce_object_name}@metadata', format='numpy')
        return X, obs, var, obsm, uns
    
    @staticmethod
    def from_sce(sce_object_name: str,
                 *,
                 slot: str = 'counts',
                 num_threads: int | np.integer | None) -> SingleCell:
        """
        Create a SingleCell dataset from a SingleCellExperiment object that has
        already been loaded into memory via the ryp Python-R bridge. To load a
        SingleCellExperiment object from disk, use e.g.
        `SingleCell('filename.rds')`.
        
        Args:
            sce_object_name: the name of the SingleCellExperiment object in the
                             ryp R workspace
            slot: the element within `{sce_object_name}@assays@data` to use as
                  `X`. Defaults to `'counts'`. If available, set to
                  `'logcounts'` to load the normalized counts. If dense, will
                  be automatically converted to a sparse array.
            num_threads: the default number of threads to use for all
                         subsequent operations on this SingleCell dataset. Also
                         sets the number of threads for this SingleCell
                         dataset's count matrix, if present. Does not affect
                         the number of threads used for data loading; this will
                         always be single-threaded for SingleCellExperiment
                         objects.
        
        Returns:
            The corresponding SingleCell dataset.
        """
        from ryp import r, to_py
        r('suppressPackageStartupMessages(library(SingleCellExperiment))')
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        check_type(slot, 'slot', str, 'a string')
        num_threads = SingleCell._process_num_threads_static(num_threads)
        if not to_py(f'inherits({sce_object_name}, "SingleCellExperiment")'):
            classes = to_py(f'class({sce_object_name})', squeeze=False)
            error_message = (
                f'the R object named by sce_object_name, {sce_object_name}, '
                f'must be a SingleCellExperiment object, but has ')
            if len(classes) == 0:
                error_message += 'no classes'
            elif len(classes) == 1:
                error_message += f'the class {classes[0]!r}'
            else:
                error_message += (
                    f'the classes '
                    f'{", ".join(f"{c!r}" for c in classes[:-1])} and '
                    f'{classes[-1]!r}')
            raise TypeError(error_message)
        X, obs, var, obsm, uns = \
            SingleCell._from_sce(sce_object_name, slot=slot, slot_name='slot')
        return SingleCell(X=X, obs=obs, var=var, obsm=obsm, uns=uns,
                          num_threads=num_threads)
        
    def to_sce(self,
               sce_object_name: str,
               *,
               QC_column: str | None = 'passed_QC',
               X_key: str = 'counts') -> None:
        """
        Convert this SingleCell dataset to a SingleCellExperiment object in the
        R workspace of the ryp Python-R bridge.
        
        Make sure to remove cells failing QC with `filter_obs(QC_column)`
        first, or specify `subset=True` in `qc()`. Alternatively, to include
        cells failing QC in the SingleCellExperiment object, set `QC_column` to
        `None`.
        
        `varm` and DataFrame keys of `obsm` will not be converted.
        
        Args:
            sce_object_name: the name of the R variable to assign the
                             SingleCellExperiment object to
            QC_column: if not `None`, raise an error if this column is present
                       in `obs` and not all cells pass QC
            X_key: the name of the slot within `@assays@data` to save `X` to
        """
        # Check that `X` is present
        if self._X is None:
            error_message = (
                'X is None, so converting to a SingleCellExperiment object is '
                'not possible')
            raise ValueError(error_message)
        from ryp import r, to_py, to_r
        r('suppressPackageStartupMessages(library(SingleCellExperiment))')
        if self._X.nnz > 2_147_483_647:
            error_message = (
                f'X has {self._X.nnz:,} non-zero elements, more than '
                f'INT32_MAX (2,147,483,647), the maximum supported in R')
            raise ValueError(error_message)
        check_type(sce_object_name, 'sce_object_name', str, 'a string')
        check_R_variable_name(sce_object_name, 'sce_object_name')
        if QC_column is not None:
            check_type(QC_column, 'QC_column', str, 'a string')
            if QC_column in self._obs:
                QCed_cells = self._obs[QC_column]
                check_dtype(QCed_cells, f'obs[{QC_column!r}]',
                            pl.Boolean)
                if QCed_cells.null_count() or not QCed_cells.all():
                    error_message = (
                        f'not all cells pass QC; remove cells failing QC with '
                        f'filter_obs({QC_column!r}) or by specifying '
                        f'subset=True in qc(), or set QC_column=None to '
                        f'include them in the SingleCellExperiment object')
                    raise ValueError(error_message)
        check_type(X_key, 'X_key', str, 'a string')
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        for df, df_name in (self._obs, 'obs'), (self._var, 'var'):
            for column, dtype in df.schema.items():
                if dtype.base_type() not in valid_dtypes:
                    error_message = (
                        f'{df_name}[{column!r}] has the data type '
                        f'{dtype.base_type()!r}, which is not supported when '
                        f'converting to a SingleCellExperiment object')
                    raise TypeError(error_message)
        try:
            obs_names = self.obs_names
            var_names = self.var_names
            for names, names_name in (obs_names, 'obs_names'), \
                    (var_names, 'var_names'):
                null_count = names.null_count()
                if null_count:
                    error_message = (
                        f'{names_name} contains {null_count:,} null values, '
                        f'but must not contain any when converting to a '
                        f'SingleCellExperiment object')
                    raise ValueError(error_message)
            to_r(self._X.T, '.SingleCell.X.T', rownames=var_names,
                 colnames=obs_names)
            try:
                to_r(self._obs.drop(obs_names.name), '.SingleCell.obs',
                     rownames=obs_names)
                try:
                    to_r(self._var.drop(var_names.name),
                         '.SingleCell.var', rownames=var_names)
                    try:
                        r(f'{sce_object_name} = SingleCellExperiment('
                          f'assays = list({X_key} = .SingleCell.X.T), '
                          f'colData = S4Vectors::DataFrame('
                          f'    .SingleCell.obs, check.names=FALSE), '
                          f'rowData = S4Vectors::DataFrame('
                          f'    .SingleCell.var, check.names=FALSE))')
                    finally:
                        r('rm(.SingleCell.var)')
                finally:
                    r('rm(.SingleCell.obs)')
            finally:
                r('rm(.SingleCell.X.T)')
            if self._obsm:
                for key, value in self._obsm.items():
                    if isinstance(value, pl.DataFrame):
                        continue
                    to_r(value, '.SingleCell.value', rownames=obs_names,
                             colnames=[f'{key}_{i}'
                                       for i in range(1, value.shape[1] + 1)])
                    try:
                        r(f'reducedDim({sce_object_name}, {key!r}) = '
                          f'.SingleCell.value')
                    finally:
                        r('rm(.SingleCell.value)')
            if self._uns:
                to_r(self._uns, '.SingleCell.uns')
                try:
                    r(f'{sce_object_name}@metadata = .SingleCell.uns')
                finally:
                    r('rm(.SingleCell.uns)')
        except:
            if to_py(f'exists({sce_object_name!r})'):
                r(f'rm({sce_object_name})')
            raise
    
    # noinspection PyTypeChecker
    def copy(self,
             *,
             deep: bool = False,
             num_threads: int | np.integer | None) -> SingleCell:
        """
        Make a copy of this SingleCell dataset.
        
        Args:
            deep: whether to perform a deep or shallow copy. Since polars
                  DataFrames are immutable, `obs` and `var` will always point
                  to the same underlying data as the original. The difference
                  when `deep=True` is that `X` and any NumPy arrays in `obsm`,
                  `varm`, `obsp`, `varp`, and `uns` will point to fresh copies
                  of the underlying data, instead of the same data as the
                  original SingleCell dataset. When `deep=False`, any
                  modifications to these NumPy arrays will modify both copies!
            num_threads: the number of threads to use when making a deep copy
                         of `X`. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Does not affect the copied SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`. Can only be
                         specified when `deep=True` and `X` is not `None`.

        Returns:
            A copy of the SingleCell dataset.
        """
        check_type(deep, 'deep', bool, 'Boolean')
        if deep:
            num_threads = self._process_num_threads(num_threads)
            if self._X is not None:
                original_num_threads = self._X._num_threads
                try:
                    self._X._num_threads = num_threads
                    X = self._X.copy()
                finally:
                    self._X._num_threads = original_num_threads
            else:
                if num_threads is not None:
                    error_message = \
                        'num_threads can only be specified when X is not None'
                    raise ValueError(error_message)
                X = None
            obsm = {key: value if isinstance(value, pl.DataFrame) else
                         value.copy() for key, value in self._obsm.items()}
            varm = {key: value if isinstance(value, pl.DataFrame) else
                         value.copy() for key, value in self._varm.items()}
            obsp = {key: value.copy() for key, value in self._obsp.items()}
            varp = {key: value.copy() for key, value in self._varp.items()}
            uns = SingleCell._copy_uns(self._uns, deep=True)
        else:
            if num_threads is not None:
                error_message = \
                    'num_threads can only be specified when deep=True'
                raise ValueError(error_message)
            X = self._X
            obsm = self._obsm.copy()
            varm = self._varm.copy()
            obsp = self._obsp.copy()
            varp = self._varp.copy()
            uns = SingleCell._copy_uns(self._uns)
        return SingleCell(X=X, obs=self._obs, var=self._var, obsm=obsm,
                          varm=varm, obsp=obsp, varp=varp, uns=uns,
                          num_threads=self._num_threads)
    
    def concat_obs(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False,
                   num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Concatenate one or more other SingleCell datasets with this one,
        cell-wise. All datasets must have distinct `obs_names`.
        
        By default, all datasets must have the same `var`, `varm`, `varp`, and
        `uns`. They must also have the same columns in `obs` and the same keys
        in `obsm`, with the same data types. `obsp` will be discarded during
        the concatenation.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of `var`, i.e. the `var_names`) before
        concatenating. Subset to columns of `var` and keys of `varm`, `varp`,
        and `uns` that are identical in all datasets after this subsetting.
        Also, subset to columns of `obs` and keys of `obsm` that are present in
        all datasets, and have the same data types. All datasets' `obs_names`
        must have the same name and data type, and similarly for their
        `var_names`.
        
        The one exception to the `obs` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `obs`.
        
        If the datasets' `X` are a mix of CSR and CSC sparse arrays, they will
        all be coerced to CSR.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes, columns of `obs` and `var`,
                      and keys of `obsm`, `varm` and `uns` common to all
                      datasets before concatenating, rather than raising an
                      error on any mismatches
            num_threads: the number of threads to use when concatenating. Does
                         not affect the concatenated SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         first dataset's `num_threads`.
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        if self._X is not None:
            if all(dataset._X is not None for dataset in datasets):
                X_present = True
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        else:
            if all(dataset._X is None for dataset in datasets):
                X_present = False
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that `obs_names` and `var_names` have the same name and
            # data type across all datasets
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            
            # Subset to genes in common across all datasets
            genes_in_common = self.var_names\
                .filter(self.var_names
                        .is_in(pl.concat([dataset.var_names
                                          for dataset in datasets[1:]])))
            if len(genes_in_common) == 0:
                error_message = \
                    'no genes are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[:, genes_in_common] for dataset in datasets]
            
            # Subset to columns of `var` and keys of `varm`, `varp`, and `uns`
            # that are identical in all datasets after this subsetting
            var_columns_in_common = [
                column.name for column in datasets[0]._var[:, 1:]
                if all(column.name in dataset._var and
                       dataset._var[column.name].equals(column)
                       for dataset in datasets[1:])]
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in self._varm.items()
                if all(key in dataset._varm and
                       type(value) is type(dataset._varm[key]) and
                       (dataset._varm[key].dtype == value.dtype and
                        array_equal(dataset._varm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].equals(value))
                       for dataset in datasets[1:])]
            varp_keys_in_common = [
                key for key, value in self._varp.items()
                if all(key in dataset._varp and
                       dataset._varp[key].dtype == value.dtype and
                       sparse_equal(dataset._varp[key], value)
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and dataset._uns[key] == value
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._var = dataset._var.select(dataset.var_names,
                                                   *var_columns_in_common)
                dataset._varm = {key: dataset._varm[key]
                                 for key in varm_keys_in_common}
                dataset._varp = {key: dataset._varp[key]
                                 for key in varp_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            
            # Subset to columns of `obs` and keys of `obsm` that are present in
            # all datasets, and have the same data types. Also include columns
            # of `obs` that are Enum in some datasets and Categorical in
            # others, or Enum in all datasets but with different categories in
            # each dataset; cast these to Categorical.
            obs_mismatched_categoricals = {
                column for column, dtype in self._obs[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._obs and
                       dataset._obs[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._obs[column].dtype == dtype
                           for dataset in datasets[1:])}
            obs_columns_in_common = [
                column
                for column, dtype in islice(self._obs.schema.items(), 1, None)
                if column in obs_mismatched_categoricals or
                   all(column in dataset._obs and
                       dataset._obs[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._obs[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in obs_mismatched_categoricals}
            for dataset in datasets:
                # noinspection PyUnresolvedReferences
                dataset._obs = dataset._obs\
                    .select(dataset.obs_names, *obs_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in self._obsm.items()
                if all(key in dataset._obsm and
                       type(dataset._obsm[key]) is type(value) and
                       (dataset._obsm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obsm = {key: dataset._obsm[key]
                                 for key in obsm_keys_in_common}
        else:  # non-flexible
            # Check that all `var`, `varm`, `varp`, and `uns` are identical
            var = self._var
            for dataset in datasets[1:]:
                if not dataset._var.equals(var):
                    error_message = (
                        'all SingleCell datasets must have the same var, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            varm = self._varm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._varm.keys() != varm.keys() or \
                        any(type(dataset._varm[key]) is not type(value) or
                            (dataset._varm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._varm[key].schema != value.schema)
                            for key, value in varm.items()) or not \
                        all(array_equal(dataset._varm[key], value)
                            for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same varm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            varp = self._varp
            for dataset in datasets[1:]:
                if dataset._varp.keys() != varp.keys() or \
                        any(type(dataset._varp[key]) is not type(value) or
                            dataset._varp[key].dtype != value.dtype
                            for key, value in varp.items()) or not \
                        all(sparse_equal(dataset._varp[key], value)
                            for key, value in varp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same varp, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            
            # Check that all `obs` have the same columns and data types
            schema = self._obs.schema
            for dataset in datasets[1:]:
                if dataset._obs.schema != schema:
                    if dataset._obs.columns != self._obs.columns:
                        error_message = (
                            'all SingleCell datasets must have the same '
                            'columns in obs, unless flexible=True')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            'all SingleCell datasets must have the same data '
                            'type for each column of obs, unless '
                            'flexible=True')
                        raise TypeError(error_message)
            
            # Check that all `obsm` have the same keys and data types
            obsm = self._obsm
            for dataset in datasets[1:]:
                if dataset._obsm.keys() != obsm.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'obsm, unless flexible=True')
                    raise ValueError(error_message)
                # noinspection PyUnresolvedReferences
                if any(type(dataset._obsm[key]) is not type(value) or
                       (dataset._obsm[key].dtype != value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].schema != value.schema)
                       for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data '
                        'type for each key in obsm, unless flexible=True')
                    raise TypeError(error_message)
        
        # Concatenate; output should be CSR when there's a mix of inputs
        obs = pl.concat([dataset._obs for dataset in datasets])
        num_unique = obs[:, 0].n_unique()
        if num_unique < len(obs):
            error_message = (
                f'obs_names contains {len(obs) - num_unique:,} duplicates '
                f'after concatenation')
            raise ValueError(error_message)
        if X_present:
            if all(isinstance(dataset._X, csr_array) for dataset in datasets):
                X = sparse_major_stack([dataset._X for dataset in datasets],
                                       num_threads=num_threads)
            elif all(isinstance(dataset._X, csc_array)
                     for dataset in datasets):
                X = sparse_minor_stack([dataset._X for dataset in datasets],
                                       num_threads=num_threads)
            else:
                X = sparse_major_stack([dataset._X.tocsr()
                                        if isinstance(dataset._X, csc_array)
                                        else dataset._X
                                        for dataset in datasets],
                                       num_threads=num_threads)
        else:
            X = None
        # noinspection PyTypeChecker
        obsm = {key: concatenate([dataset._obsm[key] for dataset in datasets],
                                 num_threads=num_threads)
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._obsm[key] for dataset in datasets])
                for key, value in datasets[0]._obsm.items()}
        return SingleCell(X=X, obs=obs, var=datasets[0]._var, obsm=obsm,
                          varm=datasets[0]._varm, varp=datasets[0]._varp,
                          uns=datasets[0]._uns,
                          num_threads=datasets[0]._num_threads)

    def concat_var(self,
                   datasets: SingleCell | Iterable[SingleCell],
                   *more_datasets: SingleCell,
                   flexible: bool = False,
                   num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Concatenate one or more other SingleCell datasets with this one,
        gene-wise. This is much less common than the cell-wise concatenation
        provided by `concat_obs()`. All datasets must have distinct
        `var_names`.
        
        By default, all datasets must have the same `obs`, `obsm`, `obsp`, and
        `uns`. They must also have the same columns in `var` and the same keys
        in `varm`, with the same data types. `varp` will be discarded during
        the concatenation.
        
        Conversely, if `flexible=True`, subset to cells present in all datasets
        (according to the first column of `obs`, i.e. the `obs_names`) before
        concatenating. Subset to columns of `obs` and keys of `obsm`, `obsp`,
        and `uns` that are identical in all datasets after this subsetting.
        Also, subset to columns of `var` and keys of `varm` that are present in
        all datasets, and have the same data types. All datasets' `obs_names`
        must have the same name and data type, and similarly for their
        `var_names`.
        
        The one exception to the `var` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `var`.
        
        If the datasets' `X` are a mix of CSR and CSC sparse arrays, they will
        all be coerced to CSR.
        
        Args:
            datasets: one or more SingleCell datasets to concatenate with this
                      one
            *more_datasets: additional SingleCell datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to cells, columns of `obs` and `var`,
                      and keys of `obsm`, `varm` and `uns` common to all
                      datasets before concatenating, rather than raising an
                      error on any mismatches
            num_threads: the number of threads to use when concatenating. Does
                         not affect the concatenated SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         first dataset's `num_threads`.
        
        Returns:
            The concatenated SingleCell dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other SingleCell dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', SingleCell,
                    'SingleCell datasets')
        if self._X is not None:
            if all(dataset._X is not None for dataset in datasets):
                X_present = True
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        else:
            if all(dataset._X is None for dataset in datasets):
                X_present = False
            else:
                error_message = (
                    'some datasets being concatenated have X missing, while '
                    'others do not')
                raise ValueError(error_message)
        check_type(flexible, 'flexible', bool, 'Boolean')
        # Perform either flexible or non-flexible concatenation
        if flexible:
            # Check that `obs_names` and `var_names` have the same name and
            # data type across all datasets
            obs_names_name = self.obs_names.name
            if not all(dataset.obs_names.name == obs_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of obs (the obs_names column)')
                raise ValueError(error_message)
            var_names_name = self.var_names.name
            if not all(dataset.var_names.name == var_names_name
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same name for the '
                    'first column of var (the var_names column)')
                raise ValueError(error_message)
            obs_names_dtype = self.obs_names.dtype
            if not all(dataset.obs_names.dtype == obs_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of obs (the obs_names column)')
                raise TypeError(error_message)
            var_names_dtype = self.var_names.dtype
            if not all(dataset.var_names.dtype == var_names_dtype
                       for dataset in datasets[1:]):
                error_message = (
                    'not all SingleCell datasets have the same data type for '
                    'the first column of var (the var_names column)')
                raise TypeError(error_message)
            
            # Subset to cells in common across all datasets
            cells_in_common = self.obs_names\
                .filter(self.obs_names
                        .is_in(pl.concat([dataset.obs_names
                                          for dataset in datasets[1:]])))
            if len(cells_in_common) == 0:
                error_message = \
                    'no cells are shared across all SingleCell datasets'
                raise ValueError(error_message)
            datasets = [dataset[cells_in_common] for dataset in datasets]
            
            # Subset to columns of `obs` and keys of `obsm`, `obsp`, and `uns`
            # that are identical in all datasets after this subsetting
            obs_columns_in_common = [
                column.name for column in datasets[0]._obs[:, 1:]
                if all(column.name in dataset._obs and
                       dataset._obs[column.name].equals(column)
                       for dataset in datasets[1:])]
            # noinspection PyUnresolvedReferences
            obsm_keys_in_common = [
                key for key, value in self._obsm.items()
                if all(key in dataset._obsm and
                       type(value) is type(dataset._obsm[key]) and
                       (dataset._obsm[key].dtype == value.dtype and
                        array_equal(dataset._obsm[key], value)
                        if isinstance(value, np.ndarray) else
                        dataset._obsm[key].equals(value))
                       for dataset in datasets[1:])]
            obsp_keys_in_common = [
                key for key, value in self._obsp.items()
                if all(key in dataset._obsp and
                       dataset._obsp[key].dtype == value.dtype and
                       sparse_equal(dataset._obsp[key], value)
                       for dataset in datasets[1:])]
            # noinspection PyTypeChecker,PyUnresolvedReferences
            uns_keys_in_common = [
                key for key, value in self._uns.items()
                if isinstance(value, dict) and
                   all(isinstance(dataset._uns[key], dict) and
                       SingleCell._eq_uns(value, dataset._uns[key],
                                          different_order_ok=True)
                       for dataset in datasets[1:]) or
                   isinstance(value, np.ndarray) and
                   all(isinstance(dataset._uns[key], np.ndarray) and
                       array_equal(dataset._uns[key], value)
                       for dataset in datasets[1:]) or
                   all(not isinstance(dataset._uns[key], (dict, np.ndarray))
                       and dataset._uns[key] == value
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._obs = dataset._obs.select(dataset.obs_names,
                                                   *obs_columns_in_common)
                dataset._obsm = {key: dataset._obsm[key]
                                 for key in obsm_keys_in_common}
                dataset._obsp = {key: dataset._obsp[key]
                                 for key in obsp_keys_in_common}
                dataset._uns = {key: dataset._uns[key]
                                for key in uns_keys_in_common}
            
            # Subset to columns of `var` and keys of `varm` that are present in
            # all datasets, and have the same data types. Also include columns
            # of `var` that are Enum in some datasets and Categorical in
            # others, or Enum in all datasets but with different categories in
            # each dataset; cast these to Categorical.
            var_mismatched_categoricals = {
                column for column, dtype in self._var[:, 1:]
                .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                if all(column in dataset._var and
                       dataset._var[column].dtype in (pl.Categorical, pl.Enum)
                       for dataset in datasets[1:]) and
                   not all(dataset._var[column].dtype == dtype
                           for dataset in datasets[1:])}
            var_columns_in_common = [
                column
                for column, dtype in islice(self._var.schema.items(), 1, None)
                if column in var_mismatched_categoricals or
                   all(column in dataset._var and
                       dataset._var[column].dtype == dtype
                       for dataset in datasets[1:])]
            cast_dict = {column: pl.Enum(
                pl.concat([dataset._var[column].cat.get_categories()
                           for dataset in datasets])
                .unique(maintain_order=True))
                for column in var_mismatched_categoricals}
            for dataset in datasets:
                # noinspection PyUnresolvedReferences
                dataset._var = dataset._var\
                    .select(dataset.var_names, *var_columns_in_common)\
                    .cast(cast_dict)
            # noinspection PyUnresolvedReferences
            varm_keys_in_common = [
                key for key, value in self._varm.items()
                if all(key in dataset._varm and
                       type(dataset._varm[key]) is type(value) and
                       (dataset._varm[key].dtype == value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema == value.schema)
                       for dataset in datasets[1:])]
            for dataset in datasets:
                dataset._varm = {key: dataset._varm[key]
                                 for key in varm_keys_in_common}
        else:  # non-flexible
            # Check that all `obs`, `obsm`, `obsp`, and `uns` are identical
            obs = self._obs
            for dataset in datasets[1:]:
                if not dataset._obs.equals(obs):
                    error_message = (
                        'all SingleCell datasets must have the same obs, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            obsm = self._obsm
            for dataset in datasets[1:]:
                # noinspection PyUnresolvedReferences
                if dataset._obsm.keys() != obsm.keys() or \
                        any(type(dataset._obsm[key]) is not type(value) or
                            (dataset._obsm[key].dtype != value.dtype
                             if isinstance(value, np.ndarray) else
                             dataset._obsm[key].schema != value.schema)
                            for key, value in obsm.items()) or not \
                        all(array_equal(dataset._obsm[key], value)
                            for key, value in obsm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same obsm, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            obsp = self._obsp
            for dataset in datasets[1:]:
                if dataset._obsp.keys() != obsp.keys() or \
                        any(type(dataset._obsp[key]) is not type(value) or
                            dataset._obsp[key].dtype != value.dtype
                            for key, value in obsp.items()) or not \
                        all(sparse_equal(dataset._obsp[key], value)
                            for key, value in obsp.items()):
                    error_message = (
                        'all SingleCell datasets must have the same obsp, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            for dataset in datasets[1:]:
                if not SingleCell._eq_uns(self._uns, dataset._uns):
                    error_message = (
                        'all SingleCell datasets must have the same uns, '
                        'unless flexible=True')
                    raise ValueError(error_message)
            
            # Check that all `var` have the same columns and data types
            schema = self._var.schema
            for dataset in datasets[1:]:
                if dataset._var.schema != schema:
                    if dataset._var.columns != self._var.columns:
                        error_message = (
                            'all SingleCell datasets must have the same '
                            'columns in var, unless flexible=True')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            'all SingleCell datasets must have the same data '
                            'type for each column of var, unless '
                            'flexible=True')
                        raise TypeError(error_message)
            
            # Check that all `varm` have the same keys and data types
            varm = self._varm
            for dataset in datasets[1:]:
                if dataset._varm.keys() != varm.keys():
                    error_message = (
                        'all SingleCell datasets must have the same keys in '
                        'varm, unless flexible=True')
                    raise ValueError(error_message)
                # noinspection PyUnresolvedReferences
                if any(type(dataset._varm[key]) is not type(value) or
                       (dataset._varm[key].dtype != value.dtype
                        if isinstance(value, np.ndarray) else
                        dataset._varm[key].schema != value.schema)
                       for key, value in varm.items()):
                    error_message = (
                        'all SingleCell datasets must have the same data '
                        'type for each key in varm, unless flexible=True')
                    raise TypeError(error_message)
        
        # Concatenate; output should be CSR when there's a mix of inputs
        var = pl.concat([dataset._var for dataset in datasets])
        num_unique = var[:, 0].n_unique()
        if num_unique != len(var):
            error_message = (
                f'var_names contains {len(var) - num_unique:,} duplicates '
                f'after concatenation')
            raise ValueError(error_message)
        if X_present:
            if all(isinstance(dataset._X, csr_array) for dataset in datasets):
                X = sparse_minor_stack([dataset._X for dataset in datasets],
                                       num_threads=num_threads)
            elif all(isinstance(dataset._X, csc_array)
                     for dataset in datasets):
                X = sparse_major_stack([dataset._X for dataset in datasets],
                                       num_threads=num_threads)
            else:
                X = sparse_minor_stack([dataset._X.tocsr()
                                        if isinstance(dataset._X, csc_array)
                                        else dataset._X
                                        for dataset in datasets],
                                       num_threads=num_threads)
        else:
            X = None
        # noinspection PyTypeChecker
        varm = {key: concatenate([dataset._varm[key] for dataset in datasets],
                                 num_threads=num_threads)
                if isinstance(value, np.ndarray) else
                pl.concat([dataset._varm[key] for dataset in datasets])
                for key, value in datasets[0]._varm.items()}
        return SingleCell(X=X, obs=datasets[0]._obs, var=var,
                          obsm=datasets[0]._obsm, varm=varm,
                          obsp=datasets[0]._obsp, uns=datasets[0]._uns,
                          num_threads=datasets[0]._num_threads)

    def split_by_obs(self,
                     column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     sort: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            Iterable[tuple[str, SingleCell]]:
        """
        The opposite of `concat_obs()`: splits a SingleCell dataset into a
        tuple of SingleCell datasets, one per unique value of a column of
        `obs`.

        Args:
            column: a String, Enum, or Categorical column of `obs` to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain `null` entries: the corresponding cells
                    will not be included in the result.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will not be
                       selected when splitting.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
            num_threads: the number of threads to use when splitting `X`. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Can only be specified when `X` is not `None`.
        
        Yields:
            For each unique value of `column`, a tuple of the value and a
            SingleCell dataset subset to cells where `column` has that value.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        column = self._get_column('obs', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  QC_column=QC_column, allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        num_threads = self._process_num_threads(num_threads)
        values = column.value_counts(sort=True).to_series().drop_nulls() \
            if sort else column.unique(maintain_order=True)
        if QC_column is None:
            for value in values:
                yield value, self.filter_obs(column == value,
                                             num_threads=num_threads)
        else:
            for value in values:
                yield value, self.filter_obs(column.eq(value) & QC_column,
                                             num_threads=num_threads)
    
    def split_by_var(self,
                     column: SingleCellColumn,
                     *,
                     sort: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            Iterable[str, SingleCell]:
        """
        The opposite of `concat_var()`: splits a SingleCell dataset into a
        tuple of SingleCell datasets, one per unique value of a column of
        `var`.

        Args:
            column: a String, Enum, or Categorical column of `var` to split by.
                    Can be a column name, a polars expression, a polars Series,
                    a 1D NumPy array, or a function that takes in this
                    SingleCell dataset and returns a polars Series or 1D NumPy
                    array. Can contain `null` entries: the corresponding genes
                    will not be included in the result.
            sort: if `True`, sort the SingleCell datasets in the returned tuple
                  in decreasing size. If `False`, sort in order of each value's
                  first appearance in `column`.
            num_threads: the number of threads to use when splitting `X`. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Can only be specified when `X` is not `None`.
        
        Yields:
            For each unique value of `column`, a tuple of the value and a
            SingleCell dataset subset to genes where `column` has that value.
        """
        column = self._get_column('var', column, 'column',
                                  (pl.String, pl.Categorical, pl.Enum),
                                  allow_null=True)
        check_type(sort, 'sort', pl.Boolean, 'Boolean')
        num_threads = self._process_num_threads(num_threads)
        values = column.value_counts(sort=True).to_series().drop_nulls() \
            if sort else column.unique(maintain_order=True)
        for value in values:
            # noinspection PyTypeChecker
            yield value, self.filter_var(column == value,
                                         num_threads=num_threads)
    
    # noinspection PyTypeChecker
    def tocsr(self,
              *,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting `X` to a
        `csr_array`. Raise an error if `X` is already a `csr_array`.
        
        Args:
            num_threads: the number of threads to use when converting to CSR.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Does not affect the copied SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A copy of this SingleCell dataset, with `X` as a `csr_array`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so converting to CSR is not possible'
            raise ValueError(error_message)
        if isinstance(self._X, csr_array):
            error_message = 'X is already a csr_array'
            raise TypeError(error_message)
        num_threads = self._process_num_threads(num_threads)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X.tocsr(), obs=self._obs, var=self._var,
                              obsm=self._obsm, varm=self._varm,
                              obsp=self._obsp, varp=self._varp, uns=self._uns,
                              num_threads=self._num_threads)
        finally:
            self._X._num_threads = original_num_threads
    
    # noinspection PyTypeChecker
    def tocsc(self,
              *,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Make a copy of this SingleCell dataset, converting `X` to a csc_array.
        Raise an error if `X` is already a `csc_array`.
        
        This function is provided for completeness, but `csr_array` is a far
        better format than `csc_array` for cell-wise operations like
        pseudobulking, so using `tocsc()` is rarely advisable.
        
        Args:
            num_threads: the number of threads to use when converting to CSC.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Does not affect the copied SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A copy of this SingleCell dataset, with `X` as a `csc_array`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so converting to CSC is not possible'
            raise ValueError(error_message)
        if isinstance(self._X, csc_array):
            error_message = 'X is already a csc_array'
            raise TypeError(error_message)
        num_threads = self._process_num_threads(num_threads)
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            return SingleCell(X=self._X.tocsc(), obs=self._obs, var=self._var,
                              obsm=self._obsm, varm=self._varm,
                              obsp=self._obsp, varp=self._varp, uns=self._uns,
                              num_threads=self._num_threads)
        finally:
            self._X._num_threads = original_num_threads
    
    # noinspection PyTypeChecker
    def filter_obs(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both
        `obs`/`obsm` and `X`.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering `X`. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Does not affect the filtered SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`. Can only be
                         specified when `X` is not `None`.
            **constraints: column filters: `name=value` filters to cells
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to cells passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        obs = self._obs\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        indices = obs['_SingleCell_index'].to_numpy()
        if self._X is None:
            if num_threads is not None:
                error_message = \
                    'num_threads can only be specified when deep=True'
                raise ValueError(error_message)
            return SingleCell(X=None,
                              obs=obs.drop('_SingleCell_index'), var=self._var,
                              obsm={key: value[indices]
                                    for key, value in self._obsm.items()},
                              varm=self._varm,
                              obsp={key: value[np.ix_(indices, indices)]
                                    for key, value in self._obsp.items()},
                              varp=self._varp, uns=self._uns,
                              num_threads=self._num_threads)
        else:
            num_threads = self._process_num_threads(num_threads)
            original_num_threads = self._X._num_threads
            try:
                self._X._num_threads = num_threads
                return SingleCell(X=self._X[indices],
                                  obs=obs.drop('_SingleCell_index'),
                                  var=self._var,
                                  obsm={key: value[indices]
                                        for key, value in self._obsm.items()},
                                  varm=self._varm,
                                  obsp={key: value[np.ix_(indices, indices)]
                                        for key, value in self._obsp.items()},
                                  varp=self._varp, uns=self._uns,
                                  num_threads=self._num_threads)
            finally:
                self._X._num_threads = original_num_threads
    
    # noinspection PyTypeChecker
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   num_threads: int | np.integer | None = None,
                   **constraints: Any) -> SingleCell:
        """
        Equivalent to `df.filter()` from polars, but applied to both
        `var`/`varm` and `X`.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            num_threads: the number of threads to use when filtering `X`. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Does not affect the filtered SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`. Can only be
                         specified when `X` is not `None`.
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new SingleCell dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        var = self._var\
            .with_columns(_SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32))\
            .filter(*predicates, **constraints)
        indices = var['_SingleCell_index'].to_numpy()
        if self._X is None:
            if num_threads is not None:
                error_message = \
                    'num_threads can only be specified when deep=True'
                raise ValueError(error_message)
            return SingleCell(X=None,
                              obs=self._obs, var=var.drop('_SingleCell_index'),
                              obsm=self._obsm,
                              varm={key: value[indices]
                                    for key, value in self._varm.items()},
                              obsp=self._obsp,
                              varp={key: value[np.ix_(indices, indices)]
                                    for key, value in self._varp.items()},
                              uns=self._uns, num_threads=self._num_threads)
        else:
            num_threads = self._process_num_threads(num_threads)
            original_num_threads = self._X._num_threads
            try:
                self._X._num_threads = num_threads
                return SingleCell(X=self._X[:, indices],
                                  obs=self._obs,
                                  var=var.drop('_SingleCell_index'),
                                  obsm=self._obsm,
                                  varm={key: value[indices]
                                        for key, value in self._varm.items()},
                                  obsp=self._obsp,
                                  varp={key: value[np.ix_(indices, indices)]
                                        for key, value in self._varp.items()},
                                  uns=self._uns, num_threads=self._num_threads)
            finally:
                self._X._num_threads = original_num_threads
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to `obs`.
        `obs_names` will be automatically included as the first column, if not
        included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.select(*exprs, **named_exprs)`, and `obs_names` as the
            first column unless already included explicitly.
        """
        obs = self._obs.select(*exprs, **named_exprs)
        if self.obs_names.name in obs:
            error_message = (
                f'one of the selected columns is the obs_names, '
                f'{self.obs_names.name!r}, but the obs_names will always be '
                f'selected automatically as the first column and thus should '
                f'not be specified explicitly')
            raise ValueError(error_message)
        obs = obs.select(self.obs_names, pl.all())
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> SingleCell:
        """
        Equivalent to `df.select()` from polars, but applied to `var`.
        `var_names` will be automatically included as the first column, if not
        included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.select(*exprs, **named_exprs)`, and `var_names` as the
            first column unless already included explicitly.
        """
        var = self._var.select(*exprs, **named_exprs)
        if self.var_names.name in var:
            error_message = (
                f'one of the selected columns is the var_names, '
                f'{self.var_names.name!r}, but the var_names will always be '
                f'selected automatically as the first column and thus should '
                f'not be specified explicitly')
            raise ValueError(error_message)
        var = var.select(self.var_names, pl.all())
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)

    def select_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `obsm` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `obsm` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key in keys},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def select_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `varm` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `varm` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to select {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key in keys},
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def select_obsp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `obsp` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `obsp` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsp:
                error_message = \
                    f'tried to select {key!r}, which is not a key of obsp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp={key: value for key, value in self._obsp.items()
                                if key in keys},
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
        
    def select_varp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `varp` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `varp` subset to the specified
            key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varp:
                error_message = \
                    f'tried to select {key!r}, which is not a key of varp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp={key: value for key, value in self._varp.items()
                                if key in keys},
                          uns=self._uns, num_threads=self._num_threads)
    
    def select_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Subsets `uns` to the specified key(s).
        
        Args:
            keys: key(s) to select
            *more_keys: additional keys to select, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with `uns` subset to the specified key(s).
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to select {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns={key: value for key, value in self._uns.items()
                                if key in keys},
                          num_threads=self._num_threads)
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to `obs`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `obs=obs.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(*exprs, **named_exprs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            SingleCell:
        """
        Equivalent to `df.with_columns()` from polars, but applied to `var`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new SingleCell dataset with
            `var=var.with_columns(*exprs, **named_exprs)`.
        """
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.with_columns(*exprs, **named_exprs),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_obsm(self,
                  obsm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_obsm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to `obsm`, overwriting existing keys with the
        same names if present.
        
        Args:
            obsm: a dictionary of keys to add to (or overwrite in) `obsm`
            **more_obsm: additional keys to add to (or overwrite in) `obsm`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `obsm`.
        """
        check_type(obsm, 'obsm', dict, 'a dictionary')
        for key, value in obsm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsm must be strings, but new obsm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        obsm |= more_obsm
        if len(obsm) == 0:
            error_message = \
                'obsm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in obsm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of obsm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new obsm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of obsm must be NumPy arrays or polars '
                    f'DataFrames, but new obsm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(obsm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_varm(self,
                  varm: dict[str, np.ndarray[2, Any] | pl.DataFrame] = {},
                  **more_varm: np.ndarray[2, Any]) -> SingleCell:
        """
        Adds one or more keys to `varm`, overwriting existing keys with the
        same names if present.
        
        Args:
            varm: a dictionary of keys to add to (or overwrite in) `varm`
            **more_varm: additional keys to add to (or overwrite in) `varm`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `varm`.
        """
        check_type(varm, 'varm', dict, 'a dictionary')
        for key, value in varm.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varm must be strings, but new varm contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        varm |= more_varm
        if len(varm) == 0:
            error_message = \
                'varm is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        for key, value in varm.items():
            if isinstance(value, np.ndarray):
                if value.ndim != 2:
                    error_message = (
                        f'all values of varm must be 2D NumPy arrays or '
                        f'polars DataFrames, but new varm[{key!r}] is a '
                        f'{value.ndim:,}D NumPy array')
                    raise ValueError(error_message)
            elif not isinstance(value, pl.DataFrame):
                error_message = (
                    f'all values of varm must be NumPy arrays or polars '
                    f'DataFrames, but new varm[{key!r}] has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
            if len(value) != self._X.shape[0]:
                error_message = (
                    f'len(varm[{key!r}]) is {len(value):,}, but X.shape[0] is '
                    f'{self._X.shape[0]:,}')
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm | varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_obsp(self,
                  obsp: dict[str, csr_array | csc_array] = {},
                  **more_obsp: csr_array | csc_array) -> SingleCell:
        """
        Adds one or more keys to `obsp`, overwriting existing keys with the
        same names if present.
        
        Args:
            obsp: a dictionary of keys to add to (or overwrite in) `obsp`
            **more_obsp: additional keys to add to (or overwrite in) `obsp`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `obsp`.
        """
        check_type(obsp, 'obsp', dict, 'a dictionary')
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        obsp |= more_obsp
        if len(obsp) == 0:
            error_message = \
                'obsp is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        num_cells = self._X.shape[0]
        new_obsp = self._obsp.copy()
        for key, value in obsp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of obsp must be strings, but new obsp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of obsp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new obsp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_cells:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[0] is '
                        f'{num_cells:,}')
                    raise ValueError(error_message)
            new_obsp[key] = value
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=new_obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_varp(self,
                  varp: dict[str, sparse.csr_array | sparse.csc_array |
                                  sparse.csr_matrix | sparse.csc_matrix] = {},
                  **more_varp: sparse.csr_array | sparse.csc_array |
                               sparse.csr_matrix | sparse.csc_matrix) -> \
            SingleCell:
        """
        Adds one or more keys to `varp`, overwriting existing keys with the
        same names if present.
        
        Args:
            varp: a dictionary of keys to add to (or overwrite in) `varp`
            **more_varp: additional keys to add to (or overwrite in) `varp`,
                         specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `varp`.
        """
        check_type(varp, 'varp', dict, 'a dictionary')
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        varp |= more_varp
        if len(varp) == 0:
            error_message = \
                'varp is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        num_genes = self._X.shape[1]
        new_varp = self._varp.copy()
        for key, value in varp.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of varp must be strings, but new varp contains '
                    f'a key of type {type(key).__name__!r}')
                raise TypeError(error_message)
            if isinstance(value, (csr_array, csc_array)):
                pass
            elif isinstance(value, (sparse.csr_array, sparse.csr_matrix)):
                value = csr_array(value)
            elif isinstance(value, (sparse.csc_array, sparse.csc_matrix)):
                value = csc_array(value)
            else:
                error_message = (
                    f'every value of varp must be a csr_array, csc_array, '
                    f'csr_matrix, or csc_matrix, but new varp{key!r}] has '
                    f'type {type(value).__name__!r}')
                raise TypeError(error_message)
            for dim in range(2):
                if value.shape[dim] != num_genes:
                    error_message = (
                        f'new varp[{key!r}].shape[{dim}] is '
                        f'{value.shape[dim]:,}, but X.shape[1] is '
                        f'{num_genes:,}')
                    raise ValueError(error_message)
            new_varp[key] = value
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=new_varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def with_uns(self,
                 uns: dict[str, UnsDict] = {},
                 **more_uns: UnsItem) -> SingleCell:
        """
        Adds one or more keys to `uns`, overwriting existing keys with the same
        names if present.
        
        Args:
            uns: a dictionary of keys to add to (or overwrite in) `uns`
            **more_uns: additional keys to add to (or overwrite in) `uns`,
                        specified as keyword arguments

        Returns:
            A new SingleCell dataset with the new key(s) added to or
            overwritten in `uns`.
        """
        check_type(uns, 'uns', dict, 'a dictionary')
        for key, value in uns.items():
            if not isinstance(key, str):
                error_message = (
                    f'all keys of uns must be strings, but new uns contains a '
                    f'key of type {type(key).__name__!r}')
                raise TypeError(error_message)
        uns |= more_uns
        if len(uns) == 0:
            error_message = \
                'uns is empty and no keyword arguments were specified'
            raise ValueError(error_message)
        valid_uns_types = str, int, np.integer, float, np.floating, \
            bool, np.bool_, np.ndarray
        for description, value in SingleCell._iter_uns(uns):
            if not isinstance(value, valid_uns_types):
                error_message = (
                    f'all values of uns must be scalars (strings, numbers or '
                    f'Booleans) or NumPy arrays, or nested dictionaries '
                    f'thereof, but {description} has type '
                    f'{type(value).__name__!r}')
                raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns | uns,
                          num_threads=self._num_threads)
    
    def drop_X(self):
        """
        Create a new SingleCell dataset with `X` removed, to reduce memory use.
        
        Returns:
            A new SingleCell dataset with `X` set to `None`.
        """
        if self._X is None:
            error_message = 'X is None, so it cannot be dropped'
            raise TypeError(error_message)
        return SingleCell(obs=self._obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from `obs`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                              positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs.drop(columns),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `columns` and `more_columns`
        removed from `var`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
        
        Returns:
            A new SingleCell dataset with the column(s) removed.
        """
        columns = to_tuple(columns) + more_columns
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.drop(columns), obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def drop_obsm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `obsm`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            obsm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of obsm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm={key: value for key, value in self._obsm.items()
                                if key not in keys},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def drop_varm(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `varm`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            varm.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varm:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of varm'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm={key: value for key, value in self._varm.items()
                                if key not in keys},
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def drop_obsp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `obsp`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            obsp.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._obsp:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of obsp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp={key: value for key, value in self._obsp.items()
                                if key not in keys},
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def drop_varp(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `varp`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            varp.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._varp:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of varp'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp={key: value for key, value in self._varp.items()
                                if key not in keys},
                          uns=self._uns, num_threads=self._num_threads)
    
    def drop_uns(self, keys: str | Iterable[str], *more_keys: str) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with `keys` and `more_keys` removed
        from `uns`.
        
        Args:
            keys: key(s) to drop
            *more_keys: additional keys to drop, specified as positional
                        arguments
        
        Returns:
            A new SingleCell dataset with the specified key(s) removed from
            uns.
        """
        keys = to_tuple_checked(keys, 'keys', str, 'strings')
        check_types(more_keys, 'more_keys', str, 'strings')
        keys += more_keys
        for key in keys:
            if key not in self._uns:
                error_message = \
                    f'tried to drop {key!r}, which is not a key of uns'
                raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns={key: value for key, value in self._uns.items()
                                if key not in keys},
                          num_threads=self._num_threads)
    
    def rename_obs(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of `obs` renamed.
        
        Rename column(s) of `obs`.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of `obs` renamed.
        """
        return SingleCell(X=self._X, obs=self._obs.rename(mapping),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def rename_var(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with column(s) of `var` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the column(s) of `var` renamed.
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.rename(mapping), obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def rename_obsm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `obsm` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `obsm` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._obsm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of obsm'
                    raise ValueError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            obsm = {mapping.get(key, key): value
                    for key, value in self._obsm.items()}
        elif isinstance(mapping, Callable):
            obsm = {}
            for key, value in self._obsm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename obsm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._obsm:
                    error_message = (
                        f'tried to rename obsm[{key!r}] to obsm[{new_key!r}], '
                        f'but obsm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                obsm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var, obsm=obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def rename_varm(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `varm` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `varm` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._varm:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of varm'
                    raise ValueError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
            varm = {mapping.get(key, key): value
                    for key, value in self._varm.items()}
        elif isinstance(mapping, Callable):
            varm = {}
            for key, value in self._varm.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename varm[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._varm:
                    error_message = (
                        f'tried to rename varm[{key!r}] to varm[{new_key!r}], '
                        f'but varm[{new_key!r}] already exists')
                    raise ValueError(error_message)
                varm[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def rename_obsp(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `obsp` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `obsp` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._obsp:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of obsp'
                    raise ValueError(error_message)
                if new_key in self._obsp:
                    error_message = (
                        f'tried to rename obsp[{key!r}] to obsp[{new_key!r}], '
                        f'but obsp[{new_key!r}] already exists')
                    raise ValueError(error_message)
            obsp = {mapping.get(key, key): value
                    for key, value in self._obsp.items()}
        elif isinstance(mapping, Callable):
            obsp = {}
            for key, value in self._obsp.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename obsp[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._obsp:
                    error_message = (
                        f'tried to rename obsp[{key!r}] to obsp[{new_key!r}], '
                        f'but obsp[{new_key!r}] already exists')
                    raise ValueError(error_message)
                obsp[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def rename_varp(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `varp` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `varp` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._varp:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of varp'
                    raise ValueError(error_message)
                if new_key in self._varp:
                    error_message = (
                        f'tried to rename varp[{key!r}] to varp[{new_key!r}], '
                        f'but varp[{new_key!r}] already exists')
                    raise ValueError(error_message)
            varp = {mapping.get(key, key): value
                    for key, value in self._varp.items()}
        elif isinstance(mapping, Callable):
            varp = {}
            for key, value in self._varp.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename varp[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._varp:
                    error_message = (
                        f'tried to rename varp[{key!r}] to varp[{new_key!r}], '
                        f'but varp[{new_key!r}] already exists')
                    raise ValueError(error_message)
                varp[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def rename_uns(self, mapping: dict[str, str] | Callable[[str], str]) -> \
            SingleCell:
        """
        Create a new SingleCell dataset with key(s) of `uns` renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name

        Returns:
            A new SingleCell dataset with the key(s) of `uns` renamed.
        """
        check_types(mapping.keys(), 'mapping.keys()', str, 'strings')
        check_types(mapping.values(), 'mapping.values()', str, 'strings')
        if isinstance(mapping, dict):
            for key, new_key in mapping.items():
                if key not in self._uns:
                    error_message = \
                        f'tried to rename {key!r}, which is not a key of uns'
                    raise ValueError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
            uns = {mapping.get(key, key): value
                   for key, value in self._uns.items()}
        elif isinstance(mapping, Callable):
            uns = {}
            for key, value in self._uns.items():
                new_key = mapping(key)
                if not isinstance(new_key, str):
                    error_message = (
                        f'tried to rename uns[{key!r}] to a non-string value '
                        f'of type {type(new_key).__name__!r}')
                    raise TypeError(error_message)
                if new_key in self._uns:
                    error_message = (
                        f'tried to rename uns[{key!r}] to uns[{new_key!r}], '
                        f'but uns[{new_key!r}] already exists')
                    raise ValueError(error_message)
                uns[new_key] = value
        else:
            error_message = (
                f'mapping must be a dictionary or function, but has type '
                f'{type(mapping).__name__!r}')
            raise TypeError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=uns,
                          num_threads=self._num_threads)
    
    def cast_X(self, dtype: np._typing.DTypeLike) -> SingleCell:
        """
        Cast `X` to the specified data type.
        
        Args:
            dtype: a NumPy data type

        Returns:
            A new SingleCell dataset with `X` cast to the specified data type.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so casting it is not possible'
            raise ValueError(error_message)
        return SingleCell(X=self._X.astype(dtype),
                          obs=self._obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of `obs` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of `obs` cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs.cast(dtypes, strict=strict),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 strict: bool = True) -> SingleCell:
        """
        Cast column(s) of `var` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new SingleCell dataset with column(s) of `var` cast to the
            specified data type(s).
        """
        return SingleCell(X=self._X, obs=self._obs,
                          var=self._var.cast(dtypes, strict=strict),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Left join `obs` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join `obs` with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in `obs`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `obs` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `obs`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            obs.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._obs
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'obs[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        obs = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(obs) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> SingleCell:
        """
        Join `var` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join `var` with
            on: the name(s) of the join column(s) in both DataFrames
            left_on: the name(s) of the join column(s) in `var`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `var` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `var`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new SingleCell dataset with the columns from `other` joined to
            var.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        left = self._var
        right = other
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
            left_columns = left.select(left_on)
            right_columns = right.select(right_on)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
            left_columns = left.select(on)
            right_columns = right.select(on)
        left_cast_dict = {}
        right_cast_dict = {}
        for left_column, right_column in zip(left_columns, right_columns):
            left_dtype = left_column.dtype
            right_dtype = right_column.dtype
            if left_dtype == right_dtype:
                continue
            if (left_dtype == pl.Enum or left_dtype == pl.Categorical) and (
                    right_dtype == pl.Enum or right_dtype == pl.Categorical):
                common_dtype = \
                    pl.Enum(pl.concat([left_column.cat.get_categories(),
                                       right_column.cat.get_categories()])
                            .unique(maintain_order=True))
                left_cast_dict[left_column.name] = common_dtype
                right_cast_dict[right_column.name] = common_dtype
            else:
                error_message = (
                    f'var[{left_column.name!r}] has data type '
                    f'{left_dtype.base_type()!r}, but '
                    f'other[{right_column.name!r}] has data type '
                    f'{right_dtype.base_type()!r}')
                raise TypeError(error_message)
        if left_cast_dict is not None:
            left = left.cast(left_cast_dict)
            right = right.cast(right_cast_dict)
        # noinspection PyTypeChecker
        var = left.join(right, on=on, how='left', left_on=left_on,
                        right_on=right_on, suffix=suffix, validate=validate,
                        join_nulls=join_nulls, coalesce=coalesce)
        if len(var) > len(self):
            other_on = to_tuple(right_on if right_on is not None else on)
            assert other.select(other_on).is_duplicated().any()
            duplicate_column = other_on[0] if len(other_on) == 1 else \
                next(column for column in other_on
                     if other[column].is_duplicated().any())
            error_message = (
                f'other[{duplicate_column!r}] contains duplicate values, so '
                f'it must be deduplicated before being joined on')
            raise ValueError(error_message)
        return SingleCell(X=self._X, obs=self._obs, var=var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def peek_obs(self, row: int = 0) -> None:
        """
        Print a row of `obs` (the first row, by default) with each column on
        its own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[row].unpivot(variable_name='column'))
    
    def peek_var(self, row: int = 0) -> None:
        """
        Print a row of `var` (the first row, by default) with each column on
        its own line.
        
        Args:
            row: the index of the row to print
        """
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[row].unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False,
                      num_threads: int | np.integer | None = None) -> \
            SingleCell:
        """
        Subsample a specific number or fraction of cells.
        
        Args:
            n: the number of cells to return; mutually exclusive with
               `fraction`
            fraction: the fraction of cells to return; mutually exclusive with
                      `n`
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will not be
                       selected when subsampling, and will not count towards
                       the denominator of `fraction`; QC_column will not appear
                       in the returned SingleCell dataset, since it would be
                       redundant.
            by_column: an optional String, Enum, Categorical, or integer column
                       of `obs` to subsample by. Can be a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of cells with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       `null` entries: the corresponding cells will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled cells; if `None`,
                              subset to these cells instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `obs`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
            num_threads: the number of threads to use when subsampling `X`.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Can only be specified when `subsample_column` is
                         `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `obs`.
        """
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._obs:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of obs; did you already run subsample_obs()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        check_type(seed, 'seed', int, 'an integer')
        if subsample_column is None:
            num_threads = self._process_num_threads(num_threads)
        else:
            if num_threads is not None:
                error_message = (
                    'num_threads can only be specified when subsample_column '
                    'is None')
                raise ValueError(error_message)
        if by_column is not None:
            by_column = self._get_column(
                'obs', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column, allow_null=True)
            if QC_column is not None:
                by_column = by_column.filter(QC_column)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n / len(by_column) * pl.len())
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))\
                    .to_series()
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))\
                    .to_series()
        elif QC_column is not None:
            selected = pl.int_range(QC_column.sum(), dtype=pl.Int32,
                                    eager=True)\
                .shuffle(seed=seed)\
                .lt(n if fraction is None else (fraction * pl.len()).round())
        else:
            selected = self._obs\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))\
                .to_series()
        if QC_column is not None:
            # Back-project from QCed cells to all cells, filling with `null`
            selected = pl.when(QC_column)\
                .then(selected.gather(QC_column.cum_sum().cast(pl.Int32) - 1))
        sc = self.filter_obs(selected, num_threads=num_threads) \
            if subsample_column is None else \
            self.with_columns_obs(selected.alias(subsample_column))
        if QC_column is not None:
            # noinspection PyTypeChecker
            sc._obs = sc._obs.drop(QC_column.name)
        return sc
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: SingleCellColumn | None = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False,
                      num_threads: int | np.integer | None = None) -> \
            SingleCell:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `var` to subsample by. Can be a column name, a
                       polars expression, a polars Series, a 1D NumPy array, or
                       a function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Specifying
                       `by_column` ensures that the same fraction of genes with
                       each value of `by_column` are subsampled. When combined
                       with `n`, to make sure the total number of samples is
                       exactly `n`, some of the smallest groups may be
                       oversampled by one element, or some of the largest
                       groups may be undersampled by one element. Can contain
                       `null` entries: the corresponding genes will not be
                       included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `var`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
            num_threads: the number of threads to use when subsampling `X`.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`. By default
                         (`num_threads=None`), use `self.num_threads` cores.
                         Can only be specified when `subsample_column` is
                         `None`.
        
        Returns:
            A new SingleCell dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `var`.
        """
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite and subsample_column in self._var:
                error_message = (
                    f'subsample_column {subsample_column!r} is already a '
                    f'column of var; did you already run subsample_var()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        check_type(seed, 'seed', int, 'an integer')
        if subsample_column is None:
            num_threads = self._process_num_threads(num_threads)
        else:
            if num_threads is not None:
                error_message = (
                    'num_threads can only be specified when subsample_column '
                    'is None')
                raise ValueError(error_message)
        if by_column is not None:
            by_column = self._get_column(
                'var', by_column, 'by_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                allow_null=True)
            by_frame = by_column.to_frame()
            by_name = by_column.name
            if n is not None:
                # Get a vector of the number of elements to sample per group.
                # The total sample size should exactly match the original n; if
                # necessary, oversample the smallest groups or undersample the
                # largest groups to make this happen.
                group_counts = by_frame\
                    .group_by(by_name)\
                    .agg(pl.len(), n=(n / len(by_column) * pl.len())
                                     .round().cast(pl.Int32))\
                    .drop_nulls(by_name)
                diff = n - group_counts['n'].sum()
                if diff != 0:
                    group_counts = group_counts\
                        .sort('len', descending=diff < 0)\
                        .with_columns(n=pl.col.n +
                                        pl.int_range(pl.len(), dtype=pl.Int32)
                                        .lt(abs(diff)).cast(pl.Int32) *
                                        pl.lit(diff).sign())
                selected = by_frame\
                    .join(group_counts, on=by_name)\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt(pl.col.n))
            else:
                selected = by_frame\
                    .select(pl.int_range(pl.len(), dtype=pl.Int32)
                            .shuffle(seed=seed)
                            .over(by_name)
                            .lt((fraction * pl.len().over(by_name)).round()))
        else:
            selected = self._var\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .shuffle(seed=seed)
                        .lt(n if fraction is None else
                            (fraction * pl.len()).round()))
        selected = selected.to_series()
        return self.filter_var(selected, num_threads=num_threads) \
            if subsample_column is None else \
            self.with_columns_var(selected.alias(subsample_column))
    
    def pipe(self,
             function: Callable[[SingleCell, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a SingleCell dataset.
        
        Args:
            function: the function to apply to the SingleCell dataset. It must
                      take a SingleCell dataset as its first argument, and can
                      return any value. The function may also allow other
                      arguments after the count matrix, which can be specified
                      via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this SingleCell dataset.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[csr_array | csc_array, ...],
                                  csr_array | csc_array],
               *args: Any,
               **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `X`.
        
        Args:
            function: the function to apply to `X`. It must take the old `X` as
                      its first argument and return the new `X`. The function
                      may also take other arguments after `X`, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to `X`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so piping it is not possible'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=function(self._X, *args, **kwargs), obs=self._obs,
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_obs(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obs`.
        
        Args:
            function: the function to apply to `obs`. It must take the old
                      `obs` as its first argument and return the new `obs`. The
                      function may also take other arguments after `obs`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obs.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_var(self,
                 function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `var`.
        
        Args:
            function: the function to apply to `var`. It must take the old
                      `var` as its first argument and return the new `var`. The
                      function may also take other arguments after `var`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            var.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs,
                          var=function(self._var, *args, **kwargs),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_obsm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obsm`.
        
        Args:
            function: the function to apply to `obsm`. It must take the old
                      `obsm` as its first argument and return the new `obsm`.
                      The function may also take other arguments after `obsm`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obsm.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=function(self._obsm, *args, **kwargs),
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def pipe_obsm_key(self,
                      key: str,
                      function: Callable[[np.ndarray[2, Any] |
                                          pl.DataFrame, ...],
                                         np.ndarray[2, Any] | pl.DataFrame],
                      *args: Any,
                      **kwargs: Any) -> SingleCell:
        """
        Apply a function to a specific key in a SingleCell dataset's `obsm`.
        
        Args:
            key: the key in `obsm` to which the function will be applied.
            function: the function to apply to `obsm[key]`. It must take the
                      old `obsm[key]` as its first argument and return the new
                      `obsm[key]`. The function may also take other arguments
                      after `obsm[key]`, which can be specified via `args` and
                      `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function
    
        Returns:
            A new SingleCell dataset where the function has been applied to
            `obsm[key]`.
        """
        # Check that `key` is a key in `obsm`
        check_type(key, 'key', str, 'a string')
        if key not in self._obsm:
            error_message = f'{key!r} is not a key in obsm'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {
                              key: function(self._obsm[key], *args, **kwargs)},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def pipe_varm(self,
                  function: Callable[[dict[str, np.ndarray[2, Any] |
                                                pl.DataFrame], ...],
                                     dict[str, np.ndarray[2, Any] |
                                               pl.DataFrame]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `varm`.
        
        Args:
            function: the function to apply to `varm`. It must take the old
                      `varm` as its first argument and return the new `varm`.
                      The function may also take other arguments after `varm`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            varm.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm=function(self._varm, *args, **kwargs),
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_varm_key(self,
                      key: str,
                      function: Callable[[np.ndarray[2, Any] |
                                          pl.DataFrame, ...],
                                         np.ndarray[2, Any] | pl.DataFrame],
                      *args: Any,
                      **kwargs: Any) -> SingleCell:
        """
        Apply a function to a specific key in a SingleCell dataset's `varm`.

        Args:
            key: the key in `varm` to which the function will be applied.
            function: the function to apply to `varm[key]`. It must take the
                      old `varm[key]` as its first argument and return the new
                      `varm[key]`. The function may also take other arguments
                      after `varm[key]`, which can be specified via `args` and
                      `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            `varm[key]`.
        """
        # Check that `key` is a key in `varm`
        check_type(key, 'key', str, 'a string')
        if key not in self._varm:
            error_message = f'{key!r} is not a key in varm'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm,
                          varm=self._varm | {
                              key: function(self._varm[key], *args, **kwargs)},
                          obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def pipe_obsp(self,
                  function: Callable[[dict[str, csr_array | csc_array], ...],
                                     dict[str, csr_array | csc_array]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `obsp`.
        
        Args:
            function: the function to apply to `obsp`. It must take the old
                      `obsp` as its first argument and return the new `obsp`.
                      The function may also take other arguments after `obsp`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            obsp.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=function(self._obsp, *args, **kwargs),
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_obsp_key(self,
                      key: str,
                      function: Callable[[csr_array | csc_array, ...],
                                         csr_array | csc_array],
                      *args: Any,
                      **kwargs: Any) -> SingleCell:
        """
        Apply a function to a specific key in a SingleCell dataset's `obsp`.

        Args:
            key: the key in `obsp` to which the function will be applied.
            function: the function to apply to `obsp[key]`. It must take the
                      old `obsp[key]` as its first argument and return the new
                      `obsp[key]`. The function may also take other arguments
                      after `obsp[key]`, which can be specified via `args` and
                      `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            `obsp[key]`.
        """
        # Check that `key` is a key in `obsp`
        check_type(key, 'key', str, 'a string')
        if key not in self._obsp:
            error_message = f'{key!r} is not a key in obsp'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, 
                          obsp=self._obsp | {
                              key: function(self._obsp[key], *args, **kwargs)},
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def pipe_varp(self,
                  function: Callable[[dict[str, csr_array | csc_array], ...],
                                     dict[str, csr_array | csc_array]],
                  *args: Any,
                  **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `varp`.
        
        Args:
            function: the function to apply to `varp`. It must take the old
                      `varp` as its first argument and return the new `varp`.
                      The function may also take other arguments after `varp`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            varp.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=function(self._varp, *args, **kwargs),
                          uns=self._uns, num_threads=self._num_threads)
    
    def pipe_varp_key(self,
                      key: str,
                      function: Callable[[csr_array | csc_array, ...],
                      csr_array | csc_array],
                      *args: Any,
                      **kwargs: Any) -> SingleCell:
        """
        Apply a function to a specific key in a SingleCell dataset's `varp`.

        Args:
            key: the key in `varp` to which the function will be applied.
            function: the function to apply to `varp[key]`. It must take the
                      old `varp[key]` as its first argument and return the new
                      `varp[key]`. The function may also take other arguments
                      after `varp[key]`, which can be specified via `args` and
                      `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            `varp[key]`.
        """
        # Check that `key` is a key in `varp`
        check_type(key, 'key', str, 'a string')
        if key not in self._varp:
            error_message = f'{key!r} is not a key in varp'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp | {
                              key: function(self._varp[key], *args, **kwargs)},
                          uns=self._uns, num_threads=self._num_threads)
    
    def pipe_uns(self,
                 function: Callable[[UnsDict, ...], UnsDict],
                 *args: Any,
                 **kwargs: Any) -> SingleCell:
        """
        Apply a function to a SingleCell dataset's `uns`.
        
        Args:
            function: the function to apply to `uns`. It must take the old
                      `uns` as its first argument and return the new `uns`. The
                      function may also take other arguments after `uns`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            uns.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp,
                          uns=function(self._uns, *args, **kwargs),
                          num_threads=self._num_threads)
    
    def pipe_uns_key(self,
                     key: str,
                     function: Callable[[UnsDict, ...], UnsDict],
                     *args: Any,
                     **kwargs: Any) -> SingleCell:
        """
        Apply a function to a specific key in a SingleCell dataset's `uns`.

        Args:
            key: the key in `uns` to which the function will be applied.
            function: the function to apply to `uns[key]`. It must take the
                      old `uns[key]` as its first argument and return the new
                      `uns[key]`. The function may also take other arguments
                      after `uns[key]`, which can be specified via `args` and
                      `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new SingleCell dataset where the function has been applied to
            `uns[key]`.
        """
        # Check that `key` is a key in `uns`
        check_type(key, 'key', str, 'a string')
        if key not in self._uns:
            error_message = f'{key!r} is not a key in uns'
            raise ValueError(error_message)
        
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp,
                          uns=self._uns | {
                              key: function(self._uns[key], *args, **kwargs)},
                          num_threads=self._num_threads)
    
    def qc(self,
           custom_filter: SingleCellColumn | None = None,
           *,
           subset: bool = True,
           QC_column: str = 'passed_QC',
           max_mito_fraction: int | float | np.integer | np.floating |
                              None = 0.05,
           min_genes: int | np.integer | None = 100,
           nonzero_MALAT1: bool = True,
           remove_doublets: bool = False,
           batch_column: SingleCellColumn | None = None,
           doublet_fraction: float | np.floating | None = None,
           num_doublet_genes: int | np.integer = 500,
           allow_float: bool = False,
           overwrite: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Adds a Boolean column to `obs` indicating which cells passed quality
        control (QC), or subsets to these cells if `subset=True`.
        
        By default, filters to cells with a cell-type confidence of ≥90%, ≤10%
        mitochondrial reads, ≥100 genes detected, and nonzero MALAT1 or Malat1
        expression. Can also filter out doublets when `remove_doublets=True`.
        
        Raises an error if any cell names appear more than once in `obs_names`
        (they can be deduplicated with `make_obs_names_unique()`) or any gene
        names appear more than once in `var_names` (they can be deduplicated
        with `make_var_names_unique()`).
        
        Args:
            custom_filter: an optional Boolean column of `obs` containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be a column name,
                           a polars expression, a polars Series, a 1D NumPy
                           array, or a function that takes in this SingleCell
                           dataset and returns a polars Series or 1D NumPy
                           array.
            subset: whether to subset to cells passing QC, instead of merely
                    adding a `QC_column` to `obs`. This will roughly double
                    memory usage, but speed up subsequent operations.
            QC_column: the name of a Boolean column to add to `obs` indicating
                       which cells passed QC, if `subset=False`. Gives an error
                       if `obs` already has a column with this name, unless
                       `overwrite=True`.
            max_mito_fraction: if not `None`, filter to cells with <= this
                               fraction of mitochondrial counts. The default of
                               5% matches Seurat's recommended value.
            min_genes: if not `None`, filter to cells with >= this many genes
                       detected (with non-zero count). The default of 100
                       matches Scanpy's recommended value, while Seurat
                       recommends a minimum of 200.
            nonzero_MALAT1: if `True`, filter out cells with 0 expression of
                            the nuclear-expressed lncRNA MALAT1, which likely
                            represent empty droplets or poor-quality cells
                            (biorxiv.org/content/10.1101/2024.07.14.603469v1).
                            There must be exactly one gene in `obs_names` with
                            the name `'MALAT1'` or `'Malat1'` to use this
                            filter.
            remove_doublets: if `True`, remove predicted doublets (see
                             `find_doublets()`). Doublet detection uses the
                             cxds algorithm to score each cell, then thresholds
                             this continuous score to a binary one (doublet
                             versus non-doublet) using a threshold derived from
                             simulated doublets.
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Only used during doublet
                          detection; doublet detection will be performed
                          separately for each batch. Set to `None` if all cells
                          belong to the same sequencing batch. Can only be
                          specified when `remove_doublets=True`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described in
                              `find_doublets()`.
            num_doublet_genes: the number of highly variable genes, i.e. genes
                               expressed in as close to 50% of cells as
                               possible, to use during doublet detection. This
                               parameter usually has a minimal influence on
                               accuracy as long as it is sufficiently large (in
                               the hundreds), so increasing it further will
                               mainly just increase runtime. If
                               `num_doublet_genes` is greater than the number
                               of genes in the dataset, all genes will be used.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check.
                         Note that all steps except mitochondrial percent
                         filtering give the same result on normalized counts,
                         so as long as `max_mito_fraction=None` is specified
                         (not typically recommended), this function will give
                         the same result on raw and normalized counts.
            overwrite: if `False`, raise an error if `uns['QCed']` is `True`
                       (indicating the dataset has already been QCed) or
                       `QC_column` is already present in `obs`; if `True`,
                       disable these two sanity checks and, when
                       `subset=False`, overwrite `QC_column` if present.
            verbose: whether to print how many cells were filtered out at each
                     step of the QC process
            num_threads: the number of threads to use when filtering based on
                         mitochondrial counts and MALAT1 expression, and for
                         doublet detection. Set `num_threads=-1` to use all
                         available cores, as determined by `os.cpu_count()`,
                         or leave unset to use `self.num_threads` cores. Does
                         not affect the QCed SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset with `QC_column` added to `obs`, or subset
            to QCed cells if `subset=True`, and `uns['QCed']` set to `True`.
        """
        # Check that `QC_column` is a string
        check_type(QC_column, 'QC_column', str, 'a string')
        
        # Check that `overwrite` is Boolean;
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # If `overwrite=False`, check that `uns['QCed']=False` and that
        # `QC_column` is not present in `obs`
        if not overwrite:
            if self._uns['QCed']:
                error_message = (
                    "uns['QCed'] is True; did you already run qc()? Specify "
                    "overwrite=True, set uns['QCed'] = False, or run "
                    "with_uns(QCed=False) to bypass this check.")
                raise ValueError(error_message)
            if QC_column in self._obs:
                error_message = (
                    f'QC_column {QC_column!r} is already a column of obs; did '
                    f'you already run qc()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present
        X = self._X
        if X is None:
            error_message = 'X is None, so QCing is not possible'
            raise ValueError(error_message)
        
        # Check that `obs_names` and `var_names` are unique
        num_unique = self.obs_names.n_unique()
        if num_unique < len(self._obs):
            error_message = (
                f'obs_names contains {num_unique - len(self._obs):,} '
                f'duplicates; deduplicate with make_obs_names_unique()')
            raise ValueError(error_message)
        num_unique = self.var_names.n_unique()
        if num_unique < len(self._var):
            error_message = (
                f'var_names contains {num_unique - len(self._var):,} '
                f'duplicates; deduplicate with make_var_names_unique()')
            raise ValueError(error_message)
        
        # Get the `custom_filter`, if specified
        if custom_filter is not None:
            custom_filter = self._get_column(
                'obs', custom_filter, 'custom_filter', pl.Boolean)
        
        # Check that `subset` is Boolean; if `subset=True`, check that
        # `QC_column` has the default value of `'passed_QC'`
        check_type(subset, 'subset', bool, 'Boolean')
        if subset and QC_column != 'passed_QC':
            error_message = 'QC_column can only be specified when subset=False'
            raise ValueError(error_message)
        
        # If `max_mito_fraction` was specified, check that it is a number
        # between 0 and 1, inclusive
        if max_mito_fraction is not None:
            check_type(max_mito_fraction, 'max_mito_fraction', (int, float),
                       'a number between 0 and 1, inclusive')
            check_bounds(max_mito_fraction, 'max_mito_fraction', 0, 1)
        
        # If `min_genes` was specified, check that it is a non-negative integer
        if min_genes is not None:
            check_type(min_genes, 'min_genes', int, 'a non-negative integer')
            check_bounds(min_genes, 'min_genes', 0)
        
        # Check that `nonzero_MALAT1` and `remove_doublets` are Boolean
        check_type(nonzero_MALAT1, 'nonzero_MALAT1', bool, 'Boolean')
        check_type(remove_doublets, 'remove_doublets', bool, 'Boolean')
        
        # If `batch_column` was specified, get it after checking that
        # `remove_doublets=True`
        if batch_column is not None:
            if not remove_doublets:
                error_message = (
                    'batch_column must be None when remove_doublets is False, '
                    'since its only use within qc() is for doublet detection')
                raise ValueError(error_message)
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'))
        
        # If `doublet_fraction` was specified, check that it is a number
        # between 0 and 1, exclusive
        if doublet_fraction is not None:
            check_type(doublet_fraction, 'doublet_fraction', float,
                       'a number greater than 0 and less than 1')
            check_bounds(doublet_fraction, 'doublet_fraction', 0, 1,
                         left_open=True, right_open=True)
        
        # Check that `num_doublet_genes` is a positive integer
        check_type(num_doublet_genes, 'num_doublet_genes', int,
                   'a positive integer')
        check_bounds(num_doublet_genes, 'num_doublet_genes', 1)
        
        # Check that `allow_float` is Boolean
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # If `allow_float=False`, raise an error if `X` is floating-point
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'qc() requires raw counts but X has data type '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)
        
        # Apply the custom filter, if specified
        if verbose:
            print(f'Starting with {len(self):,} {plural("cell", len(self))}.')
        mask = None
        if custom_filter is not None:
            if verbose:
                print('Applying the custom filter...')
            mask = custom_filter
            if verbose:
                num_cells = mask.sum()
                print(
                    f'{num_cells:,} '
                    f'{"cell remains" if num_cells == 1 else "cells remain"} '
                    f'after applying the custom filter.')
        
        # Filter to cells with ≤ `100 * max_mito_fraction`% mitochondrial
        # counts, if `max_mito_fraction` was specified
        if max_mito_fraction is not None:
            if verbose:
                print(f'Filtering to cells with ≤{100 * max_mito_fraction}% '
                      f'mitochondrial counts...')
            var_names = self.var_names
            if var_names.dtype != pl.String:
                var_names = var_names.cast(pl.String)
            mt_genes = var_names.str.to_uppercase().str.starts_with('MT-')
            if not mt_genes.any():
                error_message = (
                    'no genes are mitochondrial (start with "MT-", '
                    'case-insensitively); this may happen if your var_names '
                    'are Ensembl IDs (ENSG) rather than gene symbols (in '
                    'which case you should set the gene symbols as the '
                    'var_names with set_var_names()), or if mitochondrial '
                    'genes have already been filtered out (in which case you '
                    'can set max_mito_fraction to None)')
                raise ValueError(error_message)
            mito_mask = np.empty(X.shape[0], dtype=bool)
            if isinstance(X, csr_array):
                cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def mito_mask(const numeric[::1] data,
                      const signed_integer[::1] indices,
                      const signed_integer[::1] indptr,
                      char[::1] mt_genes,
                      const float max_mito_fraction,
                      char[::1] mito_mask,
                      unsigned num_threads):
            
            cdef unsigned row, row_sum, mt_sum, thread_index, \
                num_genes = indptr.shape[0] - 1
            cdef unsigned long col
            cdef pair[unsigned, unsigned] row_range
            
            num_threads = min(num_threads, num_genes)
            if num_threads == 1:
                for row in range(num_genes):
                    row_sum = 0
                    mt_sum = 0
                    for col in range(<unsigned long> indptr[row],
                                     <unsigned long> indptr[row + 1]):
                        row_sum = row_sum + <unsigned> data[col]
                        if mt_genes[indices[col]]:
                            mt_sum = mt_sum + <unsigned> data[col]
                    mito_mask[row] = (<float> mt_sum / row_sum) <= max_mito_fraction
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(indptr, thread_index, num_threads)
                    for row in range(row_range.first, row_range.second):
                        row_sum = mt_sum = 0
                        for col in range(<unsigned long> indptr[row],
                                         <unsigned long> indptr[row + 1]):
                            row_sum = row_sum + <unsigned> data[col]
                            if mt_genes[indices[col]]:
                                mt_sum = mt_sum + <unsigned> data[col]
                        mito_mask[row] = \
                            (<float> mt_sum / row_sum) <= max_mito_fraction
                        ''', warn_undeclared=False)['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            mito_mask=mito_mask, num_threads=num_threads)
            else:
                cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.vector cimport vector
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def mito_mask(const numeric[::1] data,
                      const signed_integer[::1] indices,
                      const signed_integer[::1] indptr,
                      char[::1] mt_genes,
                      const float max_mito_fraction,
                      char[::1] mito_mask,
                      unsigned num_threads):
            
            cdef unsigned thread_index, gene, row_sum, mt_sum, \
                num_genes = indptr.shape[0] - 1, num_cells = mito_mask.shape[0]
            cdef unsigned long cell
            cdef pair[unsigned, unsigned] col_range
            cdef vector[unsigned] row_sums_buffer, mt_sums_buffer
            cdef vector[vector[unsigned]] thread_row_sums, thread_mt_sums
            cdef unsigned[::1] row_sums, mt_sums
            
            num_threads = min(num_threads, num_genes)
            if num_threads == 1:
                row_sums_buffer.resize(num_cells)
                row_sums = <unsigned[:num_cells]> row_sums_buffer.data()
                mt_sums_buffer.resize(num_cells)
                mt_sums = <unsigned[:num_cells]> mt_sums_buffer.data()
                for gene in range(num_genes):
                    if mt_genes[gene]:
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            row_sums[indices[cell]] += <unsigned> data[cell]
                            mt_sums[indices[cell]] += <unsigned> data[cell]
                    else:
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            row_sums[indices[cell]] += <unsigned> data[cell]
                for cell in range(num_cells):
                    mito_mask[cell] = \
                        (<float> mt_sums[cell] / row_sums[cell]) <= max_mito_fraction
            else:
                # Store total counts per cell and total mitochondrial counts per cell
                # for each thread in a temporary buffer, then aggregate at the end.
                thread_row_sums.resize(num_threads)
                thread_mt_sums.resize(num_threads)
                with nogil:
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_row_sums[thread_index].resize(num_cells)
                        thread_mt_sums[thread_index].resize(num_cells)
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for gene in range(col_range.first, col_range.second):
                            if mt_genes[gene]:
                                for cell in range(<unsigned long> indptr[gene],
                                                  <unsigned long> indptr[gene + 1]):
                                    thread_row_sums[thread_index][indices[cell]] += \
                                        <unsigned> data[cell]
                                    thread_mt_sums[thread_index][indices[cell]] += \
                                        <unsigned> data[cell]
                            else:
                                for cell in range(<unsigned long> indptr[gene],
                                                  <unsigned long> indptr[gene + 1]):
                                    thread_row_sums[thread_index][indices[cell]] += \
                                        <unsigned> data[cell]
                
                    # Populate the mask
                    for cell in prange(num_cells, num_threads=num_threads):
                        row_sum = 0
                        mt_sum = 0
                        for thread_index in range(num_threads):
                            row_sum = row_sum + thread_row_sums[thread_index][cell]
                            mt_sum = mt_sum + thread_mt_sums[thread_index][cell]
                        mito_mask[cell] = \
                            (<float> mt_sum / row_sum) <= max_mito_fraction
                        ''', warn_undeclared=False)['mito_mask'](
                            data=X.data, indices=X.indices, indptr=X.indptr,
                            mt_genes=mt_genes.to_numpy(),
                            max_mito_fraction=max_mito_fraction,
                            mito_mask=mito_mask, num_threads=num_threads)
            mito_mask = pl.Series(mito_mask)
            if not mito_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'≤{100 * max_mito_fraction}% mitochondrial counts')
                raise ValueError(error_message)
            if mask is None:
                mask = mito_mask
            else:
                mask &= mito_mask
            if verbose:
                num_cells = mask.sum()
                print(
                    f'{num_cells:,} '
                    f'{"cell remains" if num_cells == 1 else "cells remain"} '
                    f'after filtering to cells with '
                    f'≤{100 * max_mito_fraction}% mitochondrial counts.')
        
        # Filter to cells with ≥ `min_genes` genes detected, if specified
        if min_genes is not None:
            if verbose:
                print(f'Filtering to cells with ≥{min_genes:,} '
                      f'{plural("gene", min_genes)} detected (with non-zero '
                      f'count)...')
            gene_mask = pl.Series(getnnz(X, axis=1, num_threads=num_threads) >=
                                  min_genes)
            if not gene_mask.any():
                error_message = (
                    f'no cells remain after filtering to cells with '
                    f'≥{min_genes:,} {plural("gene", min_genes)} detected')
                raise ValueError(error_message)
            if mask is None:
                mask = gene_mask
            else:
                mask &= gene_mask
            if verbose:
                num_cells = mask.sum()
                print(
                    f'{num_cells:,} '
                    f'{"cell remains" if num_cells == 1 else "cells remain"} '
                    f'after filtering to cells with ≥{min_genes:,} '
                    f'{plural("gene", min_genes)} detected.')
        
        # Filter to cells with non-zero MALAT1 expression, if
        # `nonzero_MALAT1=True`
        if nonzero_MALAT1:
            if verbose:
                print(f'Filtering to cells with non-zero MALAT1 expression...')
            MALAT1_index = self._var\
                .select(pl.arg_where(pl.col(self.var_names.name)
                                     .is_in(('MALAT1', 'Malat1'))))
            if len(MALAT1_index) == 0:
                error_message = (
                    f"neither 'MALAT1' nor 'Malat1' was found in var_names; "
                    f"this may happen if your var_names are Ensembl IDs "
                    f"(ENSG) rather than gene symbols (in which case you "
                    f"should set the gene symbols as the var_names with "
                    f"set_var_names()). Alternatively, set "
                    f"nonzero_MALAT1=False to disable filtering on MALAT1 "
                    f"expression.")
                raise ValueError(error_message)
            if len(MALAT1_index) == 2:
                error_message = (
                    "both 'MALAT1' and 'Malat1' were found in var_names; if "
                    "this is intentional, rename one of them before running "
                    "qc(), or set nonzero_MALAT1=False to disable filtering "
                    "on MALAT1 expression")
                raise ValueError(error_message)
            MALAT1_index = MALAT1_index.item()
            # The code below is a faster version of:
            # `MALAT1_mask = (X[:, [MALAT1_index]] != 0).toarray().squeeze()`
            if isinstance(X, csr_array):
                MALAT1_mask = np.empty(X.shape[0], dtype=bool)
                if X.has_sorted_indices:
                    # Use binary search
                    cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, threadid
        from libcpp.algorithm cimport lower_bound
    
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def get_MALAT1_mask_csr(
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const signed_integer MALAT1_index,
                char[::1] MALAT1_mask,
                unsigned num_threads):
            
            cdef unsigned row, thread_index, num_cells = indptr.shape[0] - 1
            cdef signed_integer* start
            cdef signed_integer* end
            cdef signed_integer* found
            cdef pair[unsigned, unsigned] row_range
        
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                for row in range(num_cells):
                    start = <signed_integer*> &indices[indptr[row]]
                    end = start + indptr[row + 1] - indptr[row]
                    found = lower_bound(start, end, MALAT1_index)
                    MALAT1_mask[row] = found != end and found[0] == MALAT1_index
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(indptr, thread_index, num_threads)
                    for row in range(row_range.first, row_range.second):
                        start = <signed_integer*> &indices[indptr[row]]
                        end = start + indptr[row + 1] - indptr[row]
                        found = lower_bound(start, end, MALAT1_index)
                        MALAT1_mask[row] = found != end and found[0] == MALAT1_index
                    ''', warn_undeclared=False)['get_MALAT1_mask_csr'](
                        indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
                else:
                    if verbose:
                        print('Warning: X does not have sorted indices, so '
                              'some operations may be slower. You may want to '
                              'sort indices with `X.sort_indices()` (an '
                              'in-place operation) as the first step after '
                              'loading, though be aware that this may take a '
                              'while.')
                    MALAT1_mask = np.empty(X.shape[0], dtype=bool)
                    
                    # Use brute-force search
                    cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, threadid
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def get_MALAT1_mask_csr_unsorted(
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const signed_integer MALAT1_index,
                char[::1] MALAT1_mask,
                unsigned num_threads):
            
            cdef unsigned row, thread_index, num_cells = indptr.shape[0] - 1
            cdef unsigned long col
            cdef pair[unsigned, unsigned] row_range
            
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                for row in range(num_cells):
                    for col in range(<unsigned long> indptr[row],
                                     <unsigned long> indptr[row + 1]):
                        if indices[col] == MALAT1_index:
                            MALAT1_mask[row] = True
                            break
                    else:
                        MALAT1_mask[row] = False
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(indptr, thread_index, num_threads)
                    for row in range(row_range.first, row_range.second):
                        for col in range(<unsigned long> indptr[row],
                                         <unsigned long> indptr[row + 1]):
                            if indices[col] == MALAT1_index:
                                MALAT1_mask[row] = True
                                break
                    else:
                        MALAT1_mask[row] = False
                    ''', warn_undeclared=False)\
                        ['get_MALAT1_mask_csr_unsorted'](
                        indices=X.indices, indptr=X.indptr,
                        MALAT1_index=MALAT1_index, MALAT1_mask=MALAT1_mask,
                        num_threads=num_threads)
            else:
                start = X.indptr[MALAT1_index]
                end = X.indptr[MALAT1_index + 1]
                MALAT1_mask = np.zeros(X.shape[0], dtype=bool)
                MALAT1_mask[X.indices[start:end]] = True
            MALAT1_mask = pl.Series(MALAT1_mask)
            if mask is None:
                mask = MALAT1_mask
            else:
                mask &= MALAT1_mask
            if verbose:
                num_cells = mask.sum()
                print(
                    f'{num_cells:,} '
                    f'{"cell remains" if num_cells == 1 else "cells remain"} '
                    f'after filtering to cells with non-zero MALAT1 '
                    f'expression.')
        
        # Remove predicted doublets, if `remove_doublets=True`. Exclude cells
        # that have failed earlier QC steps from being considered in the
        # doublet detection by passing `QC_column=mask`.
        if remove_doublets:
            if verbose:
                print('Removing predicted doublets...')
            singlets = pl.Series(SingleCell._find_doublets(
                X=self._X, batch_column=batch_column, QC_column=mask,
                doublet_fraction=doublet_fraction, num_genes=num_doublet_genes,
                return_scores=False, num_threads=num_threads))
            if mask is None:
                mask = singlets
            else:
                mask &= singlets
            if verbose:
                num_cells = mask.sum()
                print(
                    f'{num_cells:,} '
                    f'{"cell remains" if num_cells == 1 else "cells remain"} '
                    f'after removing predicted doublets.')
        
        # Add the mask of QCed cells as a column, or subset if `subset=True`
        if mask is None:
            error_message = 'no QC filters were specified'
            raise ValueError(error_message)
        if subset:
            if verbose:
                print(f'Subsetting to cells passing QC (note: you can reduce '
                      f'memory usage by specifying subset=False)...')
            sc = self.filter_obs(mask)
        else:
            if verbose:
                print(f'Adding a Boolean column, obs[{QC_column!r}], '
                      f'indicating which cells passed QC...')
            sc = SingleCell(X=X, obs=self._obs.with_columns(pl.lit(mask)
                                                            .alias(QC_column)),
                            var=self._var, obsm=self._obsm, varm=self._varm,
                            uns=self._uns, num_threads=self._num_threads)
        sc._uns['QCed'] = True
        return sc
    
    # noinspection PyTypeChecker
    @staticmethod
    def _find_doublets_old(X: csr_array | csc_array,
                           batch_column: SingleCellColumn | None,
                           QC_column: SingleCellColumn | None,
                           doublet_fraction: float | np.floating | None,
                           num_genes: int | np.integer,
                           return_scores: bool,
                           num_threads: int | np.integer | None) -> \
            tuple[np.ndarray[1, np.dtype[np.bool_]],
                  np.ndarray[1, np.dtype[np.float32]]]:
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507). Used by
        `qc()` (when `remove_doublets=True`) and `find_doublets()`.
        
        Args:
            X: the count matrix; may be either normalized or unnormalized
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described in
                              `find_doublets()`.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.

        Returns:
            A NumPy array with the binary doublet calls (`True` for singlets,
            `False` for doublets), or a tuple of two arrays with the doublet
            calls and doublet scores when `return_scores=True`. Scores will be
            uninitialized for cells not in any batch.
        """
        # Define Cython functions
        is_csr = isinstance(X, csr_array)
        cython_functions = cython_inline(r'''
            from cython.parallel cimport prange
            from libcpp.cmath cimport abs, erfc, exp, floor, log, log1p, sqrt
            
            ctypedef fused numeric:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            ctypedef fused signed_integer:
                int
                long
            
            cdef extern from * nogil:
                """
                #define atomic_add(x, y) _Pragma("omp atomic") x += y
                """
                void atomic_add(unsigned &x, unsigned y)
            
            cdef double log_one_half = -0.6931471805599453
            cdef double log_sqrt_2_pi = 0.91893853320467274
            cdef double one_over_sqrt_2 = 0.70710678118654752
            cdef double one_over_sqrt_pi = 0.5641895835477563
            
            cdef double[5] gamma_A = [
                8.11614167470508450300E-4, -5.95061904284301438324E-4,
                7.93650340457716943945E-4, -2.77777777730099687205E-3,
                8.33333333333331927722E-2]
            
            cdef double[6] gamma_B = [
                -1.37825152569120859100E3, -3.88016315134637840924E4,
                -3.31612992738871184744E5, -1.16237097492762307383E6,
                -1.72173700820839662146E6, -8.53555664245765465627E5]
            
            cdef double[6] gamma_C = [
                -3.51815701436523470549E2, -1.70642106651881159223E4,
                -2.20528590553854454839E5, -1.13933444367982507207E6,
                -2.53252307177582951285E6, -2.01889141433532773231E6]
            
            cdef double[6] P = [
                0.5641895835477550741253201704,
                1.275366644729965952479585264,
                5.019049726784267463450058,
                6.1602098531096305440906,
                7.409740605964741794425,
                2.97886562639399288862]
            
            cdef double[6] Q = [
                2.260528520767326969591866945,
                9.396034016235054150430579648,
                12.0489519278551290360340491,
                17.08144074746600431571095,
                9.608965327192787870698,
                3.3690752069827527677]
            
            cdef inline double p1evl(const double x,
                                     const double* coef,
                                     const unsigned N) noexcept nogil:
                cdef double ans
                cdef unsigned i
                
                ans = x + coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef inline double polevl(const double x,
                                      const double* coef,
                                      const unsigned N) noexcept nogil:
                cdef double ans
                cdef unsigned i
                
                ans = coef[0]
                for i in range(1, N):
                    ans = ans * x + coef[i]
                return ans
            
            cdef inline double log_erfc(const double x) noexcept nogil:
                # Based on GSL's gsl_sf_log_erfc_e at
                # github.com/ampl/gsl/blob/master/specfunc/erfc.c#L306
                
                cdef double y, series
                
                if x * x < 0.02460783300575925:
                    y = x * one_over_sqrt_pi
                    series = 0.00048204
                    series = y * series - 0.00142906
                    series = y * series + 0.0013200243174
                    series = y * series + 0.0009461589032
                    series = y * series - 0.0045563339802
                    series = y * series + 0.00556964649138
                    series = y * series + 0.00125993961762116
                    series = y * series - 0.01621575378835404
                    series = y * series + 0.02629651521057465
                    series = y * series - 0.001829764677455021
                    series = y * series - 0.09439510239319526
                    series = y * series + 0.28613578213673563
                    series = y * series + 1
                    series = y * series + 1
                    return -2 * y * series
                elif x > 8:
                    return log(polevl(x, &P[0], 6) / p1evl(x, &Q[0], 6)) - \
                        x * x
                else:
                    return log(erfc(x))
            
            cdef inline double gammaln(double x) noexcept nogil:
                # Simplified from github.com/scipy/scipy/blob/main/scipy/
                # special/xsf/cephes/gamma.h, based on the knowledge that `x`
                # will always be positive and finite when calculating terms in
                # the binomial distribution
                
                cdef double p, q, u, z
                
                if x < 13:
                    z = 1
                    p = 0
                    u = x
                    while u >= 3:
                        p -= 1
                        u = x + p
                        z *= u
                    while u < 2:
                        z /= u
                        p += 1
                        u = x + p
                    z = abs(z)
                    if u == 2:
                        return log(z)
                    p -= 2
                    x = x + p
                    p = x * polevl(x, &gamma_B[0], 6) / \
                        p1evl(x, &gamma_C[0], 6)
                    return log(z) + p
                elif x >= 1000:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    if x > 1e8:
                        return q
                    p = 1.0 / (x * x)
                    p = ((7.9365079365079365079365e-4 * p -
                          2.7777777777777777777778e-3) *
                         p + 0.0833333333333333333333) / x
                    return q + p
                else:
                    q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                    p = 1.0 / (x * x)
                    return q + polevl(p, &gamma_A[0], 5) / x
            
            cdef inline double binom_logsf_term(
                    const unsigned j,
                    const unsigned n,
                    const double log_p,
                    const double log1p_q,
                    const double gammaln_n_plus_1) noexcept nogil:
                return gammaln_n_plus_1 - gammaln(j + 1) - \
                    gammaln(n - j + 1) + j * log_p + (n - j) * log1p_q
            
            cdef inline double binom_logsf(const unsigned k,
                                           const unsigned n,
                                           const double p) noexcept nogil:
                cdef unsigned j, j_max
                cdef double mu, sigma, z
                cdef double sum_exp, max_term, term, log_p, log1p_q, \
                    gammaln_n_plus_1
                
                # Use the normal approximation when n * p and n * (1 - p) are
                # both greater than 500; add 0.5 for continuity correction
                if n * p > 500 and n * (1 - p) > 500:
                    mu = n * p
                    sigma = sqrt(mu * (1 - p))
                    z = (k + 0.5 - mu) / sigma
                    return log_erfc(z * one_over_sqrt_2) + log_one_half
                
                # Otherwise, compute the exact binomial p-value
                log_p = log(p)
                log1p_q = log1p(-p)
                gammaln_n_plus_1 = gammaln(n + 1)
            
                # Find `j_max`, the value of `j` with the largest binomial term
                # (the `term` variabel below). For a binomial distribution, the
                # mode (maximum probability) occurs at `floor((n + 1) * p)`.
                # However, since we are summing from `k + 1` to `n`, not 0 to
                # `n`, we need to ensure `j_max` is at least `k + 1`.
                j_max = <unsigned> floor((n + 1) * p)
                if j_max <= k:
                    j_max = k + 1
                
                max_term = binom_logsf_term(j_max, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                
                # Sum the terms of the binomial via the logsumexp trick. This
                # improves numerical stability by subtracting off the max term
                # from each term before exponentiation, then adding back the
                # max term at the end.
                sum_exp = 0
                for j in range(k + 1, n + 1):
                    term = binom_logsf_term(j, n, log_p, log1p_q,
                                            gammaln_n_plus_1)
                    sum_exp += exp(term - max_term)
            
                return log(sum_exp) + max_term
            
            def compute_obs(
                    const unsigned[::1] detection_count,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    unsigned[:, ::1] obs,
                    const unsigned num_threads):
                
                # obs[i, j] is the number of cells in which exactly one of the
                # two genes i and j is expressed. If we define:
                # - (1) as the number of cells where gene i is expressed
                # - (2) as the number of cells where gene j is expressed
                # - (3) as the number of cells where both genes i and j are
                #   expressed
                # then obs[i, j] = (1) + (2) - 2 * (3)
                cdef unsigned cell, gene_i, gene_j, \
                    num_genes = detection_count.shape[0], \
                    num_cells = indptr.shape[0] - 1
                cdef unsigned long i, j, i_start, i_end, j_end
                
                if num_threads == 1:
                    # Initialize the upper diagonal of obs to (1) + (2)
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            obs[i, j] = detection_count[i] + detection_count[j]
                    
                    # Now subtract off 2 * (3) for the upper diagonal: iterate
                    # over all pairs of genes i and j within each cell, and
                    # subtract 2 from `obs[i, j]` for each pair
                    for cell in range(num_cells):
                        for i in range(<unsigned long> indptr[cell],
                                       <unsigned long> indptr[cell + 1]):
                            for j in range(i + 1,
                                           <unsigned long> indptr[cell + 1]):
                                obs[indices[i], indices[j]] -= 2
                else:
                    with nogil:
                        for i in prange(num_genes, num_threads=num_threads):
                            for j in range(i + 1, num_genes):
                                obs[i, j] = \
                                    detection_count[i] + detection_count[j]
                        
                        for cell in prange(num_cells, num_threads=num_threads):
                            for i in range(<unsigned long> indptr[cell],
                                           <unsigned long> indptr[cell + 1]):
                                for j in range(i + 1,
                                               <unsigned long> indptr[cell + 1]):
                                    atomic_add(obs[indices[i], indices[j]], -2)
            
            def compute_S(const unsigned[:, ::1] obs,
                          const float[::1] p,
                          const unsigned num_cells,
                          float[:, ::1] S,
                          const unsigned num_threads):
                
                cdef unsigned num_genes = p.shape[0]
                cdef unsigned i, j
                
                if num_threads == 1:
                    for i in range(num_genes):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
                else:
                    for i in prange(num_genes, nogil=True,
                                    num_threads=num_threads):
                        for j in range(i + 1, num_genes):
                            S[i, j] = binom_logsf(
                                k=obs[i, j] - 1, n=num_cells,
                                p=p[i] * (1 - p[j]) + (1 - p[i]) * p[j])
            
            def compute_cxds(
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    const float[:, ::1] S,
                    float[::1] cxds_scores,
                    const unsigned num_threads):
                
                cdef unsigned cell, gene_i, gene_j, num_cells = indptr.shape[0] - 1
                cdef unsigned long i, j, i_start, i_end, j_end
                
                # Iterate over all pairs of genes i and j within each cell, and
                # subtract `S[i, j]` from the cell's cxds score for each pair
                if num_threads == 1:
                    for cell in range(num_cells):
                        cxds_scores[cell] = 0
                        for gene_i in range(<unsigned long> indptr[cell],
                                            <unsigned long> indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1,
                                                <unsigned long> indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
                else:
                    for cell in prange(num_cells, nogil=True,
                                       num_threads=num_threads):
                        cxds_scores[cell] = 0
                        for gene_i in range(<unsigned long> indptr[cell],
                                            <unsigned long> indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1,
                                                <unsigned long> indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_scores[cell] -= S[i, j]
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            def simulate_doublets(
                    const numeric[::1] data,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    signed_integer[::1] sim_indices,
                    signed_integer[::1] sim_indptr,
                    const unsigned num_cells,
                    const unsigned long seed):
                
                cdef unsigned long i, j, i_end, j_end, nnz = 0, state = srand(seed)
                cdef unsigned sim_index, cell_i, cell_j
                
                sim_indptr[0] = 0
                for sim_index in range(1, num_cells + 1):
                    cell_i = randint(num_cells, &state)
                    cell_j = randint(num_cells, &state)
                    i = indptr[cell_i]
                    i_end = indptr[cell_i + 1]
                    j = indptr[cell_j]
                    j_end = indptr[cell_j + 1]
                    while True:
                        if indices[i] == indices[j]:
                            sim_indices[nnz] = indices[i]
                            nnz = nnz + 1
                            i = i + 1
                            j = j + 1
                            if i == i_end or j == j_end:
                                break
                        elif indices[i] < indices[j]:
                            # `rand(&state) & 1` gives a random Boolean; only
                            # coin-flip if `data[i] == 1`
                            if data[i] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[i]
                                nnz = nnz + 1
                            i = i + 1
                            if i == i_end:
                                break
                        else:
                            if data[j] > 1 or rand(&state) & 1:
                                sim_indices[nnz] = indices[j]
                                nnz = nnz + 1
                            j = j + 1
                            if j == j_end:
                                break
                    sim_indptr[sim_index] = nnz
        ''', warn_undeclared=False)
        compute_obs = cython_functions['compute_obs']
        compute_S = cython_functions['compute_S']
        compute_cxds = cython_functions['compute_cxds']
        simulate_doublets = cython_functions['simulate_doublets']
        
        # If `batch_column` was specified, get the row indices of each batch,
        # ignoring cells failing QC when `QC_column` is present in `obs`. If
        # no batches were specified but `QC_column` is present, use `QC_column`
        # as the batch labels.
        if QC_column is not None and batch_column is None:
            batch_column = QC_column
        if batch_column is not None:
            batch_column_name = batch_column.name
            # noinspection PyTypeChecker
            batches = (batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      if QC_column is None else
                      batch_column
                      .to_frame()
                      .lazy()
                      .with_columns(
                          _SingleCell_batch_indices=pl.int_range(
                              pl.len(), dtype=pl.Int32))
                      .filter(QC_column))\
                .group_by(batch_column_name, maintain_order=True)\
                .agg('_SingleCell_batch_indices')\
                .select('_SingleCell_batch_indices')\
                .collect()\
                .to_series()
        else:
            batches = None,
        
        # Preallocate
        if batch_column is not None:
            singlets = np.ones(X.shape[0], dtype=bool)
            doublet_scores = np.empty(X.shape[0], dtype=np.float32)
        obs = np.empty((num_genes, num_genes), dtype=np.uint32)
        S = np.empty((num_genes, num_genes), dtype=np.float32)
        
        # noinspection PyUnresolvedReferences
        original_num_threads = X._num_threads
        try:
            X._num_threads = num_threads
            # For each batch...
            for batch_index, batch_indices in enumerate(batches):
                # Subset to cells in this batch
                X_batch = X[batch_indices] if batch_column is not None else X
                if X_batch.shape[0] == 1:
                    continue
                
                # Get the detection count of each gene
                with Timer('getnnz'):
                    detection_count = \
                        getnnz(X_batch, axis=0, num_threads=num_threads)
                
                # Subset to the `num_genes` genes with detection rates closest
                # to 50%. Exclude genes with detection rates of 0% or 100%.
                with Timer('hvg_indices and subsetting'):
                    num_cells = X_batch.shape[0]
                    p = np.divide(detection_count, num_cells, dtype=np.float32)
                    hvg_indices = \
                        np.argsort(p * (1 - p), kind='stable')[-num_genes:]
                    least_variable_p = p[hvg_indices[-num_genes]]
                    if least_variable_p == 0 or least_variable_p == 1:
                        hvg_p = p[hvg_indices]
                        hvg_indices = hvg_indices[(hvg_p > 0) & (hvg_p < 1)]
                    hvg_indices.sort()
                    detection_count = detection_count[hvg_indices]
                    p = p[hvg_indices]
                    X_batch = X_batch[:, hvg_indices]
                
                # Convert `X_batch` to CSR, if CSC
                if not is_csr:
                    X_batch = X_batch.tocsr()
                
                # Sort indices, if not already sorted (necessary for
                # `simulate_doublets`)
                if not X.has_sorted_indices:
                    X_batch.sort_indices()
                
                # Get `obs`, where `obs[i, j]` is the number of cells that
                # express exactly one of genes `i` and `j`
                compute_obs(detection_count=detection_count,
                            indices=X_batch.indices, indptr=X_batch.indptr,
                            obs=obs, num_threads=num_threads)
                    
                # Get `S`, the upper-tail log binomial p-values of `obs`
                compute_S(obs=obs, p=p, num_cells=num_cells, S=S,
                          num_threads=num_threads)
                    
                # Calculate each cell's cxds score: the sum of `-S[i, j]`
                # across all gene pairs `i` and `j` that are both expressed by
                # the cell
                cxds_scores_batch = np.empty(num_cells, dtype=np.float32)
                compute_cxds(indices=X_batch.indices, indptr=X_batch.indptr,
                             S=S, cxds_scores=cxds_scores_batch,
                             num_threads=num_threads)
                
                # Now simulate doublets within the batch and compute their cxds
                # scores, using the original `S` matrix derived from the real
                # data. Conservatively allocate twice as much memory for the
                # indices as the original indices, since in the worst-case
                # scenario none of the indices will match up and all coinflips
                # will be 1.
                sim_indptr = np.empty_like(X_batch.indptr)
                sim_indices = np.empty(2 * len(X_batch.indices),
                                       dtype=X_batch.indices.dtype)
                simulate_doublets(data=X_batch.data, indices=X_batch.indices,
                                  indptr=X_batch.indptr,
                                  sim_indices=sim_indices,
                                  sim_indptr=sim_indptr, num_cells=num_cells,
                                  seed=batch_index)
                sim_indices = sim_indices[:sim_indptr[-1]]
                cxds_scores_sim = np.empty(num_cells, dtype=np.float32)
                compute_cxds(indices=sim_indices, indptr=sim_indptr, S=S,
                             cxds_scores=cxds_scores_sim,
                             num_threads=num_threads)
                   
                # Call doublets
                if doublet_fraction is None:
                    # Call doublets based on whether their cxds score is
                    # above the median cxds score for simulated doublets
                    singlets_batch = \
                        cxds_scores_batch < np.median(cxds_scores_sim)
                else:
                    # Call a fixed fraction of cells as doublets
                    rank = rankdata(cxds_scores_batch, method='ordinal')
                    singlets_batch = \
                        rank <= (1 - doublet_fraction) * len(rank)
            
                # If using multiple batches, map doublet labels and scores back
                # to the full dataset
                if batch_column is not None:
                    # noinspection PyUnboundLocalVariable
                    singlets[batch_indices] = singlets_batch
                    if return_scores:
                        # noinspection PyUnboundLocalVariable
                        doublet_scores[batch_indices] = cxds_scores_batch
                else:
                    singlets = singlets_batch
                    if return_scores:
                        doublet_scores = cxds_scores_batch
        finally:
            X._num_threads = original_num_threads
        if return_scores:
            return singlets, doublet_scores
        else:
            return singlets
    
    def find_doublets_old(self,
                          batch_column: SingleCellColumn | None,
                          *,
                          QC_column: SingleCellColumn | None = 'passed_QC',
                          doublet_fraction: float | np.floating | None = None,
                          num_genes: int | np.integer = 500,
                          doublet_column: str = 'doublet',
                          doublet_score_column: str = 'doublet_score',
                          overwrite: bool = False,
                          num_threads: int | np.integer | None = None):
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507).
        
        The standard way to filter out doublets is by specifying
        `remove_doublets=True` in `qc()`. If you did that, do not use this
        function! This function should only be used in the unusual scenario
        where you want to find doublets without running any other
        quality-control steps.
        
        This function gives the same result regardless of whether it is run
        before or after normalization. The actual expression value does not
        matter, only whether or not it is zero.
        
        Doublets cannot occur across sequencing batches, so make sure to
        specify `batch_column` if your dataset has multiple batches! Doublet
        detection will be done independently within each batch.
        
        Since the cxds score is continuous, it needs to be converted into a
        binary classification of doublets versus non-doublets. This problem can
        be framed as finding a cxds score threshold above which a cell is
        deemed to be a doublet. To determine this threshold, we simulate
        doublets by combining the counts from randomly selected pairs of cells,
        via the following steps:
        
        1) Sample as many random pairs of cells (with replacement) as there are
           real cells.
        2) Combine the counts from each pair of cells into a simulated doublet.
           Because cxds operates on binarized count matrices, we average the
           two cells' count matrices in a binary sense: if a gene is expressed
           in either cell, it is deemed to be expressed in the simulated
           doublet, but if it has a count of 1 in one cell and 0 in the other,
           it is randomly chosen to be either expressed or not expressed with
           equal probability (since the average count would be 0.5).
        3) Calculate cxds scores for these simulated doublets, based on the
           coexpression patterns (the `S` matrix from cxds) learned from the
           real data.
        4) Take the median cxds score of the simulated doublets as the
           threshold. In other words, if a real cell has a higher doublet score
           than the average simulated doublet, we call it a doublet.
           
        Alternatively, specify `doublet_fraction` to force a specific fraction
        of cells to be classified as doublets.
        
        Args:
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described above.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            doublet_column: the name of a Boolean column to be added to `obs`
                            containing the doublet labels, i.e. whether each
                            cell is predicted to be a doublet
            doublet_score_column: the name of a column to be added to `obs`
                                  containing each cell's doublet score. Higher
                                  scores indicate greater likelihood of being a
                                  doublet. Scores are not normalized and are
                                  not comparable across datasets or batches,
                                  but are guaranteed to be positive (since they
                                  are sums of log p-values).
            overwrite: if `True`, overwrite `doublet_column` and/or
                       `doublet_score_column` if already present in `obs`,
                       instead of raising an error
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.

        Returns:
            A new SingleCell dataset where `var` contains two additional
            columns, `doublet_column` (default: `doublet`), indicating whether
            each cell is predicted to be a doublet, and `doublet_score_column`
            (default: `'doublet_score'`), containing each cell's doublet score.
        
        Note:
            This function's cxds scores are almost exactly half the original
            implementation's, because it avoids double-counting the two genes
            in each gene pair. Slight deviations from this one-half (usually
            by less than one part in a million) may occur because this function
            uses a normal approximation to the binomial p-value to avoid long
            runtimes on large datasets.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `doublet_column` and `doublet_score_column` are strings
        # and, unless `overwrite=True`, not already in `obs`
        for column, column_name in (
                (doublet_column, 'doublet_column'),
                (doublet_score_column, 'doublet_score_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column of obs; '
                    f'did you already run find_doublets()? Set overwrite=True '
                    f'to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so doublet finding is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_doublets()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column`, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if batch_column is not None:
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column)
        
        # Check that `doublet_fraction`, if specified, is > 0 and < 1
        if doublet_fraction is not None:
            check_type(doublet_fraction, 'doublet_fraction', float,
                       'a number greater than 0 and less than 1')
            check_bounds(doublet_fraction, 'doublet_fraction', 0, 1,
                         left_open=True, right_open=True)
        
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Run doublet detection
        singlets, doublet_scores = SingleCell._find_doublets_old(
            X=self._X, batch_column=batch_column, QC_column=QC_column,
            doublet_fraction=doublet_fraction, num_genes=num_genes,
            return_scores=True, num_threads=num_threads)
        
        # Convert doublet labels and scores to polars Series to add to `obs`.
        # If `QC_column` exists, set doublet labels and scores to `null` for
        # cells failing QC.
        doublet_column = pl.Series(doublet_column, ~singlets)
        doublet_score_column = pl.Series(doublet_score_column, doublet_scores)
        if QC_column is None:
            return self.with_columns_obs(doublet_column, doublet_score_column)
        else:
            return self.with_columns_obs(
                pl.when(QC_column).then(doublet_column),
                pl.when(QC_column).then(doublet_score_column))
    
    # noinspection PyTypeChecker
    @staticmethod
    def _find_doublets(X: csr_array | csc_array,
                       batch_column: SingleCellColumn | None,
                       QC_column: SingleCellColumn | None,
                       doublet_fraction: float | np.floating | None,
                       num_genes: int | np.integer,
                       return_scores: bool,
                       num_threads: int | np.integer | None) -> \
                np.ndarray[1, np.dtype[np.bool_]] | \
                tuple[np.ndarray[1, np.dtype[np.bool_]],
                      np.ndarray[1, np.dtype[np.float32]]]:
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507). Used by
        `qc()` (when `remove_doublets=True`) and `find_doublets()`.

        Args:
            X: the count matrix; may be either normalized or unnormalized
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described in
                              `find_doublets()`.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.

        Returns:
            A NumPy array with the binary doublet calls (`True` for singlets,
            `False` for doublets), or a tuple of two arrays with the doublet
            calls and doublet scores when `return_scores=True`. Scores will be
            uninitialized for cells not in any batch.
        """
        # Define Cython functions
        is_csr = isinstance(X, csr_array)
        cython_functions = cython_inline(_thread_offset_import + _heap_functions + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libc.float cimport FLT_MAX
        from libcpp.algorithm cimport nth_element, sort
        from libcpp.cmath cimport abs, erfc, exp, floor, log, log1p, sqrt
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double

        cdef extern from * nogil:
            """
            #define atomic_add(x, y) _Pragma("omp atomic") x += y
            """
            void atomic_add(unsigned &x, unsigned y)

        cdef double log_one_half = -0.6931471805599453
        cdef double log_sqrt_2_pi = 0.91893853320467274
        cdef double one_over_sqrt_2 = 0.70710678118654752
        cdef double one_over_sqrt_pi = 0.5641895835477563

        cdef double[5] gamma_A = [
            8.11614167470508450300E-4, -5.95061904284301438324E-4,
            7.93650340457716943945E-4, -2.77777777730099687205E-3,
            8.33333333333331927722E-2]

        cdef double[6] gamma_B = [
            -1.37825152569120859100E3, -3.88016315134637840924E4,
            -3.31612992738871184744E5, -1.16237097492762307383E6,
            -1.72173700820839662146E6, -8.53555664245765465627E5]

        cdef double[6] gamma_C = [
            -3.51815701436523470549E2, -1.70642106651881159223E4,
            -2.20528590553854454839E5, -1.13933444367982507207E6,
            -2.53252307177582951285E6, -2.01889141433532773231E6]

        cdef double[6] P = [
            0.5641895835477550741253201704,
            1.275366644729965952479585264,
            5.019049726784267463450058,
            6.1602098531096305440906,
            7.409740605964741794425,
            2.97886562639399288862]

        cdef double[6] Q = [
            2.260528520767326969591866945,
            9.396034016235054150430579648,
            12.0489519278551290360340491,
            17.08144074746600431571095,
            9.608965327192787870698,
            3.3690752069827527677]

        cdef inline double p1evl(const double x,
                                 const double* coef,
                                 const unsigned N) noexcept nogil:
            cdef double ans
            cdef unsigned i

            ans = x + coef[0]
            for i in range(1, N):
                ans = ans * x + coef[i]
            return ans

        cdef inline double polevl(const double x,
                                  const double* coef,
                                  const unsigned N) noexcept nogil:
            cdef double ans
            cdef unsigned i

            ans = coef[0]
            for i in range(1, N):
                ans = ans * x + coef[i]
            return ans

        cdef inline double log_erfc(const double x) noexcept nogil:
            # Based on GSL's gsl_sf_log_erfc_e at
            # github.com/ampl/gsl/blob/master/specfunc/erfc.c#L306

            cdef double y, series

            if x * x < 0.02460783300575925:
                y = x * one_over_sqrt_pi
                series = 0.00048204
                series = y * series - 0.00142906
                series = y * series + 0.0013200243174
                series = y * series + 0.0009461589032
                series = y * series - 0.0045563339802
                series = y * series + 0.00556964649138
                series = y * series + 0.00125993961762116
                series = y * series - 0.01621575378835404
                series = y * series + 0.02629651521057465
                series = y * series - 0.001829764677455021
                series = y * series - 0.09439510239319526
                series = y * series + 0.28613578213673563
                series = y * series + 1
                series = y * series + 1
                return -2 * y * series
            elif x > 8:
                return log(polevl(x, &P[0], 6) / p1evl(x, &Q[0], 6)) - \
                    x * x
            else:
                return log(erfc(x))

        cdef inline double gammaln(double x) noexcept nogil:
            # Simplified from github.com/scipy/scipy/blob/main/scipy/
            # special/xsf/cephes/gamma.h, based on the knowledge that `x`
            # will always be positive and finite when calculating terms in
            # the binomial distribution

            cdef double p, q, u, z

            if x < 13:
                z = 1
                p = 0
                u = x
                while u >= 3:
                    p -= 1
                    u = x + p
                    z *= u
                while u < 2:
                    z /= u
                    p += 1
                    u = x + p
                z = abs(z)
                if u == 2:
                    return log(z)
                p -= 2
                x = x + p
                p = x * polevl(x, &gamma_B[0], 6) / \
                    p1evl(x, &gamma_C[0], 6)
                return log(z) + p
            elif x >= 1000:
                q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                if x > 1e8:
                    return q
                p = 1.0 / (x * x)
                p = ((7.9365079365079365079365e-4 * p -
                      2.7777777777777777777778e-3) *
                     p + 0.0833333333333333333333) / x
                return q + p
            else:
                q = (x - 0.5) * log(x) - x + log_sqrt_2_pi
                p = 1.0 / (x * x)
                return q + polevl(p, &gamma_A[0], 5) / x

        cdef inline double binom_logsf_term(
                const unsigned j,
                const unsigned n,
                const double log_p,
                const double log1p_q,
                const double gammaln_n_plus_1) noexcept nogil:
            return gammaln_n_plus_1 - gammaln(j + 1) - \
                gammaln(n - j + 1) + j * log_p + (n - j) * log1p_q

        cdef inline double binom_logsf(const unsigned k,
                                       const unsigned n,
                                       const double p) noexcept nogil:
            cdef unsigned j, j_max
            cdef double mu, sigma, z
            cdef double sum_exp, max_term, term, log_p, log1p_q, \
                gammaln_n_plus_1

            # Use the normal approximation when n * p and n * (1 - p) are
            # both greater than 500; add 0.5 for continuity correction
            if n * p > 500 and n * (1 - p) > 500:
                mu = n * p
                sigma = sqrt(mu * (1 - p))
                z = (k + 0.5 - mu) / sigma
                return log_erfc(z * one_over_sqrt_2) + log_one_half

            # Otherwise, compute the exact binomial p-value
            log_p = log(p)
            log1p_q = log1p(-p)
            gammaln_n_plus_1 = gammaln(n + 1)

            # Find `j_max`, the value of `j` with the largest binomial term
            # (the `term` variabel below). For a binomial distribution, the
            # mode (maximum probability) occurs at `floor((n + 1) * p)`.
            # However, since we are summing from `k + 1` to `n`, not 0 to
            # `n`, we need to ensure `j_max` is at least `k + 1`.
            j_max = <unsigned> floor((n + 1) * p)
            if j_max <= k:
                j_max = k + 1

            max_term = binom_logsf_term(j_max, n, log_p, log1p_q,
                                        gammaln_n_plus_1)

            # Sum the terms of the binomial via the logsumexp trick. This
            # improves numerical stability by subtracting off the max term
            # from each term before exponentiation, then adding back the
            # max term at the end.
            sum_exp = 0
            for j in range(k + 1, n + 1):
                term = binom_logsf_term(j, n, log_p, log1p_q,
                                        gammaln_n_plus_1)
                sum_exp += exp(term - max_term)

            return log(sum_exp) + max_term

        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))

        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state

        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
                    
        cdef extern from * nogil:
            """
            struct CompareGreater {
                const float* data;
                CompareGreater() noexcept {}
                CompareGreater(const float* d) noexcept : data(d) {}
                bool operator()(unsigned a, unsigned b) const noexcept {
                    return data[a] > data[b];
                }
            };
            """
            cdef cppclass CompareGreater:
                CompareGreater(const float*) noexcept
                bint operator()(unsigned, unsigned) noexcept
        
        def get_hvgs(unsigned[::1] all_detection_counts,
                     unsigned[::1] detection_counts,
                     float[::1] ps,
                     unsigned[::1] hvgs,
                     float[::1] distances,
                     const unsigned num_cells,
                     const unsigned num_total_genes,
                     unsigned num_genes):
            cdef unsigned gene, detection_count, i
            cdef float p, distance, inverse_num_cells = 1.0 / num_cells
            
            # Normalize `detection_count` by `num_cells` to get the detection
            # rate `p`. Get the `num_genes` most variable genes (those with
            # detection rates closest to 50%), using a min-heap to keep track
            # of the `num_genes` smallest "distances" from 50%.
            for gene in range(num_genes):
                distances[gene] = FLT_MAX
            for gene in range(num_total_genes):
                detection_count = all_detection_counts[gene]
                p = detection_count * inverse_num_cells
                distance = abs(p - 0.5)
                if distance < distances[0]:
                    max_heap_replace_top(&hvgs[0], &distances[0], gene, distance,
                                         num_genes)
            
            # Exclude genes with detection rates of 0% or 100%, in the rare case
            # that these make it into the top `num_genes` genes
            while distances[0] == 0.5:
                max_heap_pop(&hvgs[0], &distances[0], num_genes)
                num_genes -= 1
                if num_genes == 0:
                    error_message = (
                        'all genes are present in either 0% or 100% of cells')
                    raise ValueError(error_message)
            
            # Sort the indices of the highly variable genes
            sort(&hvgs[0], &hvgs[0] + num_genes)
            
            # Populate `detection_counts` and `ps` for the highly variable genes
            for i in range(num_genes):
                gene = hvgs[i]
                detection_count = all_detection_counts[gene]
                detection_counts[i] = detection_count
                ps[i] = detection_count * inverse_num_cells
            
            # Return the number of genes (same as the input `num_genes` except in the
            # rare case mentioned above)
            return num_genes

        def compute_obs(const unsigned[::1] detection_counts,
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        unsigned[:, ::1] obs,
                        unsigned num_threads):

            # obs[i, j] is the number of cells in which exactly one of the
            # two genes i and j is expressed. If we define:
            # - (1) as the number of cells where gene i is expressed
            # - (2) as the number of cells where gene j is expressed
            # - (3) as the number of cells where both genes i and j are
            #   expressed
            # then obs[i, j] = (1) + (2) - 2 * (3)
            cdef unsigned detection_counts_i, cell, gene_i, gene_j, thread_index, \
                num_genes = detection_counts.shape[0], \
                num_cells = indptr.shape[0] - 1
            cdef unsigned long i, j, i_start, i_end, j_end
            cdef pair[unsigned, unsigned] row_range
            cdef signed_integer indices_i

            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                # Initialize the upper diagonal of obs to (1) + (2)
                for i in range(num_genes):
                    detection_counts_i = detection_counts[i]
                    for j in range(i + 1, num_genes):
                        obs[i, j] = detection_counts_i + detection_counts[j]

                # Now subtract off 2 * (3) for the upper diagonal: iterate
                # over all pairs of genes i and j within each cell, and
                # subtract 2 from `obs[i, j]` for each pair
                for cell in range(num_cells):
                    for i in range(<unsigned long> indptr[cell],
                                   <unsigned long> indptr[cell + 1]):
                        indices_i = indices[i]
                        for j in range(i + 1,
                                       <unsigned long> indptr[cell + 1]):
                            obs[indices_i, indices[j]] -= 2
            else:
                with nogil:
                    for i in prange(num_genes, num_threads=num_threads):
                        detection_counts_i = detection_counts[i]
                        for j in range(i + 1, num_genes):
                            obs[i, j] = detection_counts_i + detection_counts[j]

                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        row_range = get_thread_offset(indptr, thread_index, num_threads)
                        for cell in range(row_range.first, row_range.second):
                            for i in range(<unsigned long> indptr[cell],
                                           <unsigned long> indptr[cell + 1]):
                                indices_i = indices[i]
                                for j in range(i + 1,
                                               <unsigned long> indptr[cell + 1]):
                                    atomic_add(obs[indices_i, indices[j]], -2)

        def compute_S(const unsigned[:, ::1] obs,
                      const float[::1] ps,
                      const unsigned num_cells,
                      float[:, ::1] S,
                      const unsigned num_threads):

            cdef unsigned num_genes = ps.shape[0]
            cdef unsigned i, j
            cdef float ps_i

            if num_threads == 1:
                for i in range(num_genes):
                    ps_i = ps[i]
                    for j in range(i + 1, num_genes):
                        S[i, j] = binom_logsf(
                            k=obs[i, j] - 1, n=num_cells,
                            p=ps_i * (1 - ps[j]) + (1 - ps_i) * ps[j])
            else:
                for i in prange(num_genes, nogil=True,
                                num_threads=num_threads):
                    ps_i = ps[i]
                    for j in range(i + 1, num_genes):
                        S[i, j] = binom_logsf(
                            k=obs[i, j] - 1, n=num_cells,
                            p=ps_i * (1 - ps[j]) + (1 - ps_i) * ps[j])

        def compute_cxds(
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const float[:, ::1] S,
                float[::1] cxds_scores,
                unsigned num_threads):

            cdef unsigned cell, gene_i, gene_j, thread_index, \
                num_cells = indptr.shape[0] - 1
            cdef unsigned long i, j, i_start, i_end, j_end
            cdef float cxds_score
            cdef pair[unsigned, unsigned] row_range

            # Iterate over all pairs of genes i and j within each cell, and
            # subtract `S[i, j]` from the cell's cxds score for each pair
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                for cell in range(num_cells):
                    cxds_score = 0
                    for gene_i in range(<unsigned long> indptr[cell],
                                        <unsigned long> indptr[cell + 1]):
                        i = indices[gene_i]
                        for gene_j in range(gene_i + 1,
                                            <unsigned long> indptr[cell + 1]):
                            j = indices[gene_j]
                            cxds_score = cxds_score - S[i, j]
                    cxds_scores[cell] = cxds_score
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(indptr, thread_index, num_threads)
                    for cell in range(row_range.first, row_range.second):
                        cxds_score = 0
                        for gene_i in range(<unsigned long> indptr[cell],
                                            <unsigned long> indptr[cell + 1]):
                            i = indices[gene_i]
                            for gene_j in range(gene_i + 1,
                                                <unsigned long> indptr[cell + 1]):
                                j = indices[gene_j]
                                cxds_score = cxds_score - S[i, j]
                        cxds_scores[cell] = cxds_score
        
        def simulate_doublets(
                const numeric[::1] data,
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                signed_integer[::1] sim_indices,
                signed_integer[::1] sim_indptr,
                const unsigned num_cells,
                const unsigned long seed):

            cdef unsigned long i, j, i_end, j_end, nnz = 0, state = srand(seed)
            cdef unsigned sim_index, cell_i, cell_j

            sim_indptr[0] = 0
            for sim_index in range(1, num_cells + 1):
                cell_i = randint(num_cells, &state)
                cell_j = randint(num_cells, &state)
                i = indptr[cell_i]
                i_end = indptr[cell_i + 1]
                j = indptr[cell_j]
                j_end = indptr[cell_j + 1]
                while True:
                    if indices[i] == indices[j]:
                        sim_indices[nnz] = indices[i]
                        nnz = nnz + 1
                        i = i + 1
                        j = j + 1
                        if i == i_end or j == j_end:
                            break
                    elif indices[i] < indices[j]:
                        # `rand(&state) & 1` gives a random Boolean; only
                        # coin-flip if `data[i] == 1`
                        if data[i] > 1 or rand(&state) & 1:
                            sim_indices[nnz] = indices[i]
                            nnz = nnz + 1
                        i = i + 1
                        if i == i_end:
                            break
                    else:
                        if data[j] > 1 or rand(&state) & 1:
                            sim_indices[nnz] = indices[j]
                            nnz = nnz + 1
                        j = j + 1
                        if j == j_end:
                            break
                sim_indptr[sim_index] = nnz
            
        def call_doublets(const float[::1] cxds_scores,
                          const float median_cxds_score_sim,
                          const unsigned[::1] batch_indices,
                          unsigned[::1] doublet_indices,
                          char[::1] singlets,
                          float[::1] doublet_scores,
                          const float doublet_fraction,
                          const unsigned num_threads):
            cdef unsigned i, num_doublets, num_cells = cxds_scores.shape[0], \
                parallel_threshold = max(10_000, num_threads)
            
            if doublet_fraction == -1:  # `doublet_fraction is None`
                # Call doublets based on whether their cxds score is
                # above the median cxds score for simulated doublets;
                # `doublet_indices` is unused
            
                if batch_indices.shape[0] == 0:  # `batch_column is None`
                    if num_threads == 1 or num_cells < parallel_threshold:
                        for i in range(num_cells):
                            singlets[i] = cxds_scores[i] < median_cxds_score_sim
                    else:
                        for i in prange(num_cells, nogil=True, num_threads=num_threads):
                            singlets[i] = cxds_scores[i] < median_cxds_score_sim
                else:  # `batch_column is not None`
                    # Same as when `batch_column is None`, but use `batch_indices[i]`
                    # instead of `i` on the left, and also assign `doublet_scores`
                    
                    if doublet_scores.shape[0] == 0:  # `not return_scores`
                        if num_threads == 1 or num_cells < parallel_threshold:
                            for i in range(num_cells):
                                singlets[batch_indices[i]] = cxds_scores[i] < median_cxds_score_sim
                        else:
                            for i in prange(num_cells, nogil=True, num_threads=num_threads):
                                singlets[batch_indices[i]] = cxds_scores[i] < median_cxds_score_sim
                    else:  # `return scores`
                        if num_threads == 1 or num_cells < parallel_threshold:
                            for i in range(num_cells):
                                singlets[batch_indices[i]] = cxds_scores[i] < median_cxds_score_sim
                                doublet_scores[batch_indices[i]] = cxds_scores[i]
                        else:
                            for i in prange(num_cells, nogil=True, num_threads=num_threads):
                                singlets[batch_indices[i]] = cxds_scores[i] < median_cxds_score_sim
                                doublet_scores[batch_indices[i]] = cxds_scores[i]
            else:  # `doublet_fraction is not None`
                # Call a fixed fraction of cells as doublets;
                # `median_cxds_score_sim` is unused
                
                num_doublets = <unsigned>(num_cells * doublet_fraction)
                for i in range(num_cells):
                    doublet_indices[i] = i
                nth_element(&doublet_indices[0], &doublet_indices[0] + num_doublets,
                            &doublet_indices[0] + num_cells,
                            CompareGreater(&cxds_scores[0]))
                
                if batch_indices.shape[0] == 0:  # `batch_column is None`
                    if num_threads == 1 or num_doublets < parallel_threshold:
                        for i in range(num_doublets):
                            singlets[doublet_indices[i]] = False
                    else:
                        for i in prange(num_doublets, nogil=True,
                                        num_threads=num_threads):
                            singlets[doublet_indices[i]] = False
                    if num_threads == 1 or num_cells - num_doublets < parallel_threshold:
                        for i in range(num_doublets, num_cells):
                            singlets[doublet_indices[i]] = True
                    else:
                        for i in prange(num_doublets, num_cells, nogil=True,
                                        num_threads=num_threads):
                            singlets[doublet_indices[i]] = True
                else:  # `batch_column is not None`
                    # Same as when `batch_column is None`, but use
                    # `batch_indices[doublet_indices[i]]` instead of
                    # `doublet_indices[i]` and `batch_indices[i]` instead of `i` on the
                    # left
                    
                    if num_threads == 1 or num_doublets < parallel_threshold:
                        for i in range(num_doublets):
                            singlets[batch_indices[doublet_indices[i]]] = False
                    else:
                        for i in prange(num_doublets, nogil=True,
                                        num_threads=num_threads):
                            singlets[batch_indices[doublet_indices[i]]] = False
                    if num_threads == 1 or num_cells - num_doublets < parallel_threshold:
                        for i in range(num_doublets, num_cells):
                            singlets[batch_indices[doublet_indices[i]]] = True
                    else:
                        for i in prange(num_doublets, num_cells, nogil=True,
                                        num_threads=num_threads):
                            singlets[batch_indices[doublet_indices[i]]] = True
                    if doublet_scores.shape[0] > 0:  # `return_scores`
                        if num_threads == 1 or num_cells < parallel_threshold:
                            for i in range(num_cells):
                                doublet_scores[batch_indices[i]] = cxds_scores[i]
                        else:
                            for i in prange(num_cells, nogil=True,
                                            num_threads=num_threads):
                                doublet_scores[batch_indices[i]] = cxds_scores[i]
        ''', warn_undeclared=False)
        get_hvgs = cython_functions['get_hvgs']
        compute_obs = cython_functions['compute_obs']
        compute_S = cython_functions['compute_S']
        compute_cxds = cython_functions['compute_cxds']
        simulate_doublets = cython_functions['simulate_doublets']
        call_doublets = cython_functions['call_doublets']
        
        # If `batch_column` was specified, get the row indices of each batch,
        # ignoring cells failing QC when `QC_column` is present in `obs`. If
        # no batches were specified but `QC_column` is present, use `QC_column`
        # as the batch labels.
        if QC_column is not None and batch_column is None:
            batch_column = QC_column
        if batch_column is not None:
            batch_column_name = batch_column.name
            # noinspection PyTypeChecker
            batches = (batch_column
                       .to_frame()
                       .lazy()
                       .with_columns(_SingleCell_batch_indices=pl.int_range(
                           pl.len(), dtype=pl.UInt32))
                       if QC_column is None else
                       batch_column
                       .to_frame()
                       .lazy()
                       .with_columns(
                           _SingleCell_batch_indices=pl.int_range(
                               pl.len(), dtype=pl.UInt32))
                       .filter(QC_column)) \
                .group_by(batch_column_name, maintain_order=True) \
                .agg('_SingleCell_batch_indices') \
                .select('_SingleCell_batch_indices') \
                .collect() \
                .to_series()
        else:
            batches = pl.Series([], dtype=pl.UInt32),
        
        # Preallocate
        singlets = np.empty(X.shape[0], dtype=bool)
        doublet_scores = np.empty(X.shape[0] if return_scores else 0,
                                  dtype=np.float32)
        max_cells = batches.list.len().max() \
            if batch_column is not None else X.shape[0]
        if doublet_fraction is not None:
            doublet_indices = np.empty(max_cells, dtype=np.uint32)
        else:
            # Set `doublet_fraction` to `-1` if `None`, so it can be passed to
            # Cython as a float
            doublet_fraction = -1
            doublet_indices = np.array([], dtype=np.uint32)
        num_total_genes = X.shape[1]
        all_detection_counts = np.empty(num_total_genes, dtype=np.uint32)
        detection_counts = np.empty(num_genes, dtype=np.uint32)
        ps = np.empty(num_genes, dtype=np.float32)
        hvgs = np.empty(num_genes, dtype=np.uint32)
        distances = np.empty(num_genes, dtype=np.float32)
        obs_buffer = np.empty(num_genes * num_genes, dtype=np.uint32)
        S_buffer = np.empty(num_genes * num_genes, dtype=np.float32)
        cxds_scores_buffer = np.empty(max_cells, dtype=np.float32)
        sim_indptr_buffer = np.empty(max_cells + 1, dtype=X.indptr.dtype)
        cxds_scores_sim_buffer = np.empty(max_cells, dtype=np.float32)
        
        # noinspection PyUnresolvedReferences
        original_num_threads = X._num_threads
        try:
            X._num_threads = num_threads
            # For each batch...
            for batch_index, batch_indices in enumerate(batches):
                # Subset to cells in this batch
                if batch_column is None:
                    X_batch = X
                else:
                    if len(batch_indices) == 1:
                        continue
                    X_batch = X[batch_indices]
                
                # Get the detection count of each gene
                getnnz(X_batch, axis=0, num_threads=num_threads,
                       nnz=all_detection_counts)
                    
                # Normalize `detection_counts` by `num_cells` to get the
                # detection rate `p`. Subset to the `num_genes` genes with
                # detection rates closest to 50%. Exclude genes with detection
                # rates of 0% or 100%.
                num_cells = X_batch.shape[0]
                
                batch_num_genes = get_hvgs(
                    all_detection_counts=all_detection_counts,
                    detection_counts=detection_counts, ps=ps, hvgs=hvgs,
                    distances=distances, num_cells=num_cells,
                    num_total_genes=num_total_genes, num_genes=num_genes)
                detection_counts = detection_counts[:batch_num_genes]
                ps = ps[:batch_num_genes]
                X_batch = X_batch[:, hvgs[:batch_num_genes]]
                
                # Convert `X_batch` to CSR, if CSC
                if not is_csr:
                    X_batch = X_batch.tocsr()
                
                # Sort indices, if not already sorted (necessary for
                # `simulate_doublets`)
                if not X.has_sorted_indices:
                    X_batch.sort_indices()
                
                # Get `obs`, where `obs[i, j]` is the number of cells that
                # express exactly one of genes `i` and `j`
                obs = obs_buffer[:batch_num_genes * batch_num_genes]\
                    .reshape((batch_num_genes, batch_num_genes))
                compute_obs(detection_counts=detection_counts,
                            indices=X_batch.indices, indptr=X_batch.indptr,
                            obs=obs, num_threads=num_threads)
                
                # Get `S`, the upper-tail log binomial p-values of `obs`
                S = S_buffer[:batch_num_genes * batch_num_genes]\
                    .reshape((batch_num_genes, batch_num_genes))
                compute_S(obs=obs, ps=ps, num_cells=num_cells, S=S,
                          num_threads=num_threads)
                
                # Calculate each cell's cxds score: the sum of `-S[i, j]`
                # across all gene pairs `i` and `j` that are both expressed by
                # the cell
                cxds_scores = cxds_scores_buffer[:num_cells]
                compute_cxds(indices=X_batch.indices, indptr=X_batch.indptr,
                             S=S, cxds_scores=cxds_scores,
                             num_threads=num_threads)
                
                # Now simulate doublets within the batch and compute their cxds
                # scores, using the original `S` matrix derived from the real
                # data. Conservatively allocate twice as much memory for the
                # indices as the original indices, since in the worst-case
                # scenario none of the indices will match up and all coinflips
                # will be 1.
                sim_indptr = sim_indptr_buffer[:num_cells + 1]
                sim_indices = np.empty(2 * len(X_batch.indices),
                                       dtype=X_batch.indices.dtype)
                simulate_doublets(data=X_batch.data, indices=X_batch.indices,
                                  indptr=X_batch.indptr,
                                  sim_indices=sim_indices,
                                  sim_indptr=sim_indptr, num_cells=num_cells,
                                  seed=batch_index)
                sim_indices = sim_indices[:sim_indptr[-1]]
                cxds_scores_sim = cxds_scores_sim_buffer[:num_cells]
                compute_cxds(indices=sim_indices, indptr=sim_indptr, S=S,
                             cxds_scores=cxds_scores_sim,
                             num_threads=num_threads)
                
                # Call doublets. If using multiple batches, map doublet labels
                # (and scores, if `return_scores`) back to the full dataset.
                # noinspection PyUnresolvedReferences
                call_doublets(cxds_scores=cxds_scores,
                              median_cxds_score_sim=np.median(cxds_scores_sim)
                                  if doublet_fraction == -1 else 0,
                              doublet_indices=doublet_indices,
                              batch_indices=batch_indices.to_numpy(),
                              singlets=singlets,
                              doublet_scores=doublet_scores,
                              doublet_fraction=doublet_fraction,
                              num_threads=num_threads)
        finally:
            X._num_threads = original_num_threads
        if return_scores:
            if batch_column is None:
                # noinspection PyUnboundLocalVariable
                doublet_scores = cxds_scores
            return singlets, doublet_scores
        else:
            return singlets
    
    def find_doublets(self,
                      batch_column: SingleCellColumn | None,
                      *,
                      QC_column: SingleCellColumn | None = 'passed_QC',
                      doublet_fraction: float | np.floating | None = None,
                      num_genes: int | np.integer = 500,
                      doublet_column: str = 'doublet',
                      doublet_score_column: str | None = 'doublet_score',
                      overwrite: bool = False,
                      num_threads: int | np.integer | None = None):
        """
        Find doublets using cxds (co-expression-based doublet scoring;
        academic.oup.com/bioinformatics/article/36/4/1150/5566507).

        The standard way to filter out doublets is by specifying
        `remove_doublets=True` in `qc()`. If you did that, do not use this
        function! This function should only be used in the unusual scenario
        where you want to find doublets without running any other
        quality-control steps.

        This function gives the same result regardless of whether it is run
        before or after normalization. The actual expression value does not
        matter, only whether or not it is zero.

        Doublets cannot occur across sequencing batches, so make sure to
        specify `batch_column` if your dataset has multiple batches! Doublet
        detection will be done independently within each batch.

        Since the cxds score is continuous, it needs to be converted into a
        binary classification of doublets versus non-doublets. This problem can
        be framed as finding a cxds score threshold above which a cell is
        deemed to be a doublet. To determine this threshold, we simulate
        doublets by combining the counts from randomly selected pairs of cells,
        via the following steps:

        1) Sample as many random pairs of cells (with replacement) as there are
           real cells.
        2) Combine the counts from each pair of cells into a simulated doublet.
           Because cxds operates on binarized count matrices, we average the
           two cells' count matrices in a binary sense: if a gene is expressed
           in either cell, it is deemed to be expressed in the simulated
           doublet, but if it has a count of 1 in one cell and 0 in the other,
           it is randomly chosen to be either expressed or not expressed with
           equal probability (since the average count would be 0.5).
        3) Calculate cxds scores for these simulated doublets, based on the
           coexpression patterns (the `S` matrix from cxds) learned from the
           real data.
        4) Take the median cxds score of the simulated doublets as the
           threshold. In other words, if a real cell has a higher doublet score
           than the average simulated doublet, we call it a doublet.

        Alternatively, specify `doublet_fraction` to force a specific fraction
        of cells to be classified as doublets.

        Args:
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Doublet detection will be
                          performed separately for each batch. Set to `None` if
                          all cells belong to the same sequencing batch.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their doublet labels and doublet scores set to
                       `null`.
            doublet_fraction: an optional fraction of cells (within each batch,
                              if `batch_column` is specified) to be classified
                              as doublets. If `None`, automatically detect the
                              threshold via the approach described above.
            num_genes: the number of highly variable genes, i.e. genes
                       expressed in as close to 50% of cells as possible, to
                       use during doublet detection. This parameter usually has
                       a minimal influence on accuracy as long as it is
                       sufficiently large (in the hundreds), so increasing it
                       further will mainly just increase runtime. If
                       `num_genes` is greater than the number of genes in the
                       dataset, all genes will be used.
            doublet_column: the name of a Boolean column to be added to `obs`
                            containing the doublet labels, i.e. whether each
                            cell is predicted to be a doublet
            doublet_score_column: the name of a column to be added to `obs`
                                  containing each cell's doublet score. Higher
                                  scores indicate greater likelihood of being a
                                  doublet. Scores are not normalized and are
                                  not comparable across datasets or batches,
                                  but are guaranteed to be positive (since they
                                  are sums of log p-values). Set
                                  `doublet_score_column=None` to not return
                                  doublet scores, for a slight memory reduction
                                  and speed increase.
            overwrite: if `True`, overwrite `doublet_column` and/or
                       `doublet_score_column` if already present in `obs`,
                       instead of raising an error
            num_threads: the number of threads to use when finding doublets.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.

        Returns:
            A new SingleCell dataset where `var` contains two additional
            columns, `doublet_column` (default: `doublet`), indicating whether
            each cell is predicted to be a doublet, and `doublet_score_column`
            (default: `'doublet_score'`), containing each cell's doublet score.

        Note:
            This function's cxds scores are almost exactly half the original
            implementation's, because it avoids double-counting the two genes
            in each gene pair. Slight deviations from this one-half (usually
            by less than one part in a million) may occur because this function
            uses a normal approximation to the binomial p-value to avoid long
            runtimes on large datasets.

        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        """
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `doublet_column` and (if not `None`)
        # `doublet_score_column` are strings and, unless `overwrite=True`, not
        # already in `obs`
        check_type(doublet_column, 'doublet_column', str, 'a string')
        if not overwrite and doublet_column in self._obs:
            error_message = (
                f'doublet_column {doublet_column!r} is already a column of '
                f'obs; did you already run find_doublets()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        return_scores = doublet_score_column is not None
        if return_scores:
            check_type(doublet_score_column, 'doublet_score_column', str,
                       'a string')
            if not overwrite and doublet_score_column in self._obs:
                error_message = (
                    f'doublet_score_column {doublet_score_column!r} is '
                    f'already a column of obs; did you already run '
                    f'find_doublets()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so doublet finding is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_doublets()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column`, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if batch_column is not None:
            batch_column = self._get_column(
                'obs', batch_column, 'batch_column',
                (pl.String, pl.Categorical, pl.Enum, 'integer'),
                QC_column=QC_column)
        
        # Check that `doublet_fraction`, if specified, is > 0 and < 1
        if doublet_fraction is not None:
            check_type(doublet_fraction, 'doublet_fraction', float,
                       'a number greater than 0 and less than 1')
            check_bounds(doublet_fraction, 'doublet_fraction', 0, 1,
                         left_open=True, right_open=True)
        
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Run doublet detection
        singlets = SingleCell._find_doublets(
            X=self._X, batch_column=batch_column, QC_column=QC_column,
            doublet_fraction=doublet_fraction, num_genes=num_genes,
            return_scores=return_scores, num_threads=num_threads)
        
        # Convert doublet labels (and scores, if `doublet_score_column` is not
        # `None`) to polars Series to add to `obs`. If `QC_column` exists, set
        # doublet labels (and scores) to `null` for cells failing QC.
        if not return_scores:
            doublet_column = pl.Series(doublet_column, ~singlets)
            if QC_column is None:
                return self.with_columns_obs(doublet_column)
            else:
                return self.with_columns_obs(
                    pl.when(QC_column).then(doublet_column))
        else:
            singlets, doublet_scores = singlets
            doublet_column = pl.Series(doublet_column, ~singlets)
            doublet_score_column = \
                pl.Series(doublet_score_column, doublet_scores)
            if QC_column is None:
                return self.with_columns_obs(doublet_column,
                                             doublet_score_column)
            else:
                return self.with_columns_obs(
                    pl.when(QC_column).then(doublet_column),
                    pl.when(QC_column).then(doublet_score_column))
    
    def make_obs_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make `obs_names` unique by appending `'-1'` to the second occurence of
        a given name, `'-2'` to the third occurrence, and so on, where `'-'`
        can be switched to a different string via the `separator` argument.
        Raises an error if any `obs_names` already contain `separator`.
        
        Args:
            separator: the string connecting the original name and the integer
                       suffix

        Returns:
            A new SingleCell dataset with the `obs_names` made unique.
        """
        check_type(separator, 'separator', str, 'a string')
        if self.obs_names.str.contains(separator).any():
            error_message = (
                f'some obs_names already contain the separator {separator!r}; '
                f'did you already run make_obs_names_unique()? If not, set '
                f'the separator argument to a different string.')
            raise ValueError(error_message)
        obs_names = pl.col(self.obs_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(obs_names)
        return SingleCell(X=self._X,
                          obs=self._obs.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(obs_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(obs_names)),
                          var=self._var, obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp, varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def make_var_names_unique(self, separator: str = '-') -> SingleCell:
        """
        Make `var_names` unique by appending `'-1'` to the second occurence of
        a given name, `'-2'` to the third occurrence, and so on, where `'-'`
        can be switched to a different string via the `separator` argument.
        Raises an error if any `var_names` already contain `separator`.
        
        Args:
            separator: the string connecting the original name and the integer
                       suffix

        Returns:
            A new SingleCell dataset with the `var_names` made unique.
        """
        var_names = pl.col(self.var_names.name)
        num_times_seen = pl.int_range(pl.len(), dtype=pl.Int32).over(var_names)
        return SingleCell(X=self._X,
                          obs=self._obs,
                          var=self._var.with_columns(
                              pl.when(num_times_seen > 0)
                              .then(var_names + separator +
                                    num_times_seen.cast(str))
                              .otherwise(var_names)),
                          obsm=self._obsm, varm=self._varm, obsp=self._obsp,
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def get_sample_covariates(self,
                              ID_column: SingleCellColumn,
                              *,
                              QC_column: SingleCellColumn |
                                         None = 'passed_QC') -> pl.DataFrame:
        """
        Get a DataFrame of sample-level covariates, i.e. the columns of `obs`
        that are the same for all cells within each sample.
        
        Args:
            ID_column: a column of `obs` containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
        
        Returns:
            A DataFrame of the sample-level covariates, with ID_column (sorted)
            as the first column.
        """
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column)
        ID_column_name = ID_column.name
        obs = self._obs
        if QC_column is not None:
            obs = obs.filter(QC_column)
            ID_column = ID_column.filter(QC_column)
        return obs\
            .select(ID_column,
                    *obs
                    .group_by(ID_column)
                    .n_unique()
                    .pipe(filter_columns,
                          (pl.exclude(ID_column_name)
                           if ID_column_name in obs else pl.all()).max().eq(1))
                    .columns)\
            .unique(ID_column_name)\
            .sort(ID_column_name)
    
    def pseudobulk(self,
                   ID_column: SingleCellColumn,
                   cell_type_column: SingleCellColumn,
                   *,
                   QC_column: SingleCellColumn | None = 'passed_QC',
                   additional_obs: pl.DataFrame | None = None,
                   include_nulls: bool = False,
                   sort_genes: bool = False,
                   num_threads: int | np.integer | None = None,
                   verbose: bool = True) -> Pseudobulk:
        """
        Pseudobulk a SingleCell dataset with sample ID and cell type columns.
        
        Operates on raw counts, so cannot be run after `normalize()`. Must be
        run after `qc()`.
        
        Counts from cells with the same pair of values in `ID_column` and
        `cell_type_column` will be summed to a single value. Cells with `null`
        in either column are excluded, unless `include_nulls=True`.
        
        You can run this function multiple times at different cell type
        resolutions by setting a different `cell_type_column` each time, then
        combining the results afterwards with the `|` operator (assuming none
        of the cell types overlap between the two resolutions):
        
        ```
        pb_broad = sc.pseudobulk('ID', 'broad_cell_type')
        pb_fine = sc.pseudobulk('ID', 'fine_grained_cell_type')
        pb = pb_broad | pb_fine
        ```
        
        Args:
            ID_column: a column of `obs` containing sample IDs. Can be a column
                       name, a polars expression, a polars Series, a 1D NumPy
                       array, or a function that takes in this SingleCell
                       dataset and returns a polars Series or 1D NumPy array.
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be excluded
                       from the pseudobulk.
            additional_obs: an optional DataFrame of additional sample-level
                            covariates, which will be joined to the
                            pseudobulk's `obs` for each cell type
            include_nulls: whether to exclude cells with `null` values in
                           `ID_column` and/or `cell_type_column` from the
                           pseudobulk. If `include_nulls=True`, `null` will be
                           treated just like any other value. This means that,
                           for instance, all cells from a given cell type that
                           have `null` as the sample ID will be pseudobulked
                           together, as will all cells from a given sample ID
                           that have `null` as the cell type.
            sort_genes: whether to sort genes in alphabetical order in the
                        pseudobulk; by default, genes appear in the same order
                        as in the SingleCell dataset
            num_threads: the number of threads to use when pseudobulking;
                         parallelism happens across {sample, cell type} pairs
                         (or just samples, if `cell_type_column` is `None`).
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. For count matrices stored in
                         the usual CSR format, parallelization takes place
                         across cell types and samples, so specifying more
                         cores than the number of cell type-sample pairs may
                         not improve performance. Does not affect the
                         Pseudobulk dataset's `num_threads`; this will always
                         be the same as the SingleCell dataset's `num_threads`.
            verbose: whether to print the number of cells excluded when
                     `include_nulls=False`
        
        Returns:
            A Pseudobulk dataset with `X` (the pseudobulked counts), `obs`
            (metadata per sample), and `var` (metadata per gene) fields, each
            of which are dictionaries across cell types. The columns of each
            cell type's `obs` will be:
            - `ID_column`
            - `'num_cells'` (the number of cells for that sample and cell type)
            followed by whichever columns of the SingleCell dataset's `obs` are
            constant across samples. `var` will be identical to the SingleCell
            dataset's `var`.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        # Check that `X` is present
        X = self._X
        if X is None:
            error_message = 'X is None, so pseudobulking is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed and not normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "pseudobulk()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = (
                "uns['normalized'] is True; did you already run normalize()?")
            raise ValueError(error_message)
        
        # Check inputs
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        original_ID_column = ID_column
        ID_column = self._get_column('obs', ID_column, 'ID_column',
                                     (pl.String, pl.Categorical, pl.Enum,
                                      'integer'), QC_column=QC_column,
                                     allow_null=True)
        ID_column_name = ID_column.name
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column,
                             allow_null=True)
        cell_type_column_name = cell_type_column.name
        num_cells_column_name = 'num_cells'
        for column_description, column_name in ('ID_column', ID_column_name), \
                ('cell_type_column', cell_type_column_name):
            if column_name == num_cells_column_name:
                error_message = (
                    f'{column_description} has the name '
                    f'{num_cells_column_name!r}, which conflicts with the '
                    f'name of the column to be added to the Pseudobulk '
                    f'dataset containing the number of cells of each cell '
                    f'type')
                raise ValueError(error_message)
        check_type(include_nulls, 'include_nulls', bool, 'Boolean')
        check_type(sort_genes, 'sort_genes', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        if additional_obs is not None:
            check_type(additional_obs, 'additional_obs', pl.DataFrame,
                       'a polars DataFrame')
            if ID_column_name not in additional_obs:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f'{ID_column_description} is not a column of '
                    f'additional_obs')
                raise ValueError(error_message)
            if ID_column.dtype != additional_obs[ID_column_name].dtype:
                ID_column_description = SingleCell._describe_column(
                    'ID_column', original_ID_column)
                error_message = (
                    f"{ID_column_description} has a different data type in "
                    f"additional_obs than in this SingleCell dataset's obs")
                raise TypeError(error_message)
        
        # Check that the first column of `var` is String, Enum, Categorical,
        # or integer: this is a requirement of the Pseudobulk class. (The first
        # column of `obs` must be as well, but this will always be true by
        # construction, since it will always be the sample ID.)
        if self.var_names.dtype not in (pl.Categorical, pl.Enum, pl.String) \
                and self.var_names.dtype not in pl.INTEGER_DTYPES:
            error_message = (
                f'the first column of var (var_names) has data type '
                f'{self.var_names.dtype!r}, but must be String, Enum, '
                f'Categorical, or integer')
            raise ValueError(error_message)
        
        # Get the row indices that will be pseudobulked across for each group
        # (cell type-sample pair), ignoring cells failing QC when `QC_column`
        # is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (pl.LazyFrame((cell_type_column, ID_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, ID_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name, ID_column_name)\
            .agg('_SingleCell_group_indices',
                 pl.len().alias(num_cells_column_name))\
            .sort(cell_type_column_name, ID_column_name)\
            .collect()
        
        # Exclude cells with null values in `ID_column` and/or
        # `cell_type_column`, if `include_nulls=False`
        if not include_nulls:
            if verbose:
                excluded = groups\
                    .filter(pl.any_horizontal(pl.col(
                        cell_type_column_name, ID_column_name).is_null()))
                num_ID_null_only = excluded\
                    .filter(pl.col(ID_column_name).is_null(),
                            pl.col(cell_type_column_name).is_not_null())\
                    [num_cells_column_name]\
                    .sum()
                num_cell_type_null_only = excluded\
                    .filter(pl.col(ID_column_name).is_not_null(),
                            pl.col(cell_type_column_name).is_null())\
                    [num_cells_column_name]\
                    .sum()
                num_excluded = excluded[num_cells_column_name].sum()
                num_both_null = \
                    num_excluded - num_ID_null_only - num_cell_type_null_only
                if num_excluded > 0:
                    print(f'Excluding {num_excluded:,} '
                          f'{plural("cell", num_excluded)} when '
                          f'pseudobulking: {num_both_null:,} with nulls in '
                          f'both the ID ({ID_column_name!r}) and cell-type '
                          f'({cell_type_column_name!r}) columns, '
                          f'{num_ID_null_only:,} with nulls in just the ID '
                          f'column, and {num_cell_type_null_only:,} with '
                          f'nulls in just the cell-type column.')
                    groups = groups.drop_nulls()
            else:
                groups = groups.drop_nulls()
            if len(groups) == 0:
                error_message = (
                    f'no cells remain after excluding cells with nulls in the '
                    f'the ID and/or cell-type columns')
                raise ValueError(error_message)
                
        # Pseudobulk, storing the result in a preallocated NumPy array
        result = np.empty((len(groups), X.shape[1]), dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = groups[num_cells_column_name].cum_sum().to_numpy()
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_sum_csr(
                        const numeric[::1] data,
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const unsigned[::1] group_indices,
                        const unsigned[::1] group_ends,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned group, cell, row, num_groups = group_ends.shape[0], \
                        num_genes = result.shape[1]
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type-sample pair)...
                        for group in range(num_groups):
                            # Initialize all elements of the group to 0
                            result[group, :] = 0
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse
                                # matrix
                                row = group_indices[cell]
                                
                                # For each gene (column) that's non-zero for
                                # this cell...
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    # Add the value at this cell and gene to
                                    # the total for this group and gene
                                    result[group, indices[gene]] += \
                                        <unsigned> data[gene]
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            result[group, :] = 0
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    result[group, indices[gene]] += \
                                        <unsigned> data[gene]
                ''', warn_undeclared=False)['groupby_sum_csr'](
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    group_indices=group_indices, group_ends=group_ends,
                    result=result, num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.UInt32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = group_map.null_count() > 0
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, threadid
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def groupby_sum_csc(
                const numeric[::1] data,
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const int[::1] group_map,
                const bint has_missing,
                unsigned[:, ::1] result,
                unsigned num_threads):
            cdef unsigned gene, thread_index, num_genes = result.shape[1]
            cdef int group
            cdef unsigned long cell
            cdef pair[unsigned, unsigned] col_range
            
            result[:] = 0
            num_threads = min(num_threads, num_genes)
            if num_threads == 1:
                if has_missing:
                    # For each gene (column of the sparse array)...
                    for gene in range(num_genes):
                        # For each cell (row) that's non-zero for this gene...
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            # Get the group index for this cell (-1 if it failed QC)
                            group = group_map[indices[cell]]
                            if group == -1:
                                continue
                            
                            # Add the value at this cell and gene to the total for this
                            # group and gene
                            result[group, gene] += <unsigned> data[cell]
                else:
                    for gene in range(num_genes):
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            group = group_map[indices[cell]]
                            result[group, gene] += <unsigned> data[cell]
            else:
                if has_missing:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for gene in range(col_range.first, col_range.second):
                            for cell in range(<unsigned long> indptr[gene],
                                              <unsigned long> indptr[gene + 1]):
                                group = group_map[indices[cell]]
                                if group == -1:
                                    continue
                                result[group, gene] += <unsigned> data[cell]
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for gene in range(col_range.first, col_range.second):
                            for cell in range(<unsigned long> indptr[gene],
                                              <unsigned long> indptr[gene + 1]):
                                group = group_map[indices[cell]]
                                result[group, gene] += <unsigned> data[cell]
                    ''', warn_undeclared=False)['groupby_sum_csc'](
                        data=X.data, indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=result, num_threads=num_threads)
        
        # Sort genes, if `sort_genes=True`
        cell_type_var = self._var
        if sort_genes:
            result = result[:, cell_type_var[:, 0].arg_sort().to_numpy()]
            cell_type_var = cell_type_var.sort(cell_type_var.columns[0])
        
        # Get sample covariates
        obs = self._obs
        if QC_column is not None:
            obs = obs.filter(QC_column)
            ID_column = ID_column.filter(QC_column)
        sample_covariates = obs\
            .select(ID_column,
                    *obs
                    .group_by(ID_column)
                    .n_unique()
                    .pipe(filter_columns,
                          (pl.exclude(ID_column_name)
                           if ID_column_name in obs else pl.all()).max().eq(1))
                    .columns)\
            .unique(ID_column_name)\
            .sort(ID_column_name)
        
        # Break up the results by cell type
        X, obs, var = {}, {}, {}
        start_index = 0
        for cell_type, count in groups[cell_type_column_name]\
                .value_counts().sort(cell_type_column_name).iter_rows():
            end_index = start_index + count
            X[cell_type] = result[start_index:end_index]
            obs[cell_type] = groups.lazy()\
                .select(ID_column_name, num_cells_column_name)\
                .slice(start_index, count)\
                .join(sample_covariates.lazy(), on=ID_column_name, how='left')\
                .pipe(lambda df: df.join(additional_obs.lazy(),
                                         on=ID_column_name, how='left')
                      if additional_obs is not None else df)\
                .pipe(lambda df: df if QC_column is None else
                                 df.drop(QC_column.name))\
                .collect()
            var[cell_type] = cell_type_var
            start_index = end_index
        
        # Propagate the SingleCell dataset's `num_threads` to the Pseudobulk
        # dataset, unless there's no free-threading, in which case set the
        # Pseudobulk's `num_threads` to 1.
        try:
            # noinspection PyUnresolvedReferences
            has_GIL = sys._is_gil_enabled()
        except AttributeError:
            has_GIL = True
        num_threads = 1 if has_GIL else self._num_threads
        
        return Pseudobulk(X=X, obs=obs, var=var, num_threads=num_threads)
    
    def hvg(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            batch_column: SingleCellColumn | None |
                          Sequence[SingleCellColumn | None] = None,
            num_genes: int | np.integer = 2000,
            min_cells: int | np.integer = 3,
            exclude: str | Sequence[str] | None = None,
            flavor: Literal['seurat_v3', 'seurat_v3_paper'] = 'seurat_v3',
            span: int | float | np.integer | np.floating = 0.3,
            hvg_column: str = 'highly_variable',
            rank_column: str = 'highly_variable_rank',
            allow_float: bool = False,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Select highly variable genes using the same approach as Seurat.
        
        Operates on raw counts, so must be run before `normalize()` (but after
        `qc()`). When run with multiple datasets, only considers genes present
        in every dataset.
        
        By default, uses the same approach as Seurat's `FindVariableFeatures()`
        function, and Scanpy's `scanpy.pp.highly_variable_genes()` function
        with the `flavor` argument set to the non-default value `'seurat_v3'`.

        Requires the scikit-misc package; install with:
        pip install --no-deps --no-build-isolation scikit-misc
        
        The general idea is that since genes with higher mean expression tend
        to have higher variance in expression (because they have more non-zero
        values), we want to select genes that have a high variance *relative to
        their mean expression*. Otherwise, we'd only be picking highly
        expressed genes! To correct for the mean-variance relationship, fit a
        LOESS curve fit to the mean-variance trend.
        
        Args:
            others: optional SingleCell datasets to jointly compute highly
                    variable genes across, alongside this one. Each dataset
                    will be treated as a separate batch. If `batch_column` is
                    not `None`, each dataset AND each distinct value of
                    `batch_column` within each dataset will be treated as a
                    separate batch. Variances will be computed per batch and
                    then aggregated (see `flavor`) across batches.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
                       When `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `hvg()`, except that the
                          `min_cells` filter will always be calculated
                          per-dataset rather than per-batch. Variances will be
                          computed per batch and then aggregated (see `flavor`)
                          across batches. Set to `None` to treat each dataset
                          as having a single batch. When `others` is specified,
                          `batch_column` can be a length-`1 + len(others)`
                          sequence of columns, expressions, Series, functions,
                          or `None` for each dataset (for `self`, followed by
                          each dataset in `others`).
            num_genes: the number of highly variable genes to select. The
                       default of 2000 matches Seurat and Scanpy's recommended
                       value. Fewer than `num_genes` genes will be selected if
                       not enough genes have non-zero count in >= `min_cells`
                       cells (or when `min_cells` is `None`, if not enough
                       genes are present).
            min_cells: if not `None`, filter to genes detected (with non-zero
                       count) in >= this many cells in every dataset, before
                       calculating highly variable genes. The default value of
                       3 matches Seurat and Scanpy's recommended value. Note
                       that genes with zero variance in any dataset will always
                       be filtered out, even if `min_cells` is 0.
            exclude: one or more optional case-insensitive regular expressions
                     matching genes to exclude from the highly variable gene
                     calculation. For instance, to exclude mitochondrial genes
                     (starting with `'MT-'`) and ribosomal genes (starting with
                     `'RPL-'`, `'RPS'`, `'MRPL'`, or `'MRPS'`), specify
                     `exclude=('^MT-', '^RPL', '^RPS', '^MRPL', '^MRPS')`.
            flavor: the highly variable gene algorithm to use. Must be one of
                    `seurat_v3` and `seurat_v3_paper`, both of which match the
                    algorithms with the same name in Scanpy. Both algorithms
                    select genes based on two criteria: 1) which genes are
                    ranked as most variable (taking the median of the ranks
                    across batches where the gene is among the top `num_genes`
                    highly variable genes) and 2) the number of batches in
                    which a gene is ranked in among the top `num_genes` in
                    variability. `seurat_v3` ranks genes by 1) and uses 2) to
                    tiebreak, whereas `seurat_v3_paper` ranks genes by 2) and
                    uses 1) to tiebreak. When there is only one batch, both
                    algorithms are the same and only rank based on 1).
            span: the span of the LOESS fit; higher values will lead to more
                  smoothing
            hvg_column: the name of a Boolean column to be added to (each
                        dataset's) `var` indicating the highly variable genes
            rank_column: the name of an integer column to be added to (each
                         dataset's) `var` with the rank of each highly variable
                         gene's variance (1 = highest variance, 2 =
                         next-highest, etc.); will be `null` for non-highly
                         variable genes. In the very unlikely event of ties,
                         the gene that appears first in `var` will get the
                         lowest rank.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            overwrite: if `True`, overwrite `hvg_column` and/or `rank_column`
                       if already present in `var`, instead of raising an error
            verbose: whether to print the number of genes present in every
                     dataset, when jointly computing highly variable genes
                     across multiple datasets
            num_threads: the number of threads to use when finding highly
                         variable genes. Set `num_threads=-1` to use all
                         available cores, as determined by `os.cpu_count()`, or
                         leave unset to use `self.num_threads` cores. Does not
                         affect the `num_threads` of the returned SingleCell
                         dataset(s); this will always be the same as the
                         `num_threads` of the original dataset(s).
        
        Returns:
            A new SingleCell dataset where `var` contains an additional Boolean
            column, `hvg_column` (default: `'highly_variable'`), indicating the
            `num_genes` most highly variable genes, and `rank_column` (default:
            'highly_variable_rank') indicating the (one-based) rank of each
            highly variable gene's variance, with `null` values for non-highly
            variable genes. Or, if additional SingleCell dataset(s) are
            specified via the `others` argument, a length-`1 + len(others)`
            tuple of SingleCell datasets with these two columns added: `self`,
            followed by each dataset in `others`.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        
        Note:
            This function may not give identical results to Seurat and Scanpy.
            It avoids floating-point summation, which is more numerically
            stable than Scanpy and Seurat's calculations. If multiple genes are
            tied as the `num_genes`-th most highly variable gene in a batch or
            dataset, this function includes all of them, whereas Seurat and
            Scanpy arbitrarily pick one (or a subset) of them. Also, this
            function uses the ordering from a stable sort to break ties when
            selecting the final list of highly variable genes, instead of the
            unstable sort used by Seurat and Scanpy.
        """
        # noinspection PyUnresolvedReferences
        from skmisc.loess import loess
        
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `hvg_column` and `rank_column` are strings and, unless
        # `overwrite=True`, not already in `var` for any dataset
        suffix = ' for at least one dataset' if others else ''
        for column, column_name in (hvg_column, 'hvg_column'), \
                (rank_column, 'rank_column'):
            check_type(column, column_name, str, 'a string')
            if not overwrite and \
                    any(column in dataset._var for dataset in datasets):
                error_message = (
                    f'{column_name} {column!r} is already a column of '
                    f'var{suffix}; did you already run hvg()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present for every dataset
        if any(dataset._X is None for dataset in datasets):
            error_message = (
                f'X is None{suffix}, so highly variable gene finding is not '
                f'possible')
            raise ValueError(error_message)
        
        # Check that all datasets are QCed and not normalized
        if not all(dataset._uns['QCed'] for dataset in datasets):
            error_message = (
                f"uns['QCed'] is False{suffix}; did you forget to run qc() "
                f"before hvg()? Set uns['QCed'] = True or run "
                f"with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if any(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"hvg() requires raw counts but uns['normalized'] is "
                f"True{suffix}; did you already run normalize()?")
            raise ValueError(error_message)
        
        # Check that there are at least 3 cells in each dataset (since LOESS
        # seems to need at least three observations to converge)
        if any(len(dataset._obs) < 3 for dataset in datasets):
            error_message = (
                f'there are fewer than 3 cells{suffix}, so highly variable '
                f'genes cannot be calculated')
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column` from every dataset, if not `None`
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        
        # Check that `num_genes` is a positive integer
        check_type(num_genes, 'num_genes', int, 'a positive integer')
        check_bounds(num_genes, 'num_genes', 1)
        
        # Check that `min_cells` is a positive integer and at least as large as
        # the number of cells in each dataset
        check_type(min_cells, 'min_cells', int, 'a non-negative integer')
        check_bounds(min_cells, 'min_cells', 0)
        if any(len(dataset._obs) < min_cells for dataset in datasets):
            suffix = ' for at least one dataset' if others else ''
            error_message = (
                f'the number of cells in this dataset ({len(self._obs):,}) '
                f'is less than min_cells ({min_cells:,}){suffix}; increase '
                f'min_cells')
            raise ValueError(error_message)
        
        # Check that `exclude`, if specified, is a string or sequence thereof
        if exclude is not None:
            exclude = to_tuple_checked(exclude, 'exclude', str, 'strings')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # If `allow_float=False`, raise an error if `X` is floating-point
        # for any dataset
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float:
            for dataset in datasets:
                X = dataset._X
                if np.issubdtype(X.dtype, np.floating):
                    error_message = (
                        f'hvg() requires raw counts but X has data type '
                        f'{str(X.dtype)!r}, a floating-point data type. If '
                        f'you are sure that all values are raw integer '
                        f'counts, i.e. that (X.data == X.data.astype(int))'
                        f'.all(), then set allow_float=True.')
                    raise TypeError(error_message)
        
        # Get the universe of genes we'll be considering: those present in all
        # datasets, and not matching `exclude` (if specified). If there are
        # multiple datasets, also get the indices of these genes in each
        # dataset (with `null` for genes not present in that particular
        # dataset). Raise an error if no genes are found in all datasets.
        if others:
            # The use of `align_frames` here is a bit wasteful memory-wise,
            # because it creates an identical `'gene'` column for every
            # DataFrame in `genes_and_indices`. Fortunately, it's only one
            # small string column per dataset.
            genes_and_indices = pl.align_frames(*(
                dataset.var[:, 0]
                .to_frame('gene')
                .pipe(lambda df: df.filter(~pl.col.gene.str.contains(
                    '(?i)' + '|'.join(exclude)))  # case-insensitive
                    if exclude is not None else df)
                .with_columns(index=pl.int_range(pl.len(), dtype=pl.UInt32))
                for dataset in datasets), on='gene', how='inner')
            # noinspection PyTypeChecker
            genes_in_all_datasets = genes_and_indices[0]['gene']
            # noinspection PyTypeChecker
            num_genes_in_all_datasets = len(genes_in_all_datasets)
            if num_genes_in_all_datasets == 0:
                error_message = 'no genes are present in every dataset'
                if exclude is not None:
                    error_message += (
                        f', after applying the '
                        f'{plural("filter", len(exclude))} specified via the '
                        f'exclude argument')
                raise ValueError(error_message)
            if verbose:
                print(f'{num_genes_in_all_datasets:,} '
                      f'{plural("gene", num_genes_in_all_datasets)} are '
                      f'present in every dataset.')
            # noinspection PyTypeChecker
            dataset_gene_indices = [df['index'] for df in genes_and_indices]
            del genes_and_indices
        else:
            genes_in_all_datasets = self.var_names\
                .rename('gene')\
                .to_frame()\
                .pipe(lambda df: df.filter(~pl.col.gene.str.contains(
                    '(?i)' + '|'.join(exclude)))  # case-insensitive
                    if exclude is not None else df)\
                .to_series()
        
        # Get the batches to calculate variance across (datasets + batches
        # within each dataset)
        if others:
            if batch_column is None:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X, QC_column.to_numpy()
                            if QC_column is not None else None, gene_indices)
                           for dataset, QC_column, gene_indices in
                           zip(datasets, QC_columns, dataset_gene_indices))
            else:
                # noinspection PyUnboundLocalVariable
                batches = ((dataset._X,
                            (batch_column.eq(batch) if QC_column is None else
                             batch_column.eq(batch) & QC_column).to_numpy()
                            if batch is not None else
                            (QC_column.to_numpy() if QC_column is not None else
                             None), gene_indices)
                           for dataset, QC_column, batch_column, gene_indices
                           in zip(datasets, QC_columns, batch_columns,
                                  dataset_gene_indices)
                           for batch in ((None,) if batch_column is None else
                                         batch_column.unique()))
        else:
            X = self._X
            batch_column = batch_columns[0]
            if batch_column is None:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = (X, QC_columns[0].to_numpy(), None),
                else:
                    batches = (X, None, None),
            else:
                if QC_column is not None and QC_columns[0] is not None:
                    batches = ((X, (batch_column.eq(batch) & QC_columns[0])
                                   .to_numpy(), None)
                               for batch in batch_column.unique())
                else:
                    batches = ((X, batch_column.eq(batch).to_numpy(), None)
                               for batch in batch_column.unique())
        
        # Define Cython functions
        cython_functions = cython_inline(_thread_offset_import + _uninitialized_vector_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.vector cimport vector
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def gene_mean_and_variance_csr(const numeric[::1] data,
                                       const signed_integer[::1] indices,
                                       const signed_integer[::1] indptr,
                                       const long[::1] cell_indices,
                                       const unsigned long num_cells,
                                       const unsigned num_dataset_genes,
                                       float[::1] mean,
                                       float[::1] var,
                                       unsigned[::1] nonzero_count,
                                       const unsigned num_threads):
                
            cdef unsigned gene, cell, thread_index
            cdef unsigned long num_elements, i, j, value, chunk_size, start, end, \
                total_sum, total_sum_of_squares, total_nonzero_count
            cdef float inv_num_cells = 1.0 / num_cells, \
                inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
            cdef vector[unsigned long] sum_buffer, sum_of_squares_buffer
            cdef vector[vector[unsigned long]] thread_sum, thread_sum_of_squares
            cdef vector[vector[unsigned]] thread_nonzero_counts
            cdef unsigned long[::1] sum, sum_of_squares
            
            if num_threads == 1:
                sum_buffer.resize(num_dataset_genes)
                sum = <unsigned long[:num_dataset_genes]> sum_buffer.data()
                sum_of_squares_buffer.resize(num_dataset_genes)
                sum_of_squares = <unsigned long[:num_dataset_genes]> \
                    sum_of_squares_buffer.data()
                nonzero_count[:] = 0
                if cell_indices.shape[0] == 0:
                    # Iterate over all elements of the count
                    # matrix, ignoring which cell they're from
                    num_elements = indices.shape[0]
                    for i in range(num_elements):
                        gene = indices[i]
                        value = <unsigned long> data[i]
                        sum[gene] += value
                        sum_of_squares[gene] += value * value
                        nonzero_count[gene] += 1
                else:
                    # Only iterate over the elements from cells
                    # in `cell_indices` (i.e. cells in this
                    # batch, and/or passing QC)
                    for j in range(num_cells):
                        cell = cell_indices[j]
                        for i in range(<unsigned long> indptr[cell],
                                       <unsigned long> indptr[cell + 1]):
                            gene = indices[i]
                            value = <unsigned long> data[i]
                            sum[gene] += value
                            sum_of_squares[gene] += value * value
                            nonzero_count[gene] += 1
                
                # Calculate means and variances from the sums
                # and squared sums
                for gene in range(num_dataset_genes):
                    mean[gene] = sum[gene] * inv_num_cells
                    var[gene] = inv_num_pairs_of_cells * (
                        num_cells * sum_of_squares[gene] -
                        sum[gene] * sum[gene])
            else:
                thread_sum.resize(num_threads)
                thread_sum_of_squares.resize(num_threads)
                thread_nonzero_counts.resize(num_threads)
                with nogil:
                    if cell_indices.shape[0] == 0:
                        # Partition the work by elements, not
                        # cells, for better load-balancing in
                        # case cells have substantially
                        # different library sizes
                        num_elements = indices.shape[0]
                        chunk_size = (num_elements + num_threads - 1) / num_threads
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_sum[thread_index].resize(num_dataset_genes)
                            thread_sum_of_squares[thread_index].resize(num_dataset_genes)
                            thread_nonzero_counts[thread_index].resize(num_dataset_genes)
                            
                            start = thread_index * chunk_size
                            end = min(start + chunk_size, num_elements)
                            for i in range(start, end):
                                gene = indices[i]
                                value = <unsigned long> data[i]
                                thread_sum[thread_index][gene] += value
                                thread_sum_of_squares[thread_index][gene] += \
                                    value * value
                                thread_nonzero_counts[thread_index][gene] += 1
                    else:
                        # Partition the work by cells
                        chunk_size = (num_cells + num_threads - 1) / num_threads
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_sum[thread_index].resize(num_dataset_genes)
                            thread_sum_of_squares[thread_index].resize(num_dataset_genes)
                            thread_nonzero_counts[thread_index].resize(num_dataset_genes)
    
                            start = thread_index * chunk_size
                            end = min(start + chunk_size, num_cells)
                            for j in range(start, end):
                                cell = cell_indices[j]
                                for i in range(<unsigned long> indptr[cell],
                                               <unsigned long> indptr[cell + 1]):
                                    gene = indices[i]
                                    value = <unsigned long> data[i]
                                    thread_sum[thread_index][gene] += value
                                    thread_sum_of_squares[thread_index][gene] += \
                                        value * value
                                    thread_nonzero_counts[thread_index][gene] += 1
                    
                    # Calculate means and variances by aggregating the
                    # sums and squared sums across threads
                    for gene in prange(num_dataset_genes, num_threads=num_threads):
                        total_sum = 0
                        total_sum_of_squares = 0
                        total_nonzero_count = 0
                        for thread_index in range(num_threads):
                            total_sum = total_sum + \
                                thread_sum[thread_index][gene]
                            total_sum_of_squares = total_sum_of_squares + \
                                thread_sum_of_squares[thread_index][gene]
                            total_nonzero_count = total_nonzero_count + \
                                thread_nonzero_counts[thread_index][gene]
                        mean[gene] = total_sum * inv_num_cells
                        var[gene] = inv_num_pairs_of_cells * (
                            num_cells * total_sum_of_squares -
                            total_sum * total_sum)
                        nonzero_count[gene] = total_nonzero_count
        
        def gene_mean_and_variance_csc(const numeric[::1] data,
                                       const signed_integer[::1] indices,
                                       const signed_integer[::1] indptr,
                                       char[::1] cell_mask,
                                       const unsigned long num_cells,
                                       const unsigned num_dataset_genes,
                                       float[::1] mean,
                                       float[::1] var,
                                       unsigned[::1] nonzero_count,
                                       unsigned num_threads):
            
            cdef unsigned thread_index
            cdef unsigned long i, value, sum, sum_of_squares, nnz, gene
            cdef float inv_num_cells = 1.0 / num_cells, \
                inv_num_pairs_of_cells = \
                1.0 / (num_cells * (num_cells - 1))
            cdef pair[unsigned, unsigned] col_range
            
            num_threads = min(num_threads, num_dataset_genes)
            if num_threads == 1:
                if cell_mask.shape[0] == 0:
                    for gene in range(num_dataset_genes):
                        # Calculate the sum and squared sum for
                        # this gene, across cells with non-zero
                        # counts for the gene
                        sum = 0
                        sum_of_squares = 0
                        nnz = 0
                        for i in range(<unsigned long> indptr[gene],
                                       <unsigned long> indptr[gene + 1]):
                            value = <unsigned long> data[i]
                            sum += value
                            sum_of_squares += value * value
                            nnz += 1
                        nonzero_count[gene] = nnz
                        # Calculate the mean and variance from the
                        # sum and squared sum
                        mean[gene] = sum * inv_num_cells
                        var[gene] = inv_num_pairs_of_cells * (
                            num_cells * sum_of_squares - sum * sum)
                else:
                    # Same as the version without a cell mask, but only include cells
                    # where `cell_mask` is `True`
                    for gene in range(num_dataset_genes):
                        sum = 0
                        sum_of_squares = 0
                        nnz = 0
                        for i in range(<unsigned long> indptr[gene],
                                       <unsigned long> indptr[gene + 1]):
                            if cell_mask[indices[i]]:
                                value = <unsigned long> data[i]
                                sum += value
                                sum_of_squares += value * value
                                nnz += 1
                        nonzero_count[gene] = nnz
                        mean[gene] = sum * inv_num_cells
                        var[gene] = inv_num_pairs_of_cells * (
                            num_cells * sum_of_squares - sum * sum)
            else:
                if cell_mask.shape[0] == 0:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for gene in range(col_range.first, col_range.second):
                            sum = 0
                            sum_of_squares = 0
                            nnz = 0
                            for i in range(<unsigned long> indptr[gene],
                                           <unsigned long> indptr[gene + 1]):
                                value = <unsigned long> data[i]
                                sum = sum + value
                                sum_of_squares = \
                                    sum_of_squares + value * value
                                nnz = nnz + 1
                            nonzero_count[gene] = nnz
                            mean[gene] = sum * inv_num_cells
                            var[gene] = inv_num_pairs_of_cells * (
                                num_cells * sum_of_squares - sum * sum)
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for gene in range(col_range.first, col_range.second):
                            sum = 0
                            sum_of_squares = 0
                            nnz = 0
                            for i in range(<unsigned long> indptr[gene],
                                           <unsigned long> indptr[gene + 1]):
                                if cell_mask[indices[i]]:
                                    value = <unsigned long> data[i]
                                    sum = sum + value
                                    sum_of_squares = \
                                        sum_of_squares + value * value
                                    nnz = nnz + 1
                            nonzero_count[gene] = nnz
                            mean[gene] = sum * inv_num_cells
                            var[gene] = inv_num_pairs_of_cells * (
                                num_cells * sum_of_squares - sum * sum)
        
        def clipped_sum_csr(const numeric[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            const unsigned num_cells,
                            const unsigned num_dataset_genes,
                            const long[::1] cell_indices,
                            const float[::1] clip_val,
                            float[::1] batch_counts_sum,
                            float[::1] squared_batch_counts_sum,
                            unsigned num_threads):
            cdef unsigned cell, gene, thread_index, chunk_size, start, end
            cdef unsigned long i, j, value, total_batch_counts_sum, \
                total_squared_batch_counts_sum, total_num_out_of_range
            cdef vector[unsigned long] batch_counts_sum_int_buffer, \
                squared_batch_counts_sum_int_buffer, num_out_of_range_buffer
            cdef vector[vector[unsigned long]] thread_batch_counts_sum_int, \
                thread_squared_batch_counts_sum_int, thread_num_out_of_range
            cdef pair[unsigned, unsigned] row_range
            cdef unsigned long[::1] batch_counts_sum_int, \
                squared_batch_counts_sum_int, num_out_of_range
            
            # Key insight: the things we're summing are integers
            # except when `value > clip_val[gene]`, in which case
            # we are adding a (floating-point) constant, so keep
            # track of this case separately and add it at the end,
            # to minimize floating-point error
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                batch_counts_sum_int_buffer.resize(num_dataset_genes)
                batch_counts_sum_int = <unsigned long[:num_dataset_genes]> \
                    batch_counts_sum_int_buffer.data()
                squared_batch_counts_sum_int_buffer.resize(num_dataset_genes)
                squared_batch_counts_sum_int = <unsigned long[:num_dataset_genes]> \
                    squared_batch_counts_sum_int_buffer.data()
                num_out_of_range_buffer.resize(num_dataset_genes)
                num_out_of_range = <unsigned long[:num_dataset_genes]> \
                    num_out_of_range_buffer.data()
                if cell_indices.shape[0] == 0:
                    num_cells = indptr.shape[0] - 1
                    for cell in range(num_cells):
                        for i in range(<unsigned long> indptr[cell],
                                       <unsigned long> indptr[cell + 1]):
                            gene = indices[i]
                            value = <unsigned long> data[i]
                            if value > clip_val[gene]:
                                num_out_of_range[gene] += 1
                            else:
                                batch_counts_sum_int[gene] += value
                                squared_batch_counts_sum_int[gene] += \
                                    value * value
                else:
                    for j in range(<unsigned long> cell_indices.shape[0]):
                        cell = cell_indices[j]
                        for i in range(<unsigned long> indptr[cell],
                                       <unsigned long> indptr[cell + 1]):
                            gene = indices[i]
                            value = <unsigned long> data[i]
                            if value > clip_val[gene]:
                                num_out_of_range[gene] += 1
                            else:
                                batch_counts_sum_int[gene] += value
                                squared_batch_counts_sum_int[gene] += \
                                    value * value
                for gene in range(num_dataset_genes):
                    batch_counts_sum[gene] = \
                        batch_counts_sum_int[gene] + \
                        num_out_of_range[gene] * clip_val[gene]
                    squared_batch_counts_sum[gene] = \
                        squared_batch_counts_sum_int[gene] + \
                        num_out_of_range[gene] * clip_val[gene] * \
                        clip_val[gene]
            else:
                thread_batch_counts_sum_int.resize(num_threads)
                thread_squared_batch_counts_sum_int.resize(num_threads)
                thread_num_out_of_range.resize(num_threads)
                with nogil:
                    if cell_indices.shape[0] == 0:
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_batch_counts_sum_int[thread_index].resize(
                                num_dataset_genes)
                            thread_squared_batch_counts_sum_int[thread_index].resize(
                                num_dataset_genes)
                            thread_num_out_of_range[thread_index].resize(
                                num_dataset_genes)
                            
                            row_range = get_thread_offset(indptr, thread_index,
                                                          num_threads)
                            for cell in range(row_range.first, row_range.second):
                                for i in range(<unsigned long> indptr[cell],
                                               <unsigned long> indptr[cell + 1]):
                                    gene = indices[i]
                                    value = <unsigned long> data[i]
                                    if value > clip_val[gene]:
                                        thread_num_out_of_range[
                                            thread_index][gene] += 1
                                    else:
                                        thread_batch_counts_sum_int[
                                            thread_index][gene] += value
                                        thread_squared_batch_counts_sum_int[
                                            thread_index][gene] += value * value
                    else:
                        chunk_size = (num_cells + num_threads - 1) / num_threads
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_batch_counts_sum_int[thread_index].resize(
                                num_dataset_genes)
                            thread_squared_batch_counts_sum_int[thread_index].resize(
                                num_dataset_genes)
                            thread_num_out_of_range[thread_index].resize(
                                num_dataset_genes)
                            
                            start = thread_index * chunk_size
                            end = min(start + chunk_size, num_cells)
                            for j in range(start, end):
                                cell = cell_indices[j]
                                for i in range(<unsigned long> indptr[cell],
                                               <unsigned long> indptr[cell + 1]):
                                    gene = indices[i]
                                    value = <unsigned long> data[i]
                                    if value > clip_val[gene]:
                                        thread_num_out_of_range[
                                            thread_index][gene] += 1
                                    else:
                                        thread_batch_counts_sum_int[
                                            thread_index][gene] += value
                                        thread_squared_batch_counts_sum_int[
                                            thread_index][gene] += value * value
                    
                    for gene in prange(num_dataset_genes,
                                       num_threads=num_threads):
                        total_batch_counts_sum = 0
                        total_squared_batch_counts_sum = 0
                        total_num_out_of_range = 0
                        for thread_index in range(num_threads):
                            total_batch_counts_sum = \
                                total_batch_counts_sum + \
                                thread_batch_counts_sum_int[thread_index][gene]
                            total_squared_batch_counts_sum = \
                                total_squared_batch_counts_sum + \
                                thread_squared_batch_counts_sum_int[thread_index][gene]
                            total_num_out_of_range = \
                                total_num_out_of_range + \
                                thread_num_out_of_range[thread_index][gene]
                        batch_counts_sum[gene] = \
                            total_batch_counts_sum + \
                            total_num_out_of_range * clip_val[gene]
                        squared_batch_counts_sum[gene] = \
                            total_squared_batch_counts_sum + \
                            total_num_out_of_range * \
                            clip_val[gene] * clip_val[gene]
        
        def clipped_sum_csc(const numeric[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            char[::1] cell_mask,
                            const float[::1] clip_val,
                            float[::1] batch_counts_sum,
                            float[::1] squared_batch_counts_sum,
                            unsigned num_threads):
            cdef unsigned cell, gene, thread_index, num_genes = indptr.shape[0] - 1
            cdef unsigned long i, value, batch_counts_sum_int, \
                squared_batch_counts_sum_int, num_out_of_range
            cdef pair[unsigned, unsigned] col_range
            
            num_threads = min(num_threads, num_genes)
            if num_threads == 1:
                if cell_mask.shape[0] == 0:
                    for gene in range(num_genes):
                        batch_counts_sum_int = 0
                        squared_batch_counts_sum_int = 0
                        num_out_of_range = 0
                        for i in range(<unsigned long> indptr[gene],
                                       <unsigned long> indptr[gene + 1]):
                            value = <unsigned long> data[i]
                            if value > clip_val[gene]:
                                num_out_of_range += 1
                            else:
                                batch_counts_sum_int += value
                                squared_batch_counts_sum_int += \
                                    value * value
                        batch_counts_sum[gene] = \
                            batch_counts_sum_int + \
                            num_out_of_range * clip_val[gene]
                        squared_batch_counts_sum[gene] = \
                            squared_batch_counts_sum_int + \
                            num_out_of_range * \
                            clip_val[gene] * clip_val[gene]
                else:
                    for gene in range(num_genes):
                        batch_counts_sum_int = 0
                        squared_batch_counts_sum_int = 0
                        num_out_of_range = 0
                        for i in range(<unsigned long> indptr[gene],
                                       <unsigned long> indptr[gene + 1]):
                            cell = indices[i]
                            if cell_mask[cell]:
                                value = <unsigned long> data[i]
                                if value > clip_val[gene]:
                                    num_out_of_range += 1
                                else:
                                    batch_counts_sum_int += value
                                    squared_batch_counts_sum_int += \
                                        value * value
                        batch_counts_sum[gene] = \
                            batch_counts_sum_int + \
                            num_out_of_range * clip_val[gene]
                        squared_batch_counts_sum[gene] = \
                            squared_batch_counts_sum_int + \
                            num_out_of_range * \
                            clip_val[gene] * clip_val[gene]
            else:
                if cell_mask.shape[0] == 0:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index,
                                                      num_threads)
                        for gene in range(col_range.first, col_range.second):
                            batch_counts_sum_int = 0
                            squared_batch_counts_sum_int = 0
                            num_out_of_range = 0
                            for i in range(<unsigned long> indptr[gene],
                                           <unsigned long> indptr[gene + 1]):
                                value = <unsigned long> data[i]
                                if value > clip_val[gene]:
                                    num_out_of_range = num_out_of_range + 1
                                else:
                                    batch_counts_sum_int = \
                                        batch_counts_sum_int + value
                                    squared_batch_counts_sum_int = \
                                        squared_batch_counts_sum_int + \
                                        value * value
                            batch_counts_sum[gene] = \
                                batch_counts_sum_int + \
                                num_out_of_range * clip_val[gene]
                            squared_batch_counts_sum[gene] = \
                                squared_batch_counts_sum_int + \
                                num_out_of_range * \
                                clip_val[gene] * clip_val[gene]
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index,
                                                      num_threads)
                        for gene in range(col_range.first, col_range.second):
                            batch_counts_sum_int = 0
                            squared_batch_counts_sum_int = 0
                            num_out_of_range = 0
                            for i in range(<unsigned long> indptr[gene],
                                           <unsigned long> indptr[gene + 1]):
                                cell = indices[i]
                                if cell_mask[cell]:
                                    value = <unsigned long> data[i]
                                    if value > clip_val[gene]:
                                        num_out_of_range = num_out_of_range + 1
                                    else:
                                        batch_counts_sum_int = \
                                            batch_counts_sum_int + value
                                        squared_batch_counts_sum_int = \
                                            squared_batch_counts_sum_int + \
                                            value * value
                            batch_counts_sum[gene] = \
                                batch_counts_sum_int + \
                                num_out_of_range * clip_val[gene]
                            squared_batch_counts_sum[gene] = \
                                squared_batch_counts_sum_int + \
                                num_out_of_range * \
                                clip_val[gene] * clip_val[gene]
        ''', warn_undeclared=False)
        gene_mean_and_variance_csr = \
            cython_functions['gene_mean_and_variance_csr']
        gene_mean_and_variance_csc = \
            cython_functions['gene_mean_and_variance_csc']
        clipped_sum_csr = cython_functions['clipped_sum_csr']
        clipped_sum_csc = cython_functions['clipped_sum_csc']
        
        # Get the variance of each gene in each batch across cells passing QC
        norm_gene_vars = []
        for X, cell_mask, gene_indices in batches:
            num_dataset_genes = X.shape[1]
            mean = np.empty(num_dataset_genes, dtype=np.float32)
            var = np.empty(num_dataset_genes, dtype=np.float32)
            nonzero_count = np.empty(num_dataset_genes, dtype=np.uint32)
            is_csr = isinstance(X, csr_array)
            if is_csr:
                if cell_mask is None:
                    cell_indices = np.array([], dtype=np.int64)
                    num_cells = X.shape[0]
                else:
                    cell_indices = np.flatnonzero(cell_mask)
                    num_cells = len(cell_indices)
                gene_mean_and_variance_csr(
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    cell_indices=cell_indices, num_cells=num_cells,
                    num_dataset_genes=num_dataset_genes, mean=mean, var=var,
                    nonzero_count=nonzero_count, num_threads=num_threads)
            else:
                if cell_mask is None:
                    cell_mask = np.array([], dtype=bool)
                    num_cells = X.shape[0]
                else:
                    num_cells = cell_mask.sum()
                gene_mean_and_variance_csc(
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    cell_mask=cell_mask, num_cells=num_cells,
                    num_dataset_genes=num_dataset_genes, mean=mean,
                    var=var, nonzero_count=nonzero_count,
                    num_threads=num_threads)
            
            not_constant = var > 0
            y = np.log10(var[not_constant])
            x = np.log10(mean[not_constant])
            model = loess(x, y, span=span)
            try:
                model.fit()
            except ValueError as e:
                error_message = (
                    f'LOESS model fitting failed; this is unusual and tends '
                    f'to only happen when there are very few cells'
                    f' (e.g. under 500), which '
                    f'{"is" if num_cells < 500 else "is not"} the case here')
                raise ValueError(error_message) from e
            
            estimated_variance = np.empty(num_dataset_genes, dtype=np.float32)
            estimated_variance[not_constant] = model.outputs.fitted_values
            estimated_variance[~not_constant] = 0
            estimated_stddev = np.sqrt(10 ** estimated_variance)
            clip_val = mean + estimated_stddev * np.sqrt(num_cells,
                                                         dtype=np.float32)
            batch_counts_sum = np.empty(num_dataset_genes, dtype=np.float32)
            squared_batch_counts_sum = np.empty(num_dataset_genes,
                                                dtype=np.float32)
            if is_csr:
                # noinspection PyUnboundLocalVariable
                clipped_sum_csr(
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    num_cells=num_cells, num_dataset_genes=num_dataset_genes,
                    cell_indices=cell_indices, clip_val=clip_val,
                    batch_counts_sum=batch_counts_sum,
                    squared_batch_counts_sum=squared_batch_counts_sum,
                    num_threads=num_threads)
            else:
                clipped_sum_csc(
                    data=X.data, indices=X.indices, indptr=X.indptr,
                    cell_mask=cell_mask, clip_val=clip_val,
                    batch_counts_sum=batch_counts_sum,
                    squared_batch_counts_sum=squared_batch_counts_sum,
                    num_threads=num_threads)
            norm_gene_var = pl.Series(
                (1 / ((num_cells - 1) * np.square(estimated_stddev))) *
                ((num_cells * np.square(mean)) + squared_batch_counts_sum -
                 2 * batch_counts_sum * mean))
            
            # If `min_cells` is non-zero, set variances to `null` for genes
            # with a non-zero count less than `min_cells`
            if min_cells:
                norm_gene_var = norm_gene_var\
                    .set(pl.Series(nonzero_count < min_cells), None)
            
            # If there are multiple datasets, `norm_gene_var` is currently with
            # respect to the genes in `dataset.var_names`; map back to the
            # genes in `genes_in_any_dataset`, filling with `null`
            if others:
                norm_gene_var = norm_gene_var[gene_indices]
            norm_gene_vars.append(norm_gene_var)
        
        rank = pl.exclude('gene').rank('min', descending=True)
        final_rank = pl.struct(
            ('median_rank', 'nbatches') if flavor == 'seurat_v3' else
            ('nbatches', 'median_rank')).rank('ordinal')
        
        # Note: the expression for `median_rank` can be replaced by
        # `pl.median_horizontal(pl.exclude('gene'))` once polars implements it
        hvgs = pl.DataFrame([genes_in_all_datasets] + norm_gene_vars)\
            .lazy()\
            .pipe(lambda df: df.drop_nulls(pl.selectors.exclude('gene'))
                             if min_cells or others else df)\
            .with_columns(pl.when(rank <= num_genes).then(rank))\
            .with_columns(nbatches=pl.sum_horizontal(pl.exclude('gene')
                                                     .is_null()),
                          median_rank=pl.concat_list(pl.exclude('gene'))
                                      .explode()
                                      .median().over(pl.int_range(
                                          pl.len(), dtype=pl.UInt32)))\
            .select('gene', (final_rank <= num_genes).alias(hvg_column),
                    pl.when(final_rank <= num_genes).then(final_rank)
                    .alias(rank_column))\
            .collect()
        
        # Return a new SingleCell dataset (or a tuple of datasets, if others
        # is non-empty) containing the highly variable genes
        for dataset_index, dataset in enumerate(datasets):
            new_var = dataset._var\
                .join(hvgs.rename({'gene': dataset.var_names.name}),
                      on=dataset.var_names.name, how='left')\
                .with_columns(pl.col(hvg_column).fill_null(False))
            datasets[dataset_index] = \
                SingleCell(X=dataset._X, obs=dataset._obs, var=new_var,
                           obsm=dataset._obsm, varm=dataset._varm,
                           uns=dataset._uns, num_threads=dataset._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    def normalize_old(self,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  method: Literal['PFlog1pPF', 'log1pPF',
                                  'logCP10k'] = 'PFlog1pPF',
                  allow_float: bool = False,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Normalize this SingleCell dataset's counts using one of three methods.
        All three methods normalize each cell independently of the rest and
        log-transform the counts in some way, but differ in the details.
        
        By default, uses the PFlog1pPF method introduced in Booeshaghi et al.
        2022 (biorxiv.org/content/10.1101/2022.05.06.490859v1.full). With
        `method='logCP10k'`, it matches the default settings of Seurat's
        `NormalizeData()` function, aside from differences in floating-point
        error.
        
        PFlog1pPF is a three-step process:
        1. Divide each cell's counts by a "size factor", namely the total
        number of counts for that cell, divided by the mean number of counts
        across all cells. Booeshaghi et al. call this process "proportional
        fitting" (PF). In NumPy, proportional fitting can be implemented as:
        ```
        total_counts_per_cell = X.sum(axis=1)
        size_factor = total_counts_per_cell / total_counts_per_cell.mean()
        X = X / size_factor
        ```
        2. Take the logarithm of each entry plus 1, i.e. `log1p()`.
        3. Run an additional round of proportional fitting.
        
        If `method='log1pPF'`, only performs steps 1 and 2 and leaves out step
        3. Booeshaghi et al. call this method "log1pPF". Ahlmann-Eltze and
        Huber 2023 (nature.com/articles/s41592-023-01814-1) recommend this
        method and argue that it outperforms log(CPM) normalization. However,
        Booeshaghi et al. note that log1pPF does not fully normalize for read
        depth, because the log transform of step 2 partially undoes the
        normalization introduced by step 1. This is the reasoning behind their
        use of step 3: to restore full depth normalization. By default,
        Scanpy's `normalize_total()` uses a variation of proportional fitting
        that divides by the median instead of the mean, so it's closest to
        `method='log1pPF'`.
        
        If `method='logCP10k'`, uses 10,000 for the denominator of the size
        factors instead of `X.sum(axis=1).mean()`, and leaves out step 3. This
        method is not recommended because it implicitly assumes an
        unrealistically large amount of overdispersion, and performs worse than
        log1pPF and PFlog1pPF in Ahlmann-Eltze and Huber and Booeshaghi et
        al.'s benchmarks. Seurat's `NormalizeData()` uses logCP10k
        normalization.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will still be
                       normalized, but will not count towards the calculation
                       of the mean total count across cells when `method` is
                       `'PFlog1pPF'` or `'log1pPF'`. Has no effect when
                       `method` is `'logCP10k'`.
            method: the normalization method to use (see above)
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            num_threads: the number of threads to use when normalizing. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.

        
        Returns:
            A new SingleCell dataset with the normalized counts, and
            `uns['normalized']` set to `True`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so normalizing is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed and not already normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() (and "
                "possibly hvg()) before normalize()? Set uns['QCed'] = True "
                "or run with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = \
                "uns['normalized'] is True; did you already run normalize()?"
            raise ValueError(error_message)
        
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Check that `method` is one of the three valid methods
        if method not in ('PFlog1pPF', 'log1pPF', 'logCP10k'):
            error_message = \
                "method must be one of 'PFlog1pPF', 'log1pPF', or 'logCP10k'"
            raise ValueError(error_message)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # If `allow_float=False`, raise an error if `X` is floating-point
        X = self._X
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'normalize() requires raw counts but X has data type '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)

        def sparse_matrix_vector_multiply(
                sparse_matrix: csr_array | csc_array,
                vector: np.ndarray[1, np.dtype[np.integer | np.floating]],
                *,
                axis: Literal[0] | Literal[1],
                inplace: bool,
                num_threads: int | np.integer) -> csr_array | csc_array | None:
            """
            Elementwise multiply a sparse array/matrix and a vector, either rowwise or
            columnwise.
            
            Args:
                sparse_matrix: a CSR or CSC sparse array or matrix
                vector: a 1D numeric vector
                axis: the axis to perform the operation across: columnwise (`axis=0`)
                      or rowwise (`axis=1`)
                inplace: whether to multiply in-place
                num_threads: the number of threads to use for the operation.
            
            Returns:
                A float32 sparse array/matrix with the result of the
                multiplication, or `None` if operating in-place (`return_dtype`
                is `None`).
            """
            data = sparse_matrix.data
            indices = sparse_matrix.indices
            indptr = sparse_matrix.indptr
            
            # If in-place...
            if inplace:
                if isinstance(sparse_matrix, csc_array) == axis:
                    # Rowwise for CSR, or columnwise for CSC
                    cython_inline('''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused numeric2:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def multiply_inplace(numeric[::1] data,
                                             const signed_integer[::1] indices,
                                             const signed_integer[::1] indptr,
                                             const numeric2[::1] vector,
                                             const unsigned num_threads):
                            cdef unsigned long i
                            cdef unsigned j
                            
                            if num_threads == 1:
                                for j in range(<unsigned> indptr.shape[0] - 1):
                                    for i in range(<unsigned long> indptr[j],
                                                   <unsigned long> indptr[j + 1]):
                                        data[i] *= <numeric> vector[j]
                            else:
                                for j in prange(<unsigned> indptr.shape[0] - 1,
                                                nogil=True, num_threads=num_threads):
                                    for i in range(<unsigned long> indptr[j],
                                                   <unsigned long> indptr[j + 1]):
                                        data[i] *= <numeric> vector[j]
                        ''', warn_undeclared=False)['multiply_inplace'](
                            data, indices, indptr, vector, num_threads)
                else:
                    # Rowwise for CSC, or columnwise for CSR
                    cython_inline('''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused numeric2:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def multiply_inplace(numeric[::1] data,
                                             const signed_integer[::1] indices,
                                             const signed_integer[::1] indptr,
                                             const numeric2[::1] vector,
                                             const unsigned num_threads):
                            cdef unsigned long i
                            
                            if num_threads == 1:
                                for i in range(<unsigned long> data.shape[0]):
                                    data[i] *= <numeric> vector[indices[i]]
                            else:
                                for i in prange(<unsigned long> data.shape[0],
                                                nogil=True, num_threads=num_threads):
                                    data[i] *= <numeric> vector[indices[i]]
                        ''', warn_undeclared=False)['multiply_inplace'](
                            data, indices, indptr, vector, num_threads)
            
            # If not in-place...
            else:
                # Allocate the output sparse matrix/array's data array
                result = np.empty_like(sparse_matrix.data, dtype=np.float32)
                
                # Run the operation with Cython
                if isinstance(sparse_matrix, csc_array) == axis:
                    # Rowwise for CSR, or columnwise for CSC
                    cython_inline('''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused numeric2:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def multiply(numeric[::1] data,
                                     const signed_integer[::1] indices,
                                     const signed_integer[::1] indptr,
                                     const numeric2[::1] vector,
                                     float[::1] result,
                                     const unsigned num_threads):
                            cdef unsigned long i
                            cdef unsigned j
                            
                            if num_threads == 1:
                                for j in range(<unsigned> indptr.shape[0] - 1):
                                    for i in range(<unsigned long> indptr[j],
                                                   <unsigned long> indptr[j + 1]):
                                        result[i] = data[i] * vector[j]
                            else:
                                for j in prange(<unsigned> indptr.shape[0] - 1,
                                                nogil=True, num_threads=num_threads):
                                    for i in range(<unsigned long> indptr[j],
                                                   <unsigned long> indptr[j + 1]):
                                        result[i] = data[i] * vector[j]
                        ''', warn_undeclared=False)['multiply'](
                            data, indices, indptr, vector, result, num_threads)
                else:
                    # Rowwise for CSC, or columnwise for CSR
                    cython_inline(r'''
                        from cython.parallel cimport prange
                        
                        ctypedef fused numeric:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused numeric2:
                            int
                            unsigned
                            long
                            unsigned long
                            float
                            double
                        
                        ctypedef fused signed_integer:
                            int
                            long
                        
                        def multiply(const numeric[::1] data,
                                     const signed_integer[::1] indices,
                                     const signed_integer[::1] indptr,
                                     const numeric2[::1] vector,
                                     float[::1] result,
                                     const unsigned num_threads):
                            cdef unsigned long i
                            
                            if num_threads == 1:
                                for i in range(<unsigned long> data.shape[0]):
                                    result[i] = data[i] * vector[indices[i]]
                            else:
                                for i in prange(<unsigned long> data.shape[0],
                                                nogil=True, num_threads=num_threads):
                                    result[i] = data[i] * vector[indices[i]]
                        ''', warn_undeclared=False)['multiply'](
                            data, indices, indptr, vector, result, num_threads)
                # Return a new sparse matrix/array with the result
                return type(sparse_matrix)((result, indices, indptr),
                                           shape=sparse_matrix.shape)
        
        # Step 1
        rowsums = X.sum(axis=1)
        inverse_size_factors = np.empty_like(rowsums, dtype=np.float32) \
            if np.issubdtype(rowsums.dtype, np.integer) else rowsums
        np.divide(10_000 if method == 'logCP10k' else
                  rowsums.mean() if QC_column is None else
                  rowsums[QC_column].mean(), rowsums, inverse_size_factors)
        X = sparse_matrix_vector_multiply(X, inverse_size_factors, axis=0,
                                          inplace=False,
                                          num_threads=num_threads)
        
        # Step 2
        cython_inline('''
            from cython.parallel cimport prange
            from libcpp.cmath cimport log1p
            
            def log1p_inplace(float[::1] data, const unsigned num_threads):
                cdef unsigned long i
                
                if num_threads == 1:
                    for i in range(<unsigned long> data.shape[0]):
                        data[i] = log1p(data[i])
                else:
                    for i in prange(<unsigned long> data.shape[0],
                                    nogil=True, num_threads=num_threads):
                        data[i] = log1p(data[i])
            ''')['log1p_inplace'](X.data, num_threads)
        
        # Step 3
        if method == 'PFlog1pPF':
            rowsums = X.sum(axis=1)
            inverse_size_factors = rowsums
            np.divide(rowsums.mean() if QC_column is None else
                      rowsums[QC_column].mean(), rowsums, inverse_size_factors)
            sparse_matrix_vector_multiply(X, inverse_size_factors, axis=0,
                                          inplace=True,
                                          num_threads=num_threads)
        sc = SingleCell(X=X, obs=self._obs, var=self._var, obsm=self._obsm,
                        varm=self._varm, uns=self._uns,
                        num_threads=self._num_threads)
        sc._uns['normalized'] = True
        return sc
    
    def normalize(self,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  method: Literal['PFlog1pPF', 'log1pPF',
                                  'logCP10k'] = 'PFlog1pPF',
                  allow_float: bool = False,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Normalize this SingleCell dataset's counts.
        
        Must be run after `hvg()` and before `PCA()`.
        
        `normalize()` supports three normalization methods. All three methods
        normalize each cell independently of the rest and log-transform the
        counts in some way, but differ in the details.
        
        By default, uses the PFlog1pPF method introduced in Booeshaghi et al.
        2022 (biorxiv.org/content/10.1101/2022.05.06.490859v1.full). With
        `method='logCP10k'`, it matches the default settings of Seurat's
        `NormalizeData()` function, aside from differences in floating-point
        error.
        
        PFlog1pPF is a three-step process:
        1. Divide each cell's counts by a "size factor", namely the total
        number of counts for that cell, divided by the mean number of counts
        across all cells. Booeshaghi et al. call this process "proportional
        fitting" (PF). In NumPy, proportional fitting can be implemented as:
        ```
        total_counts_per_cell = X.sum(axis=1)
        size_factor = total_counts_per_cell / total_counts_per_cell.mean()
        X = X / size_factor
        ```
        2. Take the logarithm of each entry plus 1, i.e. `log1p()`.
        3. Run an additional round of proportional fitting.
        
        If `method='log1pPF'`, only performs steps 1 and 2 and leaves out step
        3. Booeshaghi et al. call this method "log1pPF". Ahlmann-Eltze and
        Huber 2023 (nature.com/articles/s41592-023-01814-1) recommend this
        method and argue that it outperforms log(CPM) normalization. However,
        Booeshaghi et al. note that log1pPF does not fully normalize for read
        depth, because the log transform of step 2 partially undoes the
        normalization introduced by step 1. This is the reasoning behind their
        use of step 3: to restore full depth normalization. By default,
        Scanpy's `normalize_total()` uses a variation of proportional fitting
        that divides by the median instead of the mean, so it's closest to
        `method='log1pPF'`.
        
        If `method='logCP10k'`, uses 10,000 for the denominator of the size
        factors instead of `X.sum(axis=1).mean()`, and leaves out step 3. This
        method is not recommended because it implicitly assumes an
        unrealistically large amount of overdispersion, and performs worse than
        log1pPF and PFlog1pPF in Ahlmann-Eltze and Huber and Booeshaghi et
        al.'s benchmarks. Seurat's `NormalizeData()` uses logCP10k
        normalization.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will still be
                       normalized, but will not count towards the calculation
                       of the mean total count across cells when `method` is
                       `'PFlog1pPF'` or `'log1pPF'`. Has no effect when
                       `method` is `'logCP10k'`.
            method: the normalization method to use (see above)
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            num_threads: the number of threads to use when normalizing. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the
                         normalized SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A new SingleCell dataset with the normalized counts, and
            `uns['normalized']` set to `True`.
        """
        # Check that `X` is present
        if self._X is None:
            error_message = 'X is None, so normalizing is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed and not already normalized
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() (and "
                "possibly hvg()) before normalize()? Set uns['QCed'] = True "
                "or run with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        if self._uns['normalized']:
            error_message = \
                "uns['normalized'] is True; did you already run normalize()?"
            raise ValueError(error_message)
        
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Check that `method` is one of the three valid methods
        if method == 'PFlog1pPF':
            method_number = 2
        elif method == 'log1pPF':
            method_number = 1
        else:
            if method != 'logCP10k':
                error_message = (
                    "method must be one of 'PFlog1pPF', 'log1pPF', or "
                    "'logCP10k'")
                raise ValueError(error_message)
            method_number = 0
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # If `allow_float=False`, raise an error if `X` is floating-point
        X = self._X
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        if not allow_float and np.issubdtype(X.dtype, np.floating):
            error_message = (
                f'normalize() requires raw counts but X has data type '
                f'{str(X.dtype)!r}, a floating-point data type. If you are '
                f'sure that all values are raw integer counts, i.e. that '
                f'(X.data == X.data.astype(int)).all(), then set '
                f'allow_float=True.')
            raise TypeError(error_message)
        
        # Define Cython functions
        cython_functions = cython_inline(_thread_offset_import + _uninitialized_vector_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.cmath cimport log1p
        from libcpp.vector cimport vector
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def normalize_csr(const numeric[::1] data,
                          const signed_integer[::1] indices,
                          const signed_integer[::1] indptr,
                          char[::1] QC_column,
                          float[::1] normalized_data,
                          const unsigned num_cells,
                          const unsigned method_number,
                          unsigned num_threads):
            
            cdef bint has_QC_column = QC_column.shape[0] != 0
            cdef unsigned i, num_QCed_cells, thread_index, thread_num_QCed_cells
            cdef unsigned long row_sum, j, thread_total_sum, total_sum = 0
            cdef float normalization_factor, inverse_size_factor, \
                new_normalization_factor, new_inverse_size_factor, \
                new_row_sum, new_total_sum = 0
            cdef volatile float new_normalization_factor_volatile
            cdef pair[unsigned, unsigned] row_range
            cdef uninitialized_vector[unsigned] thread_nums_QCed_cells
            cdef uninitialized_vector[unsigned long] row_sums_buffer, thread_total_sums
            cdef uninitialized_vector[float] new_row_sums, new_row_sums_contiguous
            row_sums_buffer.resize(num_cells)
            cdef unsigned long[::1] row_sums = \
                <unsigned long[:num_cells]> row_sums_buffer.data()
            
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                # Step 1a and 1b: calculate row sums and the normalization
                # factor for the size factor calculation
                if method_number == 0:  # 'logCP10k'
                    # Just calculate the row sums, and use a constant normalization factor
                    # of 10,000
                    for i in range(num_cells):
                        row_sum = 0
                        for j in range(<unsigned long> indptr[i],
                                       <unsigned long> indptr[i + 1]):
                            row_sum += <unsigned long> data[j]
                        row_sums[i] = row_sum
                    normalization_factor = 10000
                else:
                    # Same as the logCP10k version, but take the mean of the row sums
                    # (across cells passing QC, if `QC_column` was specified) as the
                    # size factor
                    if not has_QC_column:
                        for i in range(num_cells):
                            row_sum = 0
                            for j in range(<unsigned long> indptr[i],
                                           <unsigned long> indptr[i + 1]):
                                row_sum += <unsigned long> data[j]
                            row_sums[i] = row_sum
                            total_sum += row_sum
                        num_QCed_cells = num_cells
                    else:
                        num_QCed_cells = 0
                        for i in range(num_cells):
                            row_sum = 0
                            for j in range(<unsigned long> indptr[i],
                                           <unsigned long> indptr[i + 1]):
                                row_sum += <unsigned long> data[j]
                            row_sums[i] = row_sum
                            if QC_column[i]:
                                total_sum += row_sum
                                num_QCed_cells += 1
                    normalization_factor = <float> total_sum / num_QCed_cells
                
                if method_number != 2:  # 'PFlog1pPF'
                    # Step 1c and 2: calculate each cell's inverse size factor and
                    # multiply all counts for that cell by it, then log1p-transform
                    for i in range(num_cells):
                        inverse_size_factor = normalization_factor / row_sums[i]
                        for j in range(<unsigned long> indptr[i],
                                       <unsigned long> indptr[i + 1]):
                            normalized_data[j] = log1p(data[j] * inverse_size_factor)
                else:
                    # Step 1c, 2, and 3a: in addition to calculating each cell's
                    # size factor and multiplying the counts by it, also calculate
                    # the new row sums and total sum for a second round of proportional
                    # fitting. To ensure exactly the same floating-point behavior as the
                    # parallel version with -ffast-math:
                    # 1) Sum `new_row_sums` in a separate loop after the fact.
                    # 2) If `QC_column` was specified, copy the row sums to a temporary
                    #    buffer (`new_row_sums_contiguous`) before summing, to allow the
                    #    sum to use SIMD.
                    # 3) Launder `new_normalization_factor` through an intermediate variable
                    #    declared as volatile, to force it to be calculated explicitly.
                    new_row_sums.resize(num_cells)
                    for i in range(num_cells):
                        new_row_sum = 0
                        inverse_size_factor = normalization_factor / row_sums[i]
                        for j in range(<unsigned long> indptr[i],
                                       <unsigned long> indptr[i + 1]):
                            normalized_data[j] = log1p(data[j] * inverse_size_factor)
                            new_row_sum += normalized_data[j]
                        new_row_sums[i] = new_row_sum
                    if not has_QC_column:
                        for i in range(num_cells):
                            new_total_sum += new_row_sums[i]
                    else:
                        new_row_sums_contiguous.resize(num_QCed_cells)
                        num_QCed_cells = 0
                        for i in range(num_cells):
                            if QC_column[i]:
                                new_row_sums_contiguous[num_QCed_cells] = \
                                    new_row_sums[i]
                                num_QCed_cells += 1
                        for i in range(num_QCed_cells):
                            new_total_sum += new_row_sums_contiguous[i]
                    new_normalization_factor_volatile = new_total_sum / num_QCed_cells
                    new_normalization_factor = new_normalization_factor_volatile
                    
                    # Step 3b: calculate each cell's new inverse size factor
                    # and multiply all normalized counts for that cell by it
                    for i in range(num_cells):
                        new_inverse_size_factor = \
                            new_normalization_factor / new_row_sums[i]
                        for j in range(<unsigned long> indptr[i],
                                       <unsigned long> indptr[i + 1]):
                            normalized_data[j] *= new_inverse_size_factor
            else:
                with nogil:
                    # Step 1a and 1b: calculate row sums and the normalization
                    # factor for the size factor calculation
                    if method_number == 0:  # 'logCP10k'
                        # Just calculate the row sums, and use a constant normalization factor
                        # of 10,000
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            row_range = get_thread_offset(indptr, thread_index, num_threads)
                            for i in range(row_range.first, row_range.second):
                                row_sum = 0
                                for j in range(<unsigned long> indptr[i],
                                               <unsigned long> indptr[i + 1]):
                                    row_sum = row_sum + <unsigned long> data[j]
                                row_sums[i] = row_sum
                        normalization_factor = 10000
                    else:
                        # Same as the logCP10k version, but take the mean of the row sums
                        # (across cells passing QC, if `QC_column` was specified) as the
                        # size factor
                        thread_total_sums.resize(num_threads)
                        if not has_QC_column:
                            with parallel(num_threads=num_threads):
                                thread_index = threadid()
                                row_range = get_thread_offset(indptr, thread_index, num_threads)
                                thread_total_sum = 0
                                for i in range(row_range.first, row_range.second):
                                    row_sum = 0
                                    for j in range(<unsigned long> indptr[i],
                                                   <unsigned long> indptr[i + 1]):
                                        row_sum = row_sum + <unsigned long> data[j]
                                    row_sums[i] = row_sum
                                    thread_total_sum = thread_total_sum + row_sum
                                thread_total_sums[thread_index] = thread_total_sum
                            for thread_index in range(num_threads):
                                total_sum += thread_total_sums[thread_index]
                            num_QCed_cells = num_cells
                        else:
                            num_QCed_cells = 0
                            thread_nums_QCed_cells.resize(num_threads)
                            with parallel(num_threads=num_threads):
                                thread_index = threadid()
                                row_range = get_thread_offset(indptr, thread_index, num_threads)
                                thread_total_sum = 0
                                thread_num_QCed_cells = 0
                                for i in range(row_range.first, row_range.second):
                                    row_sum = 0
                                    for j in range(<unsigned long> indptr[i],
                                                   <unsigned long> indptr[i + 1]):
                                        row_sum = row_sum + <unsigned long> data[j]
                                    row_sums[i] = row_sum
                                    if QC_column[i]:
                                        thread_total_sum = thread_total_sum + row_sum
                                        thread_num_QCed_cells = thread_num_QCed_cells + 1
                                thread_total_sums[thread_index] = thread_total_sum
                                thread_nums_QCed_cells[thread_index] = \
                                    thread_num_QCed_cells
                            for thread_index in range(num_threads):
                                total_sum += thread_total_sums[thread_index]
                                num_QCed_cells += thread_nums_QCed_cells[thread_index]
                        normalization_factor = <float> total_sum / num_QCed_cells
                    if method_number != 2:  # 'PFlog1pPF'
                        # Step 1c and 2: calculate each cell's inverse size factor and
                        # multiply all counts for that cell by it, then log1p-transform
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            row_range = get_thread_offset(indptr, thread_index, num_threads)
                            for i in range(row_range.first, row_range.second):
                                inverse_size_factor = normalization_factor / row_sums[i]
                                for j in range(<unsigned long> indptr[i],
                                               <unsigned long> indptr[i + 1]):
                                    normalized_data[j] = log1p(data[j] * inverse_size_factor)
                    else:
                        # Step 1c, 2, and 3a: in addition to calculating each cell's
                        # size factor and multiplying the counts by it, also calculate
                        # the new row sums and total sum for a second round of proportional
                        # fitting. To ensure exactly the same floating-point behavior as the
                        # single-threaded version with -ffast-math:
                        # 1) Sum `new_row_sums` in a separate single-threaded loop after the fact.
                        # 2) If `QC_column` was specified, copy the row sums to a temporary
                        #    buffer (`new_row_sums_contiguous`) before summing, to allow the
                        #    sum to use SIMD.
                        # 3) Launder `new_normalization_factor` through an intermediate variable
                        #    declared as volatile, to force it to be calculated explicitly.
                        new_row_sums.resize(num_cells)
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            row_range = get_thread_offset(indptr, thread_index, num_threads)
                            for i in range(row_range.first, row_range.second):
                                new_row_sum = 0
                                inverse_size_factor = normalization_factor / row_sums[i]
                                for j in range(<unsigned long> indptr[i],
                                               <unsigned long> indptr[i + 1]):
                                    normalized_data[j] = log1p(data[j] * inverse_size_factor)
                                    new_row_sum = new_row_sum + normalized_data[j]
                                new_row_sums[i] = new_row_sum
                        if not has_QC_column:
                            for i in range(num_cells):
                                new_total_sum += new_row_sums[i]
                        else:
                            new_row_sums_contiguous.resize(num_QCed_cells)
                            num_QCed_cells = 0
                            for i in range(num_cells):
                                if QC_column[i]:
                                    new_row_sums_contiguous[num_QCed_cells] = \
                                        new_row_sums[i]
                                    num_QCed_cells += 1
                            for i in range(num_QCed_cells):
                                new_total_sum += new_row_sums_contiguous[i]
                        new_normalization_factor_volatile = new_total_sum / num_QCed_cells
                        new_normalization_factor = new_normalization_factor_volatile
                        
                        # Step 3b: calculate each cell's new inverse size factor
                        # and multiply all normalized counts for that cell by it
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            row_range = get_thread_offset(indptr, thread_index, num_threads)
                            for i in range(row_range.first, row_range.second):
                                new_inverse_size_factor = \
                                    new_normalization_factor / new_row_sums[i]
                                for j in range(<unsigned long> indptr[i],
                                               <unsigned long> indptr[i + 1]):
                                    normalized_data[j] *= new_inverse_size_factor
        
        def normalize_csc(const numeric[::1] data,
                          const signed_integer[::1] indices,
                          const signed_integer[::1] indptr,
                          char[::1] QC_column,
                          float[::1] normalized_data,
                          const unsigned num_cells,
                          const unsigned method_number,
                          const unsigned num_threads):
            
            cdef bint has_QC_column = QC_column.shape[0] != 0
            cdef unsigned i, thread_index, num_QCed_cells
            cdef unsigned long j, start, end, chunk_size, \
                num_elements = data.shape[0], total_sum = 0
            cdef float normalization_factor, inverse_size_factor, \
                new_normalization_factor, new_inverse_size_factor, \
                new_total_sum = 0
            cdef volatile float new_normalization_factor_volatile
            cdef vector[unsigned long] row_sums
            cdef vector[vector[unsigned long]] thread_row_sums
            cdef vector[float] new_row_sums
            cdef uninitialized_vector[float] new_row_sums_contiguous
            
            if num_threads == 1:
                # Step 1a: calculate row sums
                row_sums.resize(num_cells)
                for j in range(num_elements):
                    row_sums[indices[j]] += <unsigned long> data[j]
                
                # Step 1b: calculate the normalization factor for the size
                # factor calculation
                if method_number == 0:  # 'logCP10k'
                    normalization_factor = 10000
                else:
                    # Take the mean of the row sums (across cells passing QC,
                    # if `QC_column` was specified) as the size factor
                    if not has_QC_column:
                        for i in range(num_cells):
                            total_sum += row_sums[i]
                        num_QCed_cells = num_cells
                    else:
                        num_QCed_cells = 0
                        for i in range(num_cells):
                            if QC_column[i]:
                                total_sum += row_sums[i]
                                num_QCed_cells += 1
                    normalization_factor = <float> total_sum / num_QCed_cells
                
                if method_number != 2:  # 'PFlog1pPF'
                    # Step 1c and 2: multiply each count by its cell's inverse
                    # size factor, then log1p-transform.
                    for j in range(num_elements):
                        inverse_size_factor = normalization_factor / row_sums[indices[j]]
                        normalized_data[j] = log1p(data[j] * inverse_size_factor)
                else:
                    # Step 1c, 2, and 3a: in addition to calculating each cell's
                    # size factor and multiplying the counts by it, also calculate
                    # the new row sums and total sum for a second round of proportional
                    # fitting. Launder `new_normalization_factor` through an
                    # intermediate variable declared as volatile, to force it to be
                    # calculated explicitly and thereby ensure consistent
                    # floating-point behavior under -ffast-math between the
                    # single-threaded and parallel versions.
                    new_row_sums.resize(num_cells)
                    for j in range(num_elements):
                        inverse_size_factor = normalization_factor / row_sums[indices[j]]
                        normalized_data[j] = log1p(data[j] * inverse_size_factor)
                        new_row_sums[indices[j]] += normalized_data[j]
                    if not has_QC_column:
                        for i in range(num_cells):
                            new_total_sum += new_row_sums[i]
                    else:
                        new_row_sums_contiguous.resize(num_QCed_cells)
                        num_QCed_cells = 0
                        for i in range(num_cells):
                            if QC_column[i]:
                                new_row_sums_contiguous[num_QCed_cells] = \
                                    new_row_sums[i]
                                num_QCed_cells += 1
                        for i in range(num_QCed_cells):
                            new_total_sum += new_row_sums_contiguous[i]
                    new_normalization_factor_volatile = new_total_sum / num_QCed_cells
                    new_normalization_factor = new_normalization_factor_volatile
                    
                    # Step 3b: calculate each cell's new inverse size factor
                    # and multiply all normalized counts for that cell by it
                    for j in range(num_elements):
                        new_inverse_size_factor = \
                            new_normalization_factor / new_row_sums[indices[j]]
                        normalized_data[j] *= new_inverse_size_factor
            else:
                with nogil:
                    # Step 1a: calculate row sums. Store row sums for each thread
                    # in a temporary buffer, then aggregate at the end. As an
                    # optimization, put the row sums for the last thread
                    # (`thread_index == num_threads - 1`) directly into the final
                    # `row_sums` vector.
                    thread_row_sums.resize(num_threads - 1)
                    chunk_size = (num_elements + num_threads - 1) / num_threads
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        start = thread_index * chunk_size
                        if thread_index == num_threads - 1:
                            end = num_elements
                            row_sums.resize(num_cells)
                            for j in range(start, end):
                                row_sums[indices[j]] += <unsigned long> data[j]
                        else:
                            thread_row_sums[thread_index].resize(num_cells)
                            end = start + chunk_size
                            for j in range(start, end):
                                thread_row_sums[thread_index][indices[j]] += \
                                    <unsigned long> data[j]
                    for thread_index in range(num_threads - 1):
                        for i in range(num_cells):
                            row_sums[i] += thread_row_sums[thread_index][i]
                    
                    # Step 1b: calculate the normalization factor for the size
                    # factor calculation
                    if method_number == 0:  # 'logCP10k'
                        normalization_factor = 10000
                    else:
                        # Take the mean of the row sums (across cells passing QC,
                        # if `QC_column` was specified) as the size factor
                        if not has_QC_column:
                            for i in prange(num_cells, num_threads=num_threads):
                                total_sum += row_sums[i]
                            num_QCed_cells = num_cells
                        else:
                            num_QCed_cells = 0
                            for i in prange(num_cells, num_threads=num_threads):
                                if QC_column[i]:
                                    total_sum += row_sums[i]
                                    num_QCed_cells += 1
                        normalization_factor = <float> total_sum / num_QCed_cells
                    
                    # Step 1c and 2: multiply each count by its cell's inverse
                    # size factor, then log1p-transform
                    for j in prange(num_elements, num_threads=num_threads):
                        inverse_size_factor = normalization_factor / row_sums[indices[j]]
                        normalized_data[j] = log1p(data[j] * inverse_size_factor)
                        
                    if method_number == 2:  # 'PFlog1pPF'
                        # Step 3a: calculate the new row sums and total sum for a
                        # second round of proportional fitting. This must be done
                        # single-threaded to maintain a consistent order of
                        # operations and avoid differences due to floating-point
                        # error between the single-threaded and parallel versions.
                        # Launder `new_normalization_factor` through an intermediate
                        # variable declared as volatile, to force it to be calculated
                        # explicitly and thereby ensure consistent floating-point
                        # behavior under -ffast-math between the single-threaded and
                        # parallel versions.
                        
                        row_sums.clear()  # no longer needed
                        new_row_sums.resize(num_cells)
                        for j in range(num_elements):
                            new_row_sums[indices[j]] += normalized_data[j]
                        if not has_QC_column:
                            for i in range(num_cells):
                                new_total_sum += new_row_sums[i]
                        else:
                            new_row_sums_contiguous.resize(num_QCed_cells)
                            num_QCed_cells = 0
                            for i in range(num_cells):
                                if QC_column[i]:
                                    new_row_sums_contiguous[num_QCed_cells] = \
                                        new_row_sums[i]
                                    num_QCed_cells += 1
                            for i in range(num_QCed_cells):
                                new_total_sum += new_row_sums_contiguous[i]
                        new_normalization_factor_volatile = new_total_sum / num_QCed_cells
                        new_normalization_factor = new_normalization_factor_volatile
                        
                        # Step 3b: calculate each cell's new inverse size factor
                        # and multiply all normalized counts for that cell by it
                        for j in prange(num_elements, num_threads=num_threads):
                            new_inverse_size_factor = \
                                new_normalization_factor / new_row_sums[indices[j]]
                            normalized_data[j] *= new_inverse_size_factor
            ''', warn_undeclared=False)
        if isinstance(X, csr_array):
            normalize = cython_functions['normalize_csr']
            sparse_array = csr_array
        else:
            normalize = cython_functions['normalize_csc']
            sparse_array = csc_array
        normalized_data = np.empty_like(X.data, dtype=np.float32)
        original_num_threads = X._num_threads
        normalize(data=X.data, indices=X.indices, indptr=X.indptr,
                  QC_column=QC_column.to_numpy() if QC_column is not None else
                            np.array([], dtype=bool),
                  normalized_data=normalized_data, num_cells=X.shape[0],
                  method_number=method_number, num_threads=num_threads)
        X = sparse_array((normalized_data, X.indices, X.indptr), shape=X.shape)
        X._num_threads = original_num_threads
        sc = SingleCell(X=X, obs=self._obs, var=self._var, obsm=self._obsm,
                        varm=self._varm, uns=self._uns,
                        num_threads=self._num_threads)
        sc._uns['normalized'] = True
        return sc
    
    # noinspection PyTypeChecker
    def PCA_even_older(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            seed: int | np.integer = 0,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components using irlba, the package used by Seurat.
        Operates on normalized counts (see `normalize()`).
        
        Install irlba with:
        
        from ryp import r
        r('install.packages("irlba", type="source")')
        
        IMPORTANT: if you already have a copy of irlba from CRAN (e.g.
        installed with Seurat), you will get the error:
        
        ```
        RuntimeError: in irlba(X, 50, verbose = FALSE) :
          function 'as_cholmod_sparse' not provided by package 'Matrix'
        ```
        
        This error will go away if you install irlba from source as described
        above.
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their PCs set to `NaN`. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            hvg_column: a Boolean column of `var` indicating the highly
                        variable genes. Can be a column name, a polars
                        expression, a polars Series, a 1D NumPy array, or a
                        function that takes in this SingleCell dataset and
                        returns a polars Series or 1D NumPy array. Set to
                        `None` to use all genes. When `others` is specified,
                        `hvg_column` can be a length-`1 + len(others)` sequence
                        of columns, expressions, Series, functions, or `None`
                        for each dataset (for `self`, followed by each dataset
                        in `others`).
            PC_key: the key of `obsm` where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            seed: the random seed to use for irlba when computing PCs, via R's
                  `set.seed()` function
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to set the verbose flag in irlba
            num_threads: the number of threads to use when subsetting the count
                         matrix/matrices prior to PCA. PCA will run
                         single-threaded regardless of the value of
                         `num_threads`, because it does not parallelize
                         efficiently. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores.
        
        Returns:
            A new SingleCell dataset where `obsm` contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA()` function, which requires `ScaleData()`
            to be run first, this function does not require the data to be
            scaled beforehand. Instead, it scales the data implicitly. It does
            this by providing the standard deviation and mean of the data to
            `irlba()` via its `scale` and `center` arguments, respectively.
            This approach is much more computationally efficient than explicit
            scaling.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `PC_key` is a string
        check_type(PC_key, 'PC_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `PC_key` is not already in `obsm`, unless `overwrite=True`
        suffix = ' for at least one dataset' if others else ''
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present
        if self._X is None:
            error_message = \
                'X is None, so finding principal components is not possible'
            raise ValueError(error_message)
        from ryp import r, to_py, to_r
        from sklearn.utils.sparsefuncs import mean_variance_axis
        r('suppressPackageStartupMessages(library(irlba))')
        
        # Check that all datasets are normalized
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        
        # Raise an error if `X` has an integer data type for any dataset
        for dataset in datasets:
            if np.issubdtype(dataset._X.dtype, np.integer):
                error_message = (
                    f'PCA() requires normalized counts, but X has data type '
                    f'{str(dataset._X.dtype)!r}, an integer data type'
                    f'{suffix}; did you forget to run normalize() before '
                    f'PCA()?')
                raise TypeError(error_message)
        
        # Get `QC_column` and `hvg_column` (if not `None`) from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and possibly '
                         f'normalize()) before PCA()?')
        
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`.
        num_threads = self._process_num_threads(num_threads)
        
        # Get the matrix to compute PCA across: a CSC array of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use `X[np.ix_(rows, columns)]` as a faster,
        # more memory-efficient alternative to `X[rows][:, columns]`. Use CSC
        # rather than CSR because irlba has a fast C-based implementation for
        # CSC.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = self._var.filter(hvg_columns[0]).to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [dataset._X[np.ix_(QC_col.to_numpy(),
                                                hvg_col.to_numpy())]
                              if QC_col is not None else
                              dataset._X[:, hvg_col.to_numpy()]
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        X = sparse.vstack(Xs, format='csc')
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        del Xs
        
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        
        # Run PCA with irlba (github.com/bwlewis/irlba/blob/master/R/irlba.R)
        # This section is adapted from
        # github.com/satijalab/seurat/blob/master/R/integration.R#L7276-L7317
        # Note: totalvar doesn't seem to be used by irlba, maybe a Seurat bug?
        # Note: mean_variance_axis() can be replaced by Welford's algorithm
        center, feature_var = mean_variance_axis(X, axis=0)
        scale = np.sqrt(feature_var, dtype=np.float32)
        scale.clip(min=1e-8, out=scale)
        to_r(X, '.SingleCell.X')
        try:
            to_r(center, '.SingleCell.center')
            try:
                to_r(scale, '.SingleCell.scale')
                try:
                    r(f'set.seed({seed})')
                    r(f'.SingleCell.PCs = irlba(.SingleCell.X, {num_PCs}, '
                      f'verbose={str(verbose).upper()}, '
                      f'scale=.SingleCell.scale, '
                      f'center=.SingleCell.center)')
                    try:
                        PCs = to_py('.SingleCell.PCs$u', format='numpy') * \
                              to_py('.SingleCell.PCs$d', format='numpy')
                    finally:
                        r('rm(.SingleCell.PCs)')
                finally:
                    r('rm(.SingleCell.scale)')
            finally:
                r('rm(.SingleCell.center)')
        finally:
            r('rm(.SingleCell.X)')
        
        # Store each dataset's PCs in its `obsm`
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan,
                                      dtype=np.float32)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            else:
                dataset_PCs = np.ascontiguousarray(dataset_PCs)
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    # noinspection PyTypeChecker
    def PCA_older(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            seed: int | np.integer = 0,
            faster_single_threading: bool = False,
            overwrite: bool = False,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components.
        
        Requires normalized counts, so must be run after `normalize()`.
        
        Uses the propack solver, implemented in SciPy
        (`scipy.sparse.linalg.svds(solver='propack')`).
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their PCs set to `NaN`. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            hvg_column: a Boolean column of `var` indicating the highly
                        variable genes. Can be a column name, a polars
                        expression, a polars Series, a 1D NumPy array, or a
                        function that takes in this SingleCell dataset and
                        returns a polars Series or 1D NumPy array. Set to
                        `None` to use all genes. When `others` is specified,
                        `hvg_column` can be a length-`1 + len(others)` sequence
                        of columns, expressions, Series, functions, or `None`
                        for each dataset (for `self`, followed by each dataset
                        in `others`).
            PC_key: the key of `obsm` where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            seed: the random seed to use for irlba when computing PCs, via R's
                  `set.seed()` function
            faster_single_threading: if `True`, use a different order of
                                     operations for single-threaded PCA. This
                                     gives a modest (~15-20%) boost in
                                     single-threaded performance, and lower
                                     memory usage, at the cost of no longer
                                     exactly matching the PCs produced by the
                                     multithreaded version (due to differences
                                     in floating-point error arising from the
                                     different order of operations). When
                                     `faster_single_threading=True`, `PCA()`
                                     will also give slightly different results
                                     when run with CSR vs CSC input; when
                                     multiple datasets are provided with a mix
                                     of CSR and CSC formats, all datasets are
                                     converted to the format shared by the most
                                     total cells across datasets. Must be
                                     `False` unless `num_threads=1`. If you are
                                     always going to run single-threaded and do
                                     not care about exactly matching the
                                     multithreaded version, there is no reason
                                     not to set `faster_single_threading=True`.
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            num_threads: the number of threads to use for PCA. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the returned
                         SingleCell dataset's `num_threads`; this will always
                         be the same as the original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset where `obsm` contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA()` function, which requires `ScaleData()`
            to be run first, this function does not require the data to be
            scaled beforehand. Instead, it implicitly scales the data to zero
            mean and unit variance while performing PCA.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `PC_key` is a string
        check_type(PC_key, 'PC_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `PC_key` is not already in `obsm`, unless `overwrite=True`
        suffix = ' for at least one dataset' if others else ''
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present for every dataset
        if any(dataset._X is None for dataset in datasets):
            error_message = (
                f'X is None{suffix}, so finding principal components is not '
                f'possible')
            raise ValueError(error_message)
        
        # Get `QC_column` (if not `None`) and `hvg_column` from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            allow_None=False,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and possibly '
                         f'normalize()) before PCA()?')
        
        # Check that all datasets are normalized
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        
        # Raise an error if `X` is not float32 for every dataset
        for dataset in datasets:
            if dataset._X.dtype != np.float32:
                error_message = (
                    f'PCA() requires normalized counts with data type '
                    f'float32, but X has data type '
                    f'{str(dataset._X.dtype)!r}{suffix}; did you forget to '
                    f'run normalize() before PCA()?')
                raise TypeError(error_message)
        
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`.
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `faster_single_threading` is Boolean, and `False` unless
        # `num_threads=1`
        check_type(faster_single_threading, 'faster_single_threading', bool,
                   'Boolean')
        if faster_single_threading and num_threads != 1:
            error_message = \
                'faster_single_threading must be False unless num_threads is 1'
            raise ValueError(error_message)
        
        # Get the matrix to compute PCA across: a sparse matrix of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use `X[np.ix_(rows, columns)]` as a faster,
        # more memory-efficient alternative to `X[rows][:, columns]`.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = self._var.filter(hvg_columns[0]).to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [dataset._X[np.ix_(QC_col.to_numpy(),
                                                hvg_col.to_numpy())]
                              if QC_col is not None else
                              dataset._X[:, hvg_col.to_numpy()]
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        if len(Xs) == 1:
            X = Xs[0]
        elif all(isinstance(X, csr_array) for X in Xs):
            X = sparse_major_stack(Xs, num_threads=num_threads)
        elif all(isinstance(X, csc_array) for X in Xs):
            X = sparse_minor_stack(Xs, num_threads=num_threads)
        else:
            # Mix of CSR and CSC: convert to whichever format has the most
            # total cells in that format, to reduce the number of datasets that
            # need to be flipped
            total_cells = num_cells_per_dataset.sum()
            total_csr = sum(X.shape[0] for X in Xs if isinstance(X, csr_array))
            if total_csr / total_cells > 0.5:  # more CSR than CSC
                X = sparse_major_stack([
                    X.tocsr() if isinstance(X, csc_array) else X for X in Xs],
                    num_threads=num_threads)
            else:
                X = sparse_minor_stack([
                    X.tocsc() if isinstance(X, csr_array) else X for X in Xs],
                    num_threads=num_threads)
        del Xs
        
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        
        # Define Cython functions
        cython_functions = cython_inline(r'''
    from cython.parallel cimport parallel, prange, threadid
    from libcpp.cmath cimport sqrt
    from libcpp.vector cimport vector
    
    ctypedef fused signed_integer:
        int
        long
        
    def clipped_stddev_csr(const float[::1] data,
                           const signed_integer[::1] indices,
                           const signed_integer[::1] indptr,
                           const unsigned long num_cells,
                           const unsigned num_genes,
                           float clip_val,
                           float[::1] clipped_stddev):
        # Compute `X.std(axis=0)` where `X` is CSR, clipping to a minimum of
        # `clip_val`. Only used when `num_threads=1` and
        # `faster_single_threading=True`.
        
        cdef unsigned long num_elements, i, j, start, end, \
            chunk_size
        cdef unsigned gene, cell, thread_index
        cdef float value, total_sum, total_sum_of_squares, \
            inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
        cdef vector[float] sum, sum_of_squares
        sum.resize(num_genes)
        sum_of_squares.resize(num_genes)
        
        # Iterate over all elements of the count matrix, ignoring which cell
        # they're from
        num_elements = indices.shape[0]
        for i in range(num_elements):
            gene = indices[i]
            value = data[i]
            sum[gene] += value
            sum_of_squares[gene] += value * value
        
        # Calculate standard deviations from the sums and squared sums
        for gene in range(num_genes):
            clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                num_cells * sum_of_squares[gene] - sum[gene] * sum[gene]))
            if clipped_stddev[gene] < clip_val:
                clipped_stddev[gene] = clip_val
    
    def clipped_stddev_csc(const float[::1] data,
                           const signed_integer[::1] indices,
                           const signed_integer[::1] indptr,
                           const unsigned long num_cells,
                           const unsigned num_genes,
                           float clip_val,
                           float[::1] clipped_stddev,
                           const unsigned num_threads):
        # Compute `X.std(axis=0)` where `X` is CSC, clipping to a minimum of
        # `clip_val`.
        
        cdef unsigned long i, gene
        cdef float value, sum, sum_of_squares, \
            inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
        
        if num_threads == 1:
            for gene in range(num_genes):
                # Calculate the sum and squared sum for this gene, across cells
                # with non-zero counts for the gene
                sum = 0
                sum_of_squares = 0
                for i in range(<unsigned long> indptr[gene],
                               <unsigned long> indptr[gene + 1]):
                    value = data[i]
                    sum += value
                    sum_of_squares += value * value
    
                # Calculate the scaled variance from the sum and squared sum
                clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                    num_cells * sum_of_squares - sum * sum))
                if clipped_stddev[gene] < clip_val:
                    clipped_stddev[gene] = clip_val
        else:
            for gene in prange(num_genes, nogil=True, num_threads=num_threads):
                sum = 0
                sum_of_squares = 0
                for i in range(<unsigned long> indptr[gene],
                               <unsigned long> indptr[gene + 1]):
                    value = data[i]
                    sum = sum + value
                    sum_of_squares = sum_of_squares + value * value
                clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                    num_cells * sum_of_squares - sum * sum))
                if clipped_stddev[gene] < clip_val:
                    clipped_stddev[gene] = clip_val
        
    def matvec_csr(const float[::1] data,
                   const signed_integer[::1] indices,
                   const signed_integer[::1] indptr,
                   const float[::1] V,
                   const float[::1] clipped_stddev,
                   float[::1] num_cells_buffer,
                   float[::1] num_genes_buffer):
        # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
        # and `num_cells_buffer` are vectors. Also has a parallel version,
        # `matvec_csr_parallel()`.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        # Variance scaling
        for i in range(num_genes):
            num_genes_buffer[i] = V[i] / clipped_stddev[i]
            
        # Matrix-vector multiplication and mean calculation
        for i in range(num_cells):
            num_cells_buffer[i] = 0
            for j in range(<unsigned long> indptr[i],
                           <unsigned long> indptr[i + 1]):
                num_cells_buffer[i] += data[j] * num_genes_buffer[indices[j]]
            mean += num_cells_buffer[i]
        mean /= num_cells
        
        # Mean scaling
        for i in range(num_cells):
            num_cells_buffer[i] -= mean
    
    def matvec_csr_parallel(const float[::1] data,
                            const signed_integer[::1] indices,
                            const signed_integer[::1] indptr,
                            const float[::1] V,
                            const float[::1] clipped_stddev,
                            float[::1] num_cells_buffer,
                            float[::1] num_genes_buffer,
                            const unsigned num_threads):
        # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
        # and `num_cells_buffer` are vectors. Also has a single-threaded
        # version, `matvec_csr()`.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        # Variance scaling (single-threaded since `num_genes` is just 2000
        # by default)
        for i in range(num_genes):
            num_genes_buffer[i] = V[i] / clipped_stddev[i]
        
        with nogil:
            # Matrix-vector multiplication
            for i in prange(num_cells, num_threads=num_threads):
                num_cells_buffer[i] = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_cells_buffer[i] += \
                        data[j] * num_genes_buffer[indices[j]]
            
            # Mean calculation (must be done single-threaded to be
            # deterministic, due to floating-point error)
            for i in range(num_cells):
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in prange(num_cells, num_threads=num_threads):
                num_cells_buffer[i] -= mean
    
    def matvec_csc(const float[::1] data,
                   const signed_integer[::1] indices,
                   const signed_integer[::1] indptr,
                   const float[::1] V,
                   const float[::1] clipped_stddev,
                   float[::1] num_cells_buffer,
                   float[::1] num_genes_buffer):
        # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSC and `V`
        # and `num_cells_buffer` are vectors. Does not have a parallel version.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        # Variance scaling
        for i in range(num_genes):
            num_genes_buffer[i] = V[i] / clipped_stddev[i]
        
        # Matrix-vector multiplication
        num_cells_buffer[:] = 0
        for i in range(num_genes):
            for j in range(<unsigned long> indptr[i],
                           <unsigned long> indptr[i + 1]):
                num_cells_buffer[indices[j]] += data[j] * num_genes_buffer[i]
                
        # Mean calculation
        for i in range(num_cells):
            mean += num_cells_buffer[i]
        mean /= num_cells
        
        # Mean scaling
        for i in range(num_cells):
            num_cells_buffer[i] -= mean
    
    def rmatvec_csr(const float[::1] data,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    const float[::1] V,
                    const float[::1] clipped_stddev,
                    float[::1] num_cells_buffer,
                    float[::1] num_genes_buffer):
        # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSR and `V`
        # and `num_cells_buffer` are vectors. Does not have a parallel version.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        # Mean scaling
        for i in range(num_cells):
            mean += V[i]
        mean /= num_cells
        for i in range(num_cells):
            num_cells_buffer[i] = V[i] - mean
        
        # Matrix-vector multiplication
        num_genes_buffer[:] = 0
        for i in range(num_cells):
            for j in range(<unsigned long> indptr[i],
                           <unsigned long> indptr[i + 1]):
                num_genes_buffer[indices[j]] += data[j] * num_cells_buffer[i]
        
        # Variance scaling
        for i in range(num_genes):
            num_genes_buffer[i] /= clipped_stddev[i]
    
    def rmatvec_csc(const float[::1] data,
                    const signed_integer[::1] indices,
                    const signed_integer[::1] indptr,
                    const float[::1] V,
                    const float[::1] clipped_stddev,
                    float[::1] num_cells_buffer,
                    float[::1] num_genes_buffer):
        # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
        # and `num_cells_buffer` are vectors. Also has a parallel version,
        # `rmatvec_csc_parallel()`.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        # Mean calculation
        for i in range(num_cells):
            mean += V[i]
        mean /= num_cells
        
        # Mean scaling
        for i in range(num_cells):
            num_cells_buffer[i] = V[i] - mean
        
        # Matrix-vector multiplication
        for i in range(num_genes):
            num_genes_buffer[i] = 0
            for j in range(<unsigned long> indptr[i],
                           <unsigned long> indptr[i + 1]):
                num_genes_buffer[i] += data[j] * num_cells_buffer[indices[j]]
        
        # Variance scaling
        for i in range(num_genes):
            num_genes_buffer[i] /= clipped_stddev[i]
    
    def rmatvec_csc_parallel(const float[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const float[::1] V,
                             const float[::1] clipped_stddev,
                             float[::1] num_cells_buffer,
                             float[::1] num_genes_buffer,
                             const unsigned num_threads):
        # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
        # and `num_cells_buffer` are vectors. Also has a single-threaded
        # version, `rmatvec_csc()`.
        
        cdef unsigned i, num_cells = num_cells_buffer.shape[0], \
            num_genes = num_genes_buffer.shape[0]
        cdef unsigned long j
        cdef float mean = 0
        
        with nogil:
            # Mean calculation (must be done single-threaded to be
            # deterministic, due to floating-point error)
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            
            # Mean scaling
            for i in prange(num_cells, num_threads=num_threads):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            for i in prange(num_genes, num_threads=num_threads):
                num_genes_buffer[i] = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_genes_buffer[i] += \
                        data[j] * num_cells_buffer[indices[j]]
        
        # Variance scaling (single-threaded since `num_genes` is just 2000
        # by default)
        for i in range(num_genes):
            num_genes_buffer[i] /= clipped_stddev[i]
        ''', warn_undeclared=False)
        clipped_stddev_csr = cython_functions['clipped_stddev_csr']
        clipped_stddev_csc = cython_functions['clipped_stddev_csc']
        matvec_csr = cython_functions['matvec_csr']
        matvec_csr_parallel = cython_functions['matvec_csr_parallel']
        matvec_csc = cython_functions['matvec_csc']
        rmatvec_csr = cython_functions['rmatvec_csr']
        rmatvec_csc = cython_functions['rmatvec_csc']
        rmatvec_csc_parallel = cython_functions['rmatvec_csc_parallel']
        
        # Define the linear operator to be SVDed: `X` scaled to zero mean and
        # unit variance.
        # Key ideas:
        # 1. Because `X` is a sparse matrix, mean-centering cannot be done
        #    without converting to a dense matrix. So scaling `X` cannot be
        #    done in the conventional way.
        # 2. Fortunately, we can represent scaling as a matrix product:
        #    `scale(X) = C @ X @ W`, where `W` is a diagonal matrix of the
        #    standard deviations for each column (gene) and `C` is a "centering
        #    matrix" that, when applied to any vector or matrix, yields the
        #    mean-centered version of it.
        # 3. We need to calculate the matrix-vector product of our operator
        #    with some vector `V`, i.e. `scale(X) @ V`. Using the formula from
        #    point #2, this is equivalent to `C @ X @ W @ V`. Since `W` is
        #    diagonal, this is equivalent to `C @ (X @ (V / diag(W)))`. In
        #    other words:
        #    - divide `V` (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        #    - matrix-vector multiply by `X`
        #    - mean-center the resulting vector, which has length `num_cells`
        # 4. We also need to calculate `scale(X).T @ V` for the `rmatvec()`
        #    part of our operator. Rewriting as `W.T @ X @ C.T @ V` and
        #    leveraging the fact that `C` turns out to be symmetric (as is `W`,
        #    since it's diagonal), this is equivalent to
        #    `(X @ (C @ V)) / diag(W)`. In other words:
        #    - mean-center `V`, which has length `num_cells`
        #    - matrix-vector multiply by `X.T`
        #    - divide the result (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        # 5. CSR matrix-vector multiplication can be done efficiently
        #    multithreaded, but CSC can't (except by maintaining thread-local
        #    versions of the output vector and summing them across threads at
        #    the end, which leads to differences in floating-point error
        #    depending on the number of threads). This is problematic because,
        #    regardless of whether `X` is a CSR or a CSC matrix, we need to do
        #    some multiplications involving `X.T` and others involving `X`. So
        #    if `X` is CSR, the `X.T` multiplications will be single-threaded
        #    since `X.T` is a CSC matrix. If `X` is CSC, the `X`
        #    multiplications will be single-threaded. So one of the two
        #    multiplications will always be single-threaded. There are hundreds
        #    of these matrix-vector multiplications and they take up a large
        #    majority of the total runtime for PCA, so not being able to fully
        #    multithread them is a huge disadvantage.
        # 6. To address the issue in the previous point, make both a CSR and a
        #    CSC copy of `X` when running PCA in parallel. The first
        #    multiplication (involving `X.T`) will use the CSC copy of `X`, but
        #    plugged into the CSR matrix-vector multiplication function. The
        #    second multiplication (involving `X`) will use CSR multiplication
        #    as normal. This works because plugging a CSC copy of `X` into a
        #    matrix-vector multiplication routine designed for CSR matrices (or
        #    vice versa) is equivalent to multiplying by `X.T` instead of `X`.
        #    The result: both matrix multiplications can be done in parallel.
        #    This also has the nice side benefit that the final result is the
        #    same regaredless of whether the counts are input as CSR or CSC.
        # 7. When running single-threaded with `faster_single_threaded=True`,
        #    however, just use whichever version of `X` (CSR or CSC) we happen
        #    to have available, to avoid the runtime and memory overhead of
        #    creating both versions. However, this means that CSR and CSC no
        #    longer give exactly the same PCs, due to differences in
        #    floating-point error.
        # 8. When calculating the scaled variance for each gene, use the CSC
        #    version of `X` if available, for speed. Clip tiny standard
        #    deviations (less than 1e-8) to 1e-8, like Seurat.
        num_cells, num_genes = X.shape
        clipped_stddev = np.empty(num_genes, dtype=np.float32)
        num_genes_buffer = np.empty(num_genes, dtype=np.float32)
        num_cells_buffer = np.empty(num_cells, dtype=np.float32)
        clip_val = 1e-8
        if faster_single_threading:
            data = X.data
            indices = X.indices
            indptr = X.indptr
            if isinstance(X, csr_array):
                clipped_stddev_csr(
                    data=data, indices=indices, indptr=indptr,
                    num_cells=num_cells, num_genes=num_genes,
                    clip_val=clip_val, clipped_stddev=clipped_stddev)
                
                def matvec(V):
                    matvec_csr(data=data, indices=indices, indptr=indptr, V=V,
                               clipped_stddev=clipped_stddev,
                               num_cells_buffer=num_cells_buffer,
                               num_genes_buffer=num_genes_buffer)
                    return num_cells_buffer
                
                def rmatvec(V):
                    rmatvec_csr(data=data, indices=indices, indptr=indptr, V=V,
                                clipped_stddev=clipped_stddev,
                                num_cells_buffer=num_cells_buffer,
                                num_genes_buffer=num_genes_buffer)
                    return num_genes_buffer
            
            else:
                clipped_stddev_csc(
                    data=data, indices=indices, indptr=indptr,
                    num_cells=num_cells, num_genes=num_genes,
                    clip_val=clip_val, clipped_stddev=clipped_stddev,
                    num_threads=num_threads)
                
                def matvec(V):
                    matvec_csc(data=data, indices=indices, indptr=indptr, V=V,
                               clipped_stddev=clipped_stddev,
                               num_cells_buffer=num_cells_buffer,
                               num_genes_buffer=num_genes_buffer)
                    return num_cells_buffer
                
                def rmatvec(V):
                    rmatvec_csc(data=data, indices=indices, indptr=indptr, V=V,
                                clipped_stddev=clipped_stddev,
                                num_cells_buffer=num_cells_buffer,
                                num_genes_buffer=num_genes_buffer)
                    return num_genes_buffer
        else:
            if isinstance(X, csr_array):
                X_csr = X
                X_csc = X.tocsc()
            else:
                X_csr = X.tocsr()
                X_csc = X
            csr_data = X_csr.data
            csr_indices = X_csr.indices
            csr_indptr = X_csr.indptr
            csc_data = X_csc.data
            csc_indices = X_csc.indices
            csc_indptr = X_csc.indptr
            clipped_stddev_csc(
                data=csc_data, indices=csc_indices, indptr=csc_indptr,
                num_cells=num_cells, num_genes=num_genes, clip_val=clip_val,
                clipped_stddev=clipped_stddev, num_threads=num_threads)
            
            def matvec(V):
                matvec_csr_parallel(data=csr_data, indices=csr_indices,
                                    indptr=csr_indptr, V=V,
                                    clipped_stddev=clipped_stddev,
                                    num_cells_buffer=num_cells_buffer,
                                    num_genes_buffer=num_genes_buffer,
                                    num_threads=num_threads)
                return num_cells_buffer
            
            def rmatvec(V):
                rmatvec_csc_parallel(data=csc_data, indices=csc_indices,
                                     indptr=csc_indptr, V=V,
                                     clipped_stddev=clipped_stddev,
                                     num_cells_buffer=num_cells_buffer,
                                     num_genes_buffer=num_genes_buffer,
                                     num_threads=num_threads)
                return num_genes_buffer
        # noinspection PyArgumentList
        op = LinearOperator(matvec=matvec, rmatvec=rmatvec, dtype=X.dtype,
                            shape=(X.shape[0], X.shape[1]))
        
        # Run PCA with propack. `tol` is hard-coded because it has a minimal
        # effect on both runtime and accuracy as long as `maxiter` is
        # sufficiently large (which is is by default).
        original_num_threads = X._num_threads
        X._num_threads = num_threads
        try:
            with threadpool_limits(num_threads, user_api='blas'):
                # noinspection PyTypeChecker
                U, s = svds(op, num_PCs, solver='propack', tol=1e-5,
                            random_state=seed)[:2]
                PCs = U[:, ::-1] * s[::-1]
        finally:
            X._num_threads = original_num_threads
        
        # Store each dataset's PCs in its `obsm`
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan,
                                      dtype=np.float32)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            else:
                dataset_PCs = np.ascontiguousarray(dataset_PCs)
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    # noinspection PyTypeChecker
    def PCA_old(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            subspace_size: int | np.integer = 100,
            tolerance: int | np.integer | float | np.floating = 1e-6,
            max_iterations: int | np.integer = 1000,
            seed: int | np.integer = 0,
            faster_single_threading: bool = False,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components (PCs) across cells.
        
        Requires normalized counts, so must be run after `normalize()`. By
        default, only the highly variable genes from `hvg()` are used to
        compute PCs.
        
        Uses approximate singular value decomposition (SVD) via the Implicitly
        Restarted Lanczos Bidiagonalization Algorithm (IRLBA). Seurat uses a
        different implementation of the same IRLBA algorithm.
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their PCs set to `NaN`. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            hvg_column: an optional Boolean column of `var` indicating the
                        highly variable genes. Can be a column name, a polars
                        expression, a polars Series, a 1D NumPy array, or a
                        function that takes in this SingleCell dataset and
                        returns a polars Series or 1D NumPy array. Set to
                        `None` to use all genes. When `others` is specified,
                        `hvg_column` can be a length-`1 + len(others)` sequence
                        of columns, expressions, Series, functions, or `None`
                        for each dataset (for `self`, followed by each dataset
                        in `others`).
            PC_key: the key of `obsm` where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            subspace_size: the size of the Krylov subspace used by IRLBA when
                           calculating PCs. Must be greater than or equal to
                           `num_PCs`, and about twice `num_PCs` is recommended.
            tolerance: the relative tolerance (expressed as the ratio of a
                       singular value's residual to the maximum singular value)
                       required to deem a singular value converged. IRLBA will
                       stop early, before `max_iterations` iterations, if all
                       singular values have converged.
            max_iterations: the maximum number of iterations to run IRLBA for,
                            stopping early if all singular values have
                            converged (see `tolerance`)
            seed: the random seed to use when initializing the PCs, via R's
                  `set.seed()` function
            faster_single_threading: if `True`, use a different order of
                                     operations for single-threaded PCA. This
                                     gives a modest (~15-20%) boost in
                                     single-threaded performance, and lower
                                     memory usage, at the cost of no longer
                                     exactly matching the PCs produced by the
                                     multithreaded version (due to differences
                                     in floating-point error arising from the
                                     different order of operations). When
                                     `faster_single_threading=True`, `PCA()`
                                     will also give slightly different results
                                     when run with CSR vs CSC input; when
                                     multiple datasets are provided with a mix
                                     of CSR and CSC formats, all datasets are
                                     converted to the format shared by the most
                                     total cells across datasets. Must be
                                     `False` unless `num_threads=1`. If you are
                                     always going to run single-threaded and do
                                     not care about exactly matching the
                                     multithreaded version, there is no reason
                                     not to set `faster_single_threading=True`.
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print a message when the singular values did
                     not converge to a tolerance of `tolerance` within
                     `max_iterations` iterations
            num_threads: the number of threads to use for PCA. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the returned
                         SingleCell dataset's `num_threads`; this will always
                         be the same as the original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset where `obsm` contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA()` function, which requires `ScaleData()`
            to be run first, this function does not require the data to be
            scaled beforehand. Instead, it implicitly scales the data to zero
            mean and unit variance while performing PCA.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `PC_key` is a string
        check_type(PC_key, 'PC_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `PC_key` is not already in `obsm`, unless `overwrite=True`
        suffix = ' for at least one dataset' if others else ''
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present for every dataset
        if any(dataset._X is None for dataset in datasets):
            error_message = (
                f'X is None{suffix}, so finding principal components is not '
                f'possible')
            raise ValueError(error_message)
        
        # Get `QC_column` (if not `None`) and `hvg_column` from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            allow_None=False,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and possibly '
                         f'normalize()) before PCA()?')
        
        # Check that all datasets are normalized
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        
        # Raise an error if `X` is not float32 for every dataset
        for dataset in datasets:
            if dataset._X.dtype != np.float32:
                error_message = (
                    f'PCA() requires normalized counts with data type '
                    f'float32, but X has data type '
                    f'{str(dataset._X.dtype)!r}{suffix}; did you forget to '
                    f'run normalize() before PCA()?')
                raise TypeError(error_message)
        
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        
        # Check that `subspace_size` is a positive integer, and >= `num_PCs`
        check_type(subspace_size, 'subspace_size', int, 'a positive integer')
        if subspace_size < num_PCs:
            error_message = (
                f'subspace_size is {subspace_size:,}, but must be ≥ num_PCs '
                f'({num_PCs:,})')
            raise ValueError(error_message)
        
        # Check that `tolerance` is a positive number
        check_type(tolerance, 'tolerance', (int, float), 'a positive number')
        check_bounds(tolerance, 'tolerance', 0, left_open=True)
        
        # Check that `max_iterations` is a positive integer
        check_type(max_iterations, 'max_iterations', int, 'a positive integer')
        check_bounds(max_iterations, 'max_iterations', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`.
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `faster_single_threading` is Boolean, and `False` unless
        # `num_threads=1`
        check_type(faster_single_threading, 'faster_single_threading', bool,
                   'Boolean')
        if faster_single_threading and num_threads != 1:
            error_message = \
                'faster_single_threading must be False unless num_threads is 1'
            raise ValueError(error_message)
        
        # Get the matrix to compute PCA across: a sparse matrix of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use `X[np.ix_(rows, columns)]` as a faster,
        # more memory-efficient alternative to `X[rows][:, columns]`.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = self._var.filter(hvg_columns[0]).to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [dataset._X[np.ix_(QC_col.to_numpy(),
                                                hvg_col.to_numpy())]
                              if QC_col is not None else
                              dataset._X[:, hvg_col.to_numpy()]
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        if len(Xs) == 1:
            X = Xs[0]
        elif all(isinstance(X, csr_array) for X in Xs):
            X = sparse_major_stack(Xs, num_threads=num_threads)
        elif all(isinstance(X, csc_array) for X in Xs):
            X = sparse_minor_stack(Xs, num_threads=num_threads)
        else:
            # Mix of CSR and CSC: convert to whichever format has the most
            # total cells in that format, to reduce the number of datasets that
            # need to be flipped
            total_cells = num_cells_per_dataset.sum()
            total_csr = sum(X.shape[0] for X in Xs if isinstance(X, csr_array))
            if total_csr / total_cells > 0.5:  # more CSR than CSC
                X = sparse_major_stack([
                    X.tocsr() if isinstance(X, csc_array) else X for X in Xs],
                    num_threads=num_threads)
            else:
                X = sparse_minor_stack([
                    X.tocsc() if isinstance(X, csr_array) else X for X in Xs],
                    num_threads=num_threads)
        del Xs
        
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        
        # Define Cython functions. We want to perform SVD on `X`, scaled to
        # zero mean and unit variance.
        # Key ideas:
        # 1. Because `X` is a sparse matrix, mean-centering cannot be done
        #    without converting to a dense matrix. So scaling `X` cannot be
        #    done in the conventional way.
        # 2. Fortunately, we can represent scaling as a matrix product:
        #    `scale(X) = C @ X @ W`, where `W` is a diagonal matrix of the
        #    standard deviations for each column (gene) and `C` is a "centering
        #    matrix" that, when applied to any vector or matrix, yields the
        #    mean-centered version of it.
        # 3. We need to calculate the matrix-vector product of our operator
        #    with some vector `V`, i.e. `scale(X) @ V`. Using the formula from
        #    point #2, this is equivalent to `C @ X @ W @ V`. Since `W` is
        #    diagonal, this is equivalent to `C @ (X @ (V / diag(W)))`. In
        #    other words:
        #    - divide `V` (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        #    - matrix-vector multiply by `X`
        #    - mean-center the resulting vector, which has length `num_cells`
        # 4. We also need to calculate `scale(X).T @ V` for the `rmatvec()`
        #    part of our operator. Rewriting as `W.T @ X @ C.T @ V` and
        #    leveraging the fact that `C` turns out to be symmetric (as is `W`,
        #    since it's diagonal), this is equivalent to
        #    `(X @ (C @ V)) / diag(W)`. In other words:
        #    - mean-center `V`, which has length `num_cells`
        #    - matrix-vector multiply by `X.T`
        #    - divide the result (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        # 5. CSR matrix-vector multiplication can be done efficiently
        #    multithreaded, but CSC can't (except by maintaining thread-local
        #    versions of the output vector and summing them across threads at
        #    the end, which leads to differences in floating-point error
        #    depending on the number of threads). This is problematic because,
        #    regardless of whether `X` is a CSR or a CSC matrix, we need to do
        #    some multiplications involving `X.T` and others involving `X`. So
        #    if `X` is CSR, the `X.T` multiplications will be single-threaded
        #    since `X.T` is a CSC matrix. If `X` is CSC, the `X`
        #    multiplications will be single-threaded. So one of the two
        #    multiplications will always be single-threaded. There are hundreds
        #    of these matrix-vector multiplications and they take up a large
        #    majority of the total runtime for PCA, so not being able to fully
        #    multithread them is a huge disadvantage.
        # 6. To address the issue in the previous point, make both a CSR and a
        #    CSC copy of `X` when running PCA in parallel. The first
        #    multiplication (involving `X.T`) will use the CSC copy of `X`, but
        #    plugged into the CSR matrix-vector multiplication function. The
        #    second multiplication (involving `X`) will use CSR multiplication
        #    as normal. This works because plugging a CSC copy of `X` into a
        #    matrix-vector multiplication routine designed for CSR matrices (or
        #    vice versa) is equivalent to multiplying by `X.T` instead of `X`.
        #    The result: both matrix multiplications can be done in parallel.
        #    This also has the nice side benefit that the final result is the
        #    same regaredless of whether the counts are input as CSR or CSC.
        # 7. When running single-threaded with `faster_single_threaded=True`,
        #    however, just use whichever version of `X` (CSR or CSC) we happen
        #    to have available, to avoid the runtime and memory overhead of
        #    creating both versions. However, this means that CSR and CSC no
        #    longer give exactly the same PCs, due to differences in
        #    floating-point error.
        # 8. When calculating the scaled variance for each gene, use the CSC
        #    version of `X` if available, for speed. Clip tiny standard
        #    deviations (less than 1e-8) to 1e-8, like Seurat.
        cython_functions = cython_inline(_uninitialized_vector_import + r'''
        cimport numpy as np
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libc.limits cimport UINT_MAX
        from libc.math cimport M_PI
        from libcpp.algorithm cimport fill
        from libcpp.cmath cimport abs, sqrt, log, cos
        from libcpp.vector cimport vector
        from scipy.linalg.cython_blas cimport sgemm as sgemm_, sgemv as sgemv_
        from scipy.linalg.cython_lapack cimport sgesdd
        
        ctypedef fused signed_integer:
            int
            long
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline float random_uniform(unsigned long* state) noexcept nogil:
            # Returns a random number in U(0, 1)
            return <float> rand(state) / UINT_MAX
        
        cdef inline float random_normal(unsigned long* state) noexcept nogil:
            # Samples a random number from the standard normal distribution
            # using the Box-Muller transform
            
            cdef double r, theta
            
            r = sqrt(-2 * log(random_uniform(state)))
            theta = 2 * M_PI * random_uniform(state)
            return r * cos(theta)
        
        cdef inline float norm(float[::1] array) noexcept nogil:
            cdef unsigned i
            cdef float norm = 0
            for i in range(array.shape[0]):
                norm += array[i] * array[i]
            return sqrt(norm)
        
        cdef inline void sgemv(const char trans,
                               const int m,
                               const int n,
                               const float alpha,
                               const float* a,
                               const int lda,
                               const float* x,
                               const int incx,
                               const float beta,
                               float* y,
                               const int incy) noexcept nogil:
            # Matrix multiplication wrapper that accepts scalars by value and
            # provides const-correctness
            
            sgemv_(<char*> &trans, <int*> &m, <int*> &n, <float*> &alpha,
                   <float*> a, <int*> &lda, <float*> x, <int*> &incx,
                   <float*> &beta, y, <int*> &incy)
        
        cdef inline void sgemm(const char transa,
                               const char transb,
                               const int m,
                               const int n,
                               const int k,
                               const float alpha,
                               const float* a,
                               const int lda,
                               const float* b,
                               const int ldb,
                               const float beta,
                               float* c,
                               const int ldc) noexcept nogil:
            # Matrix multiplication wrapper that accepts scalars by value and
            # provides const-correctness
            
            sgemm_(<char*> &transa, <char*> &transb, <int*> &m, <int*> &n, <int*> &k,
                   <float*> &alpha, <float*> a, <int*> &lda, <float*> b, <int*> &ldb,
                   <float*> &beta, c, <int*> &ldc)
        
        def clipped_stddev_csr(const float[::1] data,
                               const signed_integer[::1] indices,
                               const signed_integer[::1] indptr,
                               const unsigned long num_cells,
                               const unsigned num_genes,
                               float clip_val,
                               float[::1] clipped_stddev):
            # Compute `X.std(axis=0)` where `X` is CSR, clipping to a minimum of
            # `clip_val`. Only used when `num_threads=1` and
            # `faster_single_threading=True`.
            
            cdef unsigned long num_elements, i, j, start, end, \
                chunk_size
            cdef unsigned gene, cell, thread_index
            cdef float value, total_sum, total_sum_of_squares, \
                inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
            cdef vector[float] sum, sum_of_squares
            sum.resize(num_genes)
            sum_of_squares.resize(num_genes)
            
            # Iterate over all elements of the count matrix, ignoring which cell
            # they're from
            num_elements = indices.shape[0]
            for i in range(num_elements):
                gene = indices[i]
                value = data[i]
                sum[gene] += value
                sum_of_squares[gene] += value * value
            
            # Calculate standard deviations from the sums and squared sums
            for gene in range(num_genes):
                clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                    num_cells * sum_of_squares[gene] - sum[gene] * sum[gene]))
                if clipped_stddev[gene] < clip_val:
                    clipped_stddev[gene] = clip_val
        
        def clipped_stddev_csc(const float[::1] data,
                               const signed_integer[::1] indices,
                               const signed_integer[::1] indptr,
                               const unsigned long num_cells,
                               const unsigned num_genes,
                               float clip_val,
                               float[::1] clipped_stddev,
                               const unsigned num_threads):
            # Compute `X.std(axis=0)` where `X` is CSC, clipping to a minimum of
            # `clip_val`.
            
            cdef unsigned long i, gene
            cdef float value, sum, sum_of_squares, \
                inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
            
            if num_threads == 1:
                for gene in range(num_genes):
                    # Calculate the sum and squared sum for this gene, across cells
                    # with non-zero counts for the gene
                    sum = 0
                    sum_of_squares = 0
                    for i in range(<unsigned long> indptr[gene],
                                   <unsigned long> indptr[gene + 1]):
                        value = data[i]
                        sum += value
                        sum_of_squares += value * value
        
                    # Calculate the scaled variance from the sum and squared sum
                    clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                        num_cells * sum_of_squares - sum * sum))
                    if clipped_stddev[gene] < clip_val:
                        clipped_stddev[gene] = clip_val
            else:
                for gene in prange(num_genes, nogil=True, num_threads=num_threads):
                    sum = 0
                    sum_of_squares = 0
                    for i in range(<unsigned long> indptr[gene],
                                   <unsigned long> indptr[gene + 1]):
                        value = data[i]
                        sum = sum + value
                        sum_of_squares = sum_of_squares + value * value
                    clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                        num_cells * sum_of_squares - sum * sum))
                    if clipped_stddev[gene] < clip_val:
                        clipped_stddev[gene] = clip_val
            
        cdef void matvec_csr(const float[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const float[::1] V,
                             const float[::1] clipped_stddev,
                             float* num_cells_buffer,
                             float* num_genes_buffer,
                             const unsigned num_cells,
                             const unsigned num_genes,
                             const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Also has a parallel version,
            # `matvec_csr_parallel()`. `num_threads` is unused, but needed to
            # match the signature of `matvec_csr_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
                
            # Matrix-vector multiplication and mean calculation
            for i in range(num_cells):
                num_cells_buffer[i] = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_cells_buffer[i] += data[j] * num_genes_buffer[indices[j]]
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] -= mean
        
        cdef void matvec_csr_parallel(const float[::1] data,
                                      const signed_integer[::1] indices,
                                      const signed_integer[::1] indptr,
                                      const float[::1] V,
                                      const float[::1] clipped_stddev,
                                      float* num_cells_buffer,
                                      float* num_genes_buffer,
                                      const unsigned num_cells,
                                      const unsigned num_genes,
                                      const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Also has a single-threaded
            # version, `matvec_csr()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            # Variance scaling (single-threaded since `num_genes` is just 2000
            # by default)
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
            
            with nogil:
                # Matrix-vector multiplication
                for i in prange(num_cells, num_threads=num_threads):
                    num_cells_buffer[i] = 0
                    for j in range(<unsigned long> indptr[i],
                                   <unsigned long> indptr[i + 1]):
                        num_cells_buffer[i] += \
                            data[j] * num_genes_buffer[indices[j]]
                
                # Mean calculation (must be done single-threaded to be
                # deterministic, due to floating-point error)
                for i in range(num_cells):
                    mean += num_cells_buffer[i]
                mean /= num_cells
                
                # Mean scaling
                for i in prange(num_cells, num_threads=num_threads):
                    num_cells_buffer[i] -= mean
        
        cdef void matvec_csc(const float[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const float[::1] V,
                             const float[::1] clipped_stddev,
                             float* num_cells_buffer,
                             float* num_genes_buffer,
                             const unsigned num_cells,
                             const unsigned num_genes,
                             const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Does not have a parallel version.
            # `num_threads` is unused, but needed to match the signature of
            # `matvec_csr_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
            
            # Matrix-vector multiplication
            fill(num_cells_buffer, num_cells_buffer + num_cells, 0)
            for i in range(num_genes):
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_cells_buffer[indices[j]] += data[j] * num_genes_buffer[i]
                    
            # Mean calculation
            for i in range(num_cells):
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] -= mean
        
        cdef void rmatvec_csr(const float[::1] data,
                              const signed_integer[::1] indices,
                              const signed_integer[::1] indptr,
                              const float[::1] V,
                              const float[::1] clipped_stddev,
                              float* num_cells_buffer,
                              float* num_genes_buffer,
                              const unsigned num_cells,
                              const unsigned num_genes,
                              const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Does not have a parallel version.
            # `num_threads` is unused, but needed to match the signature of
            # `rmatvec_csc_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            # Mean scaling
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            for i in range(num_cells):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            fill(num_genes_buffer, num_genes_buffer + num_genes, 0)
            for i in range(num_cells):
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_genes_buffer[indices[j]] += data[j] * num_cells_buffer[i]
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]
        
        cdef void rmatvec_csc(const float[::1] data,
                              const signed_integer[::1] indices,
                              const signed_integer[::1] indptr,
                              const float[::1] V,
                              const float[::1] clipped_stddev,
                              float* num_cells_buffer,
                              float* num_genes_buffer,
                              const unsigned num_cells,
                              const unsigned num_genes,
                              const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Also has a parallel version,
            # `rmatvec_csc_parallel()`. `num_threads` is unused, but needed to match
            # the signature of `rmatvec_csc_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            # Mean calculation
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            for i in range(num_genes):
                num_genes_buffer[i] = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_genes_buffer[i] += data[j] * num_cells_buffer[indices[j]]
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]
        
        cdef void rmatvec_csc_parallel(const float[::1] data,
                                       const signed_integer[::1] indices,
                                       const signed_integer[::1] indptr,
                                       const float[::1] V,
                                       const float[::1] clipped_stddev,
                                       float* num_cells_buffer,
                                       float* num_genes_buffer,
                                       const unsigned num_cells,
                                       const unsigned num_genes,
                                       const unsigned num_threads) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Also has a single-threaded
            # version, `rmatvec_csc()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float mean = 0
            
            with nogil:
                # Mean calculation (must be done single-threaded to be
                # deterministic, due to floating-point error)
                for i in range(num_cells):
                    mean += V[i]
                mean /= num_cells
                
                # Mean scaling
                for i in prange(num_cells, num_threads=num_threads):
                    num_cells_buffer[i] = V[i] - mean
                
                # Matrix-vector multiplication
                for i in prange(num_genes, num_threads=num_threads):
                    num_genes_buffer[i] = 0
                    for j in range(<unsigned long> indptr[i],
                                   <unsigned long> indptr[i + 1]):
                        num_genes_buffer[i] += \
                            data[j] * num_cells_buffer[indices[j]]
            
            # Variance scaling (single-threaded since `num_genes` is just 2000
            # by default)
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]

        cdef inline int svd_workspace_size(float[::1, :] B,
                                           float[::1, :] U,
                                           float[::1] S,
                                           float[::1, :] Vt,
                                           int* iwork) noexcept nogil:
            # Get the optimal workspace size for SVD with `sgesdd()`. Assumes `m < n`,
            # which it is for `B`.
            
            cdef char jobz = b'S'
            cdef int info, m = B.shape[0], n = B.shape[1], lwork = -1
            cdef float workspace_size
            
            sgesdd(&jobz, &m, &n, &B[0, 0], &m, &S[0], &U[0, 0], &m, &Vt[0, 0], &m,
                   &workspace_size, &lwork, iwork, &info)
            return <int> workspace_size

        cdef inline void svd(float[::1, :] B,
                             float[::1, :] U,
                             float[::1] S,
                             float[::1, :] Vt,
                             float* work,
                             int* iwork,
                             const int lwork) noexcept nogil:
            # SVD for the case where `m < n`, which it is for `B`. Uses the optimal
            # workspace size computed by `svd_workspace_size()`.
            
            cdef char jobz = b'S'
            cdef int info, m = B.shape[0], n = B.shape[1]
            
            sgesdd(&jobz, &m, &n, &B[0, 0], &m, &S[0], &U[0, 0], &m, &Vt[0, 0], &m,
                   work, <int*> &lwork, iwork, &info)
        
        ctypedef void (*matvec_function)(const float[::1] data,
                                         const signed_integer[::1] indices,
                                         const signed_integer[::1] indptr,
                                         const float[::1] V,
                                         const float[::1] clipped_stddev,
                                         float* num_cells_buffer,
                                         float* num_genes_buffer,
                                         const unsigned num_cells,
                                         const unsigned num_genes,
                                         const unsigned num_threads) noexcept nogil
        
        def irlba(const float[::1] data_matvec,
                  const signed_integer[::1] indices_matvec,
                  const signed_integer[::1] indptr_matvec,
                  const float[::1] data_rmatvec,
                  const signed_integer[::1] indices_rmatvec,
                  const signed_integer[::1] indptr_rmatvec,
                  const float[::1] clipped_stddev,
                  const unsigned num_cells,
                  const unsigned num_genes,
                  const unsigned k,
                  const unsigned subspace_size,
                  const float tolerance,
                  const unsigned max_iterations,
                  const unsigned long seed,
                  const bint faster_single_threading,
                  const bint is_csr,
                  const unsigned num_threads):
            
            cdef unsigned iteration, i, j, row, col, num_converged, k_current = 0
            cdef unsigned long state = srand(seed)
            cdef float inverse_norm, dot_product, alpha, inverse_alpha, beta, \
                inverse_beta, remainder_norm, residual, \
                Smax = 2.4221817809573368e-05  # (float32 eps) ** (2 / 3)
            cdef int lwork
            cdef bint converged = False
            cdef uninitialized_vector[float] U_buffer, U_new_buffer, V_buffer, \
                V_new_buffer, B_buffer, Ub_buffer, Sb_buffer, Vbt_buffer, \
                remainder_buffer, work
            cdef uninitialized_vector[int] iwork
            cdef float[::1, :] U, U_new, V, V_new, B, Ub, Vbt, temp
            cdef float[::1] work2, Sb
            cdef matvec_function matvec, rmatvec
            cdef np.ndarray[np.float32_t, ndim=2] PCs
            cdef np.npy_intp dims[2]
            dims[0] = num_cells
            dims[1] = k
            
            # Allocate workspace arrays:
            
            # `U` and `U_new`: left singular vectors
            U_buffer.resize(num_cells * subspace_size)
            U = <float[:num_cells:1, :subspace_size]> U_buffer.data()
            U_new_buffer.resize(num_cells * subspace_size)
            U_new = <float[:num_cells:1, :subspace_size]> U_new_buffer.data()
            
            # `V` and `V_new`: right singular vectors
            V_buffer.resize(num_genes * subspace_size)
            V = <float[:num_genes:1, :subspace_size]> V_buffer.data()
            V_new_buffer.resize(num_genes * subspace_size)
            V_new = <float[:num_genes:1, :subspace_size]> V_new_buffer.data()
            
            # The bidiagonal matrix (`B`) and its left singular vectors (`Ub`),
            # singular values (`Sb`), and transposed right singular vectors (`Vbt`)
            B_buffer.resize(subspace_size * (subspace_size + 1))
            B = <float[:subspace_size:1, :subspace_size + 1]> B_buffer.data()
            Ub_buffer.resize(subspace_size * subspace_size)
            Ub = <float[:subspace_size:1, :subspace_size]> Ub_buffer.data()
            Sb_buffer.resize(subspace_size)
            Sb = <float[:subspace_size]> Sb_buffer.data()
            Vbt_buffer.resize(subspace_size * (subspace_size + 1))
            Vbt = <float[:subspace_size:1, :subspace_size + 1]> Vbt_buffer.data()
            
            # The remainder from the Lanczos bidiagonalization (`remainder`)
            remainder_buffer.resize(num_genes)
            cdef float[::1] remainder = <float[:num_genes]> remainder_buffer.data()
            
            # The temporary buffers used by `sgesdd()` to perform SVD of `B`
            # (`iwork` and `work`). The optimal size of the latter is
            # determined programmatically via `svd_workspace_size()`.
            iwork.resize(8 * subspace_size)
            lwork = svd_workspace_size(B, Ub, Sb, Vbt, iwork.data())
            work.resize(lwork)
            
            # Decide which function to use for matvec and rmatvec
            if faster_single_threading:
                if is_csr:
                    matvec = matvec_csr
                    rmatvec = rmatvec_csr
                else:
                    matvec = matvec_csc
                    rmatvec = rmatvec_csc
            else:
                matvec = matvec_csr_parallel
                rmatvec = rmatvec_csc_parallel
            
            # Initialize the first column of `V` with a random normal vector
            for i in range(num_genes):
                V[i, 0] = random_normal(&state)
            inverse_norm = 1 / norm(V[:, 0])
            for i in range(num_genes):
                V[i, 0] *= inverse_norm
            
            # Initialize `B` with zeros
            B[:] = 0
            
            # Now that setup is done, run the IRLBA iterations
            if num_threads == 1:
                for iteration in range(max_iterations):
                    # Perform Lanczos bidiagonalization with reorthogonalization;
                    # use for loops instead of `sgemv()` for matrix-vector
                    # multiplication to ensure deterministic parallelism
                    j = k_current
                    while True:
                        # `U[:, j] = X_scaled @ V[:, j]`; use `remainder` as a
                        # temporary buffer (it happens to have the correct type, float,
                        # and length, `num_genes`)
                        matvec(data_matvec, indices_matvec, indptr_matvec, V[:, j],
                               clipped_stddev, &U[0, j], &remainder[0], num_cells,
                               num_genes, num_threads)
                        
                        if j > 0:
                            # U[:, j] -= U[:, :j] @ (U[:, :j].T @ U[:, j])
                            # Use `work` to store the intermediate product
                            # `U[:, :j].T @ U[:, j]`, which has length `j`. This is
                            # guaranteed to fit since `sgesdd()` guarantees that `work` has
                            # at least `4 * subspace_size ** 2 + 7 * subspace_size`
                            # elements, and we only need `j`, which is less than
                            # `subspace_size`.
                            
                            # `work = U[:, :j].T @ U[:, j]`
                            # sgemv(b'T', num_cells, j, 1, &U[0, 0], num_cells, &U[0, j], 1, 0,
                            #       work.data(), 1)
                            for col in range(j):
                                dot_product = 0
                                for row in range(num_cells):
                                    dot_product = dot_product + U[row, col] * U[row, j]
                                work[col] = dot_product
                            
                            # `U[:, j] -= U[:, :j] @ work`
                            # sgemv(b'N', num_cells, j, -1, &U[0, 0], num_cells,
                            #       work.data(), 1, 1, &U[0, j], 1)
                            for row in range(num_cells):
                                dot_product = 0
                                for col in range(j):
                                    dot_product = dot_product + U[row, col] * work[col]
                                U[row, j] -= dot_product
                            
                        alpha = norm(U[:, j])
                        inverse_alpha = 1 / alpha
                        for i in range(num_cells):
                            U[i, j] *= inverse_alpha
                        B[j, j] = alpha
            
                        if j < subspace_size - 1:
                            # `V[:, j + 1] = X_scaled.T @ U[:, j]`; use `U_new` as a
                            # temporary buffer (it happens to have the correct type, float,
                            # and length, at least `num_cells`)
                            rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                    U[:, j], clipped_stddev, &U_new[0, 0], &V[0, j + 1],
                                    num_cells, num_genes, num_threads)
                            
                            # `V[:, j + 1] -= alpha * V[:, j]`
                            for i in range(num_genes):
                                V[i, j + 1] -= alpha * V[i, j]
                            
                            # `V[:, j + 1] -= V[:, :j + 1] @ (V[:, :j + 1].T @ V[:, j + 1])`;
                            # use `work` to store the intermediate product, which has length
                            # `j + 1` and is guaranteed to fit: as mentioned above, `work` has
                            # at least `4 * subspace_size ** 2 + 7 * subspace_size` elements
                            
                            # `work = V[:, :j + 1].T @ V[:, j + 1]`
                            # sgemv(b'T', num_genes, j + 1, 1, &V[0, 0], num_genes,
                            #       &V[0, j + 1], 1, 0, work.data(), 1)
                            for col in range(j + 1):
                                dot_product = 0
                                for row in range(num_genes):
                                    dot_product = dot_product + V[row, col] * V[row, j + 1]
                                work[col] = dot_product
                            
                            # `V[:, j + 1] -= V[:, :j + 1] @ work`
                            # sgemv(b'N', num_genes, j + 1, -1, &V[0, 0], num_genes,
                            #       work.data(), 1, 1, &V[0, j + 1], 1)
                            for row in range(num_genes):
                                dot_product = 0
                                for col in range(j + 1):
                                    dot_product = dot_product + V[row, col] * work[col]
                                V[row, j + 1] -= dot_product
                        else:
                            # `j == subspace_size - 1` so `V[:, j + 1]` would overwrite
                            # the end of the array. Do the same computation, but store the
                            # results in the `remainder` vector instead of `V[:, j + 1]`.
                            # `remainder = X_scaled.T @ U[:, j]`
                            rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                    U[:, j], clipped_stddev, &U_new[0, 0], &remainder[0],
                                    num_cells, num_genes, num_threads)
                            
                            # `remainder -= alpha * V[:, j]`
                            for i in range(num_genes):
                                remainder[i] -= alpha * V[i, j]
                            
                            # `work = V.T @ remainder`
                            # sgemv(b'T', num_genes, subspace_size, 1, &V[0, 0], num_genes,
                            #       &remainder[0], 1, 0, &work[0], 1)
                            for col in range(subspace_size):
                                dot_product = 0
                                for row in range(num_genes):
                                    dot_product = dot_product + V[row, col] * remainder[row]
                                work[col] = dot_product
                            
                            # `remainder -= V @ work`
                            # sgemv(b'N', num_genes, subspace_size, -1, &V[0, 0], num_genes,
                            #       &work[0], 1, 1, &remainder[0], 1)
                            for row in range(num_genes):
                                dot_product = 0
                                for col in range(subspace_size):
                                    dot_product = dot_product + V[row, col] * work[col]
                                remainder[row] -= dot_product
                            
                            break
                            
                        beta = norm(V[:, j + 1])
                        inverse_beta = 1 / beta
                        for i in range(num_genes):
                            V[i, j + 1] *= inverse_beta
                        B[j, j + 1] = beta
                        j += 1
                        if j % 10 == 9:
                            PyErr_CheckSignals()
                        
                    # Compute the SVD of the bidiagonal matrix `B`, equivalent to
                    # `Ub, Sb, Vbt = np.linalg.svd(B, full_matrices=False)`
                    svd(B, Ub, Sb, Vbt, work.data(), iwork.data(), lwork)
                    
                    # Normalize the `remainder` vector
                    remainder_norm = norm(remainder)
                    for i in range(num_genes):
                        remainder[i] /= remainder_norm
                    
                    # Update `Smax`, the largest singular value of `B` we've seen so far
                    Smax = max(Sb[0], Smax)
                    
                    # Check convergence of singular values via residuals
                    num_converged = 0
                    for i in range(k):
                        residual = remainder_norm * Ub[subspace_size - 1, i]
                        num_converged += abs(residual) < tolerance * Smax
                    if num_converged == k:
                        converged = True
                        break
                    
                    # For the next iteration, increase `k_current` to `k` + the number of
                    # converged singular values, ensuring it stays under the subspace size
                    k_current = min(max(k_current, k + num_converged), subspace_size - 1)
                    
                    # Restart with new subspace: update `U`, `V` and `B`. The `U` and `V`
                    # updates are in-place, but `sgemm()` doesn't support in-place matrix
                    # multiplication, so use `U_new` and `V_new` as the output arrays and
                    # then swap the pointers so that `U` and `V` point to the output.
                    
                    # U[:, :k_current] = U @ Ub[:, :k_current]
                    sgemm(b'N', b'N', num_cells, k_current, subspace_size, 1, &U[0, 0],
                          num_cells, &Ub[0, 0], subspace_size, 0, &U_new[0, 0], num_cells)
                    temp = U
                    U = U_new
                    U_new = temp
                    
                    # V[:, :k_current] = V @ Vbt.T[:subspace_size, :k_current]
                    sgemm(b'N', b'T', num_genes, k_current, subspace_size, 1, &V[0, 0],
                          num_genes, &Vbt[0, 0], subspace_size, 0, &V_new[0, 0], num_genes)
                    temp = V
                    V = V_new
                    V_new = temp
                    
                    for i in range(num_genes):
                        V[i, k_current] = remainder[i]
                    B[:] = 0
                    for i in range(k_current):
                        B[i, i] = Sb[i]
                        residual = remainder_norm * Ub[subspace_size - 1, i]
                        B[i, k_current] = residual
            else:
                # Same as the single-threaded version, except for `nogil` and `prange()`
                with nogil:
                    for iteration in range(max_iterations):
                        # Perform Lanczos bidiagonalization with reorthogonalization
                        j = k_current
                        while True:
                            # `U[:, j] = X_scaled @ V[:, j]`; use `remainder` as a
                            # temporary buffer (it happens to have the correct type, float,
                            # and length, `num_genes`)
                            matvec(data_matvec, indices_matvec, indptr_matvec, V[:, j],
                                   clipped_stddev, &U[0, j], &remainder[0], num_cells,
                                   num_genes, num_threads)
                            
                            if j > 0:
                                # U[:, j] -= U[:, :j] @ (U[:, :j].T @ U[:, j])
                                # Use `work` to store the intermediate product
                                # `U[:, :j].T @ U[:, j]`, which has length `j`. This is
                                # guaranteed to fit since `sgesdd()` guarantees that `work` has
                                # at least `4 * subspace_size ** 2 + 7 * subspace_size`
                                # elements, and we only need `j`, which is less than
                                # `subspace_size`.
                                
                                # `work = U[:, :j].T @ U[:, j]`
                                # sgemv(b'T', num_cells, j, 1, &U[0, 0], num_cells, &U[0, j], 1, 0,
                                #       work.data(), 1)
                                for col in prange(j, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_cells):
                                        dot_product = dot_product + U[row, col] * U[row, j]
                                    work[col] = dot_product
                                
                                # `U[:, j] -= U[:, :j] @ work`
                                # sgemv(b'N', num_cells, j, -1, &U[0, 0], num_cells,
                                #       work.data(), 1, 1, &U[0, j], 1)
                                for row in prange(num_cells, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(j):
                                        dot_product = dot_product + U[row, col] * work[col]
                                    U[row, j] -= dot_product
                                
                            alpha = norm(U[:, j])
                            inverse_alpha = 1 / alpha
                            for i in prange(num_cells, num_threads=num_threads):
                                U[i, j] *= inverse_alpha
                            B[j, j] = alpha
                
                            if j < subspace_size - 1:
                                # `V[:, j + 1] = X_scaled.T @ U[:, j]`; use `U_new` as a
                                # temporary buffer (it happens to have the correct type, float,
                                # and length, at least `num_cells`)
                                rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                        U[:, j], clipped_stddev, &U_new[0, 0], &V[0, j + 1],
                                        num_cells, num_genes, num_threads)
                                
                                # `V[:, j + 1] -= alpha * V[:, j]`
                                for i in prange(num_genes, num_threads=num_threads):
                                    V[i, j + 1] -= alpha * V[i, j]
                                
                                # `V[:, j + 1] -= V[:, :j + 1] @ (V[:, :j + 1].T @ V[:, j + 1])`;
                                # use `work` to store the intermediate product, which has length
                                # `j + 1` and is guaranteed to fit: as mentioned above, `work` has
                                # at least `4 * subspace_size ** 2 + 7 * subspace_size` elements
                                
                                # `work = V[:, :j + 1].T @ V[:, j + 1]`
                                # sgemv(b'T', num_genes, j + 1, 1, &V[0, 0], num_genes,
                                #       &V[0, j + 1], 1, 0, work.data(), 1)
                                for col in prange(j + 1, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_genes):
                                        dot_product = dot_product + V[row, col] * V[row, j + 1]
                                    work[col] = dot_product
                                
                                # `V[:, j + 1] -= V[:, :j + 1] @ work`
                                # sgemv(b'N', num_genes, j + 1, -1, &V[0, 0], num_genes,
                                #       work.data(), 1, 1, &V[0, j + 1], 1)
                                for row in prange(num_genes, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(j + 1):
                                        dot_product = dot_product + V[row, col] * work[col]
                                    V[row, j + 1] -= dot_product
                            else:
                                # `j == subspace_size - 1` so `V[:, j + 1]` would overwrite
                                # the end of the array. Do the same computation, but store the
                                # results in the `remainder` vector instead of `V[:, j + 1]`.
                                # `remainder = X_scaled.T @ U[:, j]`
                                rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                        U[:, j], clipped_stddev, &U_new[0, 0], &remainder[0],
                                        num_cells, num_genes, num_threads)
                                
                                # `remainder -= alpha * V[:, j]`
                                for i in prange(num_genes, num_threads=num_threads):
                                    remainder[i] -= alpha * V[i, j]
                                
                                # `work = V.T @ remainder`
                                # sgemv(b'T', num_genes, subspace_size, 1, &V[0, 0], num_genes,
                                #       &remainder[0], 1, 0, &work[0], 1)
                                for col in prange(subspace_size, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_genes):
                                        dot_product = dot_product + V[row, col] * remainder[row]
                                    work[col] = dot_product
                                
                                # `remainder -= V @ work`
                                # sgemv(b'N', num_genes, subspace_size, -1, &V[0, 0], num_genes,
                                #       &work[0], 1, 1, &remainder[0], 1)
                                for row in prange(num_genes, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(subspace_size):
                                        dot_product = dot_product + V[row, col] * work[col]
                                    remainder[row] -= dot_product
                                
                                break
                                
                            beta = norm(V[:, j + 1])
                            inverse_beta = 1 / beta
                            for i in range(num_genes):
                                V[i, j + 1] *= inverse_beta
                            B[j, j + 1] = beta
                            j += 1
                            if j % 10 == 9:
                                with gil:
                                    PyErr_CheckSignals()
                            
                        # Compute the SVD of the bidiagonal matrix `B`, equivalent to
                        # `Ub, Sb, Vbt = np.linalg.svd(B, full_matrices=False)`
                        svd(B, Ub, Sb, Vbt, work.data(), iwork.data(), lwork)
                        
                        # Normalize the `remainder` vector
                        remainder_norm = norm(remainder)
                        for i in range(num_genes):
                            remainder[i] /= remainder_norm
                        
                        # Update `Smax`, the largest singular value of `B` we've seen so far
                        Smax = max(Sb[0], Smax)
                        
                        # Check convergence of singular values via residuals
                        num_converged = 0
                        for i in range(k):
                            residual = remainder_norm * Ub[subspace_size - 1, i]
                            num_converged += abs(residual) < tolerance * Smax
                        if num_converged == k:
                            converged = True
                            break
                        
                        # For the next iteration, increase `k_current` to `k` + the number of
                        # converged singular values, ensuring it stays under the subspace size
                        k_current = min(max(k_current, k + num_converged), subspace_size - 1)
                        
                        # Restart with new subspace: update `U`, `V` and `B`. The `U` and `V`
                        # updates are in-place, but `sgemm()` doesn't support in-place matrix
                        # multiplication, so use `U_new` and `V_new` as the output arrays and
                        # then swap the pointers so that `U` and `V` point to the output.
                        
                        # U[:, :k_current] = U @ Ub[:, :k_current]
                        sgemm(b'N', b'N', num_cells, k_current, subspace_size, 1, &U[0, 0],
                              num_cells, &Ub[0, 0], subspace_size, 0, &U_new[0, 0], num_cells)
                        temp = U
                        U = U_new
                        U_new = temp
                        
                        # V[:, :k_current] = V @ Vbt.T[:subspace_size, :k_current]
                        sgemm(b'N', b'T', num_genes, k_current, subspace_size, 1, &V[0, 0],
                              num_genes, &Vbt[0, 0], subspace_size, 0, &V_new[0, 0], num_genes)
                        temp = V
                        V = V_new
                        V_new = temp
                        
                        for i in range(num_genes):
                            V[i, k_current] = remainder[i]
                        B[:] = 0
                        for i in range(k_current):
                            B[i, i] = Sb[i]
                            residual = remainder_norm * Ub[subspace_size - 1, i]
                            B[i, k_current] = residual
                
            # Free all no-longer-needed memory, then allocate the final NumPy array
            # for the PCs
            if &U_new[0, 0] == U_buffer.data():
                U_new_buffer.clear()
            else:
                U_buffer.clear()
            V_buffer.clear()
            V_new_buffer.clear()
            B_buffer.clear()
            Vbt_buffer.clear()
            remainder_buffer.clear()
            work.clear()
            np.import_array()
            # PCs = np.empty((num_cells, k), dtype=np.float32)
            PCs = np.PyArray_EMPTY(2, dims, np.NPY_FLOAT32, 0)
        
            # Construct final PCs from the top `k` components of `U` and `S`:
            # U[:, :k] = U[:, :subspace_size] @ Ub[:, :k]
            # PCs[:] = U[:, :k] * Sb[:k]
            # As an optimization, due to linearity we can switch the order of
            # operations to:
            # Ub[:, :k] *= Sb[:k]
            for j in range(k):
                for i in range(subspace_size):
                    Ub[i, j] *= Sb[j]
            # followed by:
            # PCs[:] = U[:, :subspace_size] @ Ub[:, :k]
            # However, since `PCs` is C-contiguous whereas `U` and `Ub` are
            # Fortran-contiguous, we actually compute:
            # PCs.T[:] = Ub[:, :k].T @ U[:, :subspace_size].T
            # and then reinterpret PCs as C-contiguous
            sgemm(b'T', b'T', k, num_cells, subspace_size, 1, &Ub[0, 0],
                  subspace_size, &U[0, 0], num_cells, 0, &PCs[0, 0], k)
            
            # Return the PCs, and whether IRLBA converged
            return PCs, converged
            ''', warn_undeclared=False)
        clipped_stddev_csr = cython_functions['clipped_stddev_csr']
        clipped_stddev_csc = cython_functions['clipped_stddev_csc']
        irlba = cython_functions['irlba']
        
        # Get the data, indices and indptr to be used for the matvec and for
        # the rmatvec. When `faster_single_threading=False`, this requires
        # calculating the "opposite" version of the array (CSC if `X` is CSR,
        # CSR if `X` is CSC). Also, get the clipped standard deviation of each
        # gene, for variance scaling; use CSC for this if available.
        num_cells, num_genes = X.shape
        clipped_stddev = np.empty(num_genes, dtype=np.float32)
        clip_val = 1e-8
        is_csr = isinstance(X, csr_array)
        if is_csr:
            # `X` is CSR, so always use CSR for the matvec. If
            # `faster_single_threading`, use CSR for the rmatvec as well,
            # otherwise use CSC for the rmatvec to enable paralellism.
            data_matvec = X.data
            indices_matvec = X.indices
            indptr_matvec = X.indptr
            if faster_single_threading:
                data_rmatvec = data_matvec
                indices_rmatvec = indices_matvec
                indptr_rmatvec = indptr_matvec
                clipped_stddev_csr(
                    data=data_matvec, indices=indices_matvec,
                    indptr=indptr_matvec, num_cells=num_cells,
                    num_genes=num_genes, clip_val=clip_val,
                    clipped_stddev=clipped_stddev)
            else:
                X_csc = X.tocsc()
                data_rmatvec = X_csc.data
                indices_rmatvec = X_csc.indices
                indptr_rmatvec = X_csc.indptr
                clipped_stddev_csc(
                    data=data_rmatvec, indices=indices_rmatvec,
                    indptr=indptr_rmatvec, num_cells=num_cells,
                    num_genes=num_genes, clip_val=clip_val,
                    clipped_stddev=clipped_stddev, num_threads=num_threads)
        else:
            # `X` is CSC, so always use CSC for the rmatvec. If
            # `faster_single_threading`, use CSC for the matvec as well,
            # otherwise use CSR for the matvec to enable paralellism.
            data_rmatvec = X.data
            indices_rmatvec = X.indices
            indptr_rmatvec = X.indptr
            if faster_single_threading:
                data_matvec = data_rmatvec
                indices_matvec = indices_rmatvec
                indptr_matvec = indptr_rmatvec
            else:
                X_csr = X.tocsr()
                data_matvec = X_csr.data
                indices_matvec = X_csr.indices
                indptr_matvec = X_csr.indptr
            clipped_stddev_csc(
                data=data_rmatvec, indices=indices_rmatvec,
                indptr=indptr_rmatvec, num_cells=num_cells,
                num_genes=num_genes, clip_val=clip_val,
                clipped_stddev=clipped_stddev, num_threads=num_threads)
        
        # Run PCA with irlba
        original_num_threads = X._num_threads
        X._num_threads = num_threads
        try:
            with threadpool_limits(num_threads, user_api='blas'):
                PCs, converged = irlba(
                    data_matvec=data_matvec, indices_matvec=indices_matvec,
                    indptr_matvec=indptr_matvec, data_rmatvec=data_rmatvec,
                    indices_rmatvec=indices_rmatvec,
                    indptr_rmatvec=indptr_rmatvec,
                    clipped_stddev=clipped_stddev, num_cells=num_cells,
                    num_genes=num_genes, k=num_PCs,
                    subspace_size=subspace_size, tolerance=tolerance,
                    max_iterations=max_iterations, seed=seed,
                    faster_single_threading=faster_single_threading,
                    is_csr=is_csr, num_threads=num_threads)
        finally:
            X._num_threads = original_num_threads
        
        # Print a message if PCs did not converge and `verbose=True`
        if not converged and verbose:
            print(f'PCA did not converge to a tolerance of {tolerance:.2g} '
                  f'after {max_iterations:,} iterations; consider increasing '
                  f'max_iterations or tolerance')
        
        # Store each dataset's PCs in its `obsm`
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan,
                                      dtype=np.float32)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    # noinspection PyTypeChecker
    def PCA(self,
            *others: SingleCell,
            QC_column: SingleCellColumn | None |
                       Sequence[SingleCellColumn | None] = 'passed_QC',
            hvg_column: SingleCellColumn |
                        Sequence[SingleCellColumn] | None = 'highly_variable',
            PC_key: str = 'PCs',
            num_PCs: int | np.integer = 50,
            subspace_size: int | np.integer = 100,
            tolerance: int | np.integer | float | np.floating = 1e-6,
            max_iterations: int | np.integer = 100,
            seed: int | np.integer = 0,
            match_parallel: bool = False,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Compute principal components (PCs) across cells.
        
        Requires normalized counts, so must be run after `normalize()`. By
        default, only the highly variable genes from `hvg()` are used to
        compute PCs.
        
        Uses approximate singular value decomposition (SVD) via the Implicitly
        Restarted Lanczos Bidiagonalization Algorithm (IRLBA). Seurat uses a
        different implementation of the same IRLBA algorithm.
        
        Args:
            others: optional SingleCell datasets to jointly compute principal
                    components across, alongside this one.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their PCs set to `NaN`. When `others` is
                       specified, `QC_column` can be a length-`1 + len(others)`
                       sequence of columns, expressions, Series, functions, or
                       `None` for each dataset (for `self`, followed by each
                       dataset in `others`).
            hvg_column: an optional Boolean column of `var` indicating the
                        highly variable genes. Set to `None` to include all
                        genes. Can be a column name, a polars expression, a
                        polars Series, a 1D NumPy array, or a function that
                        takes in this SingleCell dataset and returns a polars
                        Series or 1D NumPy array. Set to `None` to use all
                        genes. When `others` is specified, `hvg_column` can be
                        a length-`1 + len(others)` sequence of columns,
                        expressions, Series, functions, or `None` for each
                        dataset (for `self`, followed by each dataset in
                        `others`).
            PC_key: the key of `obsm` where the principal components will be
                    stored
            num_PCs: the number of top principal components to calculate
            subspace_size: the size of the Krylov subspace used by IRLBA when
                           calculating PCs. Must be greater than or equal to
                           `num_PCs`, and about twice `num_PCs` is recommended.
            tolerance: the relative tolerance (expressed as the ratio of a
                       singular value's residual to the maximum singular value)
                       required to deem a singular value converged. IRLBA will
                       stop early, before `max_iterations` iterations, if all
                       singular values have converged.
            max_iterations: the maximum number of iterations to run IRLBA for,
                            stopping early if all singular values have
                            converged (see `tolerance`)
            seed: the random seed to use when initializing the PCs, via R's
                  `set.seed()` function
            match_parallel: if `False`, use a different order of operations for
                            single-threaded PCA. This gives a modest (~15-20%)
                            boost in single-threaded performance, and lower
                            memory usage, at the cost of no longer exactly
                            matching the PCs produced by the multithreaded
                            version (due to differences in floating-point error
                            arising from the different order of operations).
                            When `match_parallel=False`, `PCA()` will also give
                            slightly different results when run with CSR vs CSC
                            input; when multiple datasets are provided with a
                            mix of CSR and CSC formats, all datasets are
                            converted to the format shared by the most total
                            cells across datasets. If `True`, exactly match the
                            results of the multithreaded version when
                            `num_threads=1`. Must be `False` unless
                            `num_threads=1`.
            overwrite: if `True`, overwrite `PC_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print a message when the singular values did
                     not converge to a tolerance of `tolerance` within
                     `max_iterations` iterations
            num_threads: the number of threads to use for PCA. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the returned
                         SingleCell dataset's `num_threads`; this will always
                         be the same as the original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset where `obsm` contains an additional key,
            `PC_key` (default: `'PCs'`), containing the top `num_PCs` principal
            components. Or, if additional SingleCell dataset(s) are specified
            via the `others` argument, a length-`1 + len(others)` tuple of
            SingleCell datasets with the PCs added: `self`, followed by each
            dataset in `others`.
        
        Note:
            Unlike Seurat's `RunPCA()` function, which requires `ScaleData()`
            to be run first, this function does not require the data to be
            scaled beforehand. Instead, it implicitly scales the data to zero
            mean and unit variance while performing PCA.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if others:
            check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `PC_key` is a string
        check_type(PC_key, 'PC_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `PC_key` is not already in `obsm`, unless `overwrite=True`
        suffix = ' for at least one dataset' if others else ''
        for dataset in datasets:
            if not overwrite and PC_key in dataset._obsm:
                error_message = (
                    f'PC_key {PC_key!r} is already a key of obsm{suffix}; did '
                    f'you already run PCA()? Set overwrite=True to overwrite.')
                raise ValueError(error_message)
        
        # Check that `X` is present for every dataset
        if any(dataset._X is None for dataset in datasets):
            error_message = (
                f'X is None{suffix}, so finding principal components is not '
                f'possible')
            raise ValueError(error_message)
            
        # Get `QC_column` and `hvg_column` (if not `None`) from every dataset
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        hvg_columns = SingleCell._get_columns(
            'var', datasets, hvg_column, 'hvg_column', pl.Boolean,
            custom_error=f'hvg_column {{}} is not a column of var{suffix}; '
                         f'did you forget to run hvg() (and possibly '
                         f'normalize()) before PCA()?')
        
        # Check that all datasets are normalized
        if not all(dataset._uns['normalized'] for dataset in datasets):
            error_message = (
                f"PCA() requires normalized counts but uns['normalized'] is "
                f"False{suffix}; did you forget to run normalize() before "
                f"PCA()?")
            raise ValueError(error_message)
        
        # Raise an error if `X` is not float32 for every dataset
        for dataset in datasets:
            if dataset._X.dtype != np.float32:
                error_message = (
                    f'PCA() requires normalized counts with data type '
                    f'float32, but X has data type '
                    f'{str(dataset._X.dtype)!r}{suffix}; did you forget to '
                    f'run normalize() before PCA()?')
                raise TypeError(error_message)
        
        # Check that `num_PCs` is a positive integer
        check_type(num_PCs, 'num_PCs', int, 'a positive integer')
        check_bounds(num_PCs, 'num_PCs', 1)
        
        # Check that `subspace_size` is a positive integer, and >= `num_PCs`
        check_type(subspace_size, 'subspace_size', int, 'a positive integer')
        if subspace_size < num_PCs:
            error_message = (
                f'subspace_size is {subspace_size:,}, but must be ≥ num_PCs '
                f'({num_PCs:,})')
            raise ValueError(error_message)
        
        # Check that `tolerance` is a positive number
        check_type(tolerance, 'tolerance', (int, float), 'a positive number')
        check_bounds(tolerance, 'tolerance', 0, left_open=True)
        
        # Check that `max_iterations` is a positive integer
        check_type(max_iterations, 'max_iterations', int, 'a positive integer')
        check_bounds(max_iterations, 'max_iterations', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`.
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `match_parallel` is Boolean, and `False` unless
        # `num_threads=1`
        check_type(match_parallel, 'match_parallel', bool, 'Boolean')
        if match_parallel and num_threads != 1:
            error_message = \
                'match_parallel must be False unless num_threads is 1'
            raise ValueError(error_message)
        
        # Get the matrix to compute PCA across: a sparse matrix of counts for
        # highly variable genes (or all genes, if `hvg_column` is `None`)
        # across cells passing QC. Use `X[np.ix_(rows, columns)]` as a faster,
        # more memory-efficient alternative to `X[rows][:, columns]`.
        original_num_threads = self._X._num_threads
        try:
            self._X._num_threads = num_threads
            if others:
                if hvg_column is None:
                    genes_in_all_datasets = self.var_names\
                        .filter(self.var_names
                                .is_in(pl.concat([dataset.var_names
                                                  for dataset in others])))
                else:
                    hvg_in_self = \
                        self._var.filter(hvg_columns[0]).to_series() \
                            if hvg_columns[0] is not None else \
                            self._var.to_series()
                    genes_in_all_datasets = hvg_in_self\
                        .filter(hvg_in_self.is_in(pl.concat([
                            dataset._var.filter(hvg_col).to_series()
                            if hvg_col is not None else
                            dataset._var.to_series()
                            for dataset, hvg_col in
                            zip(others, hvg_columns[1:])])))
                gene_indices = (
                    genes_in_all_datasets
                    .to_frame()
                    .join(dataset._var.with_columns(
                          _SingleCell_index=pl.int_range(pl.len(),
                                                         dtype=pl.Int32)),
                          left_on=genes_in_all_datasets.name,
                          right_on=dataset.var_names.name, how='left')
                    ['_SingleCell_index']
                    .to_numpy()
                    for dataset in datasets)
                if QC_column is None:
                    Xs = [dataset._X[:, genes]
                          for dataset, genes in zip(datasets, gene_indices)]
                else:
                    Xs = [dataset._X[np.ix_(QC_col.to_numpy(), genes)]
                          if QC_col is not None else dataset._X[:, genes]
                          for dataset, genes, QC_col in
                          zip(datasets, gene_indices, QC_columns)]
            else:
                if QC_column is None:
                    if hvg_column is None:
                        Xs = [dataset._X for dataset in datasets]
                    else:
                        Xs = [dataset._X[:, hvg_col.to_numpy()]
                              if hvg_col is not None else dataset._X
                              for dataset, hvg_col in
                              zip(datasets, hvg_columns)]
                else:
                    if hvg_column is None:
                        Xs = [dataset._X[QC_col.to_numpy()]
                              if QC_col is not None else dataset._X
                              for dataset, QC_col in zip(datasets, QC_columns)]
                    else:
                        Xs = [(dataset._X[np.ix_(QC_col.to_numpy(),
                                                 hvg_col.to_numpy())]
                               if QC_col is not None else
                               dataset._X[:, hvg_col.to_numpy()])
                              if hvg_col is not None else
                              (dataset._X[QC_col.to_numpy()]
                               if QC_col is not None else dataset._X)
                              for dataset, QC_col, hvg_col in
                              zip(datasets, QC_columns, hvg_columns)]
        finally:
            self._X._num_threads = original_num_threads
        num_cells_per_dataset = np.array([X.shape[0] for X in Xs])
        if len(Xs) == 1:
            X = Xs[0]
        elif all(isinstance(X, csr_array) for X in Xs):
            X = sparse_major_stack(Xs, num_threads=num_threads)
        elif all(isinstance(X, csc_array) for X in Xs):
            X = sparse_minor_stack(Xs, num_threads=num_threads)
        else:
            # Mix of CSR and CSC: convert to whichever format has the most
            # total cells in that format, to reduce the number of datasets that
            # need to be flipped
            total_cells = num_cells_per_dataset.sum()
            total_csr = sum(X.shape[0] for X in Xs if isinstance(X, csr_array))
            if total_csr / total_cells > 0.5:  # more CSR than CSC
                X = sparse_major_stack([
                    X.tocsr() if isinstance(X, csc_array) else X for X in Xs],
                    num_threads=num_threads)
            else:
                X = sparse_minor_stack([
                    X.tocsc() if isinstance(X, csr_array) else X for X in Xs],
                    num_threads=num_threads)
        del Xs
        
        # Check that `num_PCs` is at most the width of this matrix
        check_bounds(num_PCs, 'num_PCs', upper_bound=X.shape[1])
        
        # Check that there are at least two cells in total
        if X.shape[0] == 1:
            error_message = (
                f'there is only one cell, so principal components cannot be '
                f'calculated')
            raise ValueError(error_message)
        
        # Define Cython functions. We want to perform SVD on `X`, scaled to
        # zero mean and unit variance.
        # Key ideas:
        # 1. Because `X` is a sparse matrix, mean-centering cannot be done
        #    without converting to a dense matrix. So scaling `X` cannot be
        #    done in the conventional way.
        # 2. Fortunately, we can represent scaling as a matrix product:
        #    `scale(X) = C @ X @ W`, where `W` is a diagonal matrix of the
        #    standard deviations for each column (gene) and `C` is a "centering
        #    matrix" that, when applied to any vector or matrix, yields the
        #    mean-centered version of it.
        # 3. We need to calculate the matrix-vector product of our operator
        #    with some vector `V`, i.e. `scale(X) @ V`. Using the formula from
        #    point #2, this is equivalent to `C @ X @ W @ V`. Since `W` is
        #    diagonal, this is equivalent to `C @ (X @ (V / diag(W)))`. In
        #    other words:
        #    - divide `V` (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        #    - matrix-vector multiply by `X`
        #    - mean-center the resulting vector, which has length `num_cells`
        # 4. We also need to calculate `scale(X).T @ V` for the `rmatvec()`
        #    part of our operator. Rewriting as `W.T @ X @ C.T @ V` and
        #    leveraging the fact that `C` turns out to be symmetric (as is `W`,
        #    since it's diagonal), this is equivalent to
        #    `(X @ (C @ V)) / diag(W)`. In other words:
        #    - mean-center `V`, which has length `num_cells`
        #    - matrix-vector multiply by `X.T`
        #    - divide the result (which has length `num_genes)` elementwise by
        #      `diag(W)`, the genewise standard deviations
        # 5. CSR matrix-vector multiplication can be done efficiently
        #    multithreaded, but CSC can't (except by maintaining thread-local
        #    versions of the output vector and summing them across threads at
        #    the end, which leads to differences in floating-point error
        #    depending on the number of threads). This is problematic because,
        #    regardless of whether `X` is a CSR or a CSC matrix, we need to do
        #    some multiplications involving `X.T` and others involving `X`. So
        #    if `X` is CSR, the `X.T` multiplications will be single-threaded
        #    since `X.T` is a CSC matrix. If `X` is CSC, the `X`
        #    multiplications will be single-threaded. So one of the two
        #    multiplications will always be single-threaded. There are hundreds
        #    of these matrix-vector multiplications and they take up a large
        #    majority of the total runtime for PCA, so not being able to fully
        #    multithread them is a huge disadvantage.
        # 6. To address the issue in the previous point, make both a CSR and a
        #    CSC copy of `X` when running PCA in parallel. The first
        #    multiplication (involving `X.T`) will use the CSC copy of `X`, but
        #    plugged into the CSR matrix-vector multiplication function. The
        #    second multiplication (involving `X`) will use CSR multiplication
        #    as normal. This works because plugging a CSC copy of `X` into a
        #    matrix-vector multiplication routine designed for CSR matrices (or
        #    vice versa) is equivalent to multiplying by `X.T` instead of `X`.
        #    The result: both matrix multiplications can be done in parallel.
        #    This also has the nice side benefit that the final result is the
        #    same regaredless of whether the counts are input as CSR or CSC.
        # 7. When running single-threaded with `faster_single_threaded=True`,
        #    however, just use whichever version of `X` (CSR or CSC) we happen
        #    to have available, to avoid the runtime and memory overhead of
        #    creating both versions. However, this means that CSR and CSC no
        #    longer give exactly the same PCs, due to differences in
        #    floating-point error.
        # 8. When calculating the scaled variance for each gene, use the CSC
        #    version of `X` if available, for speed. Clip tiny standard
        #    deviations (less than 1e-8) to 1e-8, like Seurat.
        irlba = cython_inline(_uninitialized_vector_import + _thread_offset_import + r'''
        cimport numpy as np
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libc.limits cimport UINT_MAX
        from libc.math cimport M_PI
        from libcpp.algorithm cimport fill
        from libcpp.cmath cimport abs, sqrt, log, cos
        from libcpp.vector cimport vector
        from scipy.linalg.cython_blas cimport sgemm as sgemm_, sgemv as sgemv_
        from scipy.linalg.cython_lapack cimport sgesdd
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline float random_uniform(unsigned long* state) noexcept nogil:
            # Returns a random number in U(0, 1)
            return <float> rand(state) / UINT_MAX
        
        cdef inline float random_normal(unsigned long* state) noexcept nogil:
            # Samples a random number from the standard normal distribution
            # using the Box-Muller transform
            
            cdef double r, theta
            
            r = sqrt(-2 * log(random_uniform(state)))
            theta = 2 * M_PI * random_uniform(state)
            return r * cos(theta)
        
        cdef inline float norm(float[::1] array) noexcept nogil:
            cdef unsigned i
            cdef float norm = 0
            for i in range(array.shape[0]):
                norm += array[i] * array[i]
            return sqrt(norm)
        
        cdef inline void sgemv(const char trans,
                               const int m,
                               const int n,
                               const float alpha,
                               const float* a,
                               const int lda,
                               const float* x,
                               const int incx,
                               const float beta,
                               float* y,
                               const int incy) noexcept nogil:
            # Matrix multiplication wrapper that accepts scalars by value and
            # provides const-correctness
            
            sgemv_(<char*> &trans, <int*> &m, <int*> &n, <float*> &alpha,
                   <float*> a, <int*> &lda, <float*> x, <int*> &incx,
                   <float*> &beta, y, <int*> &incy)
        
        cdef inline void sgemm(const char transa,
                               const char transb,
                               const int m,
                               const int n,
                               const int k,
                               const float alpha,
                               const float* a,
                               const int lda,
                               const float* b,
                               const int ldb,
                               const float beta,
                               float* c,
                               const int ldc) noexcept nogil:
            # Matrix multiplication wrapper that accepts scalars by value and
            # provides const-correctness
            
            sgemm_(<char*> &transa, <char*> &transb, <int*> &m, <int*> &n, <int*> &k,
                   <float*> &alpha, <float*> a, <int*> &lda, <float*> b, <int*> &ldb,
                   <float*> &beta, c, <int*> &ldc)
            
        cdef inline void clipped_stddev_csr(const float[::1] data,
                                            const signed_integer[::1] indices,
                                            const signed_integer[::1] indptr,
                                            const unsigned long num_cells,
                                            const unsigned num_genes,
                                            float clip_val,
                                            float[::1] clipped_stddev):
            # Compute `X.std(axis=0)` where `X` is CSR, clipping to a minimum of
            # `clip_val`. Only used when `num_threads=1` and `match_parallel=False`.
            
            cdef unsigned long num_elements, i, j, start, end, \
                chunk_size
            cdef unsigned gene, cell, thread_index
            cdef float value, total_sum, total_sum_of_squares, \
                inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
            cdef vector[float] sum_buffer, sum_of_squares_buffer
            sum_buffer.resize(num_genes)
            sum_of_squares_buffer.resize(num_genes)
            cdef float[::1] sum = <float[:num_genes]> sum_buffer.data(), \
                sum_of_squares = <float[:num_genes]> sum_of_squares_buffer.data()
            
            # Iterate over all elements of the count matrix, ignoring which cell
            # they're from
            num_elements = indices.shape[0]
            for i in range(num_elements):
                gene = indices[i]
                value = data[i]
                sum[gene] += value
                sum_of_squares[gene] += value * value
            
            # Calculate standard deviations from the sums and squared sums
            for gene in range(num_genes):
                clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                    num_cells * sum_of_squares[gene] - sum[gene] * sum[gene]))
                if clipped_stddev[gene] < clip_val:
                    clipped_stddev[gene] = clip_val
        
        cdef inline void clipped_stddev_csc(const float[::1] data,
                                            const signed_integer[::1] indices,
                                            const signed_integer[::1] indptr,
                                            const unsigned long num_cells,
                                            const unsigned num_genes,
                                            float clip_val,
                                            float[::1] clipped_stddev,
                                            const unsigned num_threads,
                                            const unsigned* thread_offsets) \
                noexcept nogil:
            # Compute `X.std(axis=0)` where `X` is CSC, clipping to a minimum of
            # `clip_val`.
            
            cdef unsigned gene, thread_index, start_col, end_col
            cdef unsigned long i
            cdef float value, sum, sum_of_squares, \
                inv_num_pairs_of_cells = 1.0 / (num_cells * (num_cells - 1))
            
            if num_threads == 1:
                for gene in range(num_genes):
                    # Calculate the sum and squared sum for this gene, across cells
                    # with non-zero counts for the gene
                    sum = 0
                    sum_of_squares = 0
                    for i in range(<unsigned long> indptr[gene],
                                   <unsigned long> indptr[gene + 1]):
                        value = data[i]
                        sum += value
                        sum_of_squares += value * value
        
                    # Calculate the scaled variance from the sum and squared sum
                    clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                        num_cells * sum_of_squares - sum * sum))
                    if clipped_stddev[gene] < clip_val:
                        clipped_stddev[gene] = clip_val
            else:
                with parallel(num_threads=num_threads):
                    thread_index = threadid()
                    start_col = thread_offsets[thread_index]
                    end_col = thread_offsets[thread_index + 1]
                    for gene in range(start_col, end_col):
                        sum = 0
                        sum_of_squares = 0
                        for i in range(<unsigned long> indptr[gene],
                                       <unsigned long> indptr[gene + 1]):
                            value = data[i]
                            sum = sum + value
                            sum_of_squares = sum_of_squares + value * value
                        clipped_stddev[gene] = sqrt(inv_num_pairs_of_cells * (
                            num_cells * sum_of_squares - sum * sum))
                        if clipped_stddev[gene] < clip_val:
                            clipped_stddev[gene] = clip_val
        
        cdef void matvec_csr_fast(const float[::1] data,
                                  const signed_integer[::1] indices,
                                  const signed_integer[::1] indptr,
                                  const float[::1] V,
                                  const float[::1] clipped_stddev,
                                  float* num_cells_buffer,
                                  float* num_genes_buffer,
                                  const unsigned num_cells,
                                  const unsigned num_genes,
                                  const unsigned num_threads,
                                  const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Also has a parallel version,
            # `matvec_csr_parallel()`. `num_threads` and `thread_offsets` are unused,
            # but needed to match the signature of `matvec_csr_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float dot_product, mean = 0
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
                
            # Matrix-vector multiplication and mean calculation
            for i in range(num_cells):
                dot_product = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    dot_product += data[j] * num_genes_buffer[indices[j]]
                num_cells_buffer[i] = dot_product
                mean += dot_product
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] -= mean
        
        cdef void matvec_csr(const float[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const float[::1] V,
                             const float[::1] clipped_stddev,
                             float* num_cells_buffer,
                             float* num_genes_buffer,
                             const unsigned num_cells,
                             const unsigned num_genes,
                             const unsigned num_threads,
                             const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Also has a parallel version,
            # `matvec_csr_parallel()`. `num_threads` and `thread_offsets` are unused,
            # but needed to match the signature of `matvec_csr_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float dot_product, mean = 0
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
                
            # Matrix-vector multiplication
            for i in range(num_cells):
                dot_product = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    dot_product += data[j] * num_genes_buffer[indices[j]]
                num_cells_buffer[i] = dot_product
            
            # Mean calculation; unlike `matvec_csr_fast()`, this is a separate
            # loop to maintain the parallel version's floating-point behavior.
            for i in range(num_cells):
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] -= mean
        
        cdef void matvec_csr_parallel(const float[::1] data,
                                      const signed_integer[::1] indices,
                                      const signed_integer[::1] indptr,
                                      const float[::1] V,
                                      const float[::1] clipped_stddev,
                                      float* num_cells_buffer,
                                      float* num_genes_buffer,
                                      const unsigned num_cells,
                                      const unsigned num_genes,
                                      const unsigned num_threads,
                                      const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Also has a single-threaded
            # version, `matvec_csr()`.
            
            cdef unsigned i, thread_index, start_row, end_row
            cdef unsigned long j
            cdef float dot_product, mean = 0
            
            # Variance scaling (single-threaded since `num_genes` is just 2000
            # by default)
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
            
            # Matrix-vector multiplication
            with parallel(num_threads=num_threads):
                thread_index = threadid()
                start_row = thread_offsets[thread_index]
                end_row = thread_offsets[thread_index + 1]
                for i in range(start_row, end_row):
                    dot_product = 0
                    for j in range(<unsigned long> indptr[i],
                                   <unsigned long> indptr[i + 1]):
                        dot_product = \
                            dot_product + data[j] * num_genes_buffer[indices[j]]
                    num_cells_buffer[i] = dot_product
            
            # Mean calculation (must be done single-threaded to be
            # deterministic, due to floating-point error)
            for i in range(num_cells):
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in prange(num_cells, num_threads=num_threads):
                num_cells_buffer[i] -= mean
        
        cdef void matvec_csc(const float[::1] data,
                             const signed_integer[::1] indices,
                             const signed_integer[::1] indptr,
                             const float[::1] V,
                             const float[::1] clipped_stddev,
                             float* num_cells_buffer,
                             float* num_genes_buffer,
                             const unsigned num_cells,
                             const unsigned num_genes,
                             const unsigned num_threads,
                             const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X) @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Does not have a parallel version.
            # `num_threads` and `thread_offsets` are unused, but needed to match the
            # signature of `matvec_csr_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float num_genes_buffer_i, mean = 0
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] = V[i] / clipped_stddev[i]
            
            # Matrix-vector multiplication
            fill(num_cells_buffer, num_cells_buffer + num_cells, 0)
            for i in range(num_genes):
                num_genes_buffer_i = num_genes_buffer[i]
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_cells_buffer[indices[j]] += data[j] * num_genes_buffer_i
                    
            # Mean calculation
            for i in range(num_cells):
                mean += num_cells_buffer[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] -= mean
        
        cdef void rmatvec_csr(const float[::1] data,
                              const signed_integer[::1] indices,
                              const signed_integer[::1] indptr,
                              const float[::1] V,
                              const float[::1] clipped_stddev,
                              float* num_cells_buffer,
                              float* num_genes_buffer,
                              const unsigned num_cells,
                              const unsigned num_genes,
                              const unsigned num_threads,
                              const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSR and `V`
            # and `num_cells_buffer` are vectors. Does not have a parallel version.
            # `num_threads` and `thread_offsets` are unused, but needed to match the
            # signature of `rmatvec_csc_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float num_cells_buffer_i, mean = 0
            
            # Mean scaling
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            for i in range(num_cells):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            fill(num_genes_buffer, num_genes_buffer + num_genes, 0)
            for i in range(num_cells):
                num_cells_buffer_i = num_cells_buffer[i]
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    num_genes_buffer[indices[j]] += data[j] * num_cells_buffer_i
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]
        
        cdef void rmatvec_csc(const float[::1] data,
                              const signed_integer[::1] indices,
                              const signed_integer[::1] indptr,
                              const float[::1] V,
                              const float[::1] clipped_stddev,
                              float* num_cells_buffer,
                              float* num_genes_buffer,
                              const unsigned num_cells,
                              const unsigned num_genes,
                              const unsigned num_threads,
                              const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Also has a parallel version,
            # `rmatvec_csc_parallel()`. `num_threads` and `thread_offsets` are unused,
            # but needed to match the signature of `rmatvec_csc_parallel()`.
            
            cdef unsigned i
            cdef unsigned long j
            cdef float dot_product, mean = 0
            
            # Mean calculation
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            
            # Mean scaling
            for i in range(num_cells):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            for i in range(num_genes):
                dot_product = 0
                for j in range(<unsigned long> indptr[i],
                               <unsigned long> indptr[i + 1]):
                    dot_product += data[j] * num_cells_buffer[indices[j]]
                num_genes_buffer[i] = dot_product
            
            # Variance scaling
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]
        
        cdef void rmatvec_csc_parallel(const float[::1] data,
                                       const signed_integer[::1] indices,
                                       const signed_integer[::1] indptr,
                                       const float[::1] V,
                                       const float[::1] clipped_stddev,
                                       float* num_cells_buffer,
                                       float* num_genes_buffer,
                                       const unsigned num_cells,
                                       const unsigned num_genes,
                                       const unsigned num_threads,
                                       const unsigned* thread_offsets) noexcept nogil:
            # Compute `num_cells_buffer = scale(X).T @ V`, where `X` is CSC and `V`
            # and `num_cells_buffer` are vectors. Also has a single-threaded
            # version, `rmatvec_csc()`.
            
            cdef unsigned i, thread_index, start_col, end_col
            cdef unsigned long j
            cdef float dot_product, mean = 0
            
            # Mean calculation (must be done single-threaded to be
            # deterministic, due to floating-point error)
            for i in range(num_cells):
                mean += V[i]
            mean /= num_cells
            
            # Mean scaling
            for i in prange(num_cells, num_threads=num_threads):
                num_cells_buffer[i] = V[i] - mean
            
            # Matrix-vector multiplication
            with parallel(num_threads=num_threads):
                thread_index = threadid()
                start_col = thread_offsets[thread_index]
                end_col = thread_offsets[thread_index + 1]
                for i in range(start_col, end_col):
                    dot_product = 0
                    for j in range(<unsigned long> indptr[i],
                                   <unsigned long> indptr[i + 1]):
                        dot_product = \
                            dot_product + data[j] * num_cells_buffer[indices[j]]
                    num_genes_buffer[i] = dot_product
            
            # Variance scaling (single-threaded since `num_genes` is just 2000
            # by default)
            for i in range(num_genes):
                num_genes_buffer[i] /= clipped_stddev[i]

        cdef inline int svd_workspace_size(float[::1, :] B,
                                           float[::1, :] U,
                                           float[::1] S,
                                           float[::1, :] Vt,
                                           int* iwork) noexcept nogil:
            # Get the optimal workspace size for SVD with `sgesdd()`. Assumes `m < n`,
            # which it is for `B`.
            
            cdef char jobz = b'S'
            cdef int info, m = B.shape[0], n = B.shape[1], lwork = -1
            cdef float workspace_size
            
            sgesdd(&jobz, &m, &n, &B[0, 0], &m, &S[0], &U[0, 0], &m, &Vt[0, 0], &m,
                   &workspace_size, &lwork, iwork, &info)
            return <int> workspace_size

        cdef inline void svd(float[::1, :] B,
                             float[::1, :] U,
                             float[::1] S,
                             float[::1, :] Vt,
                             float* work,
                             int* iwork,
                             const int lwork) noexcept nogil:
            # SVD for the case where `m < n`, which it is for `B`. Uses the optimal
            # workspace size computed by `svd_workspace_size()`.
            
            cdef char jobz = b'S'
            cdef int info, m = B.shape[0], n = B.shape[1]
            
            sgesdd(&jobz, &m, &n, &B[0, 0], &m, &S[0], &U[0, 0], &m, &Vt[0, 0], &m,
                   work, <int*> &lwork, iwork, &info)
        
        ctypedef void (*matvec_function)(const float[::1] data,
                                         const signed_integer[::1] indices,
                                         const signed_integer[::1] indptr,
                                         const float[::1] V,
                                         const float[::1] clipped_stddev,
                                         float* num_cells_buffer,
                                         float* num_genes_buffer,
                                         const unsigned num_cells,
                                         const unsigned num_genes,
                                         const unsigned num_threads,
                                         const unsigned* thread_offsets) noexcept nogil
        
        def irlba(const float[::1] data_matvec,
                  const signed_integer[::1] indices_matvec,
                  const signed_integer[::1] indptr_matvec,
                  const float[::1] data_rmatvec,
                  const signed_integer[::1] indices_rmatvec,
                  const signed_integer[::1] indptr_rmatvec,
                  const unsigned num_cells,
                  const unsigned num_genes,
                  const unsigned k,
                  const unsigned subspace_size,
                  const float tolerance,
                  const unsigned max_iterations,
                  const unsigned long seed,
                  const bint match_parallel,
                  const bint is_csr,
                  const unsigned num_threads):
            
            cdef unsigned iteration, i, j, row, col, num_converged, k_current = 0
            cdef unsigned long state = srand(seed)
            cdef float inverse_norm, dot_product, alpha, inverse_alpha, beta, \
                inverse_beta, remainder_norm, residual, Sbj, clip_val = 1e-8, \
                Smax = 2.4221817809573368e-05  # (float32 eps) ** (2 / 3)
            cdef int lwork
            cdef bint converged = False
            cdef uninitialized_vector[float] clipped_stddev_buffer, U_buffer, \
                U_new_buffer, V_buffer, V_new_buffer, B_buffer, Ub_buffer, Sb_buffer, \
                Vbt_buffer, remainder_buffer, work
            cdef uninitialized_vector[int] iwork
            cdef uninitialized_vector[unsigned] matvec_thread_offsets_buffer, \
                rmatvec_thread_offsets_buffer
            cdef float[::1, :] U, U_new, V, V_new, B, Ub, Vbt, temp
            cdef float[::1] clipped_stddev, work2, Sb
            cdef unsigned* matvec_thread_offsets
            cdef unsigned* rmatvec_thread_offsets
            cdef matvec_function matvec, rmatvec
            cdef np.ndarray[np.float32_t, ndim=2] PCs
            cdef np.npy_intp dims[2]
            dims[0] = num_cells
            dims[1] = k
            
            # Allocate workspace arrays:
            
            # The clipped standard deviation of each gene
            clipped_stddev_buffer.resize(num_genes)
            clipped_stddev = <float[:num_genes]> clipped_stddev_buffer.data()
            
            # `U` and `U_new`: left singular vectors
            U_buffer.resize(num_cells * subspace_size)
            U = <float[:num_cells:1, :subspace_size]> U_buffer.data()
            U_new_buffer.resize(num_cells * subspace_size)
            U_new = <float[:num_cells:1, :subspace_size]> U_new_buffer.data()
            
            # `V` and `V_new`: right singular vectors
            V_buffer.resize(num_genes * subspace_size)
            V = <float[:num_genes:1, :subspace_size]> V_buffer.data()
            V_new_buffer.resize(num_genes * subspace_size)
            V_new = <float[:num_genes:1, :subspace_size]> V_new_buffer.data()
            
            # The bidiagonal matrix (`B`) and its left singular vectors (`Ub`),
            # singular values (`Sb`), and transposed right singular vectors (`Vbt`)
            B_buffer.resize(subspace_size * (subspace_size + 1))
            B = <float[:subspace_size:1, :subspace_size + 1]> B_buffer.data()
            Ub_buffer.resize(subspace_size * subspace_size)
            Ub = <float[:subspace_size:1, :subspace_size]> Ub_buffer.data()
            Sb_buffer.resize(subspace_size)
            Sb = <float[:subspace_size]> Sb_buffer.data()
            Vbt_buffer.resize(subspace_size * (subspace_size + 1))
            Vbt = <float[:subspace_size:1, :subspace_size + 1]> Vbt_buffer.data()
            
            # The remainder from the Lanczos bidiagonalization (`remainder`)
            remainder_buffer.resize(num_genes)
            cdef float[::1] remainder = <float[:num_genes]> remainder_buffer.data()
            
            # The temporary buffers used by `sgesdd()` to perform SVD of `B`
            # (`iwork` and `work`). The optimal size of the latter is
            # determined programmatically via `svd_workspace_size()`.
            iwork.resize(8 * subspace_size)
            lwork = svd_workspace_size(B, Ub, Sb, Vbt, iwork.data())
            work.resize(lwork)
            
            # The row offset of the start of each thread when performing the
            # matvec and rmatvec. This allows for load-balancing: each thread
            # handles about the same number of elements, even if different
            # cells express different numbers of highly variable genes.
            if num_threads > 1:
                matvec_thread_offsets_buffer.resize(num_threads + 1)
                rmatvec_thread_offsets_buffer.resize(num_threads + 1)
                matvec_thread_offsets = matvec_thread_offsets_buffer.data()
                rmatvec_thread_offsets = rmatvec_thread_offsets_buffer.data()
                get_thread_offsets(indptr_matvec, matvec_thread_offsets, num_threads)
                get_thread_offsets(indptr_rmatvec, rmatvec_thread_offsets, num_threads)
            else:
                matvec_thread_offsets = NULL
                rmatvec_thread_offsets = NULL
            
            # Decide which function to use for matvec and rmatvec. Get the
            # clipped standard deviation of each gene, for variance scaling;
            # use CSC for this if available, for speed.
            if match_parallel:
                matvec = matvec_csr
                rmatvec = rmatvec_csc
            elif num_threads == 1:
                if is_csr:
                    matvec = matvec_csr_fast
                    rmatvec = rmatvec_csr
                else:
                    matvec = matvec_csc
                    rmatvec = rmatvec_csc
            else:
                matvec = matvec_csr_parallel
                rmatvec = rmatvec_csc_parallel

            if num_threads == 1 and is_csr and not match_parallel:
                clipped_stddev_csr(data_matvec, indices_matvec, indptr_matvec,
                                   num_cells, num_genes, clip_val, clipped_stddev)
            else:
                clipped_stddev_csc(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                   num_cells, num_genes, clip_val, clipped_stddev,
                                   num_threads, rmatvec_thread_offsets)
            
            # Initialize the first column of `V` with a random normal vector
            for i in range(num_genes):
                V[i, 0] = random_normal(&state)
            inverse_norm = 1 / norm(V[:, 0])
            for i in range(num_genes):
                V[i, 0] *= inverse_norm
            
            # Initialize `B` with zeros
            B[:] = 0
            
            # Now that setup is done, run the IRLBA iterations
            if num_threads == 1:
                for iteration in range(max_iterations):
                    # Perform Lanczos bidiagonalization with reorthogonalization;
                    # use for loops instead of `sgemv()` for matrix-vector
                    # multiplication to ensure deterministic parallelism
                    j = k_current
                    while True:
                        # `U[:, j] = X_scaled @ V[:, j]`; use `remainder` as a
                        # temporary buffer (it happens to have the correct type, float,
                        # and length, `num_genes`)
                        matvec(data_matvec, indices_matvec, indptr_matvec, V[:, j],
                               clipped_stddev, &U[0, j], &remainder[0], num_cells,
                               num_genes, num_threads, matvec_thread_offsets)
                        
                        if j > 0:
                            # U[:, j] -= U[:, :j] @ (U[:, :j].T @ U[:, j])
                            # Use `work` to store the intermediate product
                            # `U[:, :j].T @ U[:, j]`, which has length `j`. This is
                            # guaranteed to fit since `sgesdd()` guarantees that `work` has
                            # at least `4 * subspace_size ** 2 + 7 * subspace_size`
                            # elements, and we only need `j`, which is less than
                            # `subspace_size`.
                            
                            # `work = U[:, :j].T @ U[:, j]`
                            # sgemv(b'T', num_cells, j, 1, &U[0, 0], num_cells, &U[0, j], 1, 0,
                            #       work.data(), 1)
                            for col in range(j):
                                dot_product = 0
                                for row in range(num_cells):
                                    dot_product = dot_product + U[row, col] * U[row, j]
                                work[col] = dot_product
                            
                            # `U[:, j] -= U[:, :j] @ work`
                            # sgemv(b'N', num_cells, j, -1, &U[0, 0], num_cells,
                            #       work.data(), 1, 1, &U[0, j], 1)
                            for row in range(num_cells):
                                dot_product = 0
                                for col in range(j):
                                    dot_product = dot_product + U[row, col] * work[col]
                                U[row, j] -= dot_product
                            
                        alpha = norm(U[:, j])
                        inverse_alpha = 1 / alpha
                        for i in range(num_cells):
                            U[i, j] *= inverse_alpha
                        B[j, j] = alpha
            
                        if j < subspace_size - 1:
                            # `V[:, j + 1] = X_scaled.T @ U[:, j]`; use `U_new` as a
                            # temporary buffer (it happens to have the correct type, float,
                            # and length, at least `num_cells`)
                            rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                    U[:, j], clipped_stddev, &U_new[0, 0], &V[0, j + 1],
                                    num_cells, num_genes, num_threads, rmatvec_thread_offsets)
                            
                            # `V[:, j + 1] -= alpha * V[:, j]`
                            for i in range(num_genes):
                                V[i, j + 1] -= alpha * V[i, j]
                            
                            # `V[:, j + 1] -= V[:, :j + 1] @ (V[:, :j + 1].T @ V[:, j + 1])`;
                            # use `work` to store the intermediate product, which has length
                            # `j + 1` and is guaranteed to fit: as mentioned above, `work` has
                            # at least `4 * subspace_size ** 2 + 7 * subspace_size` elements
                            
                            # `work = V[:, :j + 1].T @ V[:, j + 1]`
                            # sgemv(b'T', num_genes, j + 1, 1, &V[0, 0], num_genes,
                            #       &V[0, j + 1], 1, 0, work.data(), 1)
                            for col in range(j + 1):
                                dot_product = 0
                                for row in range(num_genes):
                                    dot_product = dot_product + V[row, col] * V[row, j + 1]
                                work[col] = dot_product
                            
                            # `V[:, j + 1] -= V[:, :j + 1] @ work`
                            # sgemv(b'N', num_genes, j + 1, -1, &V[0, 0], num_genes,
                            #       work.data(), 1, 1, &V[0, j + 1], 1)
                            for row in range(num_genes):
                                dot_product = 0
                                for col in range(j + 1):
                                    dot_product = dot_product + V[row, col] * work[col]
                                V[row, j + 1] -= dot_product
                        else:
                            # `j == subspace_size - 1` so `V[:, j + 1]` would overwrite
                            # the end of the array. Do the same computation, but store the
                            # results in the `remainder` vector instead of `V[:, j + 1]`.
                            # `remainder = X_scaled.T @ U[:, j]`
                            rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                    U[:, j], clipped_stddev, &U_new[0, 0], &remainder[0],
                                    num_cells, num_genes, num_threads, rmatvec_thread_offsets)
                            
                            # `remainder -= alpha * V[:, j]`
                            for i in range(num_genes):
                                remainder[i] -= alpha * V[i, j]
                            
                            # `work = V.T @ remainder`
                            # sgemv(b'T', num_genes, subspace_size, 1, &V[0, 0], num_genes,
                            #       &remainder[0], 1, 0, &work[0], 1)
                            for col in range(subspace_size):
                                dot_product = 0
                                for row in range(num_genes):
                                    dot_product = dot_product + V[row, col] * remainder[row]
                                work[col] = dot_product
                            
                            # `remainder -= V @ work`
                            # sgemv(b'N', num_genes, subspace_size, -1, &V[0, 0], num_genes,
                            #       &work[0], 1, 1, &remainder[0], 1)
                            for row in range(num_genes):
                                dot_product = 0
                                for col in range(subspace_size):
                                    dot_product = dot_product + V[row, col] * work[col]
                                remainder[row] -= dot_product
                            
                            break
                            
                        beta = norm(V[:, j + 1])
                        inverse_beta = 1 / beta
                        for i in range(num_genes):
                            V[i, j + 1] *= inverse_beta
                        B[j, j + 1] = beta
                        j += 1
                        if j % 10 == 9:
                            PyErr_CheckSignals()
                        
                    # Compute the SVD of the bidiagonal matrix `B`, equivalent to
                    # `Ub, Sb, Vbt = np.linalg.svd(B, full_matrices=False)`
                    svd(B, Ub, Sb, Vbt, work.data(), iwork.data(), lwork)
                    
                    # Normalize the `remainder` vector
                    remainder_norm = norm(remainder)
                    for i in range(num_genes):
                        remainder[i] /= remainder_norm
                    
                    # Update `Smax`, the largest singular value of `B` we've seen so far
                    Smax = max(Sb[0], Smax)
                    
                    # Check convergence of singular values via residuals
                    num_converged = 0
                    for i in range(k):
                        residual = remainder_norm * Ub[subspace_size - 1, i]
                        num_converged += abs(residual) < tolerance * Smax
                    if num_converged == k:
                        converged = True
                        break
                    
                    # For the next iteration, increase `k_current` to `k` + the number of
                    # converged singular values, ensuring it stays under the subspace size
                    k_current = min(max(k_current, k + num_converged), subspace_size - 1)
                    
                    # Restart with new subspace: update `U`, `V` and `B`. The `U` and `V`
                    # updates are in-place, but `sgemm()` doesn't support in-place matrix
                    # multiplication, so use `U_new` and `V_new` as the output arrays and
                    # then swap the pointers so that `U` and `V` point to the output.
                    
                    # U[:, :k_current] = U @ Ub[:, :k_current]
                    sgemm(b'N', b'N', num_cells, k_current, subspace_size, 1, &U[0, 0],
                          num_cells, &Ub[0, 0], subspace_size, 0, &U_new[0, 0], num_cells)
                    temp = U
                    U = U_new
                    U_new = temp
                    
                    # V[:, :k_current] = V @ Vbt.T[:subspace_size, :k_current]
                    sgemm(b'N', b'T', num_genes, k_current, subspace_size, 1, &V[0, 0],
                          num_genes, &Vbt[0, 0], subspace_size, 0, &V_new[0, 0], num_genes)
                    temp = V
                    V = V_new
                    V_new = temp
                    
                    for i in range(num_genes):
                        V[i, k_current] = remainder[i]
                    B[:] = 0
                    for i in range(k_current):
                        B[i, i] = Sb[i]
                        residual = remainder_norm * Ub[subspace_size - 1, i]
                        B[i, k_current] = residual
            else:
                # Same as the single-threaded version, except for `nogil` and `prange()`
                with nogil:
                    for iteration in range(max_iterations):
                        # Perform Lanczos bidiagonalization with reorthogonalization
                        j = k_current
                        while True:
                            # `U[:, j] = X_scaled @ V[:, j]`; use `remainder` as a
                            # temporary buffer (it happens to have the correct type, float,
                            # and length, `num_genes`)
                            matvec(data_matvec, indices_matvec, indptr_matvec, V[:, j],
                                   clipped_stddev, &U[0, j], &remainder[0], num_cells,
                                   num_genes, num_threads, matvec_thread_offsets)
                            
                            if j > 0:
                                # U[:, j] -= U[:, :j] @ (U[:, :j].T @ U[:, j])
                                # Use `work` to store the intermediate product
                                # `U[:, :j].T @ U[:, j]`, which has length `j`. This is
                                # guaranteed to fit since `sgesdd()` guarantees that `work` has
                                # at least `4 * subspace_size ** 2 + 7 * subspace_size`
                                # elements, and we only need `j`, which is less than
                                # `subspace_size`.
                                
                                # `work = U[:, :j].T @ U[:, j]`
                                # sgemv(b'T', num_cells, j, 1, &U[0, 0], num_cells, &U[0, j], 1, 0,
                                #       work.data(), 1)
                                for col in prange(j, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_cells):
                                        dot_product = dot_product + U[row, col] * U[row, j]
                                    work[col] = dot_product
                                
                                # `U[:, j] -= U[:, :j] @ work`
                                # sgemv(b'N', num_cells, j, -1, &U[0, 0], num_cells,
                                #       work.data(), 1, 1, &U[0, j], 1)
                                for row in prange(num_cells, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(j):
                                        dot_product = dot_product + U[row, col] * work[col]
                                    U[row, j] -= dot_product
                                
                            alpha = norm(U[:, j])
                            inverse_alpha = 1 / alpha
                            for i in prange(num_cells, num_threads=num_threads):
                                U[i, j] *= inverse_alpha
                            B[j, j] = alpha
                
                            if j < subspace_size - 1:
                                # `V[:, j + 1] = X_scaled.T @ U[:, j]`; use `U_new` as a
                                # temporary buffer (it happens to have the correct type, float,
                                # and length, at least `num_cells`)
                                rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                        U[:, j], clipped_stddev, &U_new[0, 0], &V[0, j + 1],
                                        num_cells, num_genes, num_threads, rmatvec_thread_offsets)
                                
                                # `V[:, j + 1] -= alpha * V[:, j]`
                                for i in prange(num_genes, num_threads=num_threads):
                                    V[i, j + 1] -= alpha * V[i, j]
                                
                                # `V[:, j + 1] -= V[:, :j + 1] @ (V[:, :j + 1].T @ V[:, j + 1])`;
                                # use `work` to store the intermediate product, which has length
                                # `j + 1` and is guaranteed to fit: as mentioned above, `work` has
                                # at least `4 * subspace_size ** 2 + 7 * subspace_size` elements
                                
                                # `work = V[:, :j + 1].T @ V[:, j + 1]`
                                # sgemv(b'T', num_genes, j + 1, 1, &V[0, 0], num_genes,
                                #       &V[0, j + 1], 1, 0, work.data(), 1)
                                for col in prange(j + 1, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_genes):
                                        dot_product = dot_product + V[row, col] * V[row, j + 1]
                                    work[col] = dot_product
                                
                                # `V[:, j + 1] -= V[:, :j + 1] @ work`
                                # sgemv(b'N', num_genes, j + 1, -1, &V[0, 0], num_genes,
                                #       work.data(), 1, 1, &V[0, j + 1], 1)
                                for row in prange(num_genes, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(j + 1):
                                        dot_product = dot_product + V[row, col] * work[col]
                                    V[row, j + 1] -= dot_product
                            else:
                                # `j == subspace_size - 1` so `V[:, j + 1]` would overwrite
                                # the end of the array. Do the same computation, but store the
                                # results in the `remainder` vector instead of `V[:, j + 1]`.
                                # `remainder = X_scaled.T @ U[:, j]`
                                rmatvec(data_rmatvec, indices_rmatvec, indptr_rmatvec,
                                        U[:, j], clipped_stddev, &U_new[0, 0], &remainder[0],
                                        num_cells, num_genes, num_threads, rmatvec_thread_offsets)
                                
                                # `remainder -= alpha * V[:, j]`
                                for i in prange(num_genes, num_threads=num_threads):
                                    remainder[i] -= alpha * V[i, j]
                                
                                # `work = V.T @ remainder`
                                # sgemv(b'T', num_genes, subspace_size, 1, &V[0, 0], num_genes,
                                #       &remainder[0], 1, 0, &work[0], 1)
                                for col in prange(subspace_size, num_threads=num_threads):
                                    dot_product = 0
                                    for row in range(num_genes):
                                        dot_product = dot_product + V[row, col] * remainder[row]
                                    work[col] = dot_product
                                
                                # `remainder -= V @ work`
                                # sgemv(b'N', num_genes, subspace_size, -1, &V[0, 0], num_genes,
                                #       &work[0], 1, 1, &remainder[0], 1)
                                for row in prange(num_genes, num_threads=num_threads):
                                    dot_product = 0
                                    for col in range(subspace_size):
                                        dot_product = dot_product + V[row, col] * work[col]
                                    remainder[row] -= dot_product
                                
                                break
                                
                            beta = norm(V[:, j + 1])
                            inverse_beta = 1 / beta
                            for i in range(num_genes):
                                V[i, j + 1] *= inverse_beta
                            B[j, j + 1] = beta
                            j += 1
                            if j % 10 == 9:
                                with gil:
                                    PyErr_CheckSignals()
                            
                        # Compute the SVD of the bidiagonal matrix `B`, equivalent to
                        # `Ub, Sb, Vbt = np.linalg.svd(B, full_matrices=False)`
                        svd(B, Ub, Sb, Vbt, work.data(), iwork.data(), lwork)
                        
                        # Normalize the `remainder` vector
                        remainder_norm = norm(remainder)
                        for i in range(num_genes):
                            remainder[i] /= remainder_norm
                        
                        # Update `Smax`, the largest singular value of `B` we've seen so far
                        Smax = max(Sb[0], Smax)
                        
                        # Check convergence of singular values via residuals
                        num_converged = 0
                        for i in range(k):
                            residual = remainder_norm * Ub[subspace_size - 1, i]
                            num_converged += abs(residual) < tolerance * Smax
                        if num_converged == k:
                            converged = True
                            break
                        
                        # For the next iteration, increase `k_current` to `k` + the number of
                        # converged singular values, ensuring it stays under the subspace size
                        k_current = min(max(k_current, k + num_converged), subspace_size - 1)
                        
                        # Restart with new subspace: update `U`, `V` and `B`. The `U` and `V`
                        # updates are in-place, but `sgemm()` doesn't support in-place matrix
                        # multiplication, so use `U_new` and `V_new` as the output arrays and
                        # then swap the pointers so that `U` and `V` point to the output.
                        
                        # U[:, :k_current] = U @ Ub[:, :k_current]
                        sgemm(b'N', b'N', num_cells, k_current, subspace_size, 1, &U[0, 0],
                              num_cells, &Ub[0, 0], subspace_size, 0, &U_new[0, 0], num_cells)
                        temp = U
                        U = U_new
                        U_new = temp
                        
                        # V[:, :k_current] = V @ Vbt.T[:subspace_size, :k_current]
                        sgemm(b'N', b'T', num_genes, k_current, subspace_size, 1, &V[0, 0],
                              num_genes, &Vbt[0, 0], subspace_size, 0, &V_new[0, 0], num_genes)
                        temp = V
                        V = V_new
                        V_new = temp
                        
                        for i in range(num_genes):
                            V[i, k_current] = remainder[i]
                        B[:] = 0
                        for i in range(k_current):
                            B[i, i] = Sb[i]
                            residual = remainder_norm * Ub[subspace_size - 1, i]
                            B[i, k_current] = residual
                
            # Free all no-longer-needed memory, then allocate the final NumPy array
            # for the PCs
            if &U_new[0, 0] == U_buffer.data():
                U_new_buffer.clear()
            else:
                U_buffer.clear()
            V_buffer.clear()
            V_new_buffer.clear()
            B_buffer.clear()
            Vbt_buffer.clear()
            remainder_buffer.clear()
            work.clear()
            np.import_array()
            # PCs = np.empty((num_cells, k), dtype=np.float32)
            PCs = np.PyArray_EMPTY(2, dims, np.NPY_FLOAT32, 0)
        
            # Construct final PCs from the top `k` components of `U` and `S`:
            # U[:, :k] = U[:, :subspace_size] @ Ub[:, :k]
            # PCs[:] = U[:, :k] * Sb[:k]
            # As an optimization, due to linearity we can switch the order of
            # operations to:
            # Ub[:, :k] *= Sb[:k]
            for j in range(k):
                Sbj = Sb[j]
                for i in range(subspace_size):
                    Ub[i, j] *= Sbj
            # followed by:
            # PCs[:] = U[:, :subspace_size] @ Ub[:, :k]
            # However, since `PCs` is C-contiguous whereas `U` and `Ub` are
            # Fortran-contiguous, we actually compute:
            # PCs.T[:] = Ub[:, :k].T @ U[:, :subspace_size].T
            # and then reinterpret PCs as C-contiguous
            sgemm(b'T', b'T', k, num_cells, subspace_size, 1, &Ub[0, 0],
                  subspace_size, &U[0, 0], num_cells, 0, &PCs[0, 0], k)
            
            # Return the PCs, and whether IRLBA converged
            return PCs, converged
            ''', warn_undeclared=False)['irlba']
        
        # Get the data, indices and indptr to be used for the matvec and for
        # the rmatvec. When `match_parallel=True`, this requires calculating
        # the "opposite" version of the array (CSC if `X` is CSR, CSR if CSC)
        is_csr = isinstance(X, csr_array)
        if is_csr:
            # `X` is CSR, so always use CSR for the matvec. If `num_threads=1`
            # and `match_parallel=False`, use CSR for the rmatvec as well,
            # otherwise use CSC for the rmatvec to enable paralellism.
            data_matvec = X.data
            indices_matvec = X.indices
            indptr_matvec = X.indptr
            if num_threads == 1 and not match_parallel:
                data_rmatvec = data_matvec
                indices_rmatvec = indices_matvec
                indptr_rmatvec = indptr_matvec
            else:
                X_csc = X.tocsc()
                data_rmatvec = X_csc.data
                indices_rmatvec = X_csc.indices
                indptr_rmatvec = X_csc.indptr
        else:
            # `X` is CSC, so always use CSC for the rmatvec. If `num_threads=1`
            # and `match_parallel=False`, use CSC for the matvec as well,
            # otherwise use CSR for the matvec to enable paralellism.
            data_rmatvec = X.data
            indices_rmatvec = X.indices
            indptr_rmatvec = X.indptr
            if num_threads == 1 and not match_parallel:
                data_matvec = data_rmatvec
                indices_matvec = indices_rmatvec
                indptr_matvec = indptr_rmatvec
            else:
                X_csr = X.tocsr()
                data_matvec = X_csr.data
                indices_matvec = X_csr.indices
                indptr_matvec = X_csr.indptr
        
        # Run PCA with irlba
        original_num_threads = X._num_threads
        X._num_threads = num_threads
        try:
            with threadpool_limits(num_threads, user_api='blas'):
                num_cells, num_genes = X.shape
                PCs, converged = irlba(
                    data_matvec=data_matvec, indices_matvec=indices_matvec,
                    indptr_matvec=indptr_matvec, data_rmatvec=data_rmatvec,
                    indices_rmatvec=indices_rmatvec,
                    indptr_rmatvec=indptr_rmatvec, num_cells=num_cells,
                    num_genes=num_genes, k=num_PCs,
                    subspace_size=subspace_size, tolerance=tolerance,
                    max_iterations=max_iterations, seed=seed,
                    match_parallel=match_parallel, is_csr=is_csr,
                    num_threads=num_threads)
        finally:
            X._num_threads = original_num_threads
        
        # Print a message if PCs did not converge and `verbose=True`
        if not converged and verbose:
            print(f'PCA did not converge to a tolerance of {tolerance:.2g} '
                  f'after {max_iterations:,} iterations; consider increasing '
                  f'max_iterations or tolerance')
        
        # Store each dataset's PCs in its `obsm`
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns, num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_PCs = PCs[start_index:end_index]
            
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_PCs_QCed = dataset_PCs
                dataset_PCs = np.full((len(dataset),
                                       dataset_PCs_QCed.shape[1]), np.nan,
                                      dtype=np.float32)
                dataset_PCs[QC_col.to_numpy()] = dataset_PCs_QCed
            
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {PC_key: dataset_PCs}, varm=self._varm,
                uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    def neighbors_old(self,
                  *,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  PC_key: str = 'PCs',
                  neighbors_key: str = 'neighbors',
                  num_neighbors: int | np.integer = 20,
                  num_clusters: int | np.integer | None = None,
                  num_probes: int | np.integer | None = None,
                  num_clustering_iterations: int | np.integer = 10,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate the `num_neighbors`-nearest neighbors of each cell.
        
        This function is intended to be run after `PCA()`; by default, it uses
        `obsm['PCs']` as the input to the nearest-neighbors calculation. It
        uses an approximate algorithm based on an inverted file (IVF), as
        implemented in the Facebook AI Similarity Search (FAISS) library.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their nearest neighbors set to `-1`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input for the
                    nearest-neighbors calculation
            neighbors_key: the key of `obsm` where the nearest neighbors will
                           be stored
            num_neighbors: the number of nearest neighbors to report for each
                           cell
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be less than the number of cells. If
                          `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            num_clustering_iterations: the maximum number of iterations of
                                       k-means clustering to perform before
                                       starting the nearest-neighbor search,
                                       stopping early if convergence is reached
            seed: the random seed to use when finding nearest neighbors
            overwrite: if `True`, overwrite `neighbors_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use when finding nearest
                         neighbors. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores. Does not affect
                         the returned SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsm[neighbors_key]` as a `len(obs)` ×
            `num_neighbors` NumPy array, where `obsm[neighbors_key][i, j]`
            stores the index of the `i`th cell's `j + 1`th nearest neighbor.
            (This differs from Scanpy and Seurat, which use a less compact
            sparse matrix representation instead.) For instance, if
            `num_neighbors=2` and the 0th cell's nearest neighbors are the 4th
            cell and the 6th cell, then `obsm[neighbors_key][0]` will be
            `np.array([4, 6])`. Note that if `QC_column` is not `None`, these
            integer indices are with respect to QCed cells, not all cells.
        """
        with ignore_sigint():
            import faiss
        # Check that `neighbors_key` is a string
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `neighbors_key` is not already a key in `obsm`, unless
        # `overwrite=True`
        if not overwrite and neighbors_key in self._obsm:
            error_message = (
                f'neighbors_key {neighbors_key!r} is already a key of obsm; '
                f'did you already run neighbors()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Check that `PC_key` is the name of a key in `obsm`
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += \
                    '; did you forget to run PCA() before neighbors()?'
            raise ValueError(error_message)
        # Get PCs, and check that they are float32 and C-contiguous
        PCs = self._obsm[PC_key]
        if PCs.dtype != np.float32:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float32'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        # Subset PCs to QCed cells only, if `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
        # Check that `num_neighbors` is between 1 and the number of cells
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        num_cells = len(PCs)
        if not 1 <= num_neighbors < num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be ≥ 1 and '
                f'less than the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        # Check that `num_clusters` is between 1 and the number of cells, and
        # that `num_probes` is between 1 and the number of clusters. If either
        # is `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells),
                                           num_cells / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters < num_cells:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and '
                    f'less than the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be ≥ 1 and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = self._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        
        # Calculate each cell's `num_neighbors + 1`-nearest neighbors with
        # faiss, where the `+ 1` is for the cell itself
        dim = PCs.shape[1]
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        index.cp.niter = num_clustering_iterations
        # noinspection PyArgumentList
        index.train(PCs)
        # noinspection PyArgumentList
        index.add(PCs)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = index.search(PCs, num_neighbors + 1)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = PCs[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} '
                          f'{plural("cell", len(needs_update_X))} '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) did '
                          f'not have enough neighbors with {index.nprobe:,} '
                          f'probes; re-running nearest-neighbors finding for '
                          f'these cells with {num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = \
                    index.search(needs_update_X, num_neighbors + 1)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        if verbose:
            # noinspection PyUnresolvedReferences
            percent = \
                (nearest_neighbor_indices[:, 0] == range(num_cells)).mean()
            print(f'{100 * percent:.3f}% of cells are correctly detected as '
                  f'their own nearest neighbors (a measure of the quality of '
                  f'the k-nearest neighbors search)')
        # Remove self-neighbors from each cell's list of nearest neighbors.
        # These are almost always in the 0th column, but occasionally later due
        # to the inaccuracy of the nearest-neighbor search. This leaves us with
        # `num_neighbors + num_extra_neighbors` nearest neighbors.
        remove_self_neighbors = cython_inline(r'''
            from cython.parallel cimport prange
        
            def remove_self_neighbors(long[:, ::1] neighbors,
                                      const unsigned num_threads):
                cdef unsigned i, j, num_cells = neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1]
                
                if num_threads == 1:
                    for i in range(num_cells):
                        # If the cell is its own nearest neighbor (almost always), skip
                        
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                        
                        # Find the position where the cell is listed as its own
                        # self-neighbor
                        
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                        
                        # Shift all neighbors before it to the right, overwriting it
                        
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                else:
                    for i in prange(num_cells, nogil=True,
                                    num_threads=num_threads):
                        if <unsigned> neighbors[i, 0] == i:
                            continue
                        for j in range(1, num_neighbors):
                            if <unsigned> neighbors[i, j] == i:
                                break
                        while j > 0:
                            neighbors[i, j] = neighbors[i, j - 1]
                            j = j - 1
                ''')['remove_self_neighbors']
        remove_self_neighbors(nearest_neighbor_indices, num_threads)
        nearest_neighbor_indices = nearest_neighbor_indices[:, 1:]
        # If `QC_column` was specified, back-project from QCed cells to all
        # cells, filling with -1
        if QC_column is not None:
            nearest_neighbor_indices_QCed = nearest_neighbor_indices
            nearest_neighbor_indices = np.full(
                (len(self), nearest_neighbor_indices_QCed.shape[1]), -1)
            # noinspection PyUnboundLocalVariable
            nearest_neighbor_indices[QCed_NumPy] = \
                nearest_neighbor_indices_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm |
                               {neighbors_key: nearest_neighbor_indices},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def neighbors(self,
                  *,
                  QC_column: SingleCellColumn | None = 'passed_QC',
                  PC_key: str = 'PCs',
                  neighbors_key: str = 'neighbors',
                  distances_key: str = 'distances',
                  num_neighbors: int | np.integer = 20,
                  num_clusters: int | np.integer | None = None,
                  min_clusters_searched: int | np.integer | None = None,
                  max_clusters_searched: int | np.integer | None = None,
                  num_candidates_per_neighbor: int | np.integer = 10,
                  num_kmeans_iterations: int | np.integer = 10,
                  kmeans_barbar: bool = False,
                  num_init_iterations: int | np.integer = 5,
                  oversampling_factor: int | np.integer | float |
                                       np.floating = 1,
                  chunk_size: int | np.integer | None = None,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate the `num_neighbors`-nearest neighbors of each cell.
        
        `neighbors()` is intended to be run after `PCA()`; by default, it uses
        `obsm['PCs']` as the input to the nearest-neighbors calculation.
        `neighbors()` must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        `neighbors()` is based on the `IndexIVFFlat` search strategy from the
        widely-used `faiss` nearest-neighbor search library. It works in two
        main steps. First, it groups nearby cells into clusters using k-means
        clustering. Second, it exhaustively searches the cluster containing the
        cell, plus a few of the clusters nearby, for nearest-neighbor
        candidates.
        
        In the first step, `neighbors()` performs k-means clustering to
        subdivide the dataset into `num_clusters` clusters. For large datasets,
        the number of clusters is the square root of the number of cells
        (rounding up to the nearest integer); for small datasets, 1% of the
        number of cells.
        
        In the second step, for each cell, the nearest `max_clusters_searched`
        clusters to the cell are identified by calculating the Euclidean
        distance between the cell and each cluster centroid. All cells in at
        least `min_clusters_searched` and at most `max_clusters_searched` of
        these nearest clusters are exhaustively searched, in order of their
        centroid's nearness to the cell, for nearest-neighbor candidates, again
        via brute-force Euclidean distance calculations. The search will stop
        early (after `min_clusters_searched` but before `max_clusters_searched`
        cluster have been searched) if, at the end of searching a cluster, at
        least `num_neighbors * num_candidates_per_neighbor` candidates have
        been searched.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their nearest neighbors set to `-1`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input for the
                    nearest-neighbors calculation
            neighbors_key: the key of `obsm` where the nearest neighbors will
                           be stored
            distances_key: the key of `obsm` where the squared Euclidean
                           distance to each nearest neighbor will be stored
            num_neighbors: the number of nearest neighbors to report for each
                           cell; must be less than or equal to the number of
                           cells
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Must be less than the number
                          of cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid going below 39, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            min_clusters_searched: the minimum number of a cell's nearest
                                   clusters to search; must be between 1 and
                                   `max_clusters_searched`. Defaults to
                                   `min(10, num_neighbors, num_clusters)`.
            max_clusters_searched: the maximum number of a cell's nearest
                                   clusters to search; must be at least
                                   `num_neighbors` (to guarantee that each
                                   cell has enough nearest-neighbor candidates
                                   even in the worst-case scenario where all
                                   the nearest clusters contain just one cell)
                                   and at most `num_clusters`. Defaults to
                                   `min(num_neighbors, num_clusters)`.
            num_candidates_per_neighbor: the target number of nearest-neighbor
                                         candidates (cells) to search per
                                         neighbor requested. The true number of
                                         candidates searched may be either
                                         lower or higher than `num_neighbors *
                                         num_candidates_per_neighbor`: lower
                                         when the `num_neighbors` nearest
                                         clusters do not have enough candidates
                                         (in the worst-case scenario where all
                                         the nearest clusters contain just one
                                         cell, only `num_neighbors` cells will
                                         be searched and every searched cell
                                         will be a nearest neighbor), and
                                         higher when the final cluster searched
                                         puts the total number of candidates
                                         slightly over this value, because
                                         clusters are always searched in their
                                         entirety.
            num_kmeans_iterations: the maximum number of iterations of
                                   k-means clustering to perform before
                                   starting the nearest-neighbor search,
                                   stopping early if convergence is reached
            kmeans_barbar: whether to use k-means|| initialization (a parallel
                           version of k-means++ from arxiv.org/abs/1203.6402)
                           to initialize the k-means clustering centroids,
                           instead of random initialization
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402). Only used when
                                 `kmeans_barbar=True`.
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++. Only used when
                                 `kmeans_barbar=True`.
            chunk_size: the chunk size to use for distance calculations in the
                        initial k-means clustering and in the nearest-neighbor
                        search itself. Setting this to a power of 2 is
                        recommended; 256 was found to be the optimal chunk size
                        by scikit-learn (for k-means clustering) and in our 
                        tests (for both steps). Defaults to 
                        `min(256, number of cells)`.
            seed: the random seed to use when finding nearest neighbors
            overwrite: if `True`, overwrite `neighbors_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use when finding nearest
                         neighbors. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores. Does not affect
                         the returned SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsm[neighbors_key]` as a `len(obs)` ×
            `num_neighbors` NumPy array, where `obsm[neighbors_key][i, j]`
            stores the index of the `i`th cell's `j + 1`th nearest neighbor.
            (This differs from Scanpy and Seurat, which use a less compact
            sparse matrix representation instead.) For instance, if
            `num_neighbors=2` and the 0th cell's nearest neighbors are the 4th
            cell and the 6th cell, then `obsm[neighbors_key][0]` will be
            `np.array([4, 6])`. Note that if `QC_column` is not `None`, these
            integer indices are with respect to QCed cells, not all cells. The
            squared Euclidean distance to each nearest neighbor will be stored
            in `obsm[distances_key]` as an array of the same shape as
            `obsm[neighbors_key]`.
        """
        # Check that `neighbors_key` is a string
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `neighbors_key` is not already a key in `obsm`, unless
        # `overwrite=True`
        if not overwrite and neighbors_key in self._obsm:
            error_message = (
                f'neighbors_key {neighbors_key!r} is already a key of obsm; '
                f'did you already run neighbors()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Check that `PC_key` is the name of a key in `obsm`
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += \
                    '; did you forget to run PCA() before neighbors()?'
            raise ValueError(error_message)
        
        # Get PCs, and check that they are float32 and C-contiguous
        PCs = self._obsm[PC_key]
        if PCs.dtype != np.float32:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float32'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        
        # Subset PCs to QCed cells only, if `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
        
        # Check that `num_neighbors` is between 1 and `num_cells - 1`
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        num_cells = len(PCs)
        if not 1 <= num_neighbors < num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be ≥ 1 and '
                f'less than the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        
        # Check that `num_clusters` is between 1 and `num_cells`; if `None`,
        # set to `ceil(min(sqrt(num_cells), num_cells / 100)))`
        if num_clusters is None:
            num_clusters = \
                int(np.ceil(min(np.sqrt(num_cells), num_cells / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters <= num_cells:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and ≤ '
                    f'the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        
        # Check that `max_clusters_searched` is between `num_neighbors` and
        # `num_clusters`; if `None`, set to `min(num_neighbors, num_clusters)`
        if max_clusters_searched is None:
            max_clusters_searched = min(num_neighbors, num_clusters)
        else:
            check_type(max_clusters_searched, 'max_clusters_searched', int,
                       'a positive integer')
            if not num_neighbors <= max_clusters_searched <= num_clusters:
                error_message = (
                    f'max_clusters_searched is {max_clusters_searched:,}, but '
                    f'must be ≥ num_neighbors ({num_neighbors:,}) and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        
        # Check that `min_clusters_searched` is between 1 and
        # `max_clusters_searched`; if `None`, set to
        # `min(10, num_neighbors, num_clusters)`
        if min_clusters_searched is None:
            min_clusters_searched = min(10, num_neighbors, num_clusters)
        else:
            check_type(min_clusters_searched, 'min_clusters_searched', int,
                       'a positive integer')
            if not 1 <= min_clusters_searched <= max_clusters_searched:
                error_message = (
                    f'min_clusters_searched is {min_clusters_searched:,}, but '
                    f'must be ≥ 1 and ≤ max_clusters_searched '
                    f'({max_clusters_searched:,})')
                raise ValueError(error_message)
       
        # Check that `num_candidates_per_neighbor` is between 1 and
        # `num_cells - 1`
        check_type(num_candidates_per_neighbor, 'num_candidates_per_neighbor',
                   int, 'a positive integer')
        if not 1 <= num_candidates_per_neighbor < num_cells:
            error_message = (
                f'num_candidates_per_neighbor is '
                f'{num_candidates_per_neighbor:,}, but must be ≥ 1 and less '
                f'than the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        
        # Check that `num_kmeans_iterations` is a positive integer
        check_type(num_kmeans_iterations, 'num_kmeans_iterations', int,
                   'a positive integer')
        check_bounds(num_kmeans_iterations, 'num_kmeans_iterations', 1)
        
        # Check that `kmeans_barbar` is Boolean
        check_type(kmeans_barbar, 'kmeans_barbar', bool, 'Boolean')
        
        # Check that `num_init_iterations` is a positive integer
        check_type(num_init_iterations, 'num_init_iterations', int,
                   'a positive integer')
        check_bounds(num_init_iterations, 'num_init_iterations', 1)
        
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        
        # If `kmeans_barbar=False`, check that `num_init_iterations` and
        # `oversampling_factor` have their default values
        if not kmeans_barbar:
            if num_init_iterations != 5:
                error_message = (
                    'num_init_iterations can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            if oversampling_factor != 1:
                error_message = (
                    'oversampling_factor can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
        
        # Check that `chunk_size` is between 1 and `num_cells`; if `None`, set
        # to `min(256, num_cells)`
        if chunk_size is None:
            chunk_size = min(256, num_cells)
        else:
            check_type(chunk_size, 'chunk_size', int, 'a positive integer')
            if not 1 <= chunk_size <= num_cells:
                error_message = (
                    f'chunk_size is {chunk_size:,}, but must be ≥ 1 and ≤ the '
                    f'number of cells ({num_cells:,})')
                raise ValueError(error_message)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Run k-means clustering on `PCs`. Since `centroids` and
        # `centroids_new` are swapped every iteration, `centroids_new` will
        # contain the final centroids when doing an odd number of k-means
        # iterations; if so, swap them at the end.
        num_dimensions = PCs.shape[1]
        cluster_labels = np.empty(num_cells, dtype=np.uint32)
        centroids = np.empty((num_clusters, num_dimensions), dtype=np.float32)
        centroids_new = np.empty((num_clusters, num_dimensions),
                                 dtype=np.float32)
        num_cells_per_cluster = np.empty(num_clusters, dtype=np.uint32)
        with threadpool_limits(1, user_api='blas'):
            kmeans = _kmeans_and_knn_functions['kmeans']
            kmeans(X=PCs, cluster_labels=cluster_labels, centroids=centroids,
                   centroids_new=centroids_new,
                   num_cells_per_cluster=num_cells_per_cluster,
                   kmeans_barbar=kmeans_barbar,
                   num_init_iterations=num_init_iterations,
                   oversampling_factor=oversampling_factor,
                   num_kmeans_iterations=num_kmeans_iterations, seed=seed,
                   chunk_size=chunk_size, num_threads=num_threads)
        if num_kmeans_iterations & 1:
            centroids = centroids_new
        del centroids_new
        
        # Find the `num_neighbors` nearest neighbors of each cell, according to
        # `PCs`
        neighbors = np.empty((num_cells, num_neighbors), dtype=np.uint32)
        distances = np.empty((num_cells, num_neighbors), dtype=np.float32)
        with threadpool_limits(1, user_api='blas'):
            knn_self = _kmeans_and_knn_functions['knn_self']
            knn_self(X=PCs, cluster_labels=cluster_labels, centroids=centroids,
                     num_cells_per_cluster=num_cells_per_cluster,
                     neighbors=neighbors, distances=distances,
                     num_neighbors=num_neighbors,
                     min_clusters_searched=min_clusters_searched,
                     max_clusters_searched=max_clusters_searched,
                     num_candidates_per_neighbor=num_candidates_per_neighbor,
                     chunk_size=chunk_size, num_threads=num_threads)
        
        # If `QC_column` was specified, back-project from QCed cells to all
        # cells, filling with -1
        if QC_column is not None:
            neighbors_QCed = neighbors
            neighbors = np.full((len(self), neighbors_QCed.shape[1]), -1)
            # noinspection PyUnboundLocalVariable
            neighbors[QCed_NumPy] = neighbors_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {neighbors_key: neighbors,
                                             distances_key: distances},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def shared_neighbors_old(self,
                         *,
                         QC_column: SingleCellColumn | None |
                                    Sequence[SingleCellColumn |
                                             None] = 'passed_QC',
                         neighbors_key: str = 'neighbors',
                         shared_neighbors_key: str = 'shared_neighbors',
                         min_shared_neighbors: int | np.integer = 3,
                         overwrite: bool = False,
                         num_threads: int | np.integer | None = None) -> \
            SingleCell:
        """
        Calculate the shared nearest neighbor graph of this dataset's cells.
        
        This function is intended to be run after `neighbors()`; by default, it
        uses `obsm['neighbors']` as the input to the shared nearest-neighbors
        calculation.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and excluded from the shared nearest neighbor graph.
            neighbors_key: the key of `obsm` containing the nearest neighbors
                           of each cell calculated with `neighbors()`, to use
                           as the input for the shared nearest neighbor graph
                           calculation
            shared_neighbors_key: the key of `obsp` where the shared nearest
                                  neighbor graph will be stored
            min_shared_neighbors: the minimum number of neighbors a pair of
                                  cells must share to include an edge between
                                  them in the shared nearest neighbor graph.
                                  With 20 nearest neighbors (the default
                                  `num_neighbors` in `neighbors()`), the
                                  default value of `min_shared_neighbors=3`
                                  corresponds to the default value of
                                  `prune.SNN = 1 / 15` in Seurat's
                                  `FindNeighbors()` function: with 3 shared
                                  neighbors, the shared nearest neighbor weight
                                  is `3 / (40 - 3)` or about 0.08, which is
                                  greater than `1 / 15`, but when there are 2,
                                  the weight is only `2 / (40 - 2)` or about
                                  0.05, which is less than `1 / 15`.
            overwrite: if `True`, overwrite `shared_neighbors_key` if already
                       present in `obsp`, instead of raising an error
            num_threads: the number of threads to use when finding shared
                         nearest neighbors. Set `num_threads=-1` to use all
                         available cores, as determined by `os.cpu_count()`, or
                         leave unset to use `self.num_threads` cores. Does not
                         affect the returned SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsp[shared_neighbors_key]` as a symmetric
            `len(obs)` × `len(obs)` sparse array, where
            `obsp[shared_neighbors_key][i, j]` stores the Jaccard index of the
            `i`th and `j`th cell's nearest neighbors: the number of cells that
            are neighbors of both `i` and `j`, divided by the number of cells
            that are neighbors of at least one of `i` and `j`. For instance, if
            20 nearest neighbors have been calculated (i.e.
            `obsm[neighbors_key].shape[1] == 20`) and 8 of the 20 cells in
            `obsm[neighbors_key][i]` are also found in
            `obsm[neighbors_key][j]`, then `obsp[shared_neighbors_key][i, j]`
            will be 0.25 (`8 / (20 + 20 - 8)`).
        """
        # Check that `shared_neighbors_key` is a string
        check_type(shared_neighbors_key, 'shared_neighbors_key', str,
                   'a string')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `shared_neighbors_key` is not already a key in `obsp`,
        # unless `overwrite=True`
        if not overwrite and shared_neighbors_key in self._obsp:
            error_message = (
                f'shared_neighbors_key {shared_neighbors_key!r} is already a '
                f'key of obsp; did you already run shared_neighbors()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += (
                    '; did you forget to run neighbors() before '
                    'shared_neighbors()?')
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}] must have integer data type, but '
                f'has data type {str(nearest_neighbor_indices.dtype)!r}')
            raise TypeError(error_message)
        # Check that `min_shared_neighbors` is less than the number of
        # neighbors
        check_type(min_shared_neighbors, 'min_shared_neighbors', int,
                   'a non-negative integer')
        num_cells, num_neighbors = nearest_neighbor_indices.shape
        if not 0 <= min_shared_neighbors < num_neighbors:
            error_message = (
                f'min_shared_neighbors is {min_shared_neighbors:,}, but must '
                f'be ≥ 0 and less than the number of neighbors in '
                f'obsm[{neighbors_key!r}] ({num_neighbors:,})')
            raise ValueError(error_message)
        # Subset neighbor indices to QCed cells only, if `QC_column` is not
        # `None`; also map each cell's row index in the new neighbors array to
        # its row index in the original neighbors array
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
            QCed_to_full_map = np.flatnonzero(QCed_NumPy)
        else:
            QCed_to_full_map = np.array([], dtype=np.int64)
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        # Compute the shared nearest neighbor graph
        indptr = np.empty(num_cells + 1, dtype=np.uint64)
        indices, data = cython_inline(r'''
            cimport numpy as np
            from cpython.exc cimport PyErr_CheckSignals
            from cython.parallel cimport parallel, prange, threadid
            from libc.string cimport memcpy
            from libcpp.vector cimport vector
            
            # Loosely based on the build_snn_graph function from
            # github.com/libscran/scran_graph_cluster
            
            def compute_SNN(const long[:, :] neighbors,
                            const long[::1] QCed_to_full_map,
                            unsigned long[::1] indptr,
                            const unsigned min_shared_neighbors,
                            str neighbors_key,
                            const unsigned num_threads):
                cdef bint has_QC_column = QCed_to_full_map.shape[0] != 0
                cdef unsigned i, j, k, thread_index, num_unique_ks, \
                    num_unique_pruned_ks, num_shared, size, \
                    num_cells = QCed_to_full_map.shape[0] \
                        if has_QC_column else neighbors.shape[0], \
                    num_neighbors = neighbors.shape[1], \
                    twice_num_neighbors = 2 * num_neighbors
                cdef long neighbor
                cdef unsigned long start
                cdef np.npy_intp nnz
                cdef vector[vector[unsigned]] reverse_neighbors, shared_neighbors
                cdef vector[vector[float]] shared_neighbor_weights
                cdef vector[unsigned] num_shared_neighbors
                cdef vector[vector[unsigned]] thread_num_shared_neighbors
                cdef np.ndarray[np.uint32_t, ndim=1] indices
                cdef np.ndarray[np.float32_t, ndim=1] data
                cdef unsigned[::1] indices_view
                cdef float[::1] data_view
                cdef str error_message
                
                # Build the "reverse" mapping: which cells have each cell as a neighbor.
                # Also check that all neighbor indices are in range.
                
                reverse_neighbors.resize(num_cells)
                for i in range(num_cells):
                    for j in range(num_neighbors):
                        neighbor = neighbors[i, j]
                        if neighbor < 0:
                            error_message = (
                                f'some nearest-neighbor indices in '
                                f'obsm[{neighbors_key!r}] are negative')
                            raise ValueError(error_message)
                        if neighbor >= num_cells:
                            error_message = (
                                f'some nearest-neighbor indices in '
                                f'obsm[{neighbors_key!r}] are >= the total '
                                f'number of cells, {num_cells:,}. This may '
                                f'happen if you subset this SingleCell '
                                f'dataset between neighbors() and '
                                f'shared_neighbors(); if so, make sure to run '
                                f'neighbors() after, not before, subsetting.')
                            raise ValueError(error_message)
                        reverse_neighbors[neighbor].push_back(i)
                
                PyErr_CheckSignals()
                
                # Calculate the number of shared neigbors for each pair of cells
                
                shared_neighbors.resize(num_cells)
                shared_neighbor_weights.resize(num_cells)
                if not has_QC_column:
                    if num_threads == 1:
                        num_shared_neighbors.resize(num_cells)
                        for i in range(num_cells):
                            # For this cell `i`, find all pairs of cells `j` and `k`
                            # where `j` is a shared nearest neighbor of `i` and `k`.
                            # Do this by looking up all neighbors (`j`) of this cell
                            # (`i`), then looking up each of these neighbors' reverse
                            # neighbors (`k`).
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    # Make a list of the unique `k`s...
                                    if num_shared_neighbors[k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    # ...and the number of times each `k` has a shared
                                    # nearest neighbor with `i`. Note: almost all elements of
                                    # `num_shared_neighbors` are 0!
                                    num_shared_neighbors[k] += 1
                            
                            # Calculate the SNN weight between `i` and each `k`: the
                            # number of shared neighbors, divided by the total number
                            # of unique cells in `i` and `k`'s neighbor lists (which is
                            # twice the number of total neighbors, minus the number of
                            # shared neighbors). Note that since `shared_neighbors` is
                            # sorted, the elements of `shared_neighbor_weights` will
                            # correspond to the unique `k`s (after pruning) in sorted order.
                            # Preallocate enough space for all `num_unique_ks` to avoid the
                            # need for dynamic resizing, even though the true required size
                            # (which we call `num_unique_pruned_ks`) is less due to the pruning.
                            
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <float> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k  # move left
                                    num_unique_pruned_ks += 1
                                num_shared_neighbors[k] = 0  # reset
                            
                            # Store the number of unique `k`s after pruning in
                            # `indptr` (we will take the cumsum later)
                            
                            indptr[i + 1] = num_unique_pruned_ks
                        
                        num_shared_neighbors.clear()  # no longer needed
                    else:
                        # Same as the single-threaded version, except that
                        # `num_shared_neighbors` is now `thread_num_shared_neighbors`
                        
                        thread_num_shared_neighbors.resize(num_threads)
                        with nogil, parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_num_shared_neighbors[thread_index].resize(num_cells)
                            for i in prange(num_cells):
                                for j in range(num_neighbors):
                                    for k in reverse_neighbors[neighbors[i, j]]:
                                        if thread_num_shared_neighbors[thread_index][k] == 0:
                                            shared_neighbors[i].push_back(k)
                                        thread_num_shared_neighbors[thread_index][k] += 1
                                num_unique_ks = shared_neighbors[i].size()
                                shared_neighbor_weights[i].reserve(num_unique_ks)
                                num_unique_pruned_ks = 0
                                for k in shared_neighbors[i]:
                                    num_shared = thread_num_shared_neighbors[thread_index][k]
                                    if num_shared >= min_shared_neighbors:
                                        shared_neighbor_weights[i].push_back(
                                            <float> num_shared /
                                            (twice_num_neighbors - num_shared))
                                        shared_neighbors[i][num_unique_pruned_ks] = k
                                        num_unique_pruned_ks = num_unique_pruned_ks + 1
                                    thread_num_shared_neighbors[thread_index][k] = 0
                                indptr[i + 1] = num_unique_pruned_ks
                            thread_num_shared_neighbors[thread_index].clear()  # no longer needed
                else:
                    # Same as the version without a QC column, but store in
                    # `indptr[QCed_to_full_map[i] + 1]` instead of `indptr[i + 1]`.
                    # Also, initialize `indptr` to 0 so that cells failing QC are not
                    # left uninitialized.
                    
                    indptr[:] = 0
                    if num_threads == 1:
                        num_shared_neighbors.resize(num_cells)
                        for i in range(num_cells):
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if num_shared_neighbors[k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    num_shared_neighbors[k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = num_shared_neighbors[k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <float> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks += 1
                                num_shared_neighbors[k] = 0
                            indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                        num_shared_neighbors.clear()  # no longer needed
                    else:
                        thread_num_shared_neighbors.resize(num_threads)
                        with nogil, parallel(num_threads=num_threads):
                            thread_index = threadid()
                            thread_num_shared_neighbors[thread_index].resize(num_cells)
                            for i in prange(num_cells):
                                for j in range(num_neighbors):
                                    for k in reverse_neighbors[neighbors[i, j]]:
                                        if thread_num_shared_neighbors[thread_index][k] == 0:
                                            shared_neighbors[i].push_back(k)
                                        thread_num_shared_neighbors[thread_index][k] += 1
                                num_unique_ks = shared_neighbors[i].size()
                                shared_neighbor_weights[i].reserve(num_unique_ks)
                                num_unique_pruned_ks = 0
                                for k in shared_neighbors[i]:
                                    num_shared = thread_num_shared_neighbors[thread_index][k]
                                    if num_shared >= min_shared_neighbors:
                                        shared_neighbor_weights[i].push_back(
                                            <float> num_shared /
                                            (twice_num_neighbors - num_shared))
                                        shared_neighbors[i][num_unique_pruned_ks] = k
                                        num_unique_pruned_ks = num_unique_pruned_ks + 1
                                    thread_num_shared_neighbors[thread_index][k] = 0
                                indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                            thread_num_shared_neighbors[thread_index].clear()  # no longer needed
                
                reverse_neighbors.clear()  # no longer needed
                PyErr_CheckSignals()
                
                # Take the cumulative sum of the values in `indptr`; initialize the
                # first element to 0
                
                indptr[0] = 0
                for i in range(2, indptr.shape[0]):
                    indptr[i] += indptr[i - 1]
                
                # Allocate `indices` and `data`: their length is the sum of the
                # numbers of unique `k`s across all cells
                
                nnz = indptr[indptr.shape[0] - 1]
                np.import_array()
                # indices = np.empty(nnz, dtype=np.uint32)
                indices = np.PyArray_EMPTY(1, &nnz, np.NPY_UINT32, 0)
                # data = np.empty(nnz, dtype=np.float32)
                data = np.PyArray_EMPTY(1, &nnz, np.NPY_FLOAT32, 0)
                
                # Populate `indices` and `data` with `shared_neighbors` and
                # `shared_neighbor_weights`, respectively. Access the arrays via
                # memoryviews with C-contiguity specified to force Cython to avoid
                # generating slower code that accounts for stride (not sure if this
                # is necessary). If `QC_column` was specified, map each element
                # of `indices` through `QCed_to_full_map` so the indices are
                # with respect to all cells, not just QCed cells.
                
                indices_view = indices
                data_view = data
                if not has_QC_column:
                    if num_threads == 1:
                        for i in range(num_cells):
                            start = indptr[i]
                            size = indptr[i + 1] - start
                            # indices_view[start:start + size] = shared_neighbors[i][:]
                            memcpy(&indices_view[start], shared_neighbors[i].data(),
                                   size * sizeof(unsigned))
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
                    else:
                        for i in prange(num_cells, num_threads=num_threads, nogil=True):
                            start = indptr[i]
                            size = indptr[i + 1] - start
                            # indices_view[start:start + size] = shared_neighbors[i][:]
                            memcpy(&indices_view[start], shared_neighbors[i].data(),
                                   size * sizeof(unsigned))
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
                else:
                    if num_threads == 1:
                        for i in range(num_cells):
                            start = indptr[QCed_to_full_map[i]]
                            size = indptr[QCed_to_full_map[i] + 1] - start
                            # indices_view[start:start + size] = QCed_to_full_map[shared_neighbors[i][:]]
                            for j in range(size):
                                indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
                    else:
                        for i in prange(num_cells, num_threads=num_threads, nogil=True):
                            start = indptr[QCed_to_full_map[i]]
                            size = indptr[QCed_to_full_map[i] + 1] - start
                            # indices_view[start:start + size] = QCed_to_full_map[shared_neighbors[i][:]]
                            for j in range(size):
                                indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                            # data_view[start:start + size] = shared_neighbor_weights[i][:]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
                
                return indices, data
            ''')['compute_SNN'](
                neighbors=nearest_neighbor_indices,
                QCed_to_full_map=QCed_to_full_map, indptr=indptr,
                min_shared_neighbors=min_shared_neighbors,
                neighbors_key=neighbors_key, num_threads=num_threads)
        snn_graph = csr_array((data, indices, indptr),
                              shape=(num_cells, num_cells))
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp | {shared_neighbors_key: snn_graph},
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def shared_neighbors(self,
                         *,
                         QC_column: SingleCellColumn | None |
                                    Sequence[SingleCellColumn |
                                             None] = 'passed_QC',
                         neighbors_key: str = 'neighbors',
                         shared_neighbors_key: str = 'shared_neighbors',
                         min_shared_neighbors: int | np.integer = 3,
                         overwrite: bool = False,
                         num_threads: int | np.integer | None = None) -> \
            SingleCell:
        """
        Calculate the shared nearest neighbor graph of this dataset's cells.
        
        This function is intended to be run after `neighbors()`; by default, it
        uses `obsm['neighbors']` as the input to the shared nearest-neighbors
        calculation.
        
        This function must be re-run if the dataset is subset; not doing so
        will raise an error.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and excluded from the shared nearest neighbor graph.
            neighbors_key: the key of `obsm` containing the nearest neighbors
                           of each cell calculated with `neighbors()`, to use
                           as the input for the shared nearest neighbor graph
                           calculation
            shared_neighbors_key: the key of `obsp` where the shared nearest
                                  neighbor graph will be stored
            min_shared_neighbors: the minimum number of neighbors a pair of
                                  cells must share to include an edge between
                                  them in the shared nearest neighbor graph.
                                  With 20 nearest neighbors (the default
                                  `num_neighbors` in `neighbors()`), the
                                  default value of `min_shared_neighbors=3`
                                  corresponds to the default value of
                                  `prune.SNN = 1 / 15` in Seurat's
                                  `FindNeighbors()` function: with 3 shared
                                  neighbors, the shared nearest neighbor weight
                                  is `3 / (40 - 3)` or about 0.08, which is
                                  greater than `1 / 15`, but when there are 2,
                                  the weight is only `2 / (40 - 2)` or about
                                  0.05, which is less than `1 / 15`.
            overwrite: if `True`, overwrite `shared_neighbors_key` if already
                       present in `obsp`, instead of raising an error
            num_threads: the number of threads to use when finding shared
                         nearest neighbors. Set `num_threads=-1` to use all
                         available cores, as determined by `os.cpu_count()`, or
                         leave unset to use `self.num_threads` cores. Does not
                         affect the returned SingleCell dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset with the indices of each cell's nearest
            neighbors stored in `obsp[shared_neighbors_key]` as a symmetric
            `len(obs)` × `len(obs)` sparse array, where
            `obsp[shared_neighbors_key][i, j]` stores the Jaccard index of the
            `i`th and `j`th cell's nearest neighbors: the number of cells that
            are neighbors of both `i` and `j`, divided by the number of cells
            that are neighbors of at least one of `i` and `j`. For instance, if
            20 nearest neighbors have been calculated (i.e.
            `obsm[neighbors_key].shape[1] == 20`) and 8 of the 20 cells in
            `obsm[neighbors_key][i]` are also found in
            `obsm[neighbors_key][j]`, then `obsp[shared_neighbors_key][i, j]`
            will be 0.25 (`8 / (20 + 20 - 8)`).
        """
        # Check that `shared_neighbors_key` is a string
        check_type(shared_neighbors_key, 'shared_neighbors_key', str,
                   'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `shared_neighbors_key` is not already a key in `obsp`,
        # unless `overwrite=True`
        if not overwrite and shared_neighbors_key in self._obsp:
            error_message = (
                f'shared_neighbors_key {shared_neighbors_key!r} is already a '
                f'key of obsp; did you already run shared_neighbors()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Get the nearest-neighbor indices, and check that they are uint32 and
        # C-contiguous
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += (
                    '; did you forget to run neighbors() before '
                    'shared_neighbors()?')
            raise ValueError(error_message)
        neighbors = self._obsm[neighbors_key]
        if neighbors.dtype != np.uint32:
            error_message = (
                f'obsm[{neighbors_key!r}] must have uint32 data type, but '
                f'has data type {str(neighbors.dtype)!r}')
            raise TypeError(error_message)
        if not neighbors.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{neighbors_key!r}] is not C-contiguous; make it '
                f'C-contiguous with '
                f'pipe_obsm_key({neighbors_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        
        # Check that `min_shared_neighbors` is less than the number of
        # neighbors
        check_type(min_shared_neighbors, 'min_shared_neighbors', int,
                   'a non-negative integer')
        num_cells, num_neighbors = neighbors.shape
        if not 0 <= min_shared_neighbors < num_neighbors:
            error_message = (
                f'min_shared_neighbors is {min_shared_neighbors:,}, but must '
                f'be ≥ 0 and less than the number of neighbors in '
                f'obsm[{neighbors_key!r}] ({num_neighbors:,})')
            raise ValueError(error_message)
        
        # Subset neighbor indices to QCed cells only, if `QC_column` is not
        # `None`; also map each cell's row index in the new neighbors array to
        # its row index in the original neighbors array
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            neighbors = neighbors[QCed_NumPy]
            QCed_to_full_map = np.flatnonzero(QCed_NumPy)
        else:
            QCed_to_full_map = np.array([], dtype=np.int64)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Compute the shared nearest neighbor graph
        indptr = np.empty(num_cells + 1, dtype=np.int64)
        indices, data = cython_inline(_thread_offset_import + r'''
        cimport numpy as np
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libc.string cimport memcpy
        from libcpp.vector cimport vector
        
        def compute_SNN(const unsigned[:, ::1] neighbors,
                        const long[::1] QCed_to_full_map,
                        long[::1] indptr,
                        const unsigned min_shared_neighbors,
                        str neighbors_key,
                        unsigned num_threads):
            # Loosely based on the build_snn_graph function from
            # github.com/libscran/scran_graph_cluster
            
            cdef bint has_QC_column = QCed_to_full_map.shape[0] != 0
            cdef unsigned i, j, k, neighbor, thread_index, num_unique_ks, \
                num_unique_pruned_ks, num_shared, size, \
                num_cells = QCed_to_full_map.shape[0] \
                    if has_QC_column else neighbors.shape[0], \
                num_neighbors = neighbors.shape[1], \
                twice_num_neighbors = 2 * num_neighbors
            cdef unsigned long start
            cdef np.npy_intp nnz
            cdef pair[unsigned, unsigned] row_range
            cdef vector[vector[unsigned]] reverse_neighbors, shared_neighbors
            cdef vector[vector[float]] shared_neighbor_weights
            cdef vector[unsigned] num_shared_neighbors_buffer
            cdef vector[vector[unsigned]] thread_num_shared_neighbors
            cdef np.ndarray[np.uint32_t, ndim=1] indices
            cdef np.ndarray[np.float32_t, ndim=1] data
            cdef unsigned[::1] num_shared_neighbors, indices_view
            cdef float[::1] data_view
            cdef str error_message
            
            # Build the "reverse" mapping: which cells have each cell as a neighbor.
            # Also check that all neighbor indices are in-range.
            reverse_neighbors.resize(num_cells)
            for i in range(num_cells):
                for j in range(num_neighbors):
                    neighbor = neighbors[i, j]
                    if neighbor >= num_cells:
                        error_message = (
                            f'some nearest-neighbor indices in '
                            f'obsm[{neighbors_key!r}] are >= the total '
                            f'number of cells, {num_cells:,}. This may '
                            f'happen if you subset this SingleCell '
                            f'dataset between neighbors() and '
                            f'shared_neighbors(); if so, make sure to run '
                            f'neighbors() after, not before, subsetting.')
                        raise ValueError(error_message)
                    reverse_neighbors[neighbor].push_back(i)
            
            PyErr_CheckSignals()
            
            # Calculate the number of shared neigbors for each pair of cells
            shared_neighbors.resize(num_cells)
            shared_neighbor_weights.resize(num_cells)
            num_threads = min(num_threads, num_cells)
            if not has_QC_column:
                if num_threads == 1:
                    num_shared_neighbors_buffer.resize(num_cells)
                    num_shared_neighbors = <unsigned[:num_cells]> \
                        num_shared_neighbors_buffer.data()
                    for i in range(num_cells):
                        # For this cell `i`, find all pairs of cells `j` and `k`
                        # where `j` is a shared nearest neighbor of `i` and `k`.
                        # Do this by looking up all neighbors (`j`) of this cell
                        # (`i`), then looking up each of these neighbors' reverse
                        # neighbors (`k`).
                        for j in range(num_neighbors):
                            for k in reverse_neighbors[neighbors[i, j]]:
                                # Make a list of the unique `k`s...
                                if num_shared_neighbors[k] == 0:
                                    shared_neighbors[i].push_back(k)
                                # ...and the number of times each `k` has a shared
                                # nearest neighbor with `i`. Note: almost all elements of
                                # `num_shared_neighbors` are 0!
                                num_shared_neighbors[k] += 1
                        
                        # Calculate the SNN weight between `i` and each `k`: the
                        # number of shared neighbors, divided by the total number
                        # of unique cells in `i` and `k`'s neighbor lists (which is
                        # twice the number of total neighbors, minus the number of
                        # shared neighbors). Note that since `shared_neighbors` is
                        # sorted, the elements of `shared_neighbor_weights` will
                        # correspond to the unique `k`s (after pruning) in sorted order.
                        # Preallocate enough space for all `num_unique_ks` to avoid the
                        # need for dynamic resizing, even though the true required size
                        # (which we call `num_unique_pruned_ks`) is less due to the pruning.
                        num_unique_ks = shared_neighbors[i].size()
                        shared_neighbor_weights[i].reserve(num_unique_ks)
                        num_unique_pruned_ks = 0
                        for k in shared_neighbors[i]:
                            num_shared = num_shared_neighbors[k]
                            if num_shared >= min_shared_neighbors:
                                shared_neighbor_weights[i].push_back(
                                    <float> num_shared /
                                    (twice_num_neighbors - num_shared))
                                shared_neighbors[i][num_unique_pruned_ks] = k  # move left
                                num_unique_pruned_ks += 1
                            num_shared_neighbors[k] = 0  # reset
                        
                        # Store the number of unique `k`s after pruning in
                        # `indptr` (we will take the cumsum later)
                        indptr[i + 1] = num_unique_pruned_ks
                    
                    num_shared_neighbors_buffer.clear()  # no longer needed
                else:
                    # Same as the single-threaded version except that
                    # `num_shared_neighbors` is now `thread_num_shared_neighbors`
                    
                    thread_num_shared_neighbors.resize(num_threads)
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_num_shared_neighbors[thread_index].resize(num_cells)
                        for i in prange(num_cells):
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if thread_num_shared_neighbors[thread_index][k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    thread_num_shared_neighbors[thread_index][k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = thread_num_shared_neighbors[thread_index][k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <float> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks = num_unique_pruned_ks + 1
                                thread_num_shared_neighbors[thread_index][k] = 0
                            indptr[i + 1] = num_unique_pruned_ks
                        thread_num_shared_neighbors[thread_index].clear()  # no longer needed
            else:
                # Same as the version without a QC column, but store in
                # `indptr[QCed_to_full_map[i] + 1]` instead of `indptr[i + 1]`.
                # Also, initialize `indptr` to 0 so that cells failing QC are not
                # left uninitialized.
                
                indptr[:] = 0
                if num_threads == 1:
                    num_shared_neighbors_buffer.resize(num_cells)
                    num_shared_neighbors = <unsigned[:num_cells]> \
                        num_shared_neighbors_buffer.data()
                    for i in range(num_cells):
                        for j in range(num_neighbors):
                            for k in reverse_neighbors[neighbors[i, j]]:
                                if num_shared_neighbors[k] == 0:
                                    shared_neighbors[i].push_back(k)
                                num_shared_neighbors[k] += 1
                        num_unique_ks = shared_neighbors[i].size()
                        shared_neighbor_weights[i].reserve(num_unique_ks)
                        num_unique_pruned_ks = 0
                        for k in shared_neighbors[i]:
                            num_shared = num_shared_neighbors[k]
                            if num_shared >= min_shared_neighbors:
                                shared_neighbor_weights[i].push_back(
                                    <float> num_shared /
                                    (twice_num_neighbors - num_shared))
                                shared_neighbors[i][num_unique_pruned_ks] = k
                                num_unique_pruned_ks += 1
                            num_shared_neighbors[k] = 0
                        indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                    num_shared_neighbors.clear()  # no longer needed
                else:
                    thread_num_shared_neighbors.resize(num_threads)
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_num_shared_neighbors[thread_index].resize(num_cells)
                        for i in prange(num_cells):
                            for j in range(num_neighbors):
                                for k in reverse_neighbors[neighbors[i, j]]:
                                    if thread_num_shared_neighbors[thread_index][k] == 0:
                                        shared_neighbors[i].push_back(k)
                                    thread_num_shared_neighbors[thread_index][k] += 1
                            num_unique_ks = shared_neighbors[i].size()
                            shared_neighbor_weights[i].reserve(num_unique_ks)
                            num_unique_pruned_ks = 0
                            for k in shared_neighbors[i]:
                                num_shared = thread_num_shared_neighbors[thread_index][k]
                                if num_shared >= min_shared_neighbors:
                                    shared_neighbor_weights[i].push_back(
                                        <float> num_shared /
                                        (twice_num_neighbors - num_shared))
                                    shared_neighbors[i][num_unique_pruned_ks] = k
                                    num_unique_pruned_ks = num_unique_pruned_ks + 1
                                thread_num_shared_neighbors[thread_index][k] = 0
                            indptr[QCed_to_full_map[i] + 1] = num_unique_pruned_ks
                        thread_num_shared_neighbors[thread_index].clear()  # no longer needed
            
            reverse_neighbors.clear()  # no longer needed
            PyErr_CheckSignals()
            
            # Take the cumulative sum of the values in `indptr`; initialize the
            # first element to 0
            indptr[0] = 0
            for i in range(2, indptr.shape[0]):
                indptr[i] += indptr[i - 1]
            
            # Allocate `indices` and `data`: their length is the sum of the
            # numbers of unique `k`s across all cells
            nnz = indptr[indptr.shape[0] - 1]
            np.import_array()
            # indices = np.empty(nnz, dtype=np.uint32)
            indices = np.PyArray_EMPTY(1, &nnz, np.NPY_UINT32, 0)
            # data = np.empty(nnz, dtype=np.float32)
            data = np.PyArray_EMPTY(1, &nnz, np.NPY_FLOAT32, 0)
            
            # Populate `indices` and `data` with `shared_neighbors` and
            # `shared_neighbor_weights`, respectively. Access the arrays via
            # memoryviews with C-contiguity specified to force Cython to avoid
            # generating slower code that accounts for stride (not sure if this
            # is necessary). If `QC_column` was specified, map each element
            # of `indices` through `QCed_to_full_map` so the indices are
            # with respect to all cells, not just QCed cells.
            indices_view = indices
            data_view = data
            if not has_QC_column:
                if num_threads == 1:
                    for i in range(num_cells):
                        start = indptr[i]
                        size = indptr[i + 1] - start
                        memcpy(&indices_view[start], shared_neighbors[i].data(),
                               size * sizeof(unsigned))
                        memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                               size * sizeof(float))
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        row_range = get_thread_offset(indptr, thread_index, num_threads)
                        for i in range(row_range.first, row_range.second):
                            start = indptr[i]
                            size = indptr[i + 1] - start
                            memcpy(&indices_view[start], shared_neighbors[i].data(),
                                   size * sizeof(unsigned))
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
            else:
                if num_threads == 1:
                    for i in range(num_cells):
                        start = indptr[QCed_to_full_map[i]]
                        size = indptr[QCed_to_full_map[i] + 1] - start
                        for j in range(size):
                            indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                        memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                               size * sizeof(float))
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        row_range = get_thread_offset(indptr, thread_index, num_threads)
                        for i in range(row_range.first, row_range.second):
                            start = indptr[QCed_to_full_map[i]]
                            size = indptr[QCed_to_full_map[i] + 1] - start
                            for j in range(size):
                                indices_view[start + j] = QCed_to_full_map[shared_neighbors[i][j]]
                            memcpy(&data_view[start], shared_neighbor_weights[i].data(),
                                   size * sizeof(float))
            
            return indices, data
            ''')['compute_SNN'](
                neighbors=neighbors, QCed_to_full_map=QCed_to_full_map, 
                indptr=indptr, min_shared_neighbors=min_shared_neighbors,
                neighbors_key=neighbors_key, num_threads=num_threads)
        snn_graph = csr_array((data, indices, indptr),
                              shape=(num_cells, num_cells))
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm, varm=self._varm,
                          obsp=self._obsp | {shared_neighbors_key: snn_graph},
                          varp=self._varp, uns=self._uns,
                          num_threads=self._num_threads)
    
    def dendrogram(self,
                *,
                # QC_column: SingleCellColumn | None |
                #            Sequence[SingleCellColumn | None] = 'passed_QC',
                shared_neighbors_key: str = 'shared_neighbors',
                cell_type_column: str = 'cell_type',
                overwrite: bool = False,
                verbose: bool = True,
                num_threads: int | np.integer | None = None) -> \
            tuple[np.ndarray[1, np.dtype[np.uint32]],
                  np.ndarray[1, np.dtype[np.uint32]],
                  np.ndarray[1, np.dtype[np.float32]],
                  np.ndarray[1, np.dtype[np.uint32]]]:  # SingleCell:
        """
        Cluster cells into cell types using hierarchical clustering.
        
        This function is intended to be run after `shared_neighbors()`; by
        default, it uses `obsm['shared_neighbors']` as the input to the
        clustering.
        
        Hierarchical clustering is performed via the Paris algorithm
        (arxiv.org/abs/1806.01664; github.com/tbonald/paris), as originally
        applied to single-cell data in the Hierarchical Graph Clustering (HGC)
        package (academic.oup.com/bioinformatics/article/37/21/3964/6294402;
        github.com/XuegongLab/HGC).
        
        Args:
            # QC_column: an optional Boolean column of `obs` indicating which
            #            cells passed QC. Can be a column name, a polars
            #            expression, a polars Series, a 1D NumPy array, or a
            #            function that takes in this SingleCell dataset and
            #            returns a polars Series or 1D NumPy array. Set to `None`
            #            to include all cells. Cells failing QC will be ignored
            #            and have their cell-type labels set to `NaN`.
            shared_neighbors_key: the key of `obsp` containing the shared
                                  nearest neighbor graph calculated with
                                  `shared_neighbors()`, to use as the input for
                                  cell-type clustering. Must be symmetric,
                                  although this is not checked for, due to
                                  speed considerations. Diagonals are assumed
                                  to be 1; the actual values are ignored.
            cell_type_column: the name of an integer column to be added to
                              obs indicating the cell-type labels
            overwrite: if `True`, overwrite `cell_type_column` if already
                       present in `obs`, instead of raising an error
            verbose: whether to print details of the cell-type clustering
            num_threads: the number of threads to use when clustering. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Note that only the initial
                         graph construction is parallelized, so the speedup
                         from parallelism will likely be modest (e.g. ~10%
                         faster with a few dozen threads compared to a single
                         thread). Does not affect the returned SingleCell
                         dataset's `num_threads`; this will always be the same
                         as the original dataset's `num_threads`.
        
        Returns:
            A tuple of four NumPy arrays of length `2 * num_cells - 1`:
            - `left_nodes`: the integer index of the left node of each split
            - `right_nodes`: the integer index of the right node of each split
            - `distances`: the distance of each split
            - `sizes`: the total size (left + right) at each split
            Or, to adapt SciPy's description at docs.scipy.org/doc/scipy/
            reference/generated/scipy.cluster.hierarchy.linkage.html: at the
            `i`th clustering iteration, clusters with indices `left_nodes[i]`
            and `right_nodes[i]` are combined to form cluster `num_cells + i`.
            A cluster with an index less than `num_cells` corresponds to one of
            the original observations. The distance between clusters
            `left_nodes[i]` and `right_nodes[i]` is given by `distances[i]`.
            `sizes[i]` represents the number of original observations in the
            newly formed cluster.
            # TODO A new SingleCell dataset where `obs[cell_type_column]`
            # contains an integer cell-type label for each cell.
        
        Note:
            This function may give an incorrect output if you specified a
            custom shared nearest-neighbor graph that a) is non-symmetric (i.e.
            `(obsp[shared_neighbors_key] != obsp[shared_neighbors_key].T).nnz`
            is non-zero), b) contains explicit zeros (i.e. if
            `(obsp[shared_neighbors_key].data == 0).any()`), or c) contains
            negative values: these are not checked for, due to speed
            considerations. In the unlikely event that your custom shared
            nearest-neighbor graph contains explicit zeros, remove them by
            running `obsp[shared_neighbors_key].eliminate_zeros()` (an in-place
            operation) first.
        """
        # Check that `cell_type_column` is a string
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `cell_type_column` is not already a column of `obs`,
        # unless `overwrite=True`
        if not overwrite and 'cell_type_column' in self._obs:
            error_message = (
                f'cell_type_column {cell_type_column!r} is already a column '
                f'of obs; did you already run cluster()? Set overwrite=True '
                f'to overwrite.')
            raise ValueError(error_message)
        
        # Get the QC column, if not None
        # if QC_column is not None:
        #     QC_column = self._get_column(
        #         'obs', QC_column, 'QC_column', pl.Boolean,
        #         allow_missing=QC_column == 'passed_QC')
        
        # Get the shared nearest neighbor graph, and check that it is float32
        check_type(shared_neighbors_key, 'shared_neighbors_key', str,
                   'a string')
        if shared_neighbors_key not in self._obsp:
            error_message = (
                f'shared_neighbors_key {shared_neighbors_key!r} is not a key '
                f'of obsp')
            if shared_neighbors_key == 'shared_neighbors':
                error_message += (
                    '; did you forget to run shared_neighbors() before '
                    'cluster()?')
            raise ValueError(error_message)
        snn_graph = self._obsp[shared_neighbors_key]
        if snn_graph.dtype != np.float32:
            error_message = (
                f'obsp[{shared_neighbors_key!r}] must have data type float32, '
                f'but has data type {str(snn_graph.dtype)!r}')
            raise TypeError(error_message)
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Perform hierarchical clustering with the Paris algorithm. Store the
        # result in three arrays, sorted by increasing distance:
        # - `left_nodes[i]` is the first node in merge `i`
        # - `right_nodes[i]` is the second node in merge `i`
        # - `distances[i]` is the distance at which merge `i` occurs
        # Note: there will always be `num_nodes - 1` merges, where `num_nodes`
        #       is the number of cells/nodes in the graph.
        # Note: because `snn_graph` is symmetric, we don't have to worry about
        #       whether it's CSR or CSC.
        num_cells = snn_graph.shape[0]
        num_merges = num_cells - 1
        left_nodes = np.empty(num_merges, dtype=np.uint32)
        right_nodes = np.empty(num_merges, dtype=np.uint32)
        distances = np.empty(num_merges, dtype=np.float32)
        sizes = np.empty(num_merges, dtype=np.uint32)
        
        # This code generally follows the original Paris clustering implementation
        # (github.com/tbonald/paris/blob/master/paris.py). Note that since the shared
        # nearest neighbors graph is assumed to be symmetric, we do not need to handle
        # CSR and CSC separately.
        cython_inline(_thread_offset_import + _uninitialized_vector_import + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, threadid
        from libc.math cimport INFINITY
        from libc.float cimport FLT_MAX
        from libc.limits cimport UINT_MAX
        from libcpp.vector cimport vector
        
        cdef extern from * nogil:
            """
            // From github.com/ktprime/emhash/blob/master/thirdparty/emilib/emilib2o.hpp,
            // as benchmarked at jacksonallan.github.io/c_cpp_hash_tables_benchmark.
           
            #include <cstdlib>
            #include <cstring>
            #include <iterator>
            #include <utility>
            #include <cassert>
        
            #ifdef _WIN32
            #  include <intrin.h>
            #ifndef __clang__
            //#  include <zmmintrin.h>
            #endif
            #elif __x86_64__
            #  include <x86intrin.h>
            #else
            # include "sse2neon.h"
            #endif
        
            #undef EMH_LIKELY
            #undef EMH_UNLIKELY
        
            // likely/unlikely
            #if (__GNUC__ >= 4 || __clang__) && _MSC_VER == 0
            #    define EMH_LIKELY(condition)   __builtin_expect(condition, 1)
            #    define EMH_UNLIKELY(condition) __builtin_expect(condition, 0)
            #else
            #    define EMH_LIKELY(condition)   condition
            #    define EMH_UNLIKELY(condition) condition
            #endif
        
            // namespace emilib2 {
        
                enum State : int8_t
                {
                    EFILLED  = -126, EDELETE = -127, EEMPTY = -128,
                    SENTINEL = 127,
                };
        
                constexpr static uint8_t EMPTY_OFFSET = 0;
                constexpr static uint8_t OFFSET_STEP  = 4;
        
            #define EMH_ITERATOR_BITS simd_bytes
        
            #ifndef AVX2_EHASH
                const static auto simd_empty  = _mm_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm_set1_epi8(EDELETE);
                const static auto simd_filled = _mm_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm_set1_epi8
                #define LOAD_UEPI8     _mm_loadu_si128
                #define MOVEMASK_EPI8  _mm_movemask_epi8
                #define CMPEQ_EPI8     _mm_cmpeq_epi8
                #define CMPGT_EPI8     _mm_cmpgt_epi8
            #elif SSE2_EMHASH == 0
                const static auto simd_empty  = _mm256_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm256_set1_epi8(EDELETE);
                const static auto simd_filled = _mm256_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm256_set1_epi8
                #define LOAD_UEPI8     _mm256_loadu_si256
                #define MOVEMASK_EPI8  _mm256_movemask_epi8
                #define CMPEQ_EPI8     _mm256_cmpeq_epi8
                #define CMPGT_EPI8     _mm256_cmpgt_epi8
        
            #elif AVX512_EHASH
                const static auto simd_empty  = _mm512_set1_epi8(EEMPTY);
                const static auto simd_delete = _mm512_set1_epi8(EDELETE);
                const static auto simd_filled = _mm512_set1_epi8(EFILLED);
                constexpr static uint8_t simd_bytes = sizeof(simd_empty) / sizeof(uint8_t);
        
                #define SET1_EPI8      _mm512_set1_epi8
                #define LOAD_UEPI8     _mm512_loadu_si512
                #define MOVEMASK_EPI8  _mm512_movemask_epi8 //avx512 error
                #define CMPEQ_EPI8     _mm512_test_epi8_mask
                #define CMPGT_EPI8     _mm512_cmpgt_epi8
            #endif
        
            inline static uint32_t CTZ(uint64_t n)
            {
            #if defined(__x86_64__) || defined(_WIN32) || (__BYTE_ORDER__ && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
        
            #elif __BIG_ENDIAN__ || (__BYTE_ORDER__ && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)
                n = __builtin_bswap64(n);
            #else
                static uint32_t endianness = 0x12345678;
                const auto is_big = *(const char *)&endianness == 0x12;
                if (is_big)
                n = __builtin_bswap64(n);
            #endif
        
            #if _WIN32
                unsigned long index;
                #if defined(_WIN64)
                    _BitScanForward64(&index, n);
                #else
                if ((uint32_t)n)
                    _BitScanForward(&index, (uint32_t)n);
                else
                    {_BitScanForward(&index, (uint32_t)(n >> 32)); index += 32; }
                #endif
            #elif defined (__LP64__) || (SIZE_MAX == UINT64_MAX) || defined (__x86_64__)
                uint32_t index = __builtin_ctzll(n);
            #elif 1
                uint32_t index = __builtin_ctzl(n);
            #endif
        
                return (uint32_t)index;
            }
        
            /// A cache-friendly hash table with open addressing, linear probing and power-of-two capacity
            template <typename KeyT, typename ValueT, typename HashT = std::hash<KeyT>, typename EqT = std::equal_to<KeyT>>
            class HashMap
            {
            private:
                using htype = HashMap<KeyT, ValueT, HashT, EqT>;
        
                using PairT = std::pair<KeyT, ValueT>;
        
            public:
                using size_t          = uint32_t;
                using value_type      = PairT;
                using reference       = PairT&;
                using const_reference = const PairT&;
        
                using mapped_type     = ValueT;
                using val_type        = ValueT;
                using key_type        = KeyT;
                using hasher          = HashT;
                using key_equal       = EqT;
        
                template<typename UType, typename std::enable_if<!std::is_integral<UType>::value, int8_t>::type = 0>
                inline int8_t hash_key2(size_t& main_bucket, const UType& key) const
                {
                    const auto key_hash = _hasher(key);
                    main_bucket = (size_t)key_hash & _mask;
                    return (int8_t)((uint64_t)key_hash % 253 + EFILLED);
                }
        
                template<typename UType, typename std::enable_if<std::is_integral<UType>::value, int8_t>::type = 0>
                inline int8_t hash_key2(size_t& main_bucket, const UType& key) const
                {
                    const auto key_hash = _hasher(key);
                    main_bucket = (size_t)key_hash & _mask;
            //        return (int8_t)((key * 0x9FB21C651E98DF25ull % 251) - 125);
                    return (int8_t)((size_t)key % 253 + EFILLED);
                }
        
                class const_iterator;
                class iterator
                {
                public:
                    using iterator_category = std::forward_iterator_tag;
                    using difference_type   = std::ptrdiff_t;
                    using value_type        = std::pair<KeyT, ValueT>;
                    using pointer           = value_type*;
                    using reference         = value_type&;
        
                    iterator() {}
                    iterator(const const_iterator& it)
                        : _map(it._map), _bucket(it._bucket), _bmask(it._bmask), _from(it._from) {}
                    iterator(const htype* hash_map, size_t bucket) : _map(hash_map), _bucket(bucket) { init(); }
            #if EMH_ITER_SAFE
                    iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { init(); }
            #else
                    iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { _bmask = 0; _from = size_t(- 1); }
            #endif
        
                    void init()
                    {
                        _from = (_bucket / EMH_ITERATOR_BITS) * EMH_ITERATOR_BITS;
                        const auto bucket_count = _map->bucket_count();
                        if (_bucket < bucket_count) {
                            _bmask = _map->filled_mask(_from);
                            _bmask &= ~((1ull << (_bucket % EMH_ITERATOR_BITS)) - 1);
                        } else {
                            _bmask = 0;
                        }
                    }
        
                    size_t operator - (const iterator& r) const
                    {
                        return _bucket - r._bucket;
                    }
        
                    size_t bucket() const
                    {
                        return _bucket;
                    }
        
                    iterator& operator++()
                    {
            #ifndef EMH_ITER_SAFE
                        if (_from == (size_t)-1) init();
            #endif
                        goto_next_element();
                        return *this;
                    }
        
                    iterator operator++(int)
                    {
            #ifndef EMH_ITER_SAFE
                        if (_from == (size_t)-1) init();
            #endif
                        iterator old(*this);
                        goto_next_element();
                        return old;
                    }
        
                    reference operator*() const { return _map->_pairs[_bucket]; }
                    pointer operator->() const { return _map->_pairs + _bucket; }
        
                    bool operator==(const iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const iterator& rhs) const { return _bucket != rhs._bucket; }
                    bool operator==(const const_iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const const_iterator& rhs) const { return _bucket != rhs._bucket; }
        
                private:
                    void goto_next_element()
                    {
                        _bmask &= _bmask - 1;
                        if (_bmask != 0) {
                            _bucket = _from + CTZ(_bmask);
                            return;
                        }
        
                        do {
                            _bmask = _map->filled_mask(_from += EMH_ITERATOR_BITS);
                        } while (_bmask == 0);
        
                        _bucket = _from + CTZ(_bmask);
                    }
        
                public:
                    const htype*  _map;
                    size_t        _bmask;
                    size_t        _bucket;
                    size_t        _from;
                };
        
                class const_iterator
                {
                public:
                    using iterator_category = std::forward_iterator_tag;
                    using difference_type   = std::ptrdiff_t;
                    using value_type        = const std::pair<KeyT, ValueT>;
                    using pointer           = value_type*;
                    using reference         = value_type&;
        
                    explicit const_iterator(const iterator& it)
                        : _map(it._map), _bucket(it._bucket), _bmask(it._bmask), _from(it._from) { init(); }
                    const_iterator(const htype* hash_map, size_t bucket) : _map(hash_map), _bucket(bucket) { init(); }
            //        const_iterator(const htype* hash_map, size_t bucket, bool) : _map(hash_map), _bucket(bucket) { init(); }
        
                    void init()
                    {
                        _from = (_bucket / EMH_ITERATOR_BITS) * EMH_ITERATOR_BITS;
                        const auto bucket_count = _map->bucket_count();
                        if (_bucket < bucket_count) {
                            _bmask = _map->filled_mask(_from);
                            _bmask &= ~((1ull << (_bucket % EMH_ITERATOR_BITS)) - 1);
                        } else {
                            _bmask = 0;
                        }
                    }
        
                    size_t bucket() const
                    {
                        return _bucket;
                    }
        
                    size_t operator - (const const_iterator& r) const
                    {
                        return _bucket - r._bucket;
                    }
        
                    const_iterator& operator++()
                    {
                        goto_next_element();
                        return *this;
                    }
        
                    const_iterator operator++(int)
                    {
                        const_iterator old(*this);
                        goto_next_element();
                        return old;
                    }
        
                    reference operator*() const { return _map->_pairs[_bucket]; }
                    pointer operator->() const { return _map->_pairs + _bucket; }
        
                    bool operator==(const iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const iterator& rhs) const { return _bucket != rhs._bucket; }
                    bool operator==(const const_iterator& rhs) const { return _bucket == rhs._bucket; }
                    bool operator!=(const const_iterator& rhs) const { return _bucket != rhs._bucket; }
        
                private:
                    void goto_next_element()
                    {
                        _bmask &= _bmask - 1;
                        if (_bmask != 0) {
                            _bucket = _from + CTZ(_bmask);
                            return;
                        }
        
                        do {
                            _bmask = _map->filled_mask(_from += EMH_ITERATOR_BITS);
                        } while (_bmask == 0);
        
                        _bucket = _from + CTZ(_bmask);
                    }
        
                public:
                    const htype*  _map;
                    size_t        _bmask;
                    size_t        _bucket;
                    size_t        _from;
                };
        
                // ------------------------------------------------------------------------
        
                HashMap(size_t n = 4) noexcept
                {
                    rehash(n);
                }
        
                HashMap(const HashMap& other) noexcept
                {
                    clone(other);
                }
        
                HashMap(HashMap&& other) noexcept
                {
                    rehash(1);
                    if (this != &other) {
                        swap(other);
                    }
                }
        
                HashMap(std::initializer_list<value_type> il) noexcept
                {
                    reserve((size_t)il.size());
                    for (auto it = il.begin(); it != il.end(); ++it)
                        insert(*it);
                }
        
                template<class InputIt>
                HashMap(InputIt first, InputIt last, size_t bucket_count = 4) noexcept
                {
                    reserve(std::distance(first, last) + bucket_count);
                    for (; first != last; ++first)
                        insert(*first);
                }
        
                HashMap& operator=(const HashMap& other) noexcept
                {
                    if (this != &other)
                        clone(other);
                    return *this;
                }
        
                HashMap& operator=(HashMap&& other) noexcept
                {
                    if (this != &other) {
                        swap(other);
                        other.clear();
                    }
                    return *this;
                }
        
                ~HashMap() noexcept
                {
                    clear_data();
                    _num_filled = 0;
                    _pairs[_num_buckets].~PairT();
                    free(_pairs);
                }
        
                void clone(const HashMap& other) noexcept
                {
                    if (other.size() == 0) {
                        clear();
                        return;
                    }
        
                    clear_data();
        
                    if (other._num_buckets != _num_buckets) {
                        _num_filled = _num_buckets = 0;
                        rehash(other._num_buckets);
                    }
        
                    if (is_copy_trivially()) {
                        const auto pairs_size = (_num_buckets + 1) * sizeof(PairT);
                        memcpy((char*)_pairs, other._pairs, pairs_size);
                    } else {
                        for (auto it = other.cbegin(); it.bucket() <= _num_buckets; ++it)
                            new(_pairs + it.bucket()) PairT(*it);
                    }
        
                    //assert(_num_buckets == other._num_buckets);
                    _num_filled = other._num_filled;
                    const auto state_size = simd_bytes + _num_buckets;
                    memcpy(_states, other._states, state_size * sizeof(_states[0]));
                    memcpy(_offset, other._offset, _num_buckets * sizeof(_offset[0]) / OFFSET_STEP + 1);
                }
        
                void swap(HashMap& other) noexcept
                {
                    std::swap(_hasher,      other._hasher);
                    std::swap(_eq,          other._eq);
                    std::swap(_states,      other._states);
                    std::swap(_pairs,       other._pairs);
                    std::swap(_num_buckets, other._num_buckets);
                    std::swap(_num_filled,  other._num_filled);
                    std::swap(_offset,      other._offset);
                    std::swap(_mask,        other._mask);
                }
        
                // -------------------------------------------------------------
        
                iterator begin() noexcept
                {
                    if (_num_filled == 0)
                        return {this, _num_buckets, false};
                    return {this, find_first_slot(0), false};
                }
        
                const_iterator cbegin() const noexcept
                {
                    if (_num_filled == 0)
                        return {this, _num_buckets};
                    return {this, find_first_slot(0)};
                }
        
                const_iterator begin() const noexcept
                {
                    return cbegin();
                }
        
                iterator end() noexcept
                {
                    return {this, _num_buckets, false};
                }
        
                const_iterator cend() const noexcept
                {
                    return {this, _num_buckets};
                }
        
                const_iterator end() const noexcept
                {
                    return cend();
                }
        
                size_t size() const
                {
                    return _num_filled;
                }
        
                bool empty() const
                {
                    return _num_filled == 0;
                }
        
                // Returns the number of buckets.
                size_t bucket_count() const
                {
                    return _num_buckets;
                }
        
                /// Returns average number of elements per bucket.
                float load_factor() const
                {
                    return _num_filled / static_cast<float>(_num_buckets);
                }
        
                float max_load_factor(float lf = 7.0f / 8)
                {
                    (void)lf;
                    return 7.0f / 8;
                }
        
                // ------------------------------------------------------------
        
                template<typename K>
                iterator find(const K& key) noexcept
                {
                    return {this, find_filled_bucket(key), false};
                }
        
                template<typename K>
                const_iterator find(const K& key) const noexcept
                {
                    return {this, find_filled_bucket(key)};
                }
        
                template<typename K>
                bool contains(const K& k) const noexcept
                {
                    return find_filled_bucket(k) != _num_buckets;
                }
        
                template<typename K>
                size_t count(const K& k) const noexcept
                {
                    return find_filled_bucket(k) != _num_buckets;
                }
        
                template<typename Key = KeyT>
                ValueT& at(const KeyT& key)
                {
                    const auto bucket = find_filled_bucket(key);
                    return _pairs[bucket].second;
                }
        
                template<typename Key = KeyT>
                const ValueT& at(const KeyT& key) const
                {
                    const auto bucket = find_filled_bucket(key);
                    return _pairs[bucket].second;
                }
        
                /// Returns the matching ValueT or nullptr if k isn't found.
                template<typename K>
                ValueT* try_get(const K& k)
                {
                    auto bucket = find_filled_bucket(k);
                    return &_pairs[bucket].second;
                }
        
                /// Const version of the above
                template<typename K>
                ValueT* try_get(const K& k) const
                {
                    auto bucket = find_filled_bucket(k);
                    return &_pairs[bucket].second;
                }
        
                template<typename Con>
                bool operator == (const Con& rhs) const
                {
                    if (size() != rhs.size())
                        return false;
        
                    for (auto it = begin(), last = end(); it != last; ++it) {
                        auto oi = rhs.find(it->first);
                        if (oi == rhs.end() || it->second != oi->second)
                            return false;
                    }
                    return true;
                }
        
                template<typename Con>
                bool operator != (const Con& rhs) const { return !(*this == rhs); }
        
                void merge(HashMap& rhs)
                {
                    if (empty()) {
                        *this = std::move(rhs);
                        return;
                    }
        
                    for (auto rit = rhs.begin(); rit != rhs.end(); ) {
                        auto fit = find(rit->first);
                        if (fit.bucket() > _mask) {
                            insert_unique(rit->first, std::move(rit->second));
                            rhs.erase(rit++);
                        } else {
                            ++rit;
                        }
                    }
                }
        
                // -----------------------------------------------------
        
                /// Returns a pair consisting of an iterator to the inserted element
                /// (or to the element that prevented the insertion)
                /// and a bool denoting whether the insertion took place.
                template<typename K, typename V>
                std::pair<iterator, bool> do_insert(K&& key, V&& val) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
        
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                std::pair<iterator, bool> do_insert(const value_type& value) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(value.first, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(value); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                std::pair<iterator, bool> do_insert(value_type&& value) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(value.first, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::move(value)); _num_filled++;
                    }
                    return { {this, bucket, false}, bempty };
                }
        
                template <class... Args>
                inline std::pair<iterator, bool> emplace(Args&&... args) noexcept
                {
                    return do_insert(std::forward<Args>(args)...);
                }
        
                std::pair<iterator, bool> insert(value_type&& value) noexcept
                {
                    return do_insert(std::move(value));
                }
        
                std::pair<iterator, bool> insert(const value_type& value) noexcept
                {
                    return do_insert(value);
                }
        
            #if 0
                iterator insert(iterator hint, const value_type& value) noexcept
                {
                    (void)hint;
                    return do_insert(value).first;
                }
            #endif
        
                template <typename Iter>
                void insert(Iter beginc, Iter endc)
                {
                    reserve(endc - beginc + _num_filled);
                    for (; beginc != endc; ++beginc)
                        do_insert(beginc->first, beginc->second);
                }
        
                template<class... Args>
                std::pair<iterator, bool> try_emplace(const KeyT& key, Args&&... args)
                {
                    //check_expand_need();
                    return do_insert(key, std::forward<Args>(args)...);
                }
        
                template<class... Args>
                std::pair<iterator, bool> try_emplace(KeyT&& key, Args&&... args)
                {
                    //check_expand_need();
                    return do_insert(std::forward<KeyT>(key), std::forward<Args>(args)...);
                }
        
                void insert(std::initializer_list<value_type> ilist) noexcept
                {
                    reserve(ilist.size() + _num_filled);
                    for (auto it = ilist.begin(); it != ilist.end(); ++it)
                        do_insert(*it);
                }
        
                template<typename K, typename V>
                size_t insert_unique(K&& key, V&& val) noexcept
                {
                    check_expand_need();
        
                    size_t main_bucket;
                    const auto key_h2 = hash_key2(main_bucket, key);
                    const auto bucket = find_empty_slot(main_bucket, main_bucket, 0);
        
                    set_states(bucket, key_h2);
                    new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    return bucket;
                }
        
                template <class M>
                std::pair<iterator, bool> insert_or_assign(const KeyT& key, M&& val) { return do_assign(key, std::forward<M>(val)); }
                template <class M>
                std::pair<iterator, bool> insert_or_assign(KeyT&& key, M&& val) { return do_assign(std::move(key), std::forward<M>(val)); }
        
                template<typename K, typename V>
                std::pair<iterator, bool> do_assign(K&& key, V&& val)
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
        
                    // Check if inserting a new val rather than overwriting an old entry
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::forward<K>(key), std::forward<V>(val)); _num_filled++;
                    } else {
                        _pairs[bucket].second = std::forward<V>(val);
                    }
        
                    return { {this, bucket, false}, bempty };
                }
        
                bool set_get(const KeyT& key, const ValueT& val, ValueT& oldv)
                {
                    //check_expand_need();
        
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    /* Check if inserting a new value rather than overwriting an old entry */
                    if (bempty) {
                        new(_pairs + bucket) PairT(key,val); _num_filled++;
                    } else
                        oldv = _pairs[bucket].second;
                    return bempty;
                }
        
                ValueT& operator[](const KeyT& key) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    /* Check if inserting a new value rather than overwriting an old entry */
                    if (bempty) {
                        new(_pairs + bucket) PairT(key, std::move(ValueT())); _num_filled++;
                    }
        
                    return _pairs[bucket].second;
                }
        
                ValueT& operator[](KeyT&& key) noexcept
                {
                    bool bempty = true;
                    const auto bucket = find_or_allocate(key, bempty);
                    if (bempty) {
                        new(_pairs + bucket) PairT(std::move(key), std::move(ValueT())); _num_filled++;
                    }
        
                    return _pairs[bucket].second;
                }
        
                // -------------------------------------------------------
        
                /// Erase an element from the hash table.
                /// return false if element was not found
                size_t erase(const KeyT& key) noexcept
                {
                    auto bucket = find_filled_bucket(key);
                    if (bucket == _num_buckets)
                        return 0;
        
                    _erase(bucket);
                    return 1;
                }
        
                void erase(const const_iterator& cit) noexcept
                {
                    _erase(cit._bucket);
                }
        
                void erase(iterator it) noexcept
                {
                    _erase(it._bucket);
                }
        
                void _erase(size_t bucket) noexcept
                {
                    _num_filled -= 1;
                    if (is_triviall_destructable())
                        _pairs[bucket].~PairT();
            #if EMH_PSL_LINEAR
                    set_states(bucket, _states[bucket + 1] == State::EEMPTY ? State::EEMPTY : State::EDELETE);
            #else
                    set_states(bucket, State::EDELETE);
            #endif
        
            #if EMH_PSL_ERASE && EMH_PSL_LINEAR
                    if (_states[bucket] == State::EEMPTY) {
                        _offset[bucket] = EMPTY_OFFSET; bucket = (bucket - 1) & _mask;
                        while (_states[bucket] == State::EDELETE) {
                            set_states(bucket, State::EEMPTY); _offset[bucket] = EMPTY_OFFSET;
                            bucket = (bucket - 1) & _mask;
                        }
                    }
            #elif 0
                    if (EMH_UNLIKELY(_num_filled == 0)) {
                        std::fill_n(_states, _num_buckets, State::EEMPTY);
                        std::fill_n(_offset, _num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
                    }
            #endif
                }
        
                iterator erase(const_iterator first, const_iterator last)
                {
                    auto iend = cend();
                    auto next = first;
                    for (; next != last && next != iend; )
                        erase(next++);
        
                    return {this, next.bucket()};
                }
        
                template<typename Pred>
                size_t erase_if(Pred pred)
                {
                    auto old_size = size();
                    for (auto it = begin(), last = end(); it != last; ) {
                        if (pred(*it))
                            erase(it);
                        ++it;
                    }
                    return old_size - size();
                }
        
                static constexpr bool is_triviall_destructable()
                {
            #if __cplusplus >= 201402L || _MSC_VER > 1600
                    return !(std::is_trivially_destructible<KeyT>::value && std::is_trivially_destructible<ValueT>::value);
            #else
                    return !(std::is_pod<KeyT>::value && std::is_pod<ValueT>::value);
            #endif
                }
        
                static constexpr bool is_copy_trivially()
                {
            #if __cplusplus >= 201402L || _MSC_VER > 1600
                    return (std::is_trivially_copyable<KeyT>::value && std::is_trivially_copyable<ValueT>::value);
            #else
                    return (std::is_pod<KeyT>::value && std::is_pod<ValueT>::value);
            #endif
                }
        
                void clear_data()
                {
                    if (is_triviall_destructable()) {
                        for (auto it = begin(); _num_filled; ++it) {
                            const auto bucket = it.bucket();
                            _pairs[bucket].~PairT();
                            _num_filled -= 1;
                        }
                    }
                }
        
                /// Remove all elements, keeping full capacity.
                void clear() noexcept
                {
                    if (_num_filled) {
                        clear_data();
                        std::fill_n(_states, _num_buckets, State::EEMPTY);
                        std::fill_n(_offset, _num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
                    }
                    _num_filled = 0;
                }
        
                void shrink_to_fit()
                {
                    rehash(_num_filled + 1);
                }
        
                bool reserve(size_t num_elems) noexcept
                {
                    size_t required_buckets = num_elems + num_elems / 6;
                    if (EMH_LIKELY(required_buckets < _num_buckets))
                        return false;
        
                    rehash(required_buckets + 2);
                    return true;
                }
        
                /// Make room for this many elements
                void rehash(size_t num_elems) noexcept
                {
                    const size_t required_buckets = num_elems;
                    if (required_buckets < _num_filled)
                        return;
        
                    auto num_buckets = _num_filled > (1u << 16) ? (1u << 16) : simd_bytes;
                    while (num_buckets < required_buckets) { num_buckets *= 2; }
        
                    const auto pairs_size = (num_buckets + 1) * sizeof(PairT);
                    const auto state_size = (simd_bytes + num_buckets);
                    //assert(state_size % 8 == 0);
        
                    const auto* new_data = (char*)malloc(pairs_size + state_size * sizeof(_states[0]) + (state_size / OFFSET_STEP) * sizeof(_offset[0]));
                    auto old_states      = _states;
        
                    auto* new_pairs = (decltype(_pairs)) new_data;
                    _states         = (decltype(_states))(new_data + pairs_size);
                    _offset         = (decltype(_offset))(_states + state_size);
        
                    auto old_num_filled  = _num_filled;
                    auto old_pairs       = _pairs;
                    auto old_buckets     = _num_buckets;
        
                    _num_filled  = 0;
                    _num_buckets = num_buckets;
                    _mask        = num_buckets - 1;
                    _pairs       = new_pairs;
        
                    //init empty
                    std::fill_n(_states, num_buckets, State::EEMPTY);
                    //set sentinel tombstone
                    std::fill_n(_states + num_buckets, simd_bytes, State::SENTINEL);
                    //fill offset to 0
                    std::fill_n(_offset, num_buckets / OFFSET_STEP + 1, EMPTY_OFFSET);
        
                    {
                        //set last packet tombstone. not equal key h2
                        new(_pairs + num_buckets) PairT(KeyT(), ValueT());
                        //size_t main_bucket;
                        //_states[num_buckets] = hash_key2(main_bucket, _pairs[num_buckets].first) + 2; //iterator end tombstone:
                        if (old_buckets && is_triviall_destructable())
                            old_pairs[old_buckets].~PairT();
                    }
        
                    for (size_t src_bucket = old_buckets - 1; _num_filled < old_num_filled; --src_bucket) {
                        if (old_states[src_bucket] >= State::EFILLED) {
                            auto& src_pair = old_pairs[src_bucket];
                            size_t main_bucket;
                            const auto key_h2 = hash_key2(main_bucket, src_pair.first);
                            const auto bucket = find_empty_slot(main_bucket, main_bucket, 0);
        
                            set_states(bucket, key_h2);
                            new(_pairs + bucket) PairT(std::move(src_pair));
                            _num_filled ++;
                            if (is_triviall_destructable())
                                src_pair.~PairT();
                        }
                    }
                    free(old_pairs);
                }
        
            private:
                // Can we fit another element?
                void check_expand_need()
                {
                    reserve(_num_filled);
                }
        
                static void prefetch_heap_block(char* ctrl)
                {
                    // Prefetch the heap-allocated memory region to resolve potential TLB
                    // misses.  This is intended to overlap with execution of calculating the hash for a key.
            #if defined(_MSC_VER) && (defined(_M_X64) || defined(_M_IX86))
                    _mm_prefetch((const char*)ctrl, _MM_HINT_T0);
            #elif defined(__GNUC__)
                    __builtin_prefetch(static_cast<const void*>(ctrl));
            #endif
                }
        
                inline uint32_t get_offset(size_t main_bucket) const
                {
            #if EMH_SAFE_PSL
                    if (EMH_UNLIKELY(_offset[main_bucket / OFFSET_STEP] > 128))
                        return (_offset[main_bucket / OFFSET_STEP] - 127) * 128;
            #endif
                    return _offset[main_bucket / OFFSET_STEP];
                }
        
                inline void set_offset(size_t main_bucket, uint32_t off)
                {
            #if EMH_SAFE_PSL
                    _offset[main_bucket / OFFSET_STEP] = off <= 128 ? off : 128 + off / 128;
            #else
                    _offset[main_bucket / OFFSET_STEP] = (uint8_t)off;
            #endif
                }
        
                inline void set_states(size_t ebucket, int8_t key_h2)
                {
                    _states[ebucket] = key_h2;
                }
        
                inline size_t get_next_bucket(size_t next_bucket, size_t offset) const
                {
            #if EMH_PSL_LINEAR == 0
                    next_bucket += offset < 6 ? simd_bytes * offset + 5 : _num_buckets / 15 + 1;
                    //next_bucket += simd_bytes * offset + 1;
            #elif EMH_PSL_LINEAR == 1
                    if (offset < 8)
                        next_bucket += simd_bytes * 2 + offset;
                    else
                        next_bucket += _num_buckets / 32 + 1;
            #else
                    next_bucket += simd_bytes;
                    if (next_bucket >= _num_buckets)
                        next_bucket = offset;
            #endif
                    return next_bucket & _mask;
                }
        
                bool is_empty(size_t bucket) const
                {
                    return _states[bucket] == State::EEMPTY;
                }
        
                // Find the main_bucket with this key, or return (size_t)-1
                template<typename K>
                size_t find_filled_bucket(const K& key) const noexcept
                {
                    size_t main_bucket;
                    const auto filled = SET1_EPI8(hash_key2(main_bucket, key));
                    auto next_bucket = main_bucket;
                    size_t offset = 0;
        
                    if (1)
                    {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
                        if (maskf) {
                            prefetch_heap_block((char*)&_pairs[next_bucket]);
                            do {
                                const auto fbucket = next_bucket + CTZ(maskf);
                                if (EMH_LIKELY(_eq(_pairs[fbucket].first, key)))
                                    return fbucket;
                                maskf &= maskf - 1;
                            } while (maskf != 0);
                        }
        
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0)
                            return _num_buckets;
                        else if (0 == get_offset(main_bucket))
                            return _num_buckets;
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                    }
        
                    while (true) {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
                        if (maskf) {
                            prefetch_heap_block((char*)&_pairs[next_bucket]);
                            do {
                                const auto fbucket = next_bucket + CTZ(maskf);
                                if (EMH_LIKELY(_eq(_pairs[fbucket].first, key)))
                                    return fbucket;
                                maskf &= maskf - 1;
                            } while (maskf != 0);
                        }
            #if 0
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0)
                            break;
            #endif
                        if (++offset > get_offset(main_bucket))
                            break;
                        next_bucket = get_next_bucket(next_bucket, offset);
                    }
        
                    return _num_buckets;
                }
        
                // Find the main_bucket with this key, or return a good empty main_bucket to place the key in.
                // In the later case, the main_bucket is expected to be filled.
                template<typename K>
                size_t find_or_allocate(const K& key, bool& bnew) noexcept
                {
                    reserve(_num_filled);
        
                    size_t main_bucket;
                    const auto key_h2 = hash_key2(main_bucket, key);
                    prefetch_heap_block((char*)&_pairs[main_bucket]);
                    const auto filled = SET1_EPI8(key_h2);
                    auto next_bucket = main_bucket, offset = 0u;
                    constexpr size_t chole = (size_t)-1;
                    size_t hole = chole;
        
                    while (true) {
                        const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[next_bucket]));
                        auto maskf  = MOVEMASK_EPI8(CMPEQ_EPI8(vec, filled));
        
                        //1. find filled
                        while (maskf != 0) {
                            const auto fbucket = next_bucket + CTZ(maskf);
                            if (_eq(_pairs[fbucket].first, key)) {
                                bnew = false;
                                return fbucket;
                            }
                            maskf &= maskf - 1;
                        }
        
                        //2. find empty
                        const auto maske = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_empty));
                        if (maske != 0) {
                            auto ebucket = next_bucket + CTZ(maske);
                            if (EMH_UNLIKELY(hole != chole))
                                ebucket = hole;
                            set_states(ebucket, key_h2);
                            return ebucket;
                        }
        
                        //3. find erased
                        else if (hole == chole) {
                            const auto maskd = MOVEMASK_EPI8(CMPEQ_EPI8(vec, simd_delete));
                            if (maskd != 0)
                                hole = next_bucket + CTZ(maskd);
                        }
        
                        //4. next round
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                        if (offset > get_offset(main_bucket))
                            break;
                    }
        
                    if (hole != chole) {
                        //set_offset(main_bucket, offset - 1);
                        set_states(hole, key_h2);
                        return hole;
                    }
        
                    const auto ebucket = find_empty_slot(main_bucket, next_bucket, offset);
                    set_states(ebucket, key_h2);
                    return ebucket;
                }
        
                inline uint32_t empty_delete(size_t gbucket) const noexcept
                {
                    const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[gbucket]));
                    return MOVEMASK_EPI8(CMPGT_EPI8(simd_filled, vec));
                }
        
                inline uint32_t filled_mask(size_t gbucket) const noexcept
                {
            #if EMH_ITERATOR_BITS == 160
                    const auto vec = _mm_slli_epi16(_mm_loadu_si128((__m128i const*) & _states[gbucket]), 7);
                    return (uint32_t)~_mm_movemask_epi8(vec) & 0xFFFF;
            #elif EMH_ITERATOR_BITS == 320
                    const auto vec = _mm256_slli_epi32(_mm256_loadu_si256((__m256i const*) & _states[gbucket]), 7);
                    return (uint32_t)~_mm256_movemask_epi8(vec);
            #else
                    const auto vec = LOAD_UEPI8((decltype(&simd_empty))(&_states[gbucket]));
                    return MOVEMASK_EPI8(CMPGT_EPI8(vec, simd_delete));
            #endif
                }
        
            #if EMH_PSL
                //unlike robin hood, only move large offset once
                size_t update_offset(size_t gbucket, size_t new_bucket, size_t offset) noexcept
                {
                    const auto kdiff  = offset / 2;
                    const auto kprobe = offset - kdiff;
                    const auto kgroup = (new_bucket - kdiff * simd_bytes) & _mask;
                    for (int i = 0; i < 8 * simd_bytes; i++) {
                        const auto kbucket = (kgroup + i) & _mask;
                        if (_offset[kbucket] == 0)
                            continue;
        
                        size_t kmain_bucket;
                        hash_key2(kmain_bucket, _pairs[kbucket].first);
                        if (kmain_bucket != kbucket)
                            continue;
        
                        //move kbucket to new_bucket, update offset
                        set_states(new_bucket, _states[kbucket]);
                        new(_pairs + new_bucket) PairT(std::move(_pairs[kbucket]));
                        _pairs[kbucket].~PairT();
        
                        if (kdiff + 1 - i / simd_bytes > get_offset(kmain_bucket))
                            set_offset(kmain_bucket, kdiff + 1 - i / simd_bytes);
                        if (kprobe + i / simd_bytes >= get_offset(gbucket))
                            set_offset(gbucket, kprobe + 1 + i / simd_bytes);
                        return kbucket;
                    }
        
                    return -1;
                }
            #endif
        
                size_t find_empty_slot(size_t main_bucket, size_t next_bucket, size_t offset) noexcept
                {
                    while (true) {
                        const auto maske = empty_delete(next_bucket);
                        if (maske != 0) {
                            const auto ebucket = CTZ(maske) + next_bucket;
                            prefetch_heap_block((char*)&_pairs[ebucket]);
                            if (offset > get_offset(main_bucket))
                                set_offset(main_bucket, offset);
                            return ebucket;
                        }
                        next_bucket = get_next_bucket(next_bucket, ++offset);
                    }
        
                    return 0;
                }
        
                size_t find_first_slot(size_t next_bucket) const noexcept
                {
                    while (true) {
                        const auto maske = filled_mask(next_bucket);
                        if (maske != 0)
                            return next_bucket + CTZ(maske);
                        next_bucket += simd_bytes;
                    }
                    return 0;
                }
        
            private:
        
                HashT   _hasher;
                EqT     _eq;
                int8_t* _states           = nullptr;
                uint8_t*_offset           = nullptr;
                PairT*  _pairs            = nullptr;
                size_t  _num_buckets      = 0;
                size_t  _mask             = 0; // _num_buckets minus one
                size_t  _num_filled       = 0;
            };
        
            // } // namespace emilib
            #undef LOAD_UEPI8
            """
            cdef cppclass UnsignedFloatHashMapIterator "HashMap<unsigned, float>::iterator":
                UnsignedFloatHashMapIterator() noexcept
                pair[unsigned, float]& operator*() noexcept const
                UnsignedFloatHashMapIterator operator++() noexcept
                UnsignedFloatHashMapIterator operator++(int) noexcept
                bint operator==(const UnsignedFloatHashMapIterator&) noexcept const
                bint operator!=(const UnsignedFloatHashMapIterator&) noexcept const
        
            cdef cppclass UnsignedFloatHashMap "HashMap<unsigned, float>":
                UnsignedFloatHashMap() noexcept
                UnsignedFloatHashMap(size_t n) noexcept
                UnsignedFloatHashMapIterator begin() noexcept const
                UnsignedFloatHashMapIterator end() noexcept const
                size_t erase(const unsigned& key) noexcept
                void clear() noexcept
                bint empty() noexcept const
                size_t size() noexcept const
                size_t find_first_slot(size_t next_bucket) noexcept const
                float& operator[](const unsigned& key) noexcept
        
        def paris(const float[::1] data,
                  const signed_integer[::1] indices,
                  const signed_integer[::1] indptr,
                  unsigned[::1] left_nodes,
                  unsigned[::1] right_nodes,
                  float[::1] distances,
                  unsigned[::1] sizes,
                  const unsigned long num_nodes,
                  unsigned num_threads):
            
            cdef unsigned a, b, c, u, v, thread_index, start_node, num_merges = 0, \
                n = num_nodes, num_dendrogram_nodes = 2 * num_nodes - 1
            cdef unsigned long i
            cdef float wu, weight, d, dmin, inv_wtot, wtot = 0
            cdef pair[unsigned, float] neighbor
            cdef pair[unsigned, unsigned] row_range
            
            # SNN graph
            cdef vector[UnsignedFloatHashMap] graph
            
            # Node weights
            cdef uninitialized_vector[float] w_buffer
            w_buffer.resize(num_dendrogram_nodes)
            cdef float[::1] w = <float[:num_dendrogram_nodes]> w_buffer.data()
            
            # Node sizes
            cdef uninitialized_vector[unsigned] s_buffer
            s_buffer.resize(num_dendrogram_nodes)
            cdef unsigned[::1] s = <unsigned[:num_dendrogram_nodes]> s_buffer.data()
            s[:] = 1
            
            # Current (merged) node index for each leaf node
            cdef uninitialized_vector[unsigned] node_map_buffer
            node_map_buffer.resize(num_nodes)
            cdef unsigned[::1] node_map = <unsigned[:num_nodes]> node_map_buffer.data()
            
            # Nearest-neighbor chain
            cdef vector[unsigned] chain
            
            # Connected components
            # cdef vector[unsigned] cc
            cdef vector[pair[unsigned, unsigned]] cc
            cdef pair[unsigned, unsigned] comp
            
            # Build the graph and calculate the node weights. Note: like in the
            # original Paris clustering implementation, self-edges (which we assume to
            # all be 1) are counted twice towards `w` and once towards `wtot`.
            graph.resize(num_dendrogram_nodes)
            num_threads = min(num_threads, num_nodes)
            if num_threads == 1:
                for u in range(num_nodes):
                    wu = 0
                    for i in range(<unsigned long> indptr[u],
                                   <unsigned long> indptr[u + 1]):
                        v = indices[i]
                        if u != v:
                            weight = data[i]
                            graph[u][v] = weight
                            wu += weight
                    node_map[u] = u
                    wu += 2  # count self-edges twice towards `w`
                    w[u] = wu
                    wtot += wu
            else:
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    row_range = get_thread_offset(indptr, thread_index, num_threads)
                    for u in range(row_range.first, row_range.second):
                        wu = 0
                        for i in range(<unsigned long> indptr[u],
                                       <unsigned long> indptr[u + 1]):
                            v = indices[i]
                            if u != v:
                                weight = data[i]
                                graph[u][v] = weight
                                wu = wu + weight
                        wu = wu + 2  # count self-edges twice towards `w`
                        w[u] = wu
                for u in range(num_nodes):
                    node_map[u] = u
                    wtot += w[u]
            wtot -= num_nodes  # count self-edges only once towards `wtot`
            inv_wtot = 1 / wtot
            
            # Cluster
            u = num_nodes
            chain.reserve(64)
            start_node = 0
            while n > 0:
                PyErr_CheckSignals()
                
                # Pick an arbitrary (non-empty) node, and add it to the chain
                while True:
                    if node_map[start_node] != UINT_MAX:
                        chain.push_back(start_node)
                        break
                    start_node += 1
                
                while not chain.empty():
                    # Pop the last node, `a`, from the chain
                    a = chain.back()
                    chain.pop_back()
                    
                    # Find `a`'s nearest neighbor, `b`
                    dmin = FLT_MAX
                    for neighbor in graph[a]:
                        v = neighbor.first
                        weight = neighbor.second
                        d = w[v] / weight
                        if d < dmin:
                            b = v
                            dmin = d
                        elif d == dmin and v < b:
                            b = v
                    
                    # If the chain is still not empty after popping `a`...
                    if not chain.empty():
                        # Pop the second-last node, `c`, from the chain
                        c = chain.back()
                        chain.pop_back()
                        if b == c:
                            # `a`'s nearest neighbor `b` was the second-last
                            # node in the chain, meaning that `a` and `b` are
                            # mutual nearest neighbors. Merge `a` and `b` (or
                            # more specifically, `node_map[a]` and
                            # `node_map[b]`).
                            left_nodes[num_merges] = node_map[a]
                            right_nodes[num_merges] = node_map[b]
                            distances[num_merges] = dmin * w[a] * inv_wtot
                            sizes[num_merges] = s[a] + s[b]
                            num_merges += 1
                            
                            # Update graph
                            if graph[a].size() < graph[b].size():
                                # Merge `a` into `b`, since `a` has fewer
                                # neighbors than `b` so this should be faster
                                # than merging `b` into `a`. For each neighbor
                                # `v` of `a`, assign `graph[a][v]`'s weight to
                                # `graph[b][v]` and `graph[v][b]`, or add its
                                # weight if `graph[b][v]` and `graph[b][v]`
                                # already exist. Then, delete `graph[a][v]` (by
                                # calling `graph[a].clear()` at the end) and
                                # `graph[v][a]`. To avoid creating self-loops,
                                # delete `graph[a][b]` and `graph[b][a]` at the
                                # start.
                                graph[a].erase(b)
                                graph[b].erase(a)
                                for neighbor in graph[a]:
                                    v = neighbor.first
                                    weight = neighbor.second
                                    graph[b][v] += weight
                                    graph[v][b] += weight
                                    graph[v].erase(a)
                                graph[a].clear()
                                
                                # Set `node_map[a]` to an invalid value (`a`
                                # will never be used again), and update
                                # `node_map[b]` to `u`, the index of the merged
                                # node we just created
                                node_map[a] = UINT_MAX
                                node_map[b] = u
                                
                                # Update the weight and size of `b` to include the
                                # contribution from `a`
                                w[b] += w[a]
                                s[b] += s[a]
                            else:
                                # Merge `b` into `a`: the reverse of the code
                                # above
                                graph[b].erase(a)
                                graph[a].erase(b)
                                for neighbor in graph[b]:
                                    v = neighbor.first
                                    weight = neighbor.second
                                    graph[a][v] += weight
                                    graph[v][a] += weight
                                    graph[v].erase(b)
                                graph[b].clear()
                                node_map[b] = UINT_MAX
                                node_map[a] = u
                                w[a] += w[b]
                                s[a] += s[b]
                            n -= 1
                            u += 1
                        else:
                            # `a`'s nearest neighbor is `b`, but `b`'s nearest
                            # neighbor is another node that's not `a`. Put
                            # `c` and `a` back into the chain, and also add
                            # `b` so we can find which node is its nearest
                            # neighbor, and continue the chain until we find a
                            # pair of nodes that are mutual nearest neighbors.
                            chain.push_back(c)
                            chain.push_back(a)
                            chain.push_back(b)
                    elif dmin != FLT_MAX:
                        # `a` was the first node we added to the chain. Add it
                        # and its nearest neighbor `b` to the chain, and keep
                        # going.
                        chain.push_back(a)
                        chain.push_back(b)
                    else:
                        # `a` has no neighbors, meaning that it comprises an
                        # entire connected component. Remove this connected
                        # component from the graph and store it in `cc`.
                        
                        # cc.push_back(s[a])
                        cc.push_back(pair[unsigned_, unsigned_](node_map[a], s[a]))
                        for neighbor in graph[a]:
                            graph[neighbor.first].erase(a)
                        graph[a].clear()
                        node_map[a] = UINT_MAX
                        n -= 1
            
            # Add connected components to the dendrogram, with a distance of
            # infinity from each other
            if not cc.empty():
                # a = cc.back()
                # cc.pop_back()
                # for b in cc:
                #     left_nodes[num_merges] = a
                #     right_nodes[num_merges] = b
                #     distances[num_merges] = INFINITY
                #     num_merges += 1
                #     a = u
                #     u += 1
                comp = cc.back()
                cc.pop_back()
                a = comp.first
                size = comp.second
                for comp in cc:
                    b = comp.first
                    size += comp.second
                    left_nodes[num_merges] = a
                    right_nodes[num_merges] = b
                    distances[num_merges] = INFINITY
                    sizes[num_merges] = size
                    num_merges += 1
                    a = u
                    u += 1
        ''', warn_undeclared=False)['paris'](
            data=snn_graph.data, indices=snn_graph.indices,
            indptr=snn_graph.indptr, left_nodes=left_nodes,
            right_nodes=right_nodes, distances=distances, sizes=sizes,
            num_nodes=num_cells, num_threads=num_threads)
        
        # Reorder the dendrogram so that distances and non-leaf node IDs are in
        # ascending order
        order = np.vstack((np.arange(num_merges), distances))
        index = np.lexsort(order)
        nindex = np.concatenate([np.arange(num_cells),
                                 num_cells + index.argsort()])
        np.take(nindex, left_nodes, out=left_nodes)
        np.take(nindex, right_nodes, out=right_nodes)
        np.take(left_nodes, index, out=left_nodes)
        np.take(right_nodes, index, out=right_nodes)
        np.take(distances, index, out=distances)
        np.take(sizes, index, out=sizes)
        
        return left_nodes, right_nodes, distances, sizes
        # cell_type_labels = (lambda a, b, c: pl.Series(
        #     np.zeros(len(self._obs if QC_column is None else QC_column))))(
        #     left_nodes, right_nodes, distances)  # not currently using `verbose`
        # if QC_column is not None:
        #     # Back-project from QCed cells to all cells, filling with `null`
        #     cell_type_labels = pl.when(QC_column)\
        #         .then(cell_type_labels
        #               .gather(QC_column.cum_sum().cast(pl.Int32) - 1))
        # return SingleCell(X=self._X,
        #                   obs=self._obs.with_columns(cell_type_labels),
        #                   var=self._var, obsm=self._obsm, varm=self._varm,
        #                   obsp=self._obsp, varp=self._varp, uns=self._uns,
        #                   num_threads=self._num_threads)
    
    def harmonize_even_older(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  max_harmony_iterations: int | np.integer = 10,
                  max_clustering_iterations: int | np.integer | None = 20,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tol_harmony: int | float | np.integer | np.floating = 1e-4,
                  tol_clustering: int | float | np.integer |
                                  np.floating = 1e-5,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  theta: int | float | np.integer | np.floating = 2,
                  tau: int | float | np.integer | np.floating = 0,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0).
        
        Harmony was originally written in R (github.com/immunogenomics/harmony)
        and has two Python ports, harmony-pytorch
        (github.com/lilab-bcb/harmony-pytorch) and harmonypy
        (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm.
                          If not specified, take the minimum of 100 and
                          floor(number of cells / 30).
            max_harmony_iterations: the maximum number of iterations to run Harmony
                              for, if convergence is not achieved. Defaults to
                              10, like the original harmony package,
                              harmony-pytorch, and harmonypy. Set to `None` to
                              use as many iterations as necessary to achieve
                              convergence.
            max_clustering_iterations: the maximum number of iterations to run the
                                 clustering step within each Harmony iteration
                                 for, if convergence is not achieved. Defaults
                                 to 20 iterations, like the original harmony
                                 package and harmonypy; this differs from
                                 the default of 200 iterations used by
                                 harmony-pytorch. Set to `None` to use as many
                                 iterations as necessary to achieve
                                 convergence.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tol_harmony: the relative tolerance used to determine whether to
                         stop Harmony before `max_harmony_iterations` iterations;
                         must be positive
            tol_clustering: the relative tolerance used to determine whether to
                            stop clustering before `max_clustering_iterations`
                            iterations; must be positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            theta: the weight of the diversity penalty term in the Harmony
                   objective function; must be non-negative
            tau: the discounting factor on theta; must be non-negative. By
                 default, `tau = 0`, so there is no discounting.
            seed: the random seed for Harmony
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        with ignore_sigint():
            import faiss
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        # Check that `Harmony_key` is a string
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `Harmony_key` is not already in `obsm` for any dataset,
        # unless `overwrite=True`
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        # Check that `num_clusters`, `max_harmony_iterations`, and
        # `max_clustering_iterations` are `None` or a positive integer; if
        # either max iterations argument is `None`, set it to `INT32_MAX`
        for parameter, parameter_name in (
                (num_clusters, 'num_clusters'),
                (max_harmony_iterations, 'max_harmony_iterations'),
                (max_clustering_iterations, 'max_clustering_iterations')):
            if parameter is not None:
                check_type(parameter, parameter_name, int,
                           'a positive integer')
                check_bounds(parameter, parameter_name, 1)
        if max_harmony_iterations is None:
            max_harmony_iterations = 2147483647
        if max_clustering_iterations is None:
            max_clustering_iterations = 2147483647
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        # Check that `tol_harmony` and `tol_clustering` are positive numbers,
        # and that `ridge_lambda`, `sigma`, `theta`, and `tau` are non-negative
        # numbers. If any is an integer, cast it to a float.
        for parameter, parameter_name in (
                (tol_harmony, 'tol_harmony'),
                (tol_clustering, 'tol_clustering')):
            check_type(parameter, parameter_name, (int, float),
                       'a positive number')
            check_bounds(parameter, parameter_name, 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma'),
                (theta, 'theta'), (tau, 'tau')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tol_harmony = float(tol_harmony)
        tol_clustering = float(tol_clustering)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        theta = float(theta)
        tau = float(tau)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Concatenate PCs (`Z`) across datasets; get labels indicating which
        # rows of these concatenated PCs come from each dataset or batch.
        # Check that the PCs are float32 and C-contiguous.
        Z = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in Z:
            dtype = PCs.dtype
            if dtype != np.float32:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float32')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
                raise ValueError(error_message)
        if QC_column is not None:
            Z = [PCs[QCed] if QCed is not None else PCs
                 for PCs, QCed in zip(Z, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, Z)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum()))
                    batch_index += 1
            batch_labels = np.concatenate(batch_labels)
        Z = np.concatenate(Z)
        # Run Harmony
        cython_functions = cython_inline(r'''
            from cpython.exc cimport PyErr_CheckSignals
            from libcpp.cmath cimport abs, exp, pow, log, sqrt
            from scipy.linalg.cython_blas cimport sgemm, sgemv
            
            cdef extern from "<utility>" namespace "std" nogil:
                T swap[T](T &a, T &b)
            
            cdef inline void matrix_vector_multiply(
                    const float[:, ::1] A,
                    const float[::1] X,
                    float[::1] Y,
                    const bint transpose,
                    const float alpha,
                    const float beta) noexcept nogil:
                # Flip `trans` since our matrix is C-major, whereas BLAS
                # expects Fortran-major
                
                cdef int m = A.shape[1], n = A.shape[0], incx = 1, incy = 1
                cdef char trans = b'N' if transpose else b'T'
                sgemv(&trans, &m, &n, <float*> &alpha, <float*> &A[0,0],
                      &m, <float*> &X[0], &incx, <float*> &beta, &Y[0],
                      &incy)
            
            cdef inline void matrix_multiply(const float[:, ::1] A,
                                             const float[:, ::1] B,
                                             float[:, ::1] C,
                                             const bint transpose_A,
                                             const bint transpose_B,
                                             const float alpha,
                                             const float beta) noexcept nogil:
                # Flip `A` <-> `B` and `shape[0]` <-> `shape[1]` since our
                # matrices are C-major, whereas BLAS expects Fortran-major
                
                cdef int m, n, k, lda, ldb
                cdef char transA, transB
                if transpose_B:
                    m = B.shape[0]
                    k = B.shape[1]
                    lda = k
                    transA = b'T'
                else:
                    m = B.shape[1]
                    k = B.shape[0]
                    lda = m
                    transA = b'N'
                if transpose_A:
                    n = A.shape[1]
                    ldb = n
                    transB = b'T'
                else:
                    n = A.shape[0]
                    ldb = k
                    transB = b'N'
                sgemm(&transA, &transB, &m, &n, &k, <float*> &alpha,
                      <float*> &B[0, 0], &lda, <float*> &A[0, 0], &ldb,
                      <float*> &beta, &C[0, 0], &m)
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            cdef inline void shuffle_array(unsigned[::1] arr, unsigned long* state) noexcept nogil:
                cdef unsigned i, j
                for i in range(arr.shape[0] - 1, 0, -1):
                    j = randint(i + 1, state)
                    swap(arr[i], arr[j])
            
            cdef inline float compute_objective(
                    float[:, ::1] Z_norm_times_Y_norm,
                    const float[:, ::1] R,
                    const float[:, ::1] E,
                    const float[:, ::1] O,
                    float[:, ::1] ratio,
                    const float[::1] theta,
                    float[::1] theta_times_ratio,
                    const float sigma,
                    const unsigned num_cells,
                    const unsigned num_clusters,
                    const unsigned num_batches) noexcept nogil:
                cdef unsigned i, j
                cdef float kmeans_error, entropy_term, diversity_penalty
                kmeans_error = entropy_term = diversity_penalty = 0
                for i in range(num_cells):
                    for j in range(num_clusters):
                        kmeans_error += \
                            R[i, j] * (1 - Z_norm_times_Y_norm[i, j])
                        entropy_term += R[i, j] * log(R[i, j])
                kmeans_error *= 2
                entropy_term *= sigma
                for i in range(num_batches):
                    for j in range(num_clusters):
                        ratio[i, j] = O[i, j] * log(
                            (O[i, j] + 1) / (E[i, j] + 1))
                matrix_vector_multiply(ratio, theta, theta_times_ratio,
                                       transpose=True, alpha=1, beta=0)
                for i in range(num_clusters):
                    diversity_penalty += theta_times_ratio[i]
                diversity_penalty *= sigma
                return kmeans_error + entropy_term + diversity_penalty
            
            def initialize(const float[:, ::1] Z_norm,
                           float[:, ::1] Y_norm,
                           float[:, ::1] Z_norm_times_Y_norm,
                           const unsigned[::1] N_b,
                           float[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           float[:, ::1] R,
                           float[::1] R_sum,
                           float[:, ::1] E,
                           float[:, ::1] O,
                           const float sigma,
                           float[:, ::1] ratio,
                           float[::1] theta,
                           float[::1] theta_times_ratio,
                           const float tau):
                cdef unsigned i, j, batch_label, num_cells = Z_norm.shape[0], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef float norm, objective, base, two_over_sigma = 2 / sigma
                
                # Initialize `Pr_b`
                
                for i in range(num_batches):
                    Pr_b[i] = <float> N_b[i] / num_cells
                
                # Initialize `R`, `R_sum`, and `O`
                
                matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                transpose_A=False, transpose_B=True, alpha=1,
                                beta=0)
                for i in range(num_cells):
                    batch_label = batch_labels[i]
                    norm = 0
                    for j in range(num_clusters):
                        R[i, j] = exp(two_over_sigma *
                                      (Z_norm_times_Y_norm[i, j] - 1))
                        norm += R[i, j]
                    norm = 1 / norm
                    for j in range(num_clusters):
                        R[i, j] *= norm
                        O[batch_label, j] += R[i, j]
                        R_sum[j] += R[i, j]
                
                # Initialize `E`
                
                for i in range(num_batches):
                    for j in range(num_clusters):
                        E[i, j] = Pr_b[i] * R_sum[j]
                
                # Apply discounting to `theta`, if specified
                
                if tau > 0:
                    for i in range(num_batches):
                        base = exp(-N_b[i] / (num_clusters * tau))
                        theta[i] = theta[i] * (1 - base * base)
                
                # Compute and return the initial value of the objective
                # function
                
                objective = compute_objective(
                    Z_norm_times_Y_norm, R, E, O, ratio, theta,
                    theta_times_ratio, sigma, num_cells, num_clusters,
                    num_batches)
                
                return objective
            
            def clustering(const float[:, ::1] Z_norm,
                           float[:, ::1] Z_norm_in,
                           float[:, ::1] Y_norm,
                           float[:, ::1] Z_norm_times_Y_norm,
                           const float[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           float[:, ::1] R,
                           float[:, ::1] R_in,
                           float[::1] R_in_sum,
                           float[:, ::1] E,
                           float[:, ::1] O,
                           float[:, ::1] ratio,
                           const float[::1] theta,
                           float[::1] theta_times_ratio,
                           unsigned[::1] idx_list,
                           const float tol,
                           const unsigned max_iter,
                           const float sigma,
                           const unsigned block_size):
                cdef unsigned i, j, k, iter, num_cells_in_block, pos, batch_label, \
                    num_cells = Z_norm.shape[0], num_PCs = Z_norm.shape[1], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef unsigned long state
                cdef float norm, old, new, objective, \
                    two_over_sigma = 2 / sigma
                cdef float exp_neg_two_over_sigma = exp(-two_over_sigma)
                cdef float[:, ::1] Z_norm_block, R_block
                cdef float past_clustering_objectives[3]
                
                for iter in range(max_iter):
                    # Compute cluster centroids
                    
                    matrix_multiply(R, Z_norm, Y_norm, transpose_A=True,
                                    transpose_B=False, alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm = norm + Y_norm[i, j] * Y_norm[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y_norm[i, j] = Y_norm[i, j] * norm
                    for i in range(num_cells):
                        idx_list[i] = i
                    state = srand(iter)
                    shuffle_array(idx_list, &state)
                    
                    # Update cells blockwise
                    
                    pos = 0
                    Z_norm_block = Z_norm_in
                    R_block = R_in
                    while pos < num_cells:
                        if pos + block_size > num_cells:
                            num_cells_in_block = num_cells - pos
                            Z_norm_block = Z_norm_block[:num_cells_in_block]
                            R_block = R_block[:num_cells_in_block]
                        else:
                            num_cells_in_block = block_size
                        
                        # Remove the cells in this block from `E` and `O`
                        
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[pos + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_in_sum[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_norm_block[i, j] = Z_norm[k, j]
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] -= Pr_b[i] * R_in_sum[j]
                        
                        # Recompute `R` for the removed cells
                        # Note: the original formula is
                        # `exp(-2 / sigma * (1 - Z_norm_block @ Y_norm.T))`,
                        # which expands to `exp(-2 / sigma) *
                        # exp(2 / sigma * Z_norm_block @ Y_norm.T))`. Since
                        # `exp(-2 / sigma)` is a constant, we fold it into
                        # `ratio`.
                        
                        matrix_multiply(Z_norm_block, Y_norm, R_block,
                                        transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    pow((E[i, j] + 1) / (O[i, j] + 1),
                                        theta[i])
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[i + pos]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                R[k, j] = exp(R_block[i, j]) * \
                                          ratio[batch_label, j]
                                norm += R[k, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[k, j] *= norm
                                R_in_sum[j] += R[k, j]
                                
                                # Add the removed cells back into `O`
                                
                                O[batch_label, j] += R[k, j]
                        
                        # Add the removed cells back into `E`
                        
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] += Pr_b[i] * R_in_sum[j]
                        
                        # Move to the next block
                        
                        pos += block_size
                    
                    # Compute the objective and decide whether we've converged
                    
                    matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                    transpose_A=False, transpose_B=True,
                                    alpha=1, beta=0)
                    objective = compute_objective(
                        Z_norm_times_Y_norm, R, E, O, ratio, theta,
                        theta_times_ratio, sigma, num_cells, num_clusters,
                        num_batches)
                    if iter < 3:
                        past_clustering_objectives[iter] = objective
                    else:
                        old = past_clustering_objectives[0] + \
                            past_clustering_objectives[1] + \
                            past_clustering_objectives[2]
                        new = past_clustering_objectives[1] + \
                            past_clustering_objectives[2] + objective
                        if old - new < tol * abs(old):
                            break
                        else:
                            past_clustering_objectives[0] = \
                                past_clustering_objectives[1]
                            past_clustering_objectives[1] = \
                                past_clustering_objectives[2]
                            past_clustering_objectives[2] = objective
                    PyErr_CheckSignals()
                return objective
            
            def correction(const float[:, ::1] Z,
                           float[:, ::1] Z_hat,
                           const float[:, ::1] R,
                           const float[:, ::1] O,
                           const float ridge_lambda,
                           const unsigned[::1] batch_labels,
                           float[::1] factor,
                           float[:, ::1] P,
                           float[:, ::1] P_t_B_inv,
                           float[:, ::1] inv_mat,
                           float[:, ::1] Phi_t_diag_R_by_X,
                           float[:, ::1] W):
                cdef unsigned i, j, k, batch_label, num_cells = Z.shape[0], \
                    num_PCs = Z.shape[1], num_batches = O.shape[0], \
                    num_clusters = O.shape[1]
                cdef float c, c_inv
                
                Z_hat[:] = Z[:]
                
                # Initialize `P` to the identity matrix
                
                P[:] = 0
                for i in range(num_batches + 1):
                    P[i, i] = 1
            
                for k in range(num_clusters):
                    # Compute `factor`, `c_inv` and `P`
                    
                    c = 0
                    for i in range(num_batches):
                        factor[i] = 1 / (O[i, k] + ridge_lambda)
                        c += O[i, k] * (1 - factor[i] * O[i, k])
                        P[0, i + 1] = -factor[i] * O[i, k]
                    c_inv = 1 / c
                    
                    # Compute `P_t_B_inv`
                    
                    P_t_B_inv[:] = 0
                    P_t_B_inv[0, 0] = c_inv
                    for i in range(1, num_batches + 1):
                        P_t_B_inv[i, i] = factor[i - 1]
                        P_t_B_inv[i, 0] = P[0, i] * c_inv
                    
                    # Compute `inv_mat`
                    
                    matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                    transpose_B=False, alpha=1, beta=0)
                    
                    # Compute `Phi_t_diag_R @ X`
                    
                    Phi_t_diag_R_by_X[:] = 0
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Phi_t_diag_R_by_X[0, j] += Z[i, j] * R[i, k]
                            Phi_t_diag_R_by_X[batch_label + 1, j] += \
                                Z[i, j] * R[i, k]
                            
                    # Compute `W`
                    
                    matrix_multiply(inv_mat, Phi_t_diag_R_by_X, W,
                                    transpose_A=False, transpose_B=False,
                                    alpha=1, beta=0)
                    
                    # Update `Z_hat`
                    
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Z_hat[i, j] = Z_hat[i, j] - \
                                W[batch_label + 1, j] * R[i, k]
                        
            def normalize_rows(const float[:, ::1] arr, float[:, ::1] out):
                cdef unsigned i, j
                cdef float norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

            def normalize_rows_inplace(float[:, ::1] arr):
                cdef unsigned i, j
                cdef float norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        arr[i, j] = arr[i, j] * norm
        ''')
        initialize = cython_functions['initialize']
        clustering = cython_functions['clustering']
        correction = cython_functions['correction']
        normalize_rows = cython_functions['normalize_rows']
        normalize_rows_inplace = cython_functions['normalize_rows_inplace']
        
        # Get dimensions of everything
        num_cells, num_PCs = Z.shape
        block_size = int(num_cells * block_proportion)
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        N_b = bincount(batch_labels, num_bins=batch_labels[-1] + 1,
                       num_threads=1)
        num_batches = len(N_b)
        
        # Allocate arrays
        Z_norm = np.empty((num_cells, num_PCs), dtype=np.float32)
        Z_norm_in = np.empty((block_size, num_PCs), dtype=np.float32)
        Z_norm_times_Y_norm = np.empty((num_cells, num_clusters),
                                       dtype=np.float32)
        Pr_b = np.empty(num_batches, dtype=np.float32)
        R = np.empty((num_cells, num_clusters), dtype=np.float32)
        R_in = np.empty((block_size, num_clusters), dtype=np.float32)
        R_in_sum = np.zeros(num_clusters, dtype=np.float32)
        E = np.empty((num_batches, num_clusters), dtype=np.float32)
        O = np.zeros((num_batches, num_clusters), dtype=np.float32)
        ratio = np.empty((num_batches, num_clusters), dtype=np.float32)
        theta = np.repeat(theta, num_batches).astype(np.float32)
        theta_times_ratio = np.empty(num_clusters, dtype=np.float32)
        idx_list = np.empty(num_cells, dtype=np.uint32)
        factor = np.empty(num_cells, np.float32)
        P = np.empty((num_batches + 1, num_batches + 1), np.float32)
        P_t_B_inv = np.empty((num_batches + 1, num_batches + 1), np.float32)
        inv_mat = np.empty((num_batches + 1, num_batches + 1), np.float32)
        Phi_t_diag_R_by_X = np.empty((num_batches + 1, num_PCs), np.float32)
        W = np.empty((num_batches + 1, num_PCs), np.float32)
        
        # Run k-means
        normalize_rows(Z, Z_norm)
        kmeans = faiss.Kmeans(num_PCs, num_clusters, seed=seed)
        kmeans.train(Z_norm)
        Y_norm = kmeans.centroids
        normalize_rows_inplace(Y_norm)
        
        # Complete initialization in Cython
        objective = initialize(
            Z_norm=Z_norm, Y_norm=Y_norm,
            Z_norm_times_Y_norm=Z_norm_times_Y_norm, N_b=N_b, Pr_b=Pr_b,
            batch_labels=batch_labels, R=R, R_sum=R_in_sum, E=E, O=O,
            sigma=sigma, ratio=ratio, theta=theta,
            theta_times_ratio=theta_times_ratio, tau=tau)
        
        if verbose:
            print(f'Initialization is complete: objective = {objective:.2f}')
        
        iteration_string = plural('iteration', max_harmony_iterations)
        
        for i in range(1, max_harmony_iterations + 1):
            prev_objective = objective
            objective = clustering(
                Z_norm=Z_norm, Z_norm_in=Z_norm_in, Y_norm=Y_norm,
                Z_norm_times_Y_norm=Z_norm_times_Y_norm, Pr_b=Pr_b,
                batch_labels=batch_labels, R=R, R_in=R_in, R_in_sum=R_in_sum,
                E=E, O=O, ratio=ratio, theta=theta,
                theta_times_ratio=theta_times_ratio, idx_list=idx_list,
                tol=tol_clustering, max_iter=max_clustering_iterations,
                sigma=sigma, block_size=block_size)
            correction(Z=Z, Z_hat=Z_norm, R=R, O=O, ridge_lambda=ridge_lambda,
                       batch_labels=batch_labels, factor=factor, P=P,
                       P_t_B_inv=P_t_B_inv, inv_mat=inv_mat,
                       Phi_t_diag_R_by_X=Phi_t_diag_R_by_X, W=W)
            
            if verbose:
                print(f'Completed {i} of {max_harmony_iterations} '
                      f'{iteration_string}: objective = {objective:.2f}')
            
            if prev_objective - objective < tol_harmony * abs(prev_objective):
                if verbose:
                    print(f'Reached convergence after {i} {iteration_string}')
                break
            
            if i == max_harmony_iterations:
                if verbose:
                    print(f'Failed to converge after {i} {iteration_string}')
                break
            
            normalize_rows_inplace(Z_norm)
        
        del batch_labels, Z, Pr_b, theta, R, E, O, Z_norm_in, Y_norm, \
            Z_norm_times_Y_norm, R_in, R_in_sum, ratio, theta_times_ratio, \
            idx_list, factor, P, P_t_B_inv, inv_mat, Phi_t_diag_R_by_X, W
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z_norm[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan, dtype=np.float32)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    def harmonize_older(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  num_kmeans_iterations: int | np.integer = 25,
                  max_harmony_iterations: int | np.integer = 10,
                  max_clustering_iterations: int | np.integer | None = 20,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tol_harmony: int | float | np.integer | np.floating = 1e-4,
                  tol_clustering: int | float | np.integer |
                                  np.floating = 1e-5,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  theta: int | float | np.integer | np.floating = 2,
                  tau: int | float | np.integer | np.floating = 0,
                  kmeans_barbar: bool = False,
                  num_init_iterations: int | np.integer = 5,
                  oversampling_factor: int | np.integer | float |
                                       np.floating = 1,
                  chunk_size: int | np.integer = 256,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0).
        
        Harmony was originally written in R (github.com/immunogenomics/harmony)
        but has two Python ports, harmony-pytorch
        (github.com/lilab-bcb/harmony-pytorch), which our implementation is
        based on, and harmonypy (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm,
                          including in the initial k-means clustering. If not
                          specified, take the minimum of 100 and
                          floor(total number of cells / 30).
            num_kmeans_iterations: the number of iterations of k-means
                                   clustering to run, as the first step of
                                   Harmony. Defaults to 25, like the original
                                   Harmony R package, harmony-pytorch, and
                                   harmonypy. However, unlike these packages,
                                   only one initialization is tried rather than
                                   10 to reduce runtime.
            max_harmony_iterations: the maximum number of iterations to run
                                    Harmony for, if convergence is not
                                    achieved. Defaults to 10, like the original
                                    Harmony R package, harmony-pytorch, and
                                    harmonypy. Set to `None` to use as many
                                    iterations as necessary to achieve
                                    convergence.
            max_clustering_iterations: the maximum number of iterations to run
                                       the clustering step within each Harmony
                                       iteration for, if convergence is not
                                       achieved. Defaults to 20 iterations,
                                       like the original harmony R package and
                                       harmonypy; this differs from the default
                                       of 200 iterations used by
                                       harmony-pytorch. Set to `None` to use as
                                       many iterations as necessary to achieve
                                       convergence.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tol_harmony: the relative tolerance used to determine whether to
                         stop Harmony before `max_harmony_iterations`
                         iterations; must be positive
            tol_clustering: the relative tolerance used to determine whether to
                            stop clustering before `max_clustering_iterations`
                            iterations; must be positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            theta: the weight of the diversity penalty term in the Harmony
                   objective function; must be non-negative
            tau: the discounting factor on theta; must be non-negative. By
                 default, `tau = 0`, so there is no discounting.
            kmeans_barbar: whether to use k-means|| initialization (a parallel
                           version of k-means++ from arxiv.org/abs/1203.6402)
                           to initialize the k-means clustering centroids,
                           instead of random initialization
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402). Only used when
                                 `kmeans_barbar=True`.
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++. Only used when
                                 `kmeans_barbar=True`.
            chunk_size: the chunk size to use for distance calculations in the
                        initial k-means clustering. Setting this to a power of
                        2 is recommended; 256 was found to be the optimal chunk 
                        size by scikit-learn and in our tests.
            seed: the random seed to use for the initial k-means clustering
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
            num_threads: the number of threads to use when concatenating
                         principal components and batch/dataset labels across
                         datasets, for the initial k-means clustering, and for
                         the matrix and matrix-vector multiplications within
                         Harmony. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores. Does not affect
                         the returned SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `Harmony_key` is a string
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `Harmony_key` is not already in `obsm` for any dataset,
        # unless `overwrite=True`
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        
        # If `num_clusters` is not `None`, check that it is a positive integer
        # We will assign `num_clusters` its default value (if `None`) and check
        # its upper bound later, once we know the total number of cells across
        # all datasets.
        if num_clusters is not None:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            check_bounds(num_clusters, 'num_clusters', 1)
        
        # Check that `num_kmeans_iterations` is a positive integer
        check_type(num_kmeans_iterations, 'num_kmeans_iterations', int,
                   'a positive integer')
        check_bounds(num_kmeans_iterations, 'num_kmeans_iterations', 1)
            
        # Check that `max_harmony_iterations` and `max_clustering_iterations`
        # are `None` or a positive integer; if `None`, set to `INT32_MAX`
        for parameter, parameter_name in (
                (max_harmony_iterations, 'max_harmony_iterations'),
                (max_clustering_iterations, 'max_clustering_iterations')):
            if parameter is not None:
                check_type(parameter, parameter_name, int,
                           'a positive integer')
                check_bounds(parameter, parameter_name, 1)
        if max_harmony_iterations is None:
            max_harmony_iterations = 2_147_483_647
        if max_clustering_iterations is None:
            max_clustering_iterations = 2_147_483_647
            
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        
        # Check that `tol_harmony` and `tol_clustering` are positive numbers,
        # and that `ridge_lambda`, `sigma`, `theta`, and `tau` are non-negative
        # numbers. If any is an integer, cast it to a float.
        for parameter, parameter_name in (
                (tol_harmony, 'tol_harmony'),
                (tol_clustering, 'tol_clustering')):
            check_type(parameter, parameter_name, (int, float),
                       'a positive number')
            check_bounds(parameter, parameter_name, 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma'),
                (theta, 'theta'), (tau, 'tau')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tol_harmony = float(tol_harmony)
        tol_clustering = float(tol_clustering)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        theta = float(theta)
        tau = float(tau)
        
        # Check that `kmeans_barbar` is Boolean
        check_type(kmeans_barbar, 'kmeans_barbar', bool, 'Boolean')
        
        # Check that `num_init_iterations` is a positive integer
        check_type(num_init_iterations, 'num_init_iterations', int,
                   'a positive integer')
        check_bounds(num_init_iterations, 'num_init_iterations', 1)
        
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        
        # If `kmeans_barbar=False`, check that `num_init_iterations` and
        # `oversampling_factor` have their default values
        if not kmeans_barbar:
            if num_init_iterations != 5:
                error_message = (
                    'num_init_iterations can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            if oversampling_factor != 1:
                error_message = (
                    'oversampling_factor can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            
        # Check that `chunk_size` is a positive integer. We will check its
        # upper bound later, once we know the total number of cells across all
        # datasets.
        check_type(chunk_size, 'chunk_size', int, 'a positive integer')
        check_bounds(chunk_size, 'chunk_size', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
            
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Concatenate PCs (`Z`) across datasets; get labels indicating which
        # rows of these concatenated PCs come from each dataset or batch. Check
        # that the PCs are float32 and C-contiguous and all have the same
        # width.
        Z = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in Z:
            dtype = PCs.dtype
            if dtype != np.float32:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float32')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
                raise ValueError(error_message)
        width = Z[0].shape[1]
        for PCs in Z[1:]:
            if PCs.shape[1] != width:
                error_message = (
                    f"two datasets' PCs have different numbers of dimensions "
                    f"({width:,} vs {PCs.shape[1]:,}")
                raise ValueError(error_message)
        if QC_column is not None:
            Z = [PCs[QCed] if QCed is not None else PCs
                 for PCs, QCed in zip(Z, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, Z)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum(),
                                                dtype=np.float32))
                    batch_index += 1
            batch_labels = concatenate(batch_labels, num_threads=num_threads)
        Z = concatenate(Z, num_threads=num_threads)
        num_cells, num_PCs = Z.shape
        
        # Set `num_clusters` to its default value if `None`; otherwise, check
        # that it is less than the total number of cells across all datasets
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        elif num_clusters >= num_cells:
            error_message = (
                f'num_clusters is {num_clusters:,}, but must be less than the '
                f'total number of cells across all datasets ({num_cells:,})')
            raise ValueError(error_message)
        
        # Also check that `chunk_size` is less than the total number of cells
        # across all datasets
        if chunk_size >= num_cells:
            error_message = (
                f'chunk_size is {chunk_size:,}, but must be less than the '
                f'total number of cells across all datasets ({num_cells:,})')
            raise ValueError(error_message)
        
        # Define Cython functions
        cython_functions = cython_inline(r'''
            from cpython.exc cimport PyErr_CheckSignals
            from libcpp.cmath cimport abs, exp, pow, log, sqrt
            from scipy.linalg.cython_blas cimport sgemm, sgemv
            
            cdef extern from "<utility>" namespace "std" nogil:
                T swap[T](T &a, T &b)
            
            cdef inline void matrix_multiply(const float[:, ::1] A,
                                             const float[:, ::1] B,
                                             float[:, ::1] C,
                                             const bint transpose_A,
                                             const bint transpose_B,
                                             const float alpha,
                                             const float beta) noexcept nogil:
                # Flip `A` <-> `B` and `shape[0]` <-> `shape[1]` since our
                # matrices are C-major, whereas BLAS expects Fortran-major
                
                cdef int m, n, k, lda, ldb
                cdef char transA, transB
                if transpose_B:
                    m = B.shape[0]
                    k = B.shape[1]
                    lda = k
                    transA = b'T'
                else:
                    m = B.shape[1]
                    k = B.shape[0]
                    lda = m
                    transA = b'N'
                if transpose_A:
                    n = A.shape[1]
                    ldb = n
                    transB = b'T'
                else:
                    n = A.shape[0]
                    ldb = k
                    transB = b'N'
                sgemm(&transA, &transB, &m, &n, &k, <float*> &alpha,
                      <float*> &B[0, 0], &lda, <float*> &A[0, 0], &ldb,
                      <float*> &beta, &C[0, 0], &m)
            
            cdef inline void matrix_vector_multiply(
                    const float[:, ::1] A,
                    const float[::1] X,
                    float[::1] Y,
                    const bint transpose,
                    const float alpha,
                    const float beta) noexcept nogil:
                # Flip `trans` since our matrix is C-major, whereas BLAS
                # expects Fortran-major
                
                cdef int m = A.shape[1], n = A.shape[0], incx = 1, incy = 1
                cdef char trans = b'N' if transpose else b'T'
                sgemv(&trans, &m, &n, <float*> &alpha, <float*> &A[0,0],
                      &m, <float*> &X[0], &incx, <float*> &beta, &Y[0],
                      &incy)
            
            cdef inline unsigned rand(unsigned long* state) noexcept nogil:
                cdef unsigned long x = state[0]
                state[0] = x * 6364136223846793005UL + 1442695040888963407UL
                cdef unsigned s = (x ^ (x >> 18)) >> 27
                cdef unsigned rot = x >> 59
                return (s >> rot) | (s << ((-rot) & 31))
            
            cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
                cdef unsigned long state = seed + 1442695040888963407UL
                rand(&state)
                return state
            
            cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
                cdef unsigned r, threshold = -bound % bound
                while True:
                    r = rand(state)
                    if r >= threshold:
                        return r % bound
            
            cdef inline void shuffle_array(unsigned[::1] arr, unsigned long* state) noexcept nogil:
                cdef unsigned i, j, temp
                for i in range(arr.shape[0] - 1, 0, -1):
                    j = randint(i + 1, state)
                    swap(arr[i], arr[j])
            
            cdef inline float compute_objective(
                    float[:, ::1] Z_norm_times_Y_norm,
                    const float[:, ::1] R,
                    const float[:, ::1] E,
                    const float[:, ::1] O,
                    float[:, ::1] ratio,
                    const float[::1] theta,
                    float[::1] theta_times_ratio,
                    const float sigma,
                    const unsigned num_cells,
                    const unsigned num_clusters,
                    const unsigned num_batches):
                cdef unsigned i, j
                cdef float kmeans_error, entropy_term, diversity_penalty
                kmeans_error = entropy_term = diversity_penalty = 0
                for i in range(num_cells):
                    for j in range(num_clusters):
                        kmeans_error += \
                            R[i, j] * (1 - Z_norm_times_Y_norm[i, j])
                        entropy_term += R[i, j] * log(R[i, j])
                kmeans_error *= 2
                entropy_term *= sigma
                for i in range(num_batches):
                    for j in range(num_clusters):
                        ratio[i, j] = O[i, j] * log(
                            (O[i, j] + 1) / (E[i, j] + 1))
                matrix_vector_multiply(ratio, theta, theta_times_ratio,
                                       transpose=True, alpha=1, beta=0)
                for i in range(num_clusters):
                    diversity_penalty += theta_times_ratio[i]
                diversity_penalty *= sigma
                return kmeans_error + entropy_term + diversity_penalty
            
            def initialize(const float[:, ::1] Z_norm,
                           float[:, ::1] Y_norm,
                           float[:, ::1] Z_norm_times_Y_norm,
                           const unsigned[::1] N_b,
                           float[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           float[:, ::1] R,
                           float[::1] R_sum,
                           float[:, ::1] E,
                           float[:, ::1] O,
                           const float sigma,
                           float[:, ::1] ratio,
                           float[::1] theta,
                           float[::1] theta_times_ratio,
                           const float tau):
                cdef unsigned i, j, batch_label, num_cells = Z_norm.shape[0], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef float norm, objective, base, two_over_sigma = 2 / sigma
                
                # Initialize `Pr_b`
                for i in range(num_batches):
                    Pr_b[i] = <float> N_b[i] / num_cells
                
                # Initialize `R`, `R_sum`, and `O`
                matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                transpose_A=False, transpose_B=True, alpha=1,
                                beta=0)
                for i in range(num_cells):
                    batch_label = batch_labels[i]
                    norm = 0
                    for j in range(num_clusters):
                        R[i, j] = exp(two_over_sigma *
                                      (Z_norm_times_Y_norm[i, j] - 1))
                        norm += R[i, j]
                    norm = 1 / norm
                    for j in range(num_clusters):
                        R[i, j] *= norm
                        O[batch_label, j] += R[i, j]
                        R_sum[j] += R[i, j]
                
                # Initialize `E`
                for i in range(num_batches):
                    for j in range(num_clusters):
                        E[i, j] = Pr_b[i] * R_sum[j]
                
                # Apply discounting to `theta`, if specified
                if tau > 0:
                    for i in range(num_batches):
                        base = exp(-N_b[i] / (num_clusters * tau))
                        theta[i] = theta[i] * (1 - base * base)
                
                # Compute and return the initial value of the objective
                # function
                objective = compute_objective(
                    Z_norm_times_Y_norm, R, E, O, ratio, theta,
                    theta_times_ratio, sigma, num_cells, num_clusters,
                    num_batches)
                
                return objective
            
            def clustering(const float[:, ::1] Z_norm,
                           float[:, ::1] Z_norm_in,
                           float[:, ::1] Y_norm,
                           float[:, ::1] Z_norm_times_Y_norm,
                           const float[::1] Pr_b,
                           const unsigned[::1] batch_labels,
                           float[:, ::1] R,
                           float[:, ::1] R_in,
                           float[::1] R_in_sum,
                           float[:, ::1] E,
                           float[:, ::1] O,
                           float[:, ::1] ratio,
                           const float[::1] theta,
                           float[::1] theta_times_ratio,
                           unsigned[::1] idx_list,
                           const float tol,
                           const unsigned max_iter,
                           const float sigma,
                           const unsigned block_size,
                           const unsigned long seed):
                cdef unsigned i, j, k, iter, num_cells_in_block, pos, batch_label, \
                    num_cells = Z_norm.shape[0], num_PCs = Z_norm.shape[1], \
                    num_batches = E.shape[0], num_clusters = E.shape[1]
                cdef unsigned long state
                cdef float norm, old, new, objective, \
                    two_over_sigma = 2 / sigma
                cdef float exp_neg_two_over_sigma = exp(-two_over_sigma)
                cdef float[:, ::1] Z_norm_block, R_block
                cdef float past_clustering_objectives[3]
                
                state = srand(seed)
                for iter in range(max_iter):
                    # Compute cluster centroids
                    matrix_multiply(R, Z_norm, Y_norm, transpose_A=True,
                                    transpose_B=False, alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm = norm + Y_norm[i, j] * Y_norm[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y_norm[i, j] = Y_norm[i, j] * norm
                    
                    # Shuffle the order of the cells
                    for i in range(num_cells):
                        idx_list[i] = i
                    shuffle_array(idx_list, &state)
                    
                    # Update cells blockwise
                    pos = 0
                    Z_norm_block = Z_norm_in
                    R_block = R_in
                    while pos < num_cells:
                        if pos + block_size > num_cells:
                            num_cells_in_block = num_cells - pos
                            Z_norm_block = Z_norm_block[:num_cells_in_block]
                            R_block = R_block[:num_cells_in_block]
                        else:
                            num_cells_in_block = block_size
                        
                        # Remove the cells in this block from `E` and `O`
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[pos + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_in_sum[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_norm_block[i, j] = Z_norm[k, j]
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] -= Pr_b[i] * R_in_sum[j]
                        
                        # Recompute `R`, the fractional soft clustering
                        # assignments of each cell to each cluster, for the
                        # removed cells. Note that the original formula is:
                        # `exp(-2 / sigma * (1 - Z_norm_block @ Y_norm.T))`,
                        # which expands to `exp(-2 / sigma) *
                        # exp(2 / sigma * Z_norm_block @ Y_norm.T))`. Since
                        # `exp(-2 / sigma)` is a constant, we fold it into
                        # `ratio`.
                        matrix_multiply(Z_norm_block, Y_norm, R_block,
                                        transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    pow((E[i, j] + 1) / (O[i, j] + 1),
                                        theta[i])
                        R_in_sum[:] = 0
                        for i in range(num_cells_in_block):
                            k = idx_list[i + pos]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                R[k, j] = exp(R_block[i, j]) * \
                                          ratio[batch_label, j]
                                norm += R[k, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[k, j] *= norm
                                R_in_sum[j] += R[k, j]
                                
                                # Add the removed cells back into `O`
                                O[batch_label, j] += R[k, j]
                        
                        # Add the removed cells back into `E`
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                E[i, j] += Pr_b[i] * R_in_sum[j]
                        
                        # Move to the next block
                        pos += block_size
                    
                    # Compute the objective and decide whether we've converged
                    matrix_multiply(Z_norm, Y_norm, Z_norm_times_Y_norm,
                                    transpose_A=False, transpose_B=True,
                                    alpha=1, beta=0)
                    objective = compute_objective(
                        Z_norm_times_Y_norm, R, E, O, ratio, theta,
                        theta_times_ratio, sigma, num_cells, num_clusters,
                        num_batches)
                    if iter < 3:
                        past_clustering_objectives[iter] = objective
                    else:
                        old = past_clustering_objectives[0] + \
                            past_clustering_objectives[1] + \
                            past_clustering_objectives[2]
                        new = past_clustering_objectives[1] + \
                            past_clustering_objectives[2] + objective
                        if old - new < tol * abs(old):
                            break
                        else:
                            past_clustering_objectives[0] = \
                                past_clustering_objectives[1]
                            past_clustering_objectives[1] = \
                                past_clustering_objectives[2]
                            past_clustering_objectives[2] = objective
                    PyErr_CheckSignals()
                return objective
            
            def correction(const float[:, ::1] Z,
                           float[:, ::1] Z_hat,
                           const float[:, ::1] R,
                           const float[:, ::1] O,
                           const float ridge_lambda,
                           const unsigned[::1] batch_labels,
                           float[::1] factor,
                           float[:, ::1] P,
                           float[:, ::1] P_t_B_inv,
                           float[:, ::1] inv_mat,
                           float[:, ::1] Phi_t_diag_R_by_X,
                           float[:, ::1] W):
                cdef unsigned i, j, k, batch_label, num_cells = Z.shape[0], \
                    num_PCs = Z.shape[1], num_batches = O.shape[0], \
                    num_clusters = O.shape[1]
                cdef float c, c_inv
                
                Z_hat[:] = Z[:]
                
                # Initialize `P` to the identity matrix
                P[:] = 0
                for i in range(num_batches + 1):
                    P[i, i] = 1
            
                for k in range(num_clusters):
                    # Compute `factor`, `c_inv` and `P`
                    c = 0
                    for i in range(num_batches):
                        factor[i] = 1 / (O[i, k] + ridge_lambda)
                        c += O[i, k] * (1 - factor[i] * O[i, k])
                        P[0, i + 1] = -factor[i] * O[i, k]
                    c_inv = 1 / c
                    
                    # Compute `P_t_B_inv`
                    P_t_B_inv[:] = 0
                    P_t_B_inv[0, 0] = c_inv
                    for i in range(1, num_batches + 1):
                        P_t_B_inv[i, i] = factor[i - 1]
                        P_t_B_inv[i, 0] = P[0, i] * c_inv
                    
                    # Compute `inv_mat`
                    matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                    transpose_B=False, alpha=1, beta=0)
                    
                    # Compute `Phi_t_diag_R @ X`
                    Phi_t_diag_R_by_X[:] = 0
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Phi_t_diag_R_by_X[0, j] += Z[i, j] * R[i, k]
                            Phi_t_diag_R_by_X[batch_label + 1, j] += \
                                Z[i, j] * R[i, k]
                            
                    # Compute `W`
                    matrix_multiply(inv_mat, Phi_t_diag_R_by_X, W,
                                    transpose_A=False, transpose_B=False,
                                    alpha=1, beta=0)
                    
                    # Update `Z_hat`
                    for i in range(num_cells):
                        batch_label = batch_labels[i]
                        for j in range(num_PCs):
                            Z_hat[i, j] = Z_hat[i, j] - \
                                W[batch_label + 1, j] * R[i, k]
                        
            def normalize_rows(const float[:, ::1] arr, float[:, ::1] out):
                cdef unsigned i, j
                cdef float norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

            def normalize_rows_inplace(float[:, ::1] arr):
                cdef unsigned i, j
                cdef float norm
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        arr[i, j] = arr[i, j] * norm
        ''')
        initialize = cython_functions['initialize']
        clustering = cython_functions['clustering']
        correction = cython_functions['correction']
        normalize_rows = cython_functions['normalize_rows']
        normalize_rows_inplace = cython_functions['normalize_rows_inplace']
        
        # Get batch counts and the number of batches
        N_b = bincount(batch_labels, num_bins=batch_labels[-1] + 1,
                       num_threads=num_threads)
        num_batches = len(N_b)
        
        # Allocate arrays
        block_size = int(num_cells * block_proportion)
        Z_norm = np.empty((num_cells, num_PCs), dtype=np.float32)
        Z_norm_in = np.empty((block_size, num_PCs), dtype=np.float32)
        Z_norm_times_Y_norm = np.empty((num_cells, num_clusters),
                                       dtype=np.float32)
        Pr_b = np.empty(num_batches, dtype=np.float32)
        R = np.empty((num_cells, num_clusters), dtype=np.float32)
        R_in = np.empty((block_size, num_clusters), dtype=np.float32)
        R_in_sum = np.zeros(num_clusters, dtype=np.float32)
        E = np.empty((num_batches, num_clusters), dtype=np.float32)
        O = np.zeros((num_batches, num_clusters), dtype=np.float32)
        ratio = np.empty((num_batches, num_clusters), dtype=np.float32)
        theta = np.repeat(theta, num_batches).astype(np.float32)
        theta_times_ratio = np.empty(num_clusters, dtype=np.float32)
        idx_list = np.empty(num_cells, dtype=np.uint32)
        factor = np.empty(num_cells, dtype=np.float32)
        P = np.empty((num_batches + 1, num_batches + 1), dtype=np.float32)
        P_t_B_inv = np.empty((num_batches + 1, num_batches + 1),
                             dtype=np.float32)
        inv_mat = np.empty((num_batches + 1, num_batches + 1),
                           dtype=np.float32)
        Phi_t_diag_R_by_X = np.empty((num_batches + 1, num_PCs),
                                     dtype=np.float32)
        W = np.empty((num_batches + 1, num_PCs), dtype=np.float32)
        
        # Run k-means clustering on `Z_norm`. Since `Y_norm` and `Y_norm_new`
        # are swapped every iteration, `Y_norm_new` will contain the final
        # `Y_norm` when doing an odd number of k-means iterations; if so, swap
        # them at the end.
        normalize_rows(Z, Z_norm)
        Y_norm = np.empty((num_clusters, num_PCs), dtype=np.float32)
        Y_norm_new = np.empty((num_clusters, num_PCs), dtype=np.float32)
        with threadpool_limits(1, user_api='blas'):
            kmeans = _kmeans_and_knn_functions['kmeans']
            kmeans(X=Z_norm, 
                   cluster_labels=np.empty(num_cells, dtype=np.uint32),
                   centroids=Y_norm, centroids_new=Y_norm_new,
                   num_cells_per_cluster=np.empty(num_clusters, 
                                                  dtype=np.uint32),
                   kmeans_barbar=kmeans_barbar, 
                   num_init_iterations=num_init_iterations,
                   oversampling_factor=oversampling_factor,
                   num_kmeans_iterations=num_kmeans_iterations, seed=seed,
                   chunk_size=chunk_size, num_threads=num_threads)
        if num_kmeans_iterations & 1:
            Y_norm = Y_norm_new
        del Y_norm_new
        normalize_rows_inplace(Y_norm)
        
        # Unlike above, here we need to use single-threaded matrix
        # multiplication to ensure consistent floating-point roundoff
        with threadpool_limits(1, user_api='blas'):
            # Complete initialization in Cython
            objective = initialize(
                Z_norm=Z_norm, Y_norm=Y_norm,
                Z_norm_times_Y_norm=Z_norm_times_Y_norm, N_b=N_b, Pr_b=Pr_b,
                batch_labels=batch_labels, R=R, R_sum=R_in_sum, E=E, O=O,
                sigma=sigma, ratio=ratio, theta=theta,
                theta_times_ratio=theta_times_ratio, tau=tau)
            
            if verbose:
                print(f'Initialization is complete: objective = '
                      f'{objective:.2f}')
                iteration_string = plural('iteration', max_harmony_iterations)
            
            for i in range(1, max_harmony_iterations + 1):
                prev_objective = objective
                objective = clustering(
                    Z_norm=Z_norm, Z_norm_in=Z_norm_in, Y_norm=Y_norm,
                    Z_norm_times_Y_norm=Z_norm_times_Y_norm, Pr_b=Pr_b,
                    batch_labels=batch_labels, R=R, R_in=R_in,
                    R_in_sum=R_in_sum, E=E, O=O, ratio=ratio, theta=theta,
                    theta_times_ratio=theta_times_ratio, idx_list=idx_list,
                    tol=tol_clustering, max_iter=max_clustering_iterations,
                    sigma=sigma, block_size=block_size, seed=seed)
                correction(Z=Z, Z_hat=Z_norm, R=R, O=O,
                           ridge_lambda=ridge_lambda,
                           batch_labels=batch_labels, factor=factor, P=P,
                           P_t_B_inv=P_t_B_inv, inv_mat=inv_mat,
                           Phi_t_diag_R_by_X=Phi_t_diag_R_by_X, W=W)
                
                if verbose:
                    if max_harmony_iterations == 2_147_483_647:
                        print(f'Completed {i:,} {iteration_string}')
                    else:
                        print(f'Completed {i:,} of {max_harmony_iterations:,} '
                              f'iterations: objective = {objective:.2f}')
                
                if prev_objective - objective < \
                        tol_harmony * abs(prev_objective):
                    if verbose:
                        print(f'Reached convergence after {i:,} '
                              f'{iteration_string}')
                    break
                
                if i == max_harmony_iterations:
                    if verbose:
                        print(f'Failed to converge after {i:,} '
                              f'{iteration_string}')
                    break
                
                normalize_rows_inplace(Z_norm)
        
        del batch_labels, Z, Pr_b, theta, R, E, O, Z_norm_in, Y_norm, \
            Z_norm_times_Y_norm, R_in, R_in_sum, ratio, theta_times_ratio, \
            idx_list, factor, P, P_t_B_inv, inv_mat, Phi_t_diag_R_by_X, W
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z_norm[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan, dtype=np.float32)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    def harmonize_old(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  max_iterations: int | np.integer = 10,
                  num_kmeans_iterations: int | np.integer = 25,
                  num_clustering_iterations: int | np.integer = 5,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tolerance: int | float | np.integer | np.floating = 1e-4,
                  ridge_lambda: int | float | np.integer | np.floating = 1,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  kmeans_barbar: bool = False,
                  num_init_iterations: int | np.integer = 5,
                  oversampling_factor: int | np.integer | float |
                                       np.floating = 1,
                  chunk_size_kmeans: int | np.integer | None = None,
                  chunk_size_Harmony: int | np.integer | None = None,
                  seed: int | np.integer = 0,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0).
        
        Harmony was originally written in R (github.com/immunogenomics/harmony)
        but has two Python ports, harmony-pytorch
        (github.com/lilab-bcb/harmony-pytorch), which our implementation is
        loosely based on, and harmonypy (github.com/slowkow/harmonypy).
        
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm,
                          including in the initial k-means clustering. If not
                          specified, take the minimum of 100 and
                          floor(total number of cells / 30).
            max_iterations: the maximum number of iterations to run Harmony
                            for, if convergence is not achieved. Defaults to
                            10, like the original Harmony R package,
                            harmony-pytorch, and harmonypy. Set to `None` to
                            use as many iterations as necessary to achieve
                            convergence.
            num_kmeans_iterations: the number of iterations of k-means
                                   clustering to run, as the first step of
                                   Harmony. Defaults to 25, like the original
                                   Harmony R package, harmony-pytorch, and
                                   harmonypy. However, unlike these packages,
                                   only one initialization is tried rather than
                                   10 to reduce runtime.
            num_clustering_iterations: the number of iterations to run the
                                       clustering step within each Harmony
                                       iteration for. Unlike the original
                                       Harmony algorithm, convergence of the
                                       clustering is not checked. Defaults to 5
                                       iterations; this differs from the 20
                                       used by the original harmony R package
                                       and harmonypy, and the 200 iterations
                                       used by harmony-pytorch (which do have
                                       convergence checks).
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tolerance: the relative tolerance used to determine whether to stop
                       Harmony before `max_iterations` iterations; must be
                       positive
            ridge_lambda: the ridge regression penalty used in the Harmony
                          correction step; must be non-negative
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            kmeans_barbar: whether to use k-means|| initialization (a parallel
                           version of k-means++ from arxiv.org/abs/1203.6402)
                           to initialize the k-means clustering centroids,
                           instead of random initialization
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402). Only used when
                                 `kmeans_barbar=True`.
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++. Only used when
                                 `kmeans_barbar=True`.
            chunk_size_kmeans: the chunk size to use for distance calculations
                               in the initial k-means clustering. Setting this
                               to a power of 2 is recommended; 256 was found to
                               be the optimal chunk size by scikit-learn and in
                               our tests. Defaults to `min(256, num_cells)`.
            chunk_size_Harmony: the chunk size to use for Harmony. Setting this
                                to a power of 2 is recommended; 512 was found
                                to be the optimal chunk size in our tests.
                                Defaults to `min(512, num_cells)`.
            seed: the random seed to use for the initial k-means clustering
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
            num_threads: the number of threads to use when concatenating
                         principal components and batch/dataset labels across
                         datasets, for the initial k-means clustering, and for
                         the matrix and matrix-vector multiplications within
                         Harmony. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores. Does not affect
                         the returned SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `Harmony_key` is a string
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `Harmony_key` is not already in `obsm` for any dataset,
        # unless `overwrite=True`
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        
        # If `num_clusters` is not `None`, check that it is a positive integer
        # We will assign `num_clusters` its default value (if `None`) and check
        # its upper bound later, once we know the total number of cells across
        # all datasets.
        if num_clusters is not None:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            check_bounds(num_clusters, 'num_clusters', 1)
        
        # Check that `max_iterations` is `None` or a positive integer; if
        # `None`, set to `INT32_MAX`
        if max_iterations is None:
            max_iterations = 2_147_483_647
        else:
            check_type(max_iterations, 'max_iterations', int,
                       'a positive integer')
            check_bounds(max_iterations, 'max_iterations', 1)
        
        # Check that `num_kmeans_iterations` is a positive integer
        check_type(num_kmeans_iterations, 'num_kmeans_iterations', int,
                   'a positive integer')
        check_bounds(num_kmeans_iterations, 'num_kmeans_iterations', 1)
        
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        
        # Check that `tolerance` is a positive number, and that `ridge_lambda`
        # and `sigma `are non-negative numbers. If any is an integer, cast it
        # to a float.
        check_type(tolerance, 'tolerance', (int, float),
                   'a positive number')
        check_bounds(tolerance, 'tolerance', 0, left_open=True)
        for parameter, parameter_name in (
                (ridge_lambda, 'ridge_lambda'), (sigma, 'sigma')):
            check_type(parameter, parameter_name, (int, float),
                       'a non-negative number')
            check_bounds(parameter, parameter_name, 0)
        tolerance = float(tolerance)
        ridge_lambda = float(ridge_lambda)
        sigma = float(sigma)
        
        # Check that `kmeans_barbar` is Boolean
        check_type(kmeans_barbar, 'kmeans_barbar', bool, 'Boolean')
        
        # Check that `num_init_iterations` is a positive integer
        check_type(num_init_iterations, 'num_init_iterations', int,
                   'a positive integer')
        check_bounds(num_init_iterations, 'num_init_iterations', 1)
        
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        
        # If `kmeans_barbar=False`, check that `num_init_iterations` and
        # `oversampling_factor` have their default values
        if not kmeans_barbar:
            if num_init_iterations != 5:
                error_message = (
                    'num_init_iterations can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            if oversampling_factor != 1:
                error_message = (
                    'oversampling_factor can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            
        # If `chunk_size_kmeans` and/or `chunk_size_Harmony` are not `None`,
        # check that they are positive integers. We will check their upper
        # bounds and set their default values (if `None`) later, once we know
        # the total number of cells across all datasets.
        for chunk_size, chunk_size_name in \
                (chunk_size_kmeans, 'chunk_size_kmeans'), \
                (chunk_size_Harmony, 'chunk_size_Harmony'):
            if chunk_size is not None:
                check_type(chunk_size, chunk_size_name, int,
                           'a positive integer')
                check_bounds(chunk_size, chunk_size_name, 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Concatenate PCs across datasets; get labels indicating which rows of
        # these concatenated PCs come from each dataset or batch. Check that
        # the PCs are float32 and C-contiguous and all have the same width.
        all_PCs = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in all_PCs:
            dtype = PCs.dtype
            if dtype != np.float32:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float32')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
                raise ValueError(error_message)
        width = all_PCs[0].shape[1]
        for PCs in all_PCs[1:]:
            if PCs.shape[1] != width:
                error_message = (
                    f"two datasets' PCs have different numbers of dimensions "
                    f"({width:,} vs {PCs.shape[1]:,}")
                raise ValueError(error_message)
        if QC_column is not None:
            all_PCs = [PCs[QCed] if QCed is not None else PCs
                       for PCs, QCed in zip(all_PCs, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, all_PCs)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum(),
                                                dtype=np.float32))
                    batch_index += 1
            batch_labels = concatenate(batch_labels, num_threads=num_threads)
        PCs = concatenate(all_PCs, num_threads=num_threads)
        num_cells, num_PCs = PCs.shape
        
        # If `num_clusters` is `None`, set it to
        # `min(100, int(num_cells / 30))`. Otherwise, check that it is less
        # than the total number of cells across all datasets.
        if num_clusters is None:
            num_clusters = min(100, int(num_cells / 30))
        elif num_clusters >= num_cells:
            error_message = (
                f'num_clusters is {num_clusters:,}, but must be less than the '
                f'total number of cells across all datasets ({num_cells:,})')
            raise ValueError(error_message)
        
        # If `chunk_size_kmeans` is `None`, set it to `min(256, num_cells)`.
        # Otherwise, check that it is less than the total number of cells
        # across all datasets.
        if chunk_size_kmeans is None:
            chunk_size_kmeans = min(256, num_cells)
        elif chunk_size_kmeans >= num_cells:
            error_message = (
                f'chunk_size_kmeans is {chunk_size_kmeans:,}, but must be '
                f'less than the total number of cells across all datasets '
                f'({num_cells:,})')
            raise ValueError(error_message)
        
        # If `chunk_size_Harmony` is `None`, set it to `min(512, num_cells)`.
        # Otherwise, check that it is less than the total number of cells
        # across all datasets.
        if chunk_size_Harmony is None:
            chunk_size_Harmony = min(512, num_cells)
        elif chunk_size_Harmony >= num_cells:
            error_message = (
                f'chunk_size_Harmony is {chunk_size_Harmony:,}, but must be '
                f'less than the total number of cells across all datasets '
                f'({num_cells:,})')
            raise ValueError(error_message)
        
        # Define Cython functions
        cython_functions = cython_inline(_uninitialized_vector_import + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.cmath cimport abs, ceil, exp, pow, log, sqrt
        from libcpp.vector cimport vector
        from scipy.linalg.cython_blas cimport sgemm
        
        ctypedef fused integer:
            unsigned
            int
            long
        
        cdef extern from "<utility>" namespace "std" nogil:
            T swap[T](T &a, T &b)
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef inline void bincount(const integer[::1] arr,
                                  unsigned[::1] counts,
                                  const unsigned num_threads) noexcept nogil:
            cdef unsigned long start, end, i, num_bins, chunk_size, \
                num_elements = arr.shape[0]
            cdef unsigned thread_index
            cdef vector[vector[unsigned]] thread_counts
            
            if num_threads == 1:
                counts[:] = 0
                for i in range(num_elements):
                    counts[arr[i]] += 1
            else:
                # Store counts for each thread in a temporary buffer, then
                # aggregate at the end. As an optimization, put the counts for
                # the last thread (`thread_index == num_threads - 1`) directly
                # into the final `counts` array.
                thread_counts.resize(num_threads - 1)
                num_bins = counts.shape[0]
                chunk_size = (num_elements + num_threads - 1) / num_threads
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    start = thread_index * chunk_size
                    if thread_index == num_threads - 1:
                        end = num_elements
                        counts[:] = 0
                        for i in range(start, end):
                            counts[arr[i]] += 1
                    else:
                        thread_counts[thread_index].resize(num_bins)
                        end = start + chunk_size
                        for i in range(start, end):
                            thread_counts[thread_index][arr[i]] += 1
                
                # Aggregate counts from all threads except the last
                for thread_index in range(num_threads - 1):
                    for i in range(num_bins):
                        counts[i] += thread_counts[thread_index][i]
        
        cdef inline void matrix_multiply(const float[:, ::1] A,
                                         const float[:, ::1] B,
                                         float[:, ::1] C,
                                         const bint transpose_A,
                                         const bint transpose_B,
                                         const float alpha,
                                         const float beta) noexcept nogil:
            # A wrapper for `sgemm()` for the case when all matrices are
            # C-major. Flip `A` <-> `B`, `shape[0]` <-> `shape[1]`, and both
            # transpose flags, since BLAS expects Fortran-major.
            
            cdef int m, n, k, lda, ldb
            cdef char transA, transB
            if transpose_B:
                m = B.shape[0]
                k = B.shape[1]
                lda = k
                transA = b'T'
            else:
                m = B.shape[1]
                k = B.shape[0]
                lda = m
                transA = b'N'
            if transpose_A:
                n = A.shape[1]
                ldb = n
                transB = b'T'
            else:
                n = A.shape[0]
                ldb = k
                transB = b'N'
            sgemm(&transA, &transB, &m, &n, &k, <float*> &alpha,
                  <float*> &B[0, 0], &lda, <float*> &A[0, 0], &ldb,
                  <float*> &beta, &C[0, 0], &m)
        
        cdef inline void normalize_rows_inplace(float[:, ::1] arr,
                                                 const unsigned num_threads) \
                noexcept nogil:
            cdef unsigned i, j
            cdef float norm
            for i in prange(arr.shape[0], num_threads=num_threads):
                norm = 0
                for j in range(arr.shape[1]):
                    norm = norm + arr[i, j] * arr[i, j]
                norm = 1 / sqrt(norm)
                for j in range(arr.shape[1]):
                    arr[i, j] = arr[i, j] * norm
        
        def normalize_rows(const float[:, ::1] arr,
                           float[:, ::1] out,
                           const unsigned num_threads):
            cdef unsigned i, j
            cdef float norm
            for i in prange(arr.shape[0], nogil=True, num_threads=num_threads):
                norm = 0
                for j in range(arr.shape[1]):
                    norm = norm + arr[i, j] * arr[i, j]
                norm = 1 / sqrt(norm)
                for j in range(arr.shape[1]):
                    out[i, j] = arr[i, j] * norm
        
        def harmony(const float[:, ::1] PCs,
                    float[:, ::1] Z,
                    float[:, ::1] Y,
                    const unsigned[::1] batch_labels,
                    const unsigned max_iterations,
                    const unsigned num_clustering_iterations,
                    const float block_proportion,
                    const float tolerance,
                    const float ridge_lambda,
                    const float sigma,
                    const unsigned chunk_size,
                    const unsigned long seed,
                    const bint verbose,
                    const unsigned num_threads):
            
            # The major data structures in the Harmony algorithm are:
            # - `Z` (cells × PCs): the row-normalized principal components
            # - `Y` (clusters × PCs): the centroid of each cluster
            # - `R` (cells × clusters): the soft assignment of each cell to each
            #   cluster, which has values between 0 and 1 and sums to 1 for each cell
            # - `O` (batches × clusters): the sum of the `R`s across all cells from
            #    each batch that are part of each cluster
            # - `E` (batches × clusters): the expected value of `O` if batches were
            #   randomly distributed across clusters. For a given batch `i` and cluster
            #   `j`, this is just `Pr_b[i]` (the fraction of cells in batch `i`) times
            #   `sum(R[:, j])` (the sum of `R` for that cluster across all batches).
            #    We define `R_sum = sum(R[:, j])`.
            # The Harmony paper also refers to ϕ, the one-hot encoded batch labels. We
            # avoid one-hot encoding for efficiency and just use the raw batch labels.
            #
            # We process cells blockwise and then chunkwise within each block. Why both
            # blocks and chunks? Because `O` and `E` need to be updated by block, so
            # parallelization has to occur within each block.
            #
            # The number of chunks may be slightly less for the last block, so
            # per-chunk arrays are allocated to have
            # `max_chunks_per_block = ceil(block_size / chunk_size)` chunks, of which
            # only the first `num_chunks_per_block` are used for each block.
            #
            # Allocate per-thread storage for `Z_chunk` and `distance = Z_chunk @ Y.T`.
            # Allocate a single buffer for all threads without worrying about false
            # sharing, since the default `chunk_size` (256) is a multiple of the cache
            # line size.
            
            cdef unsigned i, j, k, thread_index, chunk_index, chunk_start, chunk_end, \
                iteration, clustering_iteration, block_index, block_start, block_end, \
                num_chunks_per_block, batch_label, start_row, end_row, \
                num_cells = Z.shape[0], num_PCs = Z.shape[1], \
                num_clusters = Y.shape[0], \
                num_batches = batch_labels[batch_labels.shape[0] - 1] + 1, \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size, \
                block_size = <unsigned> ceil(num_cells * block_proportion), \
                num_blocks = (num_cells + block_size - 1) / block_size, \
                max_chunks_per_block = (block_size + chunk_size - 1) / chunk_size, \
                num_cells_per_thread = (num_cells + num_threads - 1) / num_threads
            cdef unsigned long state = srand(seed)
            cdef float base, kmeans_error, entropy_term, norm, R_sum, O_sum, \
                diversity_penalty, prev_objective, total, delta_Eij, delta_Oij, Eij, \
                Oij, objective, c, c_inv, batch_total, two_over_sigma = 2 / sigma, \
                exp_neg_two_over_sigma = exp(-two_over_sigma)
            cdef str iteration_string, metrics
            cdef uninitialized_vector[unsigned] N_b_buffer, cell_order_buffer
            cdef uninitialized_vector[float] Pr_b_buffer, factor_buffer, \
                R_buffer, E_buffer, O_buffer, R_sums_buffer, P_buffer, \
                P_t_B_inv_buffer, inv_mat_buffer, Phi_t_diag_R_X_t_buffer, W_buffer, \
                Y_chunk_buffer, Z_chunk_buffer, distance_buffer, delta_O_buffer, \
                delta_E_buffer, ratio_buffer, kmeans_errors, entropy_terms, \
                Phi_t_diag_R_X_t_chunk_buffer
            N_b_buffer.resize(num_batches)
            cell_order_buffer.resize(num_cells)
            Pr_b_buffer.resize(num_batches)
            factor_buffer.resize(num_cells)
            R_buffer.resize(num_cells * num_clusters)
            E_buffer.resize(num_batches * num_clusters)
            O_buffer.resize(num_batches * num_clusters)
            R_sums_buffer.resize(num_chunks * num_clusters)
            P_buffer.resize((num_batches + 1) * (num_batches + 1))
            P_t_B_inv_buffer.resize((num_batches + 1) * (num_batches + 1))
            inv_mat_buffer.resize((num_batches + 1) * (num_batches + 1))
            Phi_t_diag_R_X_t_buffer.resize(num_PCs * (num_batches + 1))
            W_buffer.resize(num_PCs * (num_batches + 1))
            Y_chunk_buffer.resize(num_chunks * num_clusters * num_PCs)
            Z_chunk_buffer.resize(num_threads * chunk_size * num_PCs)
            distance_buffer.resize(num_threads * chunk_size * num_clusters)
            delta_O_buffer.resize(num_chunks * num_batches * num_clusters)
            delta_E_buffer.resize(num_chunks * num_batches * num_clusters)
            ratio_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            kmeans_errors.resize(num_chunks)
            entropy_terms.resize(num_chunks)
            Phi_t_diag_R_X_t_chunk_buffer.resize(
                num_chunks * num_PCs * (num_batches + 1))
            cdef unsigned[::1] \
                N_b = <unsigned[:num_batches]> N_b_buffer.data(), \
                cell_order = <unsigned[:num_cells]> cell_order_buffer.data()
            cdef float[::1] \
                Pr_b = <float[:num_batches]> Pr_b_buffer.data(), \
                factor = <float[:num_cells]> factor_buffer.data(), Rk = factor
            cdef float[:, ::1] R = <float[:num_cells, :num_clusters]> R_buffer.data(), \
                E = <float[:num_batches, :num_clusters]> E_buffer.data(), \
                O = <float[:num_batches, :num_clusters]> O_buffer.data(), \
                R_sums = <float[:num_chunks, :num_clusters]> R_sums_buffer.data(), \
                P = <float[:num_batches + 1, :num_batches + 1]> P_buffer.data(), \
                P_t_B_inv = <float[:num_batches + 1, :num_batches + 1]> \
                    P_t_B_inv_buffer.data(), \
                inv_mat = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_mat_buffer.data(), \
                Phi_t_diag_R_X_t = <float[:num_PCs, :num_batches + 1]> \
                    Phi_t_diag_R_X_t_buffer.data(), \
                W = <float[:num_batches + 1, :num_PCs]> W_buffer.data()
            cdef float[:, :, ::1] \
                Y_chunk = \
                    <float[:num_chunks, :num_clusters, :num_PCs]> \
                    Y_chunk_buffer.data(), \
                Z_chunk = <float[:num_threads, :chunk_size, :num_PCs]> \
                    Z_chunk_buffer.data(), \
                distances = <float[:num_threads, :chunk_size, :num_clusters]> \
                    distance_buffer.data(), \
                delta_O = <float[:num_chunks, :num_batches, :num_clusters]> \
                    delta_O_buffer.data(), \
                delta_E = <float[:num_chunks, :num_batches, :num_clusters]> \
                    delta_E_buffer.data(), \
                ratio = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                    ratio_buffer.data(), \
                Phi_t_diag_R_X_t_chunk = \
                    <float[:num_chunks, :num_PCs, :num_batches + 1]> \
                    Phi_t_diag_R_X_t_chunk_buffer.data()
            
            with nogil:
                # Get the number (`N_b`) and fraction (`Pr_b`) of cells in each batch
                bincount(batch_labels, N_b, num_threads)
                for i in range(num_batches):
                    Pr_b[i] = <float> N_b[i] / num_cells
                N_b_buffer.clear()
                
                # Initialize `R`, `R_sum`, and `O` chunkwise
                with parallel(num_threads=min(num_threads, num_chunks)):
                    thread_index = threadid()
                    for chunk_index in prange(num_chunks):
                        chunk_start = chunk_index * chunk_size
                        chunk_end = min(chunk_start + chunk_size, num_cells)
                        matrix_multiply(Z[chunk_start:chunk_end], Y, distances[thread_index],
                                        transpose_A=False, transpose_B=True, alpha=1,
                                        beta=0)
                        kmeans_error = 0
                        entropy_term = 0
                        R_sums[chunk_index, :] = 0
                        delta_O[chunk_index, :, :] = 0
                        for i in range(chunk_start, chunk_end):
                            norm = 0
                            for j in range(num_clusters):
                                R[i, j] = exp(two_over_sigma *
                                              (distances[thread_index, i - chunk_start, j] - 1))
                                norm += R[i, j]
                            norm = 1 / norm
                            for j in range(num_clusters):
                                R[i, j] *= norm
                                R_sums[chunk_index, j] += R[i, j]
                                delta_O[chunk_index, batch_labels[i], j] += R[i, j]
                                kmeans_error = kmeans_error + \
                                    R[i, j] * (1 - distances[thread_index, i - chunk_start, j])
                                entropy_term = entropy_term + R[i, j] * log(R[i, j])
                        kmeans_errors[chunk_index] = kmeans_error
                        entropy_terms[chunk_index] = entropy_term
                
                # Initialize `E`
                for i in range(num_batches):
                    for j in range(num_clusters):
                        R_sum = 0
                        for chunk_index in range(num_chunks):
                            R_sum += R_sums[chunk_index, j]
                        E[i, j] = Pr_b[i] * R_sum
                
                # Compute the initial k-means error and entropy term, the first two
                # components of the objective function
                kmeans_error = 0
                entropy_term = 0
                for chunk_index in range(num_chunks):
                    kmeans_error += kmeans_errors[chunk_index]
                    entropy_term += entropy_terms[chunk_index]
                kmeans_error *= 2
                entropy_term *= sigma
                
                # Initialize `O` and compute the initial diversity penalty, the third
                # component of the objective function
                diversity_penalty = 0
                for i in range(num_batches):
                    for j in range(num_clusters):
                        O_sum = 0
                        for chunk_index in range(num_chunks):
                            O_sum += delta_O[chunk_index, i, j]
                        O[i, j] = O_sum
                        diversity_penalty += \
                            O_sum * log((O_sum + 1) / (E[i, j] + 1))
                diversity_penalty *= 2 * sigma
                
                # Compute the initial total objective function
                prev_objective = kmeans_error + entropy_term + diversity_penalty
                
                # Normalize each row of `Y` in-place
                normalize_rows_inplace(Y, num_threads)
                
            # Define the random order to iterate over cells in, via the "inside-out"
            # variant of the Fisher-Yates shuffle
            for i in range(num_cells):
                j = randint(i + 1, &state)
                cell_order[i] = cell_order[j]
                cell_order[j] = i
                
            if verbose:
                print(f'Initialization is complete: objective = '
                      f'{prev_objective:.2f}')
                iteration_string = 'iteration' if max_iterations == 1 else 'iterations'
            
            # Check for KeyboardInterrupts
            PyErr_CheckSignals()
            
            # Shrink `R_sums`, `delta_O`, and `delta_E` from `num_chunks` to
            # `max_chunks_per_block` along their first dimension
            R_sums_buffer.resize(max_chunks_per_block * num_clusters)
            delta_O_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            delta_E_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            R_sums = <float[:max_chunks_per_block, :num_clusters]> R_sums_buffer.data()
            delta_O = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                delta_O_buffer.data()
            delta_E = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                delta_E_buffer.data()
            
            # Now that initialization is done, start the Harmony iterations
            with nogil:
                for iteration in range(max_iterations):
                    # Perform `num_clustering_iterations` iterations of clustering
                    # within each Harmony iteration
                    for clustering_iteration in range(num_clustering_iterations):
                        # Compute `Y`, the normalized cluster centroids, chunkwise
                        for chunk_index in prange(num_chunks, num_threads=num_threads):
                            chunk_start = chunk_index * chunk_size
                            chunk_end = min(chunk_start + chunk_size, num_cells)
                            matrix_multiply(R[chunk_start:chunk_end],
                                            Z[chunk_start:chunk_end], Y_chunk[chunk_index],
                                            transpose_A=True, transpose_B=False, alpha=1,
                                            beta=0)
                        for i in prange(num_clusters, num_threads=num_threads):
                            norm = 0
                            for j in range(num_PCs):
                                total = 0
                                for chunk_index in range(num_chunks):
                                    total = total + Y_chunk[chunk_index, i, j]
                                Y[i, j] = total
                                norm = norm + total * total
                            norm = 1 / sqrt(norm)
                            for j in range(num_PCs):
                                Y[i, j] *= norm

                        # Process cells blockwise and then chunkwise within each block
                        for block_index in range(num_blocks):
                            block_start = block_index * block_size
                            block_end = min(block_start + block_size, num_cells)
                            num_chunks_per_block = \
                                (block_end - block_start + chunk_size - 1) / chunk_size
                            with parallel(num_threads=min(num_threads, num_chunks_per_block)):
                                thread_index = threadid()
                                for chunk_index in prange(num_chunks_per_block):
                                    chunk_start = chunk_index * chunk_size
                                    chunk_end = min(chunk_start + chunk_size,
                                                    block_end - block_start)
                                    
                                    # Get the chunk of `Z`, the normalized PCs, to process.
                                    # Calculate the observed-to-expected ratio
                                    # `((E + 1) / (O + 1)) ** 2` for this chunk, subtracting
                                    # the chunk's own contributions to `E` and `O` (stored in
                                    # `delta_O[chunk_index]` and `delta_E[chunk_index]`).
                                    #
                                    # Note: the formula for `R` has the term:
                                    # `exp(-2 / sigma * (1 - Z @ Y.T))`
                                    # which expands to:
                                    # `exp(-2 / sigma) * exp(2 / sigma * Z @ Y.T))`
                                    # Since `exp(-2 / sigma)` is a constant, we fold it
                                    # into `ratio`.
                                    R_sums[chunk_index, :] = 0
                                    delta_O[chunk_index, :, :] = 0
                                    for i in range(chunk_end - chunk_start):
                                        k = cell_order[block_start + chunk_start + i]
                                        batch_label = batch_labels[k]
                                        Z_chunk[thread_index, i, :] = Z[k, :]
                                        for j in range(num_clusters):
                                            R_sums[chunk_index, j] += R[k, j]
                                            delta_O[chunk_index, batch_label, j] -= R[k, j]
                                    for i in range(num_batches):
                                        for j in range(num_clusters):
                                            delta_Eij = -Pr_b[i] * R_sums[chunk_index, j]
                                            delta_Oij = delta_O[chunk_index, i, j]
                                            Eij = E[i, j] + delta_Eij
                                            Oij = O[i, j] + delta_Oij
                                            ratio[chunk_index, i, j] = \
                                                exp_neg_two_over_sigma * \
                                                (Eij + 1) / (Oij + 1) * \
                                                (Eij + 1) / (Oij + 1)
                                            delta_E[chunk_index, i, j] = delta_Eij
                                    
                                    # Update `R`, the fractional soft clustering
                                    # assignment of each cell to each cluster, for the
                                    # cells in the chunk. Normalize each row of `R` to
                                    # ensure it always sums to 1 despite floating-point
                                    # error. Also, add back this chunk's contributions
                                    # to `O` and `E` to `delta_O[chunk_index]` and
                                    # `delta_E[chunk_index]` using the newly-updated
                                    # `R`. Note: we fold `two_over_sigma` into `distances`
                                    # during the matrix multiplication to avoid having to
                                    # multiply by it after the fact.
                                    matrix_multiply(Z_chunk[thread_index], Y,
                                                    distances[thread_index],
                                                    transpose_A=False, transpose_B=True,
                                                    alpha=two_over_sigma, beta=0)
                                    R_sums[chunk_index, :] = 0
                                    for i in range(chunk_end - chunk_start):
                                        k = cell_order[block_start + chunk_start + i]
                                        batch_label = batch_labels[k]
                                        norm = 0
                                        for j in range(num_clusters):
                                            R[k, j] = exp(distances[
                                                thread_index, i, j]) * \
                                                ratio[chunk_index, batch_label, j]
                                            norm += R[k, j]
                                        norm = 1 / norm
                                        for j in range(num_clusters):
                                            R[k, j] *= norm
                                            R_sums[chunk_index, j] += R[k, j]
                                            delta_O[chunk_index, batch_label, j] += R[k, j]
                                    for i in range(num_batches):
                                        for j in range(num_clusters):
                                            delta_E[chunk_index, i, j] += \
                                                Pr_b[i] * R_sums[chunk_index, j]
                            
                            # Update `O` and `E` for this block with the `delta_E` and
                            # `delta_O` for each chunk.
                            for chunk_index in range(num_chunks_per_block):
                                for i in range(num_batches):
                                    for j in range(num_clusters):
                                        E[i, j] += delta_E[chunk_index, i, j]
                                        O[i, j] += delta_O[chunk_index, i, j]
                
                    # Check for KeyboardInterrupts after each clustering iteration
                    with gil:
                        PyErr_CheckSignals()
                
                    # Compute the objective function. Compute its first two components
                    # chunkwise: the k-means error and entropy term.
                    with parallel(num_threads=min(num_threads, num_chunks)):
                        thread_index = threadid()
                        for chunk_index in prange(num_chunks):
                            chunk_start = chunk_index * chunk_size
                            chunk_end = min(chunk_start + chunk_size, num_cells)
                            matrix_multiply(Z[chunk_start:chunk_end], Y,
                                            distances[thread_index], transpose_A=False,
                                            transpose_B=True, alpha=1, beta=0)
                            kmeans_error = 0
                            entropy_term = 0
                            for i in range(chunk_start, chunk_end):
                                for j in range(num_clusters):
                                    kmeans_error = kmeans_error + \
                                        R[i, j] * (1 - distances[
                                            thread_index, i - chunk_start, j])
                                    entropy_term = entropy_term + R[i, j] * log(R[i, j])
                            kmeans_errors[chunk_index] = kmeans_error
                            entropy_terms[chunk_index] = entropy_term
                    kmeans_error = 0
                    entropy_term = 0
                    for chunk_index in range(num_chunks):
                        kmeans_error += kmeans_errors[chunk_index]
                        entropy_term += entropy_terms[chunk_index]
                    kmeans_error *= 2
                    entropy_term *= sigma
    
                    # Compute the diversity penalty, the third component of the
                    # objective function
                    diversity_penalty = 0
                    for i in range(num_batches):
                        for j in range(num_clusters):
                            diversity_penalty += \
                                O[i, j] * log((O[i, j] + 1) / (E[i, j] + 1))
                    diversity_penalty *= 2 * sigma
                    
                    # Compute the total objective function
                    objective = kmeans_error + entropy_term + diversity_penalty
                    
                    # Apply the Harmony correction to the PCs to get the new
                    # Harmony embeddings, `Z`
                    
                    # Initialize `Z` to `PCs`
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        start_row = thread_index * num_cells_per_thread
                        end_row = min(start_row + num_cells_per_thread, num_cells)
                        Z[start_row:end_row] = PCs[start_row:end_row]
                    
                    # Initialize `P` to the identity matrix
                    P[:] = 0
                    for i in range(num_batches + 1):
                        P[i, i] = 1
                
                    for k in range(num_clusters):
                        # Compute `factor`, `c_inv` and `P`
                        c = 0
                        for i in range(num_batches):
                            factor[i] = 1 / (O[i, k] + ridge_lambda)
                            c += O[i, k] * (1 - factor[i] * O[i, k])
                            P[num_batches, i] = -factor[i] * O[i, k]
                        c_inv = 1 / c
                        
                        # Compute `P_t_B_inv`
                        P_t_B_inv[:] = 0
                        P_t_B_inv[num_batches, num_batches] = c_inv
                        for i in range(num_batches):
                            P_t_B_inv[i, i] = factor[i]
                            P_t_B_inv[i, num_batches] = P[num_batches, i] * c_inv
                        
                        # Cache `R[:, k]` to avoid strided access
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            start_row = thread_index * num_cells_per_thread
                            end_row = min(start_row + num_cells_per_thread, num_cells)
                            for i in range(start_row, end_row):
                                Rk[i] = R[i, k]
                        
                        # Compute `Phi_t_diag_R_X_t = (Phi_t_diag_R @ X).T` in chunks
                        for chunk_index in prange(num_chunks, num_threads=num_threads):
                            start_row = chunk_index * chunk_size
                            end_row = min(start_row + chunk_size, num_cells)
                            Phi_t_diag_R_X_t_chunk[chunk_index, :, :] = 0
                            for i in range(start_row, end_row):
                                batch_label = batch_labels[i]
                                for j in range(num_PCs):
                                    Phi_t_diag_R_X_t_chunk[
                                        chunk_index, j, batch_label] += PCs[i, j] * Rk[i]
                        
                        # Compute `inv_mat`
                        matrix_multiply(P_t_B_inv, P, inv_mat, transpose_A=False,
                                        transpose_B=False, alpha=1, beta=0)
                        
                        # Compute `R`-scaled PCs in chunks
                        for j in prange(num_PCs, num_threads=num_threads):
                            total = 0
                            for batch_label in range(num_batches):
                                batch_total = 0
                                for chunk_index in range(num_chunks):
                                    batch_total = batch_total + Phi_t_diag_R_X_t_chunk[
                                        chunk_index, j, batch_label]
                                Phi_t_diag_R_X_t[j, batch_label] = batch_total
                                total = total + batch_total
                            Phi_t_diag_R_X_t[j, num_batches] = total
                        
                        # Compute `W`
                        matrix_multiply(inv_mat, Phi_t_diag_R_X_t, W,
                                        transpose_A=False, transpose_B=True,
                                        alpha=1, beta=0)
                        
                        # Update `Z`
                        for i in prange(num_cells, num_threads=num_threads):
                            batch_label = batch_labels[i]
                            for j in range(num_PCs):
                                Z[i, j] -= W[batch_label, j] * Rk[i]
                    
                    with gil:
                        if verbose:
                            metrics = (
                                f'objective = {objective:.2f} (k-means error = '
                                f'{kmeans_error:.2f}, entropy term = {entropy_term:.2f}, '
                                f'diversity penalty = {diversity_penalty:.2f})')
                            if max_iterations == 2_147_483_647:
                                print(f'Completed {iteration + 1:,} {iteration_string}: {metrics}')
                            else:
                                print(f'Completed {iteration + 1:,} of {max_iterations:,} '
                                      f'iterations: {metrics}')
                        
                        # If Harmony converged, return
                        if prev_objective - objective < tolerance * abs(prev_objective):
                            if verbose:
                                print(f'Reached convergence after {iteration + 1:,} '
                                      f'{iteration_string}')
                            return
                        prev_objective = objective
                    
                        # Check for KeyboardInterrupts
                        PyErr_CheckSignals()
                    
                    # Normalize each row of `Z` in-place
                    normalize_rows_inplace(Z, num_threads)
                
            if verbose:
                print(f'Failed to converge after {max_iterations:,} '
                      f'{iteration_string}')
        ''')
        normalize_rows = cython_functions['normalize_rows']
        harmony = cython_functions['harmony']
        
        # Get `Z`, the row-normalized PCs
        Z = np.empty((num_cells, num_PCs), dtype=np.float32)
        normalize_rows(arr=PCs, out=Z, num_threads=num_threads)
        
        # Run k-means clustering on `Z`. Since `Y` and `Y_new` are swapped
        # every iteration, `Y_new` will contain the final `Y` when doing an odd
        # number of k-means iterations; if so, swap them at the end.
        Y = np.empty((num_clusters, num_PCs), dtype=np.float32)
        Y_new = np.empty((num_clusters, num_PCs), dtype=np.float32)
        with threadpool_limits(1, user_api='blas'):
            kmeans = _kmeans_and_knn_functions['kmeans']
            kmeans(X=Z, cluster_labels=np.empty(num_cells, dtype=np.uint32),
                   centroids=Y, centroids_new=Y_new,
                   num_cells_per_cluster=np.empty(num_clusters,
                                                  dtype=np.uint32),
                   kmeans_barbar=kmeans_barbar,
                   num_init_iterations=num_init_iterations,
                   oversampling_factor=oversampling_factor,
                   num_kmeans_iterations=num_kmeans_iterations, seed=seed,
                   chunk_size=chunk_size_kmeans, num_threads=num_threads)
        if num_kmeans_iterations & 1:
            Y = Y_new
        del Y_new
        
        # Run Harmony. Unlike above, here we need to use single-threaded matrix
        # multiplication to ensure consistent floating-point roundoff.
        with threadpool_limits(1, user_api='blas'):
            harmony(PCs=PCs, Z=Z, Y=Y, batch_labels=batch_labels,
                    max_iterations=max_iterations,
                    num_clustering_iterations=num_clustering_iterations,
                    block_proportion=block_proportion, tolerance=tolerance,
                    ridge_lambda=ridge_lambda, sigma=sigma,
                    chunk_size=chunk_size_Harmony, seed=seed,
                    verbose=verbose, num_threads=num_threads)
        
        del batch_labels, PCs, Y
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan, dtype=np.float32)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
        
    def harmonize(self,
                  *others: SingleCell,
                  QC_column: SingleCellColumn | None |
                             Sequence[SingleCellColumn | None] = 'passed_QC',
                  batch_column: SingleCellColumn | None |
                                Sequence[SingleCellColumn | None] = None,
                  PC_key: str = 'PCs',
                  Harmony_key: str = 'Harmony_PCs',
                  num_clusters: int | np.integer | None = None,
                  max_iterations: int | np.integer = 10,
                  num_kmeans_iterations: int | np.integer = 25,
                  max_clustering_iterations: int | np.integer = 5,
                  block_proportion: int | float | np.integer |
                                    np.floating = 0.05,
                  tolerance: int | float | np.integer | np.floating = 0.01,
                  early_stopping: bool = False,
                  clustering_tolerance: int | float | np.integer |
                                        np.floating = 0.001,
                  alpha: float | np.floating = 0.2,
                  sigma: int | float | np.integer | np.floating = 0.1,
                  kmeans_barbar: bool = False,
                  num_init_iterations: int | np.integer = 5,
                  oversampling_factor: int | np.integer | float |
                                       np.floating = 1,
                  chunk_size_kmeans: int | np.integer | None = None,
                  chunk_size_Harmony: int | np.integer | None = None,
                  seed: int | np.integer = 0,
                  original: bool = False,
                  overwrite: bool = False,
                  verbose: bool = True,
                  num_threads: int | np.integer | None = None) -> \
            SingleCell | tuple[SingleCell, ...]:
        """
        Harmonize this SingleCell dataset with other datasets, using Harmony
        (nature.com/articles/s41592-019-0619-0).
        
        Harmony was originally written in R (github.com/immunogenomics/harmony)
        but has two Python ports, harmony-pytorch
        (github.com/lilab-bcb/harmony-pytorch) and harmonypy
        (github.com/slowkow/harmonypy).
        
        Our implementation differs from the original in three key ways:
        
        First, we parallelize Harmony via an innovative nested block strategy.
        The original algorithm partitions cells randomly into blocks, each
        containing a fixed fraction (`block_proportion`, 5% by default) of the
        total cells. It iterates over each block, subtracting the contribution
        of the cells in the block to the observed and expected cluster-batch
        co-occurence matrices `O` and `E`, re-calculating the soft-clustering
        assignments `R` based on the residual `O` and `E`, then adding back the
        contribution of the cells in the block to `O` and `E` based on the new
        `R`. This approach resists straightforward parallelization because 
        updates to `R` for future blocks depend on updates to `O` and `E` from
        previous blocks, leading to convergence failure if naively
        parallelized. Instead, we parallelize within blocks by dividing them
        into chunks of `chunk_size` cells (256 by default). We process each
        chunk in parallel, subtracting only the `O` and `E` contributions of
        the chunk itself, updating `R` for the chunk, and, after and, after
        processing all chunks in the block, add back the `O` and `E`
        contributions for all chunks based on the updated `R`. Updating `O` and
        `E` at the end of each block (rather than after processing every cell
        in the dataset) ensures convergence, while the inner chunking enables
        parallelization without disrupting convergence. The original
        algorithm's chunkless strategy for updating `R`, `O`, and `E`, which is
        faster for single-threaded execution, can be re-enabled with
        `original=True, num_threads=1`.
        
        Second, we reduce the default number of clustering iterations
        (Harmony's inner loop) from 20 to 5, but always complete all 5
        iterations without early stopping based on convergence. In practice,
        the largest changes to the Harmony objective function result from
        updating the PCs (outer loop), not updating the soft-clustering
        assignments (inner loop). Our implementation makes updating the PCs
        sufficiently fast that there are rapidly diminishing returns from
        performing lots of clustering updates, versus just skipping directly to
        updating the PCs after a few clustering iterations. By skipping
        convergence checks, we reduce the number of objective function
        evaluations (which are expensive) from once per inner iteration to once
        per outer iteration. The old default of 20 iterations with early
        stopping can be re-enabled with
        `early_stopping=True, max_clustering_iterations=20`.
        
        Third, we adopt harmony-pytorch's 'fast' correction, which removes
        batch-specific variation from PCs more efficiently than the original
        Harmony implementation. We combine this with the original Harmony
        implementation's automated selection of the regularization parameter
        for the correction step (see the `alpha` parameter), which was
        introduced in Harmony version 1.2.0 and is not present in either Python
        port.
  
        Args:
            others: the other SingleCell datasets to harmonize this one with
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their Harmony embeddings set to `NaN`. When
                       `others` is specified, `QC_column` can be a
                       length-`1 + len(others)` sequence of columns,
                       expressions, Series, functions, or `None` for each
                       dataset (for `self`, followed by each dataset in
                       `others`).
            batch_column: an optional String, Enum, Categorical, or integer
                          column of `obs` indicating which batch each cell is
                          from. Can be a column name, a polars expression, a
                          polars Series, a 1D NumPy array, or a function that
                          takes in this SingleCell dataset and returns a polars
                          Series or 1D NumPy array. Each batch will be treated
                          as if it were a distinct dataset; this is exactly
                          equivalent to splitting the dataset with
                          `split_by(batch_column)` and then passing each of the
                          resulting datasets to `harmonize()`. Set to `None` to
                          treat each dataset as having a single batch. When
                          `others` is specified, `batch_column` may be a
                          length-`1 + len(others)` sequence of columns,
                          expressions, Series, functions, or `None` for each
                          dataset (for `self`, followed by each dataset in
                          `others`).
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as the input to Harmony
            Harmony_key: the key of `obsm` where the Harmony embeddings will be
                         stored; will be added in-place to both `self` and each
                         of the datasets in `others`!
            num_clusters: the number of clusters used in the Harmony algorithm,
                          including in the initial k-means clustering. If not
                          specified, use 100 clusters if ≥3000 cells, 1 cluster
                          if ≤30 cells, and `round(num_cells / 30)` if between
                          30 and 3000 cells.
            max_iterations: the maximum number of iterations to run Harmony 
                            for, if convergence is not achieved. Defaults to 
                            10, like the original Harmony R package, 
                            harmony-pytorch, and harmonypy. Set to `None` to 
                            use as many iterations as necessary to achieve
                            convergence.
            num_kmeans_iterations: the number of iterations of k-means
                                   clustering to run, as the first step of
                                   Harmony. Defaults to 25, like the original
                                   Harmony R package, harmony-pytorch, and
                                   harmonypy. However, unlike these packages,
                                   only one initialization is tried rather than
                                   10 to reduce runtime.
            max_clustering_iterations: the number of iterations to run the
                                       clustering step within each Harmony
                                       iteration for, or the maximum number of
                                       iterations if `early_stopping=True`.
                                       Unlike the original Harmony algorithm,
                                       convergence of the clustering is not
                                       checked unless `early_stopping=True`.
                                       Defaults to 5 iterations; this differs
                                       from the 20 used by the original harmony
                                       R package and harmonypy and the 200
                                       iterations used by harmony-pytorch,
                                       which do have convergence checks.
                                       Must be 4 or more when
                                       `early_stopping=True`, since Harmony's
                                       clustering convergence check requires
                                       knowing the errors from the past 3
                                       iterations.
            block_proportion: the proportion of cells to use in each batch
                              update in the clustering step; must be greater
                              than zero and less than or equal to 1
            tolerance: the relative tolerance used to determine whether to stop 
                       optimizing the Harmony embeddings before
                       `max_iterations` iterations
            early_stopping: whether to stop clustering before
                            `max_clustering_iterations` iterations if
                            convergence is reached, like in the original
                            Harmony algorithm
            clustering_tolerance: the relative tolerance used to determine
                                  whether to stop clustering before
                                  `max_clustering_iterations` iterations. Only
                                  used when `early_stopping=True`.
            alpha: the scaling factor for the ridge regression penalty used
                   when correcting the principal components to get the Harmony
                   embeddings; must be greater than 0 and less than 1. The
                   ridge penalty `lambda` is determined by `alpha` and the
                   expected number of cells, assuming independence between
                   batches and clusters:
                   `lambda = alpha * expected number of cells`.
            sigma: the weight of the entropy term in the Harmony objective
                   function; must be non-negative
            kmeans_barbar: whether to use k-means|| initialization (a parallel
                           version of k-means++ from arxiv.org/abs/1203.6402)
                           to initialize the k-means clustering centroids,
                           instead of random initialization
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402). Only used when
                                 `kmeans_barbar=True`.
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++. Only used when
                                 `kmeans_barbar=True`.
            chunk_size_kmeans: the chunk size to use for distance calculations 
                               in the initial k-means clustering. Setting this 
                               to a power of 2 is recommended; 256 was found to 
                               be the optimal chunk size by scikit-learn and in
                               our tests. Defaults to
                               `min(256, total number of cells)`.
            chunk_size_Harmony: the chunk size to use for Harmony. Setting this 
                                to a power of 2 is recommended; 512 was found 
                                to be the optimal chunk size in our tests. 
                                Defaults to `min(512, total number of cells)`.
                                Not used when `original=True`.
            seed: the random seed to use for the initial k-means clustering
            original: if `True`, use the original Harmony algorithm's blocking
                      strategy, rather than our nested chunks-within-blocks
                      strategy. `original=True` requires `num_threads=1`. This
                      gives lower memory usage and closer correspondence to the
                      original algorithm, at the cost of a) a moderate
                      (~20-25%) degradation in performance and b) no longer
                      matching the Harmony embeddings produced by the
                      multithreaded version. If `True`, exactly match the
                      results of the multithreaded version when
                      `num_threads=1`. Must be `False` unless `num_threads=1`.
            overwrite: if `True`, overwrite `Harmony_key` if already present in
                       obsm, instead of raising an error
            verbose: whether to print details of the harmonization process
            num_threads: the number of threads to use when concatenating
                         principal components and batch/dataset labels across
                         datasets, for the initial k-means clustering, and for
                         the matrix and matrix-vector multiplications within
                         Harmony. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores. Does not affect
                         the returned SingleCell dataset's `num_threads`; this
                         will always be the same as the original dataset's
                         `num_threads`.
        
        Returns:
            A length-`1 + len(others)` tuple of SingleCell datasets with the
            Harmony embeddings stored in `obsm[Harmony_key]`: `self`, followed
            by each dataset in `others`.
        """
        # If `others` was specified, check that all elements of `others` are
        # SingleCell datasets
        if not others:
            error_message = 'others cannot be empty'
            raise ValueError(error_message)
        check_types(others, 'others', SingleCell, 'SingleCell datasets')
        datasets = [self] + list(others)
        
        # Check that `Harmony_key` is a string
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `Harmony_key` is not already in `obsm` for any dataset,
        # unless `overwrite=True`
        if not overwrite and \
                any(Harmony_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'Harmony_key {Harmony_key!r} is already a key of obsm for at '
                f'least one dataset; did you already run harmonize()? Set '
                f'overwrite=True to overwrite.')
            raise ValueError(error_message)
        
        # Get `QC_column` and `batch_column` from every dataset, if not None
        QC_columns = SingleCell._get_columns(
            'obs', datasets, QC_column, 'QC_column', pl.Boolean,
            allow_missing=True)
        QC_columns_NumPy = [QC_col.to_numpy() if QC_col is not None else None
                            for QC_col in QC_columns]
        batch_columns = SingleCell._get_columns(
            'obs', datasets, batch_column, 'batch_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'),
            QC_columns=QC_columns)
        
        # Check that `PC_key` is a key of `obsm` for every dataset
        check_type(PC_key, 'PC_key', str, 'a string')
        if not all(PC_key in dataset._obsm for dataset in datasets):
            error_message = (
                f'PC_key {PC_key!r} is not a column of obs for at least one '
                f'dataset; did you forget to run PCA() before harmonize()?')
            raise ValueError(error_message)
        
        # If `num_clusters` is not `None`, check that it is a positive integer
        # We will assign `num_clusters` its default value (if `None`) and check
        # its upper bound later, once we know the total number of cells across
        # all datasets.
        if num_clusters is not None:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            check_bounds(num_clusters, 'num_clusters', 1)
        
        # Check that `max_iterations` is `None` or a positive integer; if 
        # `None`, set to `INT32_MAX`
        if max_iterations is None:
            max_iterations = 2_147_483_647
        else:
            check_type(max_iterations, 'max_iterations', int, 
                       'a positive integer')
            check_bounds(max_iterations, 'max_iterations', 1)
        
        # Check that `num_kmeans_iterations` is a positive integer
        check_type(num_kmeans_iterations, 'num_kmeans_iterations', int,
                   'a positive integer')
        check_bounds(num_kmeans_iterations, 'num_kmeans_iterations', 1)
            
        # Check that `max_clustering_iterations` is a positive integer
        check_type(max_clustering_iterations, 'max_clustering_iterations', int,
                   'a positive integer')
        check_bounds(max_clustering_iterations, 'max_clustering_iterations', 1)
        
        # Check that `block_proportion` is a number and that
        # `0 < block_proportion <= 1`
        check_type(block_proportion, 'block_proportion', (int, float),
                   'a number greater than zero and less than or equal to 1')
        check_bounds(block_proportion, 'block_proportion', 0, 1,
                     left_open=True)
        
        # Check that `tolerance` is a positive number; if an integer, cast it
        # to a float
        check_type(tolerance, 'tolerance', (int, float), 'a positive number')
        check_bounds(tolerance, 'tolerance', 0, left_open=True)
        tolerance = float(tolerance)
        
        # Check that `early_stopping` is Boolean
        check_type(early_stopping, 'early_stopping', bool, 'Boolean')
        
        # Check that `clustering_tolerance` is a positive number; if an
        # integer, cast it to a float
        check_type(clustering_tolerance, 'clustering_tolerance', (int, float),
                   'a positive number')
        check_bounds(clustering_tolerance, 'clustering_tolerance', 0,
                     left_open=True)
        clustering_tolerance = float(clustering_tolerance)
        
        # If `early_stopping=False`, check that `clustering_tolerance` has its
        # default value
        if not early_stopping:
            if clustering_tolerance != 0.001:
                error_message = (
                    'clustering_tolerance can only be specified when '
                    'early_stopping=True')
                raise ValueError(error_message)
        
        # Check that `alpha` is greater than 0 and less than 1
        check_type(alpha, 'alpha', float,
                   'a number greater than 0 and less than 1')
        check_bounds(alpha, 'alpha', 0, 1, left_open=True, right_open=True)
        
        # Check that `sigma` is a non-negative number; if an integer, cast it
        # to a float
        check_type(sigma, 'sigma', (int, float), 'a non-negative number')
        check_bounds(sigma, 'sigma', 0)
        sigma = float(sigma)
        
        # Check that `kmeans_barbar` is Boolean
        check_type(kmeans_barbar, 'kmeans_barbar', bool, 'Boolean')
        
        # Check that `num_init_iterations` is a positive integer
        check_type(num_init_iterations, 'num_init_iterations', int,
                   'a positive integer')
        check_bounds(num_init_iterations, 'num_init_iterations', 1)
        
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        
        # If `kmeans_barbar=False`, check that `num_init_iterations` and
        # `oversampling_factor` have their default values
        if not kmeans_barbar:
            if num_init_iterations != 5:
                error_message = (
                    'num_init_iterations can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            if oversampling_factor != 1:
                error_message = (
                    'oversampling_factor can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            
        # If `chunk_size_kmeans` and/or `chunk_size_Harmony` are not `None`, 
        # check that they are positive integers. We will check their upper 
        # bounds and set their default values (if `None`) later, once we know 
        # the total number of cells across all datasets.
        for chunk_size, chunk_size_name in \
                (chunk_size_kmeans, 'chunk_size_kmeans'), \
                (chunk_size_Harmony, 'chunk_size_Harmony'):
            if chunk_size is not None:
                check_type(chunk_size, chunk_size_name, int, 
                           'a positive integer')
                check_bounds(chunk_size, chunk_size_name, 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `original` is Boolean, and `False` unless `num_threads=1`
        check_type(original, 'original', bool, 'Boolean')
        if original and num_threads != 1:
            error_message = 'original must be False unless num_threads is 1'
            raise ValueError(error_message)
        
        # Concatenate PCs across datasets; get labels indicating which rows of
        # these concatenated PCs come from each dataset or batch. Check that
        # the PCs are float32 and C-contiguous and all have the same width.
        all_PCs = [dataset._obsm[PC_key] for dataset in datasets]
        for PCs in all_PCs:
            dtype = PCs.dtype
            if dtype != np.float32:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is {dtype!r} for at least one '
                    f'dataset, but must be float32')
                raise TypeError(error_message)
            if not PCs.flags['C_CONTIGUOUS']:
                error_message = (
                    f'obsm[{PC_key!r}].dtype is not C-contiguous for at least '
                    f'one dataset; make it C-contiguous with '
                    f'pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
                raise ValueError(error_message)
        width = all_PCs[0].shape[1]
        for PCs in all_PCs[1:]:
            if PCs.shape[1] != width:
                error_message = (
                    f"two datasets' PCs have different numbers of dimensions "
                    f"({width:,} vs {PCs.shape[1]:,}")
                raise ValueError(error_message)
        if QC_column is not None:
            all_PCs = [PCs[QCed] if QCed is not None else PCs
                       for PCs, QCed in zip(all_PCs, QC_columns_NumPy)]
        num_cells_per_dataset = np.array(list(map(len, all_PCs)))
        if batch_column is None:
            batch_labels = np.repeat(np.arange(len(num_cells_per_dataset),
                                               dtype=np.uint32),
                                     num_cells_per_dataset)
        else:
            batch_labels = []
            batch_index = 0
            for dataset, QC_col, batch_col in \
                    zip(datasets, QC_columns, batch_columns):
                if batch_col is not None:
                    if QC_col is not None:
                        batch_col = batch_col.filter(QC_col)
                    if batch_col.dtype in (pl.String, pl.Categorical, pl.Enum):
                        if batch_col.dtype != pl.Enum:
                            batch_col = batch_col\
                                .cast(pl.Enum(batch_col.unique().drop_nulls()))
                        batch_col = batch_col.to_physical()
                    batch_labels.append(batch_col.to_numpy() + batch_index)
                    batch_index += batch_col.n_unique()
                else:
                    batch_labels.append(np.full(batch_index,
                                                len(dataset) if QC_col is None
                                                else QC_col.sum(),
                                                dtype=np.float32))
                    batch_index += 1
            batch_labels = concatenate(batch_labels, num_threads=num_threads)
        PCs = concatenate(all_PCs, num_threads=num_threads)
        num_cells, num_PCs = PCs.shape
        
        # If `num_clusters` is `None`, set it to `num_cells / 30`, rounded to
        # the nearest integer and clipped to be between 1 and 100. Otherwise,
        # check that it is less than the total number of cells across all
        # datasets.
        if num_clusters is None:
            num_clusters = max(min(100, int(round(num_cells / 30))), 1)
        elif num_clusters >= num_cells:
            error_message = (
                f'num_clusters is {num_clusters:,}, but must be less than the '
                f'total number of cells across all datasets ({num_cells:,})')
            raise ValueError(error_message)
        
        # If `chunk_size_kmeans` is `None`, set it to `min(256, num_cells)`.
        # Otherwise, check that it is less than the total number of cells
        # across all datasets.
        if chunk_size_kmeans is None:
            chunk_size_kmeans = min(256, num_cells)
        elif chunk_size_kmeans >= num_cells:
            error_message = (
                f'chunk_size_kmeans is {chunk_size_kmeans:,}, but must be '
                f'less than the total number of cells across all datasets '
                f'({num_cells:,})')
            raise ValueError(error_message)
        
        # If `chunk_size_Harmony` is `None`, set it to `min(512, num_cells)`.
        # Otherwise, check that it is less than the total number of cells
        # across all datasets.
        if chunk_size_Harmony is None:
            chunk_size_Harmony = min(512, num_cells)
        elif chunk_size_Harmony >= num_cells:
            error_message = (
                f'chunk_size_Harmony is {chunk_size_Harmony:,}, but must be '
                f'less than the total number of cells across all datasets '
                f'({num_cells:,})')
            raise ValueError(error_message)
        
        # Define Cython functions
        cython_functions = cython_inline(_uninitialized_vector_import + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.cmath cimport abs, ceil, exp, log, sqrt
        from libcpp.vector cimport vector
        from scipy.linalg.cython_blas cimport sgemm
        
        ctypedef fused integer:
            unsigned
            int
            long
        
        cdef extern from "<utility>" namespace "std" nogil:
            T swap[T](T &a, T &b)
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef inline void bincount(const integer[::1] arr,
                                  unsigned[::1] counts,
                                  const unsigned num_threads) noexcept nogil:
            cdef unsigned long start, end, i, num_bins, chunk_size, \
                num_elements = arr.shape[0]
            cdef unsigned thread_index
            cdef vector[vector[unsigned]] thread_counts
            
            if num_threads == 1:
                counts[:] = 0
                for i in range(num_elements):
                    counts[arr[i]] += 1
            else:
                # Store counts for each thread in a temporary buffer, then
                # aggregate at the end. As an optimization, put the counts for
                # the last thread (`thread_index == num_threads - 1`) directly
                # into the final `counts` array.
                thread_counts.resize(num_threads - 1)
                num_bins = counts.shape[0]
                chunk_size = (num_elements + num_threads - 1) / num_threads
                with parallel(num_threads=num_threads):
                    thread_index = threadid()
                    start = thread_index * chunk_size
                    if thread_index == num_threads - 1:
                        end = num_elements
                        counts[:] = 0
                        for i in range(start, end):
                            counts[arr[i]] += 1
                    else:
                        thread_counts[thread_index].resize(num_bins)
                        end = start + chunk_size
                        for i in range(start, end):
                            thread_counts[thread_index][arr[i]] += 1
                
                # Aggregate counts from all threads except the last
                for thread_index in range(num_threads - 1):
                    for i in range(num_bins):
                        counts[i] += thread_counts[thread_index][i]
        
        cdef inline void matrix_multiply(const float[:, ::1] A,
                                         const float[:, ::1] B,
                                         float[:, ::1] C,
                                         const bint transpose_A,
                                         const bint transpose_B,
                                         const float alpha,
                                         const float beta) noexcept nogil:
            # A wrapper for `sgemm()` for the case when all matrices are C-major. Flip
            # `A` <-> `B`, `shape[0]` <-> `shape[1]`, and both transpose flags, since
            # BLAS expects Fortran-major. Note that `C`'s dimensions are not required
            # to match `A` and `B`'s (and often don't); `C` is only used for its
            # address.
            
            cdef int m, n, k, lda, ldb
            cdef char transA, transB
            if transpose_B:
                m = B.shape[0]
                k = B.shape[1]
                lda = k
                transA = b'T'
            else:
                m = B.shape[1]
                k = B.shape[0]
                lda = m
                transA = b'N'
            if transpose_A:
                n = A.shape[1]
                ldb = n
                transB = b'T'
            else:
                n = A.shape[0]
                ldb = k
                transB = b'N'
            sgemm(&transA, &transB, &m, &n, &k, <float*> &alpha,
                  <float*> &B[0, 0], &lda, <float*> &A[0, 0], &ldb,
                  <float*> &beta, &C[0, 0], &m)
        
        cdef inline void normalize_rows_inplace(float[:, ::1] arr):
            cdef unsigned i, j
            cdef float norm
            for i in range(arr.shape[0]):
                norm = 0
                for j in range(arr.shape[1]):
                    norm += arr[i, j] * arr[i, j]
                norm = 1 / sqrt(norm)
                for j in range(arr.shape[1]):
                    arr[i, j] = arr[i, j] * norm
        
        cdef inline void normalize_rows_inplace_parallel(
                float[:, ::1] arr,
                const unsigned num_threads) noexcept nogil:
            cdef unsigned i, j
            cdef float norm
            for i in prange(arr.shape[0], num_threads=num_threads):
                norm = 0
                for j in range(arr.shape[1]):
                    norm = norm + arr[i, j] * arr[i, j]
                norm = 1 / sqrt(norm)
                for j in range(arr.shape[1]):
                    arr[i, j] = arr[i, j] * norm
        
        def normalize_rows(const float[:, ::1] arr,
                           float[:, ::1] out,
                           const unsigned num_threads):
            cdef unsigned i, j
            cdef float norm
            if num_threads == 1:
                for i in range(arr.shape[0]):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm += arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm
            else:
                for i in prange(arr.shape[0], nogil=True, num_threads=num_threads):
                    norm = 0
                    for j in range(arr.shape[1]):
                        norm = norm + arr[i, j] * arr[i, j]
                    norm = 1 / sqrt(norm)
                    for j in range(arr.shape[1]):
                        out[i, j] = arr[i, j] * norm

        def harmony(const float[:, ::1] PCs,
                    float[:, ::1] Z,
                    float[:, ::1] Y,
                    const unsigned[::1] batch_labels,
                    const unsigned max_iterations,
                    const unsigned max_clustering_iterations,
                    const float block_proportion,
                    const float tolerance,
                    const bint early_stopping,
                    const float clustering_tolerance,
                    const float alpha,
                    const float sigma,
                    const unsigned chunk_size,
                    const unsigned long seed,
                    const bint verbose,
                    const unsigned num_threads):
            
            # The major data structures in the Harmony algorithm are:
            # - `Z` (cells × PCs): the row-normalized principal components
            # - `Y` (clusters × PCs): the centroid of each cluster
            # - `R` (cells × clusters): the soft assignment of each cell to each
            #   cluster, which has values between 0 and 1 and sums to 1 for each cell
            # - `O` (batches × clusters): the cluster-batch co-occurrence matrix, i.e.
            #    the sum of the `R`s across all cells from each batch that are part of
            #    each cluster
            # - `E` (batches × clusters): the expected value of `O` if batches were
            #   randomly distributed across clusters. For a given batch `i` and cluster
            #   `j`, this is just `Pr_b[i]` (the fraction of cells in batch `i`) times
            #   `sum(R[:, j])` (the sum of `R` for that cluster across all batches).
            #    We define `R_sum = sum(R[:, j])`.
            # The Harmony paper also refers to ϕ, the one-hot encoded batch labels. We
            # avoid one-hot encoding for efficiency and just use the raw batch labels.
            #
            # We process cells blockwise and then chunkwise within each block. Why both
            # blocks and chunks? Because `O` and `E` need to be updated by block, so
            # parallelization has to occur within each block.
            #
            # The number of chunks may be slightly less for the last block, so
            # per-chunk arrays are allocated to have
            # `max_chunks_per_block = ceil(block_size / chunk_size)` chunks, of which
            # only the first `num_chunks_per_block` are used for each block.
            #
            # Allocate per-thread storage for `Z_chunk` and `distance = Z_chunk @ Y.T`.
            # Allocate a single buffer for all threads without worrying about false
            # sharing, since the default `chunk_size` (256) is a multiple of the cache
            # line size.
            
            cdef unsigned i, j, k, thread_index, chunk_index, chunk_start, chunk_end, \
                iteration, clustering_iteration, block_index, block_start, block_end, \
                num_chunks_per_block, batch_label, start_row, end_row, \
                num_cells = Z.shape[0], num_PCs = Z.shape[1], \
                num_clusters = Y.shape[0], \
                num_batches = batch_labels[batch_labels.shape[0] - 1] + 1, \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size, \
                block_size = <unsigned> ceil(num_cells * block_proportion), \
                num_blocks = (num_cells + block_size - 1) / block_size, \
                max_chunks_per_block = (block_size + chunk_size - 1) / chunk_size, \
                num_cells_per_thread = (num_cells + num_threads - 1) / num_threads
            cdef unsigned long state = srand(seed)
            cdef float base, kmeans_error, entropy_term, norm, Rij, R_sum, O_sum, \
                diversity_penalty, prev_objective, total, delta_Eij, delta_Oij, \
                Pr_bi, Eij, Oij, Rkj, objective, last_two, old, new, ridge_lambda, \
                factor, Rki, batch_total, two_over_sigma = 2 / sigma, \
                exp_neg_two_over_sigma = exp(-two_over_sigma)
            cdef float past_clustering_objectives[3]
            cdef str metrics
            cdef uninitialized_vector[unsigned] N_b_buffer, cell_order_buffer
            cdef uninitialized_vector[float] Pr_b_buffer, Rk_buffer, \
                R_buffer, E_buffer, O_buffer, R_sums_buffer, inv_cov_2_buffer, \
                inv_cov_1_buffer, inv_cov_buffer, R_scaled_PCs_buffer, W_buffer, \
                Y_chunk_buffer, Z_chunk_buffer, distance_buffer, delta_O_buffer, \
                delta_E_buffer, ratio_buffer, kmeans_errors, entropy_terms, \
                R_scaled_PCs_chunk_buffer
            N_b_buffer.resize(num_batches)
            cell_order_buffer.resize(num_cells)
            Pr_b_buffer.resize(num_batches)
            Rk_buffer.resize(num_cells)
            R_buffer.resize(num_cells * num_clusters)
            E_buffer.resize(num_batches * num_clusters)
            O_buffer.resize(num_batches * num_clusters)
            R_sums_buffer.resize(num_chunks * num_clusters)
            inv_cov_2_buffer.resize((num_batches + 1) * (num_batches + 1))
            inv_cov_1_buffer.resize((num_batches + 1) * (num_batches + 1))
            inv_cov_buffer.resize((num_batches + 1) * (num_batches + 1))
            R_scaled_PCs_buffer.resize(num_PCs * (num_batches + 1))
            W_buffer.resize(num_PCs * (num_batches + 1))
            Y_chunk_buffer.resize(num_chunks * num_clusters * num_PCs)
            Z_chunk_buffer.resize(num_threads * chunk_size * num_PCs)
            distance_buffer.resize(num_threads * chunk_size * num_clusters)
            delta_O_buffer.resize(num_chunks * num_batches * num_clusters)
            delta_E_buffer.resize(num_chunks * num_batches * num_clusters)
            ratio_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            kmeans_errors.resize(num_chunks)
            entropy_terms.resize(num_chunks)
            R_scaled_PCs_chunk_buffer.resize(
                num_chunks * num_PCs * (num_batches + 1))
            cdef unsigned[::1] \
                N_b = <unsigned[:num_batches]> N_b_buffer.data(), \
                cell_order = <unsigned[:num_cells]> cell_order_buffer.data()
            cdef float[::1] \
                Pr_b = <float[:num_batches]> Pr_b_buffer.data(), \
                Rk = <float[:num_cells]> Rk_buffer.data()
            cdef float[:, ::1] \
                R = <float[:num_cells, :num_clusters]> R_buffer.data(), \
                E = <float[:num_batches, :num_clusters]> E_buffer.data(), \
                O = <float[:num_batches, :num_clusters]> O_buffer.data(), \
                R_sums = <float[:num_chunks, :num_clusters]> R_sums_buffer.data(), \
                inv_cov_2 = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_2_buffer.data(), \
                inv_cov_1 = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_1_buffer.data(), \
                inv_cov = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_buffer.data(), \
                R_scaled_PCs = <float[:num_PCs, :num_batches + 1]> \
                    R_scaled_PCs_buffer.data(), \
                W = <float[:num_batches + 1, :num_PCs]> W_buffer.data()
            cdef float[:, :, ::1] \
                Y_chunk = <float[:num_chunks, :num_clusters, :num_PCs]> \
                    Y_chunk_buffer.data(), \
                Z_chunk = <float[:num_threads, :chunk_size, :num_PCs]> \
                    Z_chunk_buffer.data(), \
                distances = <float[:num_threads, :chunk_size, :num_clusters]> \
                    distance_buffer.data(), \
                delta_O = <float[:num_chunks, :num_batches, :num_clusters]> \
                    delta_O_buffer.data(), \
                delta_E = <float[:num_chunks, :num_batches, :num_clusters]> \
                    delta_E_buffer.data(), \
                ratio = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                    ratio_buffer.data(), \
                R_scaled_PCs_chunk = \
                    <float[:num_chunks, :num_PCs, :num_batches + 1]> \
                    R_scaled_PCs_chunk_buffer.data()

            with nogil:
                # Get the number (`N_b`) and fraction (`Pr_b`) of cells in each batch
                bincount(batch_labels, N_b, num_threads)
                for i in range(num_batches):
                    Pr_b[i] = <float> N_b[i] / num_cells
                N_b_buffer.clear()
                
                # Initialize `R`, `R_sum`, and `O` chunkwise. Compute the initial
                # k-means error and entropy term, the first two components of the
                # objective function, for each chunk.
                with parallel(num_threads=min(num_threads, num_chunks)):
                    thread_index = threadid()
                    for chunk_index in prange(num_chunks):
                        chunk_start = chunk_index * chunk_size
                        chunk_end = min(chunk_start + chunk_size, num_cells)
                        matrix_multiply(Z[chunk_start:chunk_end], Y,
                                        distances[thread_index], transpose_A=False,
                                        transpose_B=True, alpha=1, beta=0)
                        kmeans_error = 0
                        entropy_term = 0
                        R_sums[chunk_index, :] = 0
                        delta_O[chunk_index, :, :] = 0
                        for i in range(chunk_start, chunk_end):
                            batch_label = batch_labels[i]
                            norm = 0
                            for j in range(num_clusters):
                                Rij = exp(two_over_sigma * (distances[
                                    thread_index, i - chunk_start, j] - 1))
                                R[i, j] = Rij
                                norm += Rij
                            norm = 1 / norm
                            for j in range(num_clusters):
                                Rij = R[i, j]
                                Rij = Rij * norm
                                R[i, j] = Rij
                                R_sums[chunk_index, j] += Rij
                                delta_O[chunk_index, batch_label, j] += Rij
                                kmeans_error = kmeans_error + \
                                    Rij * (1 - distances[
                                        thread_index, i - chunk_start, j])
                                entropy_term = entropy_term + Rij * log(Rij)
                        kmeans_errors[chunk_index] = kmeans_error
                        entropy_terms[chunk_index] = entropy_term
                
                # Initialize `E`
                for i in range(num_batches):
                    for j in range(num_clusters):
                        R_sum = 0
                        for chunk_index in range(num_chunks):
                            R_sum += R_sums[chunk_index, j]
                        E[i, j] = Pr_b[i] * R_sum
                
                # Aggregate the initial k-means error and entropy term across chunks
                kmeans_error = 0
                entropy_term = 0
                for chunk_index in range(num_chunks):
                    kmeans_error += kmeans_errors[chunk_index]
                    entropy_term += entropy_terms[chunk_index]
                kmeans_error *= 2
                entropy_term *= sigma
                
                # Initialize `O` and compute the initial diversity penalty, the third
                # component of the objective function
                diversity_penalty = 0
                for i in range(num_batches):
                    for j in range(num_clusters):
                        O_sum = 0
                        for chunk_index in range(num_chunks):
                            O_sum += delta_O[chunk_index, i, j]
                        O[i, j] = O_sum
                        diversity_penalty += \
                            O_sum * log((O_sum + 1) / (E[i, j] + 1))
                diversity_penalty *= 2 * sigma
                
                # Compute the initial total objective function
                prev_objective = kmeans_error + entropy_term + diversity_penalty
                
                # Normalize each row of `Y` in-place
                normalize_rows_inplace_parallel(Y, num_threads)
                
            # Define the random order to iterate over cells in, via the "inside-out"
            # variant of the Fisher-Yates shuffle
            for i in range(num_cells):
                j = randint(i + 1, &state)
                cell_order[i] = cell_order[j]
                cell_order[j] = i
                
            if verbose:
                print(f'Initialization is complete: objective = '
                      f'{prev_objective:.2f}')
            
            # Check for KeyboardInterrupts
            PyErr_CheckSignals()
            
            # Shrink `R_sums`, `delta_O`, and `delta_E` from `num_chunks` to
            # `max_chunks_per_block` along their first dimension
            R_sums_buffer.resize(max_chunks_per_block * num_clusters)
            delta_O_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            delta_E_buffer.resize(max_chunks_per_block * num_batches * num_clusters)
            R_sums = <float[:max_chunks_per_block, :num_clusters]> R_sums_buffer.data()
            delta_O = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                delta_O_buffer.data()
            delta_E = <float[:max_chunks_per_block, :num_batches, :num_clusters]> \
                delta_E_buffer.data()
            
            # Now that initialization is done, start the Harmony iterations
            with nogil:
                for iteration in range(1, max_iterations + 1):
                    # Perform `max_clustering_iterations` iterations of clustering
                    # within each Harmony iteration, stopping early if
                    # `early_stopping=True` and convergence is met
                    for clustering_iteration in range(1, max_clustering_iterations + 1):
                        # Compute `Y`, the normalized cluster centroids, chunkwise
                        for chunk_index in prange(num_chunks, num_threads=num_threads):
                            chunk_start = chunk_index * chunk_size
                            chunk_end = min(chunk_start + chunk_size, num_cells)
                            matrix_multiply(R[chunk_start:chunk_end],
                                            Z[chunk_start:chunk_end], Y_chunk[chunk_index],
                                            transpose_A=True, transpose_B=False, alpha=1,
                                            beta=0)
                        for i in prange(num_clusters, num_threads=num_threads):
                            norm = 0
                            for j in range(num_PCs):
                                total = 0
                                for chunk_index in range(num_chunks):
                                    total = total + Y_chunk[chunk_index, i, j]
                                Y[i, j] = total
                                norm = norm + total * total
                            norm = 1 / sqrt(norm)
                            for j in range(num_PCs):
                                Y[i, j] *= norm
                        
                        # Update `R`, `E`, and `O` by processing cells blockwise and
                        # then chunkwise within each block. Note that the full formula
                        # for `R` is:
                        # `((E + 1) / (O + 1)) ** 2 * exp(-2 / sigma * (1 - Z @ Y.T))`
                        # which we can calculate as `ratio * exp(distances)` where
                        # `ratio = exp(-2 / sigma) * ((E + 1) / (O + 1)) ** 2` and
                        # `distances = 2 / sigma * Z @ Y.T`. Calculating `R` this way
                        # saves a few multiplications.
                        for block_index in range(num_blocks):
                            block_start = block_index * block_size
                            block_end = min(block_start + block_size, num_cells)
                            num_chunks_per_block = \
                                (block_end - block_start + chunk_size - 1) / chunk_size
                            with parallel(num_threads=min(num_threads, num_chunks_per_block)):
                                thread_index = threadid()
                                for chunk_index in prange(num_chunks_per_block):
                                    chunk_start = chunk_index * chunk_size
                                    chunk_end = min(chunk_start + chunk_size,
                                                    block_end - block_start)
                                    
                                    # Get the chunk of `Z`, the normalized PCs, to
                                    # process. Calculate the observed-to-expected ratio
                                    # `ratio = exp(-2 / sigma) *
                                    #          ((E + 1) / (O + 1)) ** 2` for the chunk,
                                    # subtracting the chunk's own contributions to `E`
                                    # and `O`, which we store in `delta_O[chunk_index]`
                                    # and `delta_E[chunk_index]`.
                                    R_sums[chunk_index, :] = 0
                                    delta_O[chunk_index, :, :] = 0
                                    for i in range(chunk_end - chunk_start):
                                        k = cell_order[block_start + chunk_start + i]
                                        batch_label = batch_labels[k]
                                        Z_chunk[thread_index, i, :] = Z[k, :]
                                        for j in range(num_clusters):
                                            R_sums[chunk_index, j] += R[k, j]
                                            delta_O[chunk_index, batch_label, j] -= R[k, j]
                                    for i in range(num_batches):
                                        Pr_bi = Pr_b[i]
                                        for j in range(num_clusters):
                                            delta_Eij = -Pr_bi * R_sums[chunk_index, j]
                                            delta_Oij = delta_O[chunk_index, i, j]
                                            Eij = E[i, j] + delta_Eij
                                            Oij = O[i, j] + delta_Oij
                                            ratio[chunk_index, i, j] = \
                                                exp_neg_two_over_sigma * \
                                                ((Eij + 1) / (Oij + 1)) * \
                                                ((Eij + 1) / (Oij + 1))
                                            delta_E[chunk_index, i, j] = delta_Eij
                                    
                                    # Compute `distances = 2 / sigma * Z @ Y.T`
                                    matrix_multiply(
                                        Z_chunk[thread_index,
                                               :chunk_end - chunk_start],
                                        Y, distances[thread_index], transpose_A=False,
                                        transpose_B=True, alpha=two_over_sigma, beta=0)
                                    
                                    # Update `R`, the fractional soft clustering
                                    # assignment of each cell to each cluster, for the
                                    # cells in the chunk. Normalize each row of `R` to
                                    # ensure it always sums to 1 despite floating-point
                                    # error. Also, add back this chunk's contributions
                                    # to `O` and `E` to `delta_O[chunk_index]` and
                                    # `delta_E[chunk_index]` using the newly-updated `R`.
                                    R_sums[chunk_index, :] = 0
                                    for i in range(chunk_end - chunk_start):
                                        k = cell_order[block_start + chunk_start + i]
                                        batch_label = batch_labels[k]
                                        norm = 0
                                        for j in range(num_clusters):
                                            R[k, j] = exp(distances[
                                                thread_index, i, j]) * \
                                                ratio[chunk_index, batch_label, j]
                                            norm += R[k, j]
                                        norm = 1 / norm
                                        for j in range(num_clusters):
                                            Rkj = R[k, j]
                                            Rkj = Rkj * norm
                                            R[k, j] = Rkj
                                            R_sums[chunk_index, j] += Rkj
                                            delta_O[chunk_index, batch_label, j] += Rkj
                                    for i in range(num_batches):
                                        Pr_bi = Pr_b[i]
                                        for j in range(num_clusters):
                                            delta_E[chunk_index, i, j] += \
                                                Pr_bi * R_sums[chunk_index, j]
                            
                            # Update `O` and `E` for this block with the `delta_E` and
                            # `delta_O` for each chunk.
                            for chunk_index in range(num_chunks_per_block):
                                for i in range(num_batches):
                                    for j in range(num_clusters):
                                        E[i, j] += delta_E[chunk_index, i, j]
                                        O[i, j] += delta_O[chunk_index, i, j]
                        
                        # Compute the objective function, if we are done clustering or
                        # `early_stopping=True`
                        if early_stopping or \
                                clustering_iteration == max_clustering_iterations:
                            # Compute its first two components chunkwise: the k-means
                            # error and entropy term
                            with parallel(num_threads=min(num_threads, num_chunks)):
                                thread_index = threadid()
                                for chunk_index in prange(num_chunks):
                                    chunk_start = chunk_index * chunk_size
                                    chunk_end = min(chunk_start + chunk_size, num_cells)
                                    matrix_multiply(Z[chunk_start:chunk_end], Y,
                                                    distances[thread_index], transpose_A=False,
                                                    transpose_B=True, alpha=1, beta=0)
                                    kmeans_error = 0
                                    entropy_term = 0
                                    for i in range(chunk_start, chunk_end):
                                        for j in range(num_clusters):
                                            kmeans_error = kmeans_error + \
                                                R[i, j] * (1 - distances[
                                                    thread_index, i - chunk_start, j])
                                            entropy_term = entropy_term + R[i, j] * log(R[i, j])
                                    kmeans_errors[chunk_index] = kmeans_error
                                    entropy_terms[chunk_index] = entropy_term
                            kmeans_error = 0
                            entropy_term = 0
                            for chunk_index in range(num_chunks):
                                kmeans_error += kmeans_errors[chunk_index]
                                entropy_term += entropy_terms[chunk_index]
                            kmeans_error *= 2
                            entropy_term *= sigma
            
                            # Compute the diversity penalty, the third component of the
                            # objective function
                            diversity_penalty = 0
                            for i in range(num_batches):
                                for j in range(num_clusters):
                                    diversity_penalty += \
                                        O[i, j] * log((O[i, j] + 1) / (E[i, j] + 1))
                            diversity_penalty *= 2 * sigma
                            
                            # Compute the total objective function
                            objective = kmeans_error + entropy_term + diversity_penalty
                            
                            if early_stopping:
                                with gil:
                                    # Check for KeyboardInterrupts after each
                                    # clustering iteration
                                    PyErr_CheckSignals()
                                    
                                    # If `verbose=True`, print the objective function
                                    if verbose:
                                        print(f'Clustering iteration '
                                              f'{clustering_iteration:,}: objective = '
                                              f'{objective:.2f} (k-means error = '
                                              f'{kmeans_error:.2f}, entropy term = '
                                              f'{entropy_term:.2f}, diversity '
                                              f'penalty = {diversity_penalty:.2f})')
                        
                                # Exit early if the clustering converged, based on a
                                # sliding window average of the objective over the past
                                # three iterations
                                if clustering_iteration <= 3:
                                    past_clustering_objectives[clustering_iteration] = objective
                                else:
                                    last_two = past_clustering_objectives[1] + \
                                        past_clustering_objectives[2]
                                    old = past_clustering_objectives[0] + last_two
                                    new = last_two + objective
                                    if old - new < clustering_tolerance * abs(old):
                                        break
                                    else:
                                        past_clustering_objectives[0] = \
                                            past_clustering_objectives[1]
                                        past_clustering_objectives[1] = \
                                            past_clustering_objectives[2]
                                        past_clustering_objectives[2] = objective
                            else:
                                # Check for KeyboardInterrupts after each clustering iteration
                                with gil:
                                    PyErr_CheckSignals()
                        else:
                            # Check for KeyboardInterrupts after each clustering iteration
                            with gil:
                                PyErr_CheckSignals()
                    
                    # Apply the Harmony correction to the PCs to get the new
                    # Harmony embeddings, `Z`
                    
                    # Initialize `Z` to `PCs`
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        start_row = thread_index * num_cells_per_thread
                        end_row = min(start_row + num_cells_per_thread, num_cells)
                        Z[start_row:end_row] = PCs[start_row:end_row]
                    
                    # Initialize `inv_cov_2` to the identity matrix
                    inv_cov_2[:] = 0
                    for i in range(num_batches + 1):
                        inv_cov_2[i, i] = 1
                    
                    for k in range(num_clusters):
                        # Compute `inv_cov_1` and `inv_cov_2`, which will be
                        # multiplied together to get the inverse covariance.
                        # (Harmony-pytorch defines `P = inv_cov_2` and
                        # `P_t_B_inv = inv_cov_1.T`.)
                        inv_cov_1[:] = 0
                        norm = 0
                        for i in range(num_batches):
                            ridge_lambda = E[i, k] * alpha
                            factor = 1 / (O[i, k] + ridge_lambda)
                            inv_cov_1[i, i] = factor
                            factor *= -O[i, k]
                            inv_cov_2[num_batches, i] = factor
                            norm += O[i, k] * (1 + factor)
                        norm = 1 / norm
                        inv_cov_1[num_batches, num_batches] = norm
                        for i in range(num_batches):
                            inv_cov_1[num_batches, i] = \
                                inv_cov_2[num_batches, i] * norm
                        
                        # Compute the inverse covariance matrix
                        # `inv_cov = inv_cov_1.T @ inv_cov_2`
                        matrix_multiply(inv_cov_1, inv_cov_2, inv_cov,
                                        transpose_A=True, transpose_B=False, alpha=1,
                                        beta=0)
                        
                        # Cache `R[:, k]` to avoid strided access
                        with parallel(num_threads=num_threads):
                            thread_index = threadid()
                            start_row = thread_index * num_cells_per_thread
                            end_row = min(start_row + num_cells_per_thread, num_cells)
                            for i in range(start_row, end_row):
                                Rk[i] = R[i, k]
                        
                        # Compute `R`-scaled PCs in chunks; the last column
                        # (`R_scaled_PCs[:, num_batches]`) stores the sum of the other
                        # columns
                        for chunk_index in prange(num_chunks, num_threads=num_threads):
                            start_row = chunk_index * chunk_size
                            end_row = min(start_row + chunk_size, num_cells)
                            R_scaled_PCs_chunk[chunk_index, :, :] = 0
                            for i in range(start_row, end_row):
                                batch_label = batch_labels[i]
                                Rki = Rk[i]
                                for j in range(num_PCs):
                                    R_scaled_PCs_chunk[chunk_index, j, batch_label] += \
                                        PCs[i, j] * Rki
                        for j in prange(num_PCs, num_threads=num_threads):
                            total = 0
                            for batch_label in range(num_batches):
                                batch_total = 0
                                for chunk_index in range(num_chunks):
                                    batch_total = batch_total + R_scaled_PCs_chunk[
                                        chunk_index, j, batch_label]
                                R_scaled_PCs[j, batch_label] = batch_total
                                total = total + batch_total
                            R_scaled_PCs[j, num_batches] = total
                        
                        # Compute `W = inv_cov @ R_scaled_PCs.T`
                        matrix_multiply(inv_cov, R_scaled_PCs, W,
                                        transpose_A=False, transpose_B=True,
                                        alpha=1, beta=0)
                        
                        # Update `Z`, the Harmony embeddings
                        for i in prange(num_cells, num_threads=num_threads):
                            batch_label = batch_labels[i]
                            Rki = Rk[i]
                            for j in range(num_PCs):
                                Z[i, j] -= W[batch_label, j] * Rki
                    
                    with gil:
                        # If `verbose=True`, print the objective function
                        if verbose:
                            metrics = (
                                f'objective = {objective:.2f} (k-means error = '
                                f'{kmeans_error:.2f}, entropy term = {entropy_term:.2f}, '
                                f'diversity penalty = {diversity_penalty:.2f})')
                            if max_iterations == 2_147_483_647:
                                print(f'Completed {iteration:,} '
                                      f'iteration{"" if iteration == 1 else "s"}: {metrics}')
                            else:
                                print(f'Completed {iteration:,} of {max_iterations:,} '
                                      f'iteration{"" if max_iterations == 1 else "s"}: {metrics}')
                        
                        # If Harmony converged, return
                        if prev_objective - objective < tolerance * abs(prev_objective):
                            if verbose:
                                print(f'Reached convergence after {iteration:,} iteration'
                                      f'{"" if iteration == 1 else "s"}')
                            return
                        prev_objective = objective
                    
                        # Check for KeyboardInterrupts
                        PyErr_CheckSignals()
                    
                    # Normalize each row of `Z` in-place
                    normalize_rows_inplace_parallel(Z, num_threads)
                
            if verbose:
                print(f'Failed to converge after {max_iterations:,} '
                      f'iteration{"" if max_iterations == 1 else "s"}')
        
        def harmony_original(const float[:, ::1] PCs,
                             float[:, ::1] Z,
                             float[:, ::1] Y,
                             const unsigned[::1] batch_labels,
                             const unsigned max_iterations,
                             const unsigned max_clustering_iterations,
                             const float block_proportion,
                             const float tolerance,
                             const bint early_stopping,
                             const float clustering_tolerance,
                             const float alpha,
                             const float sigma,
                             const unsigned chunk_size,
                             const unsigned long seed,
                             const bint verbose):
            # A version of `harmony()` that matches the original in its strategy for
            # updating `R`, `O`, and `E`: updates occur blockwise instead of chunkwise,
            # and the contribution of the entire block to `O` and `E` is subtracted
            # before updating `R`, then added back afterwards with the new `R`. Chunks
            # are still used for certain steps, but only to reduce memory usage and
            # cache misses, and never nested inside blocks.
            
            cdef unsigned i, j, k, chunk_index, chunk_start, chunk_end, batch_label, \
                iteration, clustering_iteration, block_index, block_start, block_end, \
                num_cells = Z.shape[0], num_PCs = Z.shape[1], \
                num_clusters = Y.shape[0], \
                num_batches = batch_labels[batch_labels.shape[0] - 1] + 1, \
                num_chunks = (num_cells + chunk_size - 1) / chunk_size, \
                block_size = <unsigned> ceil(num_cells * block_proportion), \
                num_blocks = (num_cells + block_size - 1) / block_size
            cdef unsigned long state = srand(seed)
            cdef float base, kmeans_error, entropy_term, norm, Rij, \
                diversity_penalty, prev_objective, Pr_bi, Eij, Oij, Rkj, objective, \
                last_two, old, new, ridge_lambda, factor, Rki, total, \
                two_over_sigma = 2 / sigma, \
                exp_neg_two_over_sigma = exp(-two_over_sigma)
            cdef float past_clustering_objectives[3]
            cdef str metrics
            cdef uninitialized_vector[unsigned] N_b_buffer, cell_order_buffer
            cdef uninitialized_vector[float] Pr_b_buffer, R_sums_buffer, Rk_buffer, \
                distance_buffer, R_buffer, E_buffer, O_buffer, Z_block_buffer, \
                inv_cov_2_buffer, inv_cov_1_buffer, inv_cov_buffer, \
                R_scaled_PCs_buffer, W_buffer, ratio_buffer
            N_b_buffer.resize(num_batches)
            cell_order_buffer.resize(num_cells)
            Pr_b_buffer.resize(num_batches)
            R_sums_buffer.resize(num_clusters)
            Rk_buffer.resize(num_cells)
            distance_buffer.resize(num_cells * num_clusters)
            R_buffer.resize(num_cells * num_clusters)
            E_buffer.resize(num_batches * num_clusters)
            O_buffer.resize(num_batches * num_clusters)
            Z_block_buffer.resize(block_size * num_PCs)
            ratio_buffer.resize(num_batches * num_clusters)
            inv_cov_2_buffer.resize((num_batches + 1) * (num_batches + 1))
            inv_cov_1_buffer.resize((num_batches + 1) * (num_batches + 1))
            inv_cov_buffer.resize((num_batches + 1) * (num_batches + 1))
            R_scaled_PCs_buffer.resize(num_PCs * (num_batches + 1))
            W_buffer.resize(num_PCs * (num_batches + 1))
            cdef unsigned[::1] \
                N_b = <unsigned[:num_batches]> N_b_buffer.data(), \
                cell_order = <unsigned[:num_cells]> cell_order_buffer.data()
            cdef float[::1] \
                Pr_b = <float[:num_batches]> Pr_b_buffer.data(), \
                R_sums = <float[:num_clusters]> R_sums_buffer.data(), \
                Rk = <float[:num_cells]> Rk_buffer.data()
            cdef float[:, ::1] \
                distances = <float[:chunk_size, :num_clusters]> \
                    distance_buffer.data(), \
                R = <float[:num_cells, :num_clusters]> R_buffer.data(), \
                E = <float[:num_batches, :num_clusters]> E_buffer.data(), \
                O = <float[:num_batches, :num_clusters]> O_buffer.data(), \
                Z_block = <float[:block_size, :num_PCs]> Z_block_buffer.data(), \
                ratio = <float[:num_batches, :num_clusters]> ratio_buffer.data(), \
                inv_cov_2 = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_2_buffer.data(), \
                inv_cov_1 = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_1_buffer.data(), \
                inv_cov = <float[:num_batches + 1, :num_batches + 1]> \
                    inv_cov_buffer.data(), \
                R_scaled_PCs = <float[:num_PCs, :num_batches + 1]> \
                    R_scaled_PCs_buffer.data(), \
                W = <float[:num_batches + 1, :num_PCs]> W_buffer.data()
            
            # Get the number (`N_b`) and fraction (`Pr_b`) of cells in each batch
            bincount(batch_labels, N_b, num_threads=1)
            for i in range(num_batches):
                Pr_b[i] = <float> N_b[i] / num_cells
            N_b_buffer.clear()
            
            # Initialize `R`, `R_sums`, and `O` chunkwise. Compute the initial k-means
            # error and entropy term, the first two components of the objective
            # function.
            R_sums[:] = 0
            O[:] = 0
            kmeans_error = 0
            entropy_term = 0
            for chunk_index in range(num_chunks):
                chunk_start = chunk_index * chunk_size
                chunk_end = min(chunk_start + chunk_size, num_cells)
                matrix_multiply(Z[chunk_start:chunk_end], Y, distances,
                                transpose_A=False, transpose_B=True, alpha=1, beta=0)
                for i in range(chunk_start, chunk_end):
                    norm = 0
                    for j in range(num_clusters):
                        Rij = exp(two_over_sigma * (distances[i - chunk_start, j] - 1))
                        R[i, j] = Rij
                        norm += Rij
                    norm = 1 / norm
                    for j in range(num_clusters):
                        batch_label = batch_labels[i]
                        Rij = R[i, j]
                        Rij *= norm
                        R[i, j] = Rij
                        R_sums[j] += Rij
                        O[batch_label, j] += Rij
                        kmeans_error += Rij * (1 - distances[i - chunk_start, j])
                        entropy_term += Rij * log(Rij)
            kmeans_error *= 2
            entropy_term *= sigma
            
            # Initialize `E`
            for i in range(num_batches):
                for j in range(num_clusters):
                    E[i, j] = Pr_b[i] * R_sums[j]
            
            # Compute the initial diversity penalty, the third component of the
            # objective function
            diversity_penalty = 0
            for i in range(num_batches):
                for j in range(num_clusters):
                    diversity_penalty += O[i, j] * log((O[i, j] + 1) / (E[i, j] + 1))
            diversity_penalty *= 2 * sigma
            
            # Compute the initial total objective function
            prev_objective = kmeans_error + entropy_term + diversity_penalty
            
            # Normalize each row of `Y` in-place
            normalize_rows_inplace(Y)
                
            # Define the random order to iterate over cells in, via the "inside-out"
            # variant of the Fisher-Yates shuffle
            for i in range(num_cells):
                j = randint(i + 1, &state)
                cell_order[i] = cell_order[j]
                cell_order[j] = i
                
            if verbose:
                print(f'Initialization is complete: objective = {prev_objective:.2f}')
            
            # Check for KeyboardInterrupts
            PyErr_CheckSignals()
            
            # Now that initialization is done, start the Harmony iterations
            for iteration in range(1, max_iterations + 1):
                # Perform `max_clustering_iterations` iterations of clustering
                # within each Harmony iteration, stopping early if
                # `early_stopping=True` and convergence is met
                for clustering_iteration in range(1, max_clustering_iterations + 1):
                    # Compute `Y`, the normalized cluster centroids
                    matrix_multiply(R, Z, Y, transpose_A=True, transpose_B=False,
                                    alpha=1, beta=0)
                    for i in range(num_clusters):
                        norm = 0
                        for j in range(num_PCs):
                            norm += Y[i, j] * Y[i, j]
                        norm = 1 / sqrt(norm)
                        for j in range(num_PCs):
                            Y[i, j] *= norm
                    
                    # Update `R`, `E`, and `O` by processing cells blockwise. Note that
                    # the full formula for `R` is:
                    # `((E + 1) / (O + 1)) ** 2 * exp(-2 / sigma * (1 - Z @ Y.T))`
                    # which we can calculate as `ratio * exp(distances)` where
                    # `ratio = exp(-2 / sigma) * ((E + 1) / (O + 1)) ** 2` and
                    # `distances = 2 / sigma * Z @ Y.T`. Calculating `R` this way
                    # saves a few multiplications.
                    for block_index in range(num_blocks):
                        block_start = block_index * block_size
                        block_end = min(block_start + block_size, num_cells)
                        
                        # Remove the cells in this block from `E` and `O`. Copy `Z` for
                        # these cells into a contiguous array, `Z_block`.
                        R_sums[:] = 0
                        for i in range(block_end - block_start):
                            k = cell_order[block_start + i]
                            batch_label = batch_labels[k]
                            for j in range(num_clusters):
                                O[batch_label, j] -= R[k, j]
                                R_sums[j] += R[k, j]
                            for j in range(num_PCs):
                                Z_block[i, j] = Z[k, j]
                        for i in range(num_batches):
                            Pr_bi = Pr_b[i]
                            for j in range(num_clusters):
                                E[i, j] -= Pr_bi * R_sums[j]
                        
                        # Update `R`, the fractional soft clustering assignment of each
                        # cell to each cluster, for the cells in the block. Normalize
                        # each row of `R` to ensure it always sums to 1 despite
                        # floating-point error. Add the removed cells back into `O` and
                        # `E`.
                        matrix_multiply(Z_block[:block_end - block_start], Y,
                                        distances, transpose_A=False, transpose_B=True,
                                        alpha=two_over_sigma, beta=0)
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                Eij = E[i, j]
                                Oij = O[i, j]
                                ratio[i, j] = exp_neg_two_over_sigma * \
                                    ((Eij + 1) / (Oij + 1)) * ((Eij + 1) / (Oij + 1))
                        R_sums[:] = 0
                        for i in range(block_end - block_start):
                            k = cell_order[block_start + i]
                            batch_label = batch_labels[k]
                            norm = 0
                            for j in range(num_clusters):
                                Rkj = exp(distances[i, j]) * ratio[batch_label, j]
                                R[k, j] = Rkj
                                norm += Rkj
                            norm = 1 / norm
                            for j in range(num_clusters):
                                Rkj = R[k, j]
                                Rkj *= norm
                                R[k, j] = Rkj
                                R_sums[j] += Rkj
                                O[batch_label, j] += Rkj
                        for i in range(num_batches):
                            Pr_bi = Pr_b[i]
                            for j in range(num_clusters):
                                E[i, j] += Pr_bi * R_sums[j]
                    
                    # Check for KeyboardInterrupts after each clustering
                    # iteration
                    PyErr_CheckSignals()
                    
                    # Compute the objective function, if we are done clustering or
                    # `early_stopping=True`
                    if early_stopping or \
                            clustering_iteration == max_clustering_iterations:
                        # Compute its first two components chunkwise: the k-means
                        # error and entropy term
                        kmeans_error = 0
                        entropy_term = 0
                        for chunk_index in range(num_chunks):
                            chunk_start = chunk_index * chunk_size
                            chunk_end = min(chunk_start + chunk_size, num_cells)
                            matrix_multiply(Z[chunk_start:chunk_end], Y, distances,
                                            transpose_A=False, transpose_B=True,
                                            alpha=1, beta=0)
                            for i in range(chunk_start, chunk_end):
                                for j in range(num_clusters):
                                    kmeans_error += R[i, j] * \
                                        (1 - distances[i - chunk_start, j])
                                    entropy_term += R[i, j] * log(R[i, j])
                        kmeans_error *= 2
                        entropy_term *= sigma
        
                        # Compute the diversity penalty, the third component of the
                        # objective function
                        diversity_penalty = 0
                        for i in range(num_batches):
                            for j in range(num_clusters):
                                diversity_penalty += \
                                    O[i, j] * log((O[i, j] + 1) / (E[i, j] + 1))
                        diversity_penalty *= 2 * sigma
                        
                        # Compute the total objective function
                        objective = kmeans_error + entropy_term + diversity_penalty
                        
                        if early_stopping:
                            # If `verbose=True`, print the objective function
                            if verbose:
                                print(f'Clustering iteration '
                                      f'{clustering_iteration:,}: objective = '
                                      f'{objective:.2f} (k-means error = '
                                      f'{kmeans_error:.2f}, entropy term = '
                                      f'{entropy_term:.2f}, diversity '
                                      f'penalty = {diversity_penalty:.2f})')
                    
                            # Exit early if the clustering converged, based on a
                            # sliding window average of the objective over the past
                            # three iterations
                            if clustering_iteration <= 3:
                                past_clustering_objectives[clustering_iteration] = objective
                            else:
                                last_two = past_clustering_objectives[1] + \
                                    past_clustering_objectives[2]
                                old = past_clustering_objectives[0] + last_two
                                new = last_two + objective
                                if old - new < clustering_tolerance * abs(old):
                                    break
                                else:
                                    past_clustering_objectives[0] = \
                                        past_clustering_objectives[1]
                                    past_clustering_objectives[1] = \
                                        past_clustering_objectives[2]
                                    past_clustering_objectives[2] = objective
                
                # Apply the Harmony correction to the PCs to get the new Harmony
                # embeddings, `Z`
                
                # Initialize `Z` to `PCs`
                Z[:] = PCs[:]
                
                # Initialize `inv_cov_2` to the identity matrix
                inv_cov_2[:] = 0
                for i in range(num_batches + 1):
                    inv_cov_2[i, i] = 1
                
                with nogil:
                    for k in range(num_clusters):
                        # Compute `inv_cov_1` and `inv_cov_2`, which will be
                        # multiplied together to get the inverse covariance.
                        # (Harmony-pytorch defines `P = inv_cov_2` and
                        # `P_t_B_inv = inv_cov_1.T`.)
                        inv_cov_1[:] = 0
                        norm = 0
                        for i in range(num_batches):
                            ridge_lambda = E[i, k] * alpha
                            factor = 1 / (O[i, k] + ridge_lambda)
                            inv_cov_1[i, i] = factor
                            factor *= -O[i, k]
                            inv_cov_2[num_batches, i] = factor
                            norm += O[i, k] * (1 + factor)
                        norm = 1 / norm
                        inv_cov_1[num_batches, num_batches] = norm
                        for i in range(num_batches):
                            inv_cov_1[num_batches, i] = \
                                inv_cov_2[num_batches, i] * norm
                        
                        # Compute the inverse covariance matrix
                        # `inv_cov = inv_cov_1.T @ inv_cov_2`
                        matrix_multiply(inv_cov_1, inv_cov_2, inv_cov,
                                        transpose_A=True, transpose_B=False, alpha=1,
                                        beta=0)
                        
                        # Cache `R[:, k]` to avoid strided access
                        for i in range(num_cells):
                            Rk[i] = R[i, k]
                        
                        # Compute `R`-scaled PCs; the last column
                        # (`R_scaled_PCs[:, num_batches]`) stores the sum of the other
                        # columns. Surprisingly, this step, and the updates to `Z`
                        # below, is about twice as fast with
                        # `prange(..., num_threads=1)` as with `range()`.
                        R_scaled_PCs[:] = 0
                        for i in prange(num_cells, num_threads=1):
                            batch_label = batch_labels[i]
                            Rki = Rk[i]
                            for j in range(num_PCs):
                                R_scaled_PCs[j, batch_label] += Rki * PCs[i, j]
                        for j in range(num_PCs):
                            total = 0
                            for batch_label in range(num_batches):
                                total += R_scaled_PCs[j, batch_label]
                            R_scaled_PCs[j, num_batches] = total
                        
                        # Compute `W = inv_cov @ R_scaled_PCs.T`
                        matrix_multiply(inv_cov, R_scaled_PCs, W,
                                        transpose_A=False, transpose_B=True,
                                        alpha=1, beta=0)
                        
                        # Update `Z`, the Harmony embeddings.
                        for i in prange(num_cells, num_threads=1):
                            batch_label = batch_labels[i]
                            Rki = Rk[i]
                            for j in range(num_PCs):
                                Z[i, j] -= W[batch_label, j] * Rki
                
                # If `verbose=True`, print the objective function
                if verbose:
                    metrics = (
                        f'objective = {objective:.2f} (k-means error = '
                        f'{kmeans_error:.2f}, entropy term = {entropy_term:.2f}, '
                        f'diversity penalty = {diversity_penalty:.2f})')
                    if max_iterations == 2_147_483_647:
                        print(f'Completed {iteration:,} '
                              f'iteration{"" if iteration == 1 else "s"}: {metrics}')
                    else:
                        print(f'Completed {iteration:,} of {max_iterations:,} '
                              f'iteration{"" if max_iterations == 1 else "s"}: {metrics}')
                
                # If Harmony converged, return
                if prev_objective - objective < tolerance * abs(prev_objective):
                    if verbose:
                        print(f'Reached convergence after {iteration:,} iteration'
                              f'{"" if iteration == 1 else "s"}')
                    return
                prev_objective = objective
            
                # Check for KeyboardInterrupts
                PyErr_CheckSignals()
                
                # Normalize each row of `Z` in-place
                normalize_rows_inplace(Z)
            if verbose:
                print(f'Failed to converge after {max_iterations:,} '
                      f'iteration{"" if max_iterations == 1 else "s"}')
        ''')
        normalize_rows = cython_functions['normalize_rows']
        
        # Get `Z`, the row-normalized PCs
        Z = np.empty((num_cells, num_PCs), dtype=np.float32)
        normalize_rows(arr=PCs, out=Z, num_threads=num_threads)
        
        # Run k-means clustering on `Z`. Since `Y` and `Y_new` are swapped
        # every iteration, `Y_new` will contain the final `Y` when doing an odd
        # number of k-means iterations; if so, swap them at the end.
        Y = np.empty((num_clusters, num_PCs), dtype=np.float32)
        Y_new = np.empty((num_clusters, num_PCs), dtype=np.float32)
        
        with threadpool_limits(1, user_api='blas'):
            kmeans = _kmeans_and_knn_functions['kmeans']
            kmeans(X=Z, cluster_labels=np.empty(num_cells, dtype=np.uint32),
                   centroids=Y, centroids_new=Y_new,
                   num_cells_per_cluster=np.empty(num_clusters,
                                                  dtype=np.uint32),
                   kmeans_barbar=kmeans_barbar,
                   num_init_iterations=num_init_iterations,
                   oversampling_factor=oversampling_factor,
                   num_kmeans_iterations=num_kmeans_iterations, seed=seed,
                   chunk_size=chunk_size_kmeans, num_threads=num_threads)
        if num_kmeans_iterations & 1:
            Y = Y_new
        del Y_new
        
        # Run Harmony. Unlike above, here we need to use single-threaded matrix
        # multiplication to ensure consistent floating-point roundoff.
        with threadpool_limits(1, user_api='blas'):
            if original:
                harmony_original = cython_functions['harmony_original']
                harmony_original(
                    PCs=PCs, Z=Z, Y=Y, batch_labels=batch_labels,
                    max_iterations=max_iterations,
                    max_clustering_iterations=max_clustering_iterations,
                    block_proportion=block_proportion, tolerance=tolerance,
                    early_stopping=early_stopping,
                    clustering_tolerance=clustering_tolerance, alpha=alpha,
                    sigma=sigma, chunk_size=chunk_size_Harmony, seed=seed,
                    verbose=verbose)
            else:
                harmony = cython_functions['harmony']
                harmony(PCs=PCs, Z=Z, Y=Y, batch_labels=batch_labels,
                        max_iterations=max_iterations,
                        max_clustering_iterations=max_clustering_iterations,
                        block_proportion=block_proportion, tolerance=tolerance,
                        early_stopping=early_stopping,
                        clustering_tolerance=clustering_tolerance, alpha=alpha,
                        sigma=sigma, chunk_size=chunk_size_Harmony, seed=seed,
                        verbose=verbose, num_threads=num_threads)
        
        del batch_labels, PCs, Y
        
        # Store each dataset's Harmony embedding in its obsm
        for dataset_index, (dataset, QC_col, num_cells, end_index) in \
                enumerate(zip(datasets, QC_columns_NumPy,
                              num_cells_per_dataset,
                              num_cells_per_dataset.cumsum())):
            start_index = end_index - num_cells
            dataset_Harmony_embedding = Z[start_index:end_index]
            # If `QC_col` is not `None` for this dataset, back-project from
            # QCed cells to all cells, filling with `NaN`
            if QC_col is not None:
                dataset_Harmony_embedding_QCed = dataset_Harmony_embedding
                dataset_Harmony_embedding = np.full(
                    (len(dataset), dataset_Harmony_embedding_QCed.shape[1]),
                    np.nan, dtype=np.float32)
                # noinspection PyUnboundLocalVariable
                dataset_Harmony_embedding[QC_col] = \
                    dataset_Harmony_embedding_QCed
            datasets[dataset_index] = SingleCell(
                X=dataset._X, obs=dataset._obs, var=dataset._var,
                obsm=dataset._obsm | {Harmony_key: dataset_Harmony_embedding},
                varm=self._varm, uns=self._uns, num_threads=self._num_threads)
        return tuple(datasets) if others else datasets[0]
    
    def label_transfer_from_old(
            self,
            other: SingleCell,
            original_cell_type_column: SingleCellColumn,
            *,
            QC_column: SingleCellColumn | None = 'passed_QC',
            other_QC_column: SingleCellColumn | None = 'passed_QC',
            Harmony_key: str = 'Harmony_PCs',
            cell_type_column: str = 'cell_type',
            confidence_column: str = 'cell_type_confidence',
            next_best_cell_type_column: str = 'next_best_cell_type',
            next_best_confidence_column: str =
                'next_best_cell_type_confidence',
            num_neighbors: int | np.integer = 20,
            num_clusters: int | np.integer | None = None,
            num_probes: int | np.integer | None = None,
            num_clustering_iterations: int | np.integer = 10,
            seed: int | np.integer = 0,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Transfer cell-type labels from another dataset to this one, by running
        approximate k-nearest neighbors on the Harmony embeddings from
        `harmonize()`.
        
        For each cell in `self`, the transferred cell-type label is the most
        common cell-type label among the `num_neighbors` cells in `other` with
        the nearest Harmony embeddings. The cell-type confidence is the
        fraction of these neighbors that share this most common cell-type
        label.
        
        Args:
            other: the dataset to transfer cell-type labels from
            original_cell_type_column: a column of `other.obs` containing
                                       cell-type labels. Can be a column name,
                                       a polars expression, a polars Series, a
                                       1D NumPy array, or a function that takes
                                       in `other` and returns a polars Series
                                       or 1D NumPy array.
            QC_column: an optional Boolean column of `self.obs` indicating
                       which cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in `self` and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will have their cell-type labels
                       and confidences set to `null`.
            other_QC_column: an optional Boolean column of `other.obs`
                             indicating which cells passed QC. Can be a column
                             name, a polars expression, a polars Series, a 1D
                             NumPy array, or a function that takes in `other`
                             and returns a polars Series or 1D NumPy array. Set
                             to `None` to include all cells. Cells failing QC
                             will be ignored during the label transfer.
            Harmony_key: the key of `self.obsm` and `other.obsm` containing the
                         Harmony embeddings for each dataset
            cell_type_column: the name of a column to be added to `self.obs`
                              indicating each cell's most likely cell type,
                              i.e. the most common cell-type label among the
                              cell's `num_neighbors` nearest neighbors in
                              `other`
            confidence_column: the name of a column to be added to `self.obs`
                               indicating each cell's cell-type confidence,
                               i.e. the fraction of the cell's `num_neighbors`
                               nearest neighbors in `other` that share the most
                               common cell-type label. If multiple cell types
                               are equally common among the nearest neighbors,
                               tiebreak based on which of them is most common
                               in `original_cell_type_column`.
            next_best_cell_type_column: the name of a column to be added to
                                        `self.obs` indicating each cell's
                                        second-most likely cell type, i.e. the
                                        second-most common cell-type label
                                        among the cell's `num_neighbors`
                                        nearest neighbors in
                                        `original_cell_type_column`
            next_best_confidence_column: the name of a column to be added to
                                         `self.obs` indicating each cell's
                                         cell-type confidence, i.e. the
                                         fraction of the cell's `num_neighbors`
                                         nearest neighbors in
                                         `original_cell_type_column` that share
                                         the most common cell-type label. If
                                         multiple cell types are equally common
                                         among the nearest neighbors, tiebreak
                                         based on which of them is most common
                                         in `other`.
            num_neighbors: the number of nearest neighbors to use when
                           determining a cell's label. All cell-type
                           confidences will be multiples of
                           `1 / num_neighbors`.
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Called `nlist` internally by
                          faiss. Must be positive and less than the number of
                          cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells and 1% of the number of cells,
                          rounding up. The core of the heuristic,
                          `sqrt(num_cells)`, is on the order of the range
                          recommended by faiss, 4 to 16 times the square root
                          (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid this warning, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            num_probes: the number of nearest k-means clusters to search for a
                        given cell's nearest neighbors. Called `nprobe`
                        internally by faiss. Must be between 1 and
                        `num_clusters`, and should generally be a small
                        fraction of `num_clusters`. If `None`, will be set to
                        `min(num_clusters, 10)`.
            num_clustering_iterations: the maximum number of iterations of
                                       k-means clustering to perform before
                                       starting the nearest-neighbor search,
                                       stopping early if convergence is reached
            seed: the random seed to use when finding nearest neighbors
            overwrite: if `True`, overwrite `cell_type_column` and/or
                       `confidence_column` if already present in this dataset's
                       obs, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use for the nearest-neighbor
                         search. Set `num_threads=-1` to use all available
                         cores, as determined by `os.cpu_count()`, or leave
                         unset to use `self.num_threads` cores.

        Returns:
            `self`, but with two additional columns: `cell_type_column`,
            containing the transferred cell-type labels, and
            `confidence_column`, containing the cell-type confidences.
        """
        with ignore_sigint():
            import faiss
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `cell_type_column`, `confidence_column`,
        # `next_best_cell_type_column` and `next_best_confidence_column` are
        # strings and, unless `overwrite=True`, not already columns of
        # `self.obs`
        for column, column_name in (
                (cell_type_column, 'cell_type_column'),
                (confidence_column, 'confidence_column'),
                (next_best_cell_type_column, 'next_best_cell_type_column'),
                (next_best_confidence_column, 'next_best_confidence_column')):
            check_type(column, column_name, str, 'a string')
            if not overwrite and column in self._obs:
                error_message = (
                    f'{column_name} {column!r} is already a column '
                    f'of obs; did you already run label_transfer_from()? Set '
                    f'overwrite=True to overwrite.')
                raise ValueError(error_message)
        # Check that `other` is a SingleCell dataset
        check_type(other, 'other', SingleCell, 'a SingleCell dataset')
        # Get `QC_column` from `self` and `other_QC_column` from `other`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if other_QC_column is not None:
            other_QC_column = other._get_column(
                'obs', other_QC_column, 'other_QC_column', pl.Boolean,
                allow_missing=other_QC_column == 'passed_QC')
        # Get the number of cells in `self` and `other`
        num_cells_in_self = len(self._obs) if QC_column is None else \
            QC_column.sum()
        num_cells_in_other = len(other._obs) if other_QC_column is None else \
            other_QC_column.sum()
        # Get `original_cell_type_column` from `other`
        original_original_cell_type_column = original_cell_type_column
        original_cell_type_column = other._get_column(
            'obs', original_cell_type_column, 'original_cell_type_column',
            (pl.Categorical, pl.Enum, pl.String), QC_column=other_QC_column)
        # If `other_QC_column` was specified, filter the cell type labels in
        # `original_cell_type_column` to cells passing QC
        if other_QC_column is not None:
            original_cell_type_column = \
                original_cell_type_column.filter(other_QC_column)
        # Check that `original_cell_type_column` has at least two distinct cell
        # types
        most_common_cell_types = \
            original_cell_type_column.value_counts(sort=True).to_series()
        if len(most_common_cell_types) == 1:
            original_cell_type_column_description = \
                SingleCell._describe_column('original_cell_type_column',
                                            original_original_cell_type_column)
            error_message = (
                f'{original_cell_type_column_description} must have at least '
                f'two distinct cell types')
            if other_QC_column is not None:
                error_message += ' after filtering to cells passing QC'
            raise ValueError(error_message)
        # Check that `Harmony_key` is a string and in both `self.obsm` and
        # `other.obsm`
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        datasets = (self, 'self'), (other, 'other')
        for dataset, dataset_name in datasets:
            if Harmony_key not in dataset._obsm:
                error_message = (
                    f'Harmony_key {Harmony_key!r} is not a column of '
                    f'{dataset_name}.obs; did you forget to run harmonize() '
                    f'before label_transfer_from()?')
                raise ValueError(error_message)
        # Check that `num_neighbors` is a positive integer
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        check_bounds(num_neighbors, 'num_neighbors', 1)
        # Check that `num_clusters` is between 1 and `num_cells_in_other`, and
        # that `num_probes` is between 1 and `num_clusters`. If either is
        # `None`, set them to their default values.
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells_in_other),
                                           num_cells_in_other / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters <= num_cells_in_other:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and '
                    f'less than the number of cells in other '
                    f'({num_cells_in_other:,})')
                raise ValueError(error_message)
        if num_probes is None:
            num_probes = min(num_clusters, 10)
        else:
            check_type(num_probes, 'num_probes', int, 'a positive integer')
            if not 1 <= num_probes <= num_clusters:
                error_message = (
                    f'num_probes is {num_probes:,}, but must be ≥ 1 and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        # Check that `num_clustering_iterations` is a positive integer
        check_type(num_clustering_iterations, 'num_clustering_iterations', int,
                   'a positive integer')
        check_bounds(num_clustering_iterations, 'num_clustering_iterations', 1)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`. Set this as the number of threads for faiss.
        num_threads = self._process_num_threads(num_threads)
        faiss.omp_set_num_threads(num_threads)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Recode cell types so the most common is 0, the next-most common 1,
        # etc. This has the effect of breaking ties by taking the most common
        # cell type: we pick the first element in case of ties.
        cell_type_to_code = dict(zip(most_common_cell_types, range(
            len(most_common_cell_types))))
        original_cell_type_column = original_cell_type_column\
            .replace_strict(cell_type_to_code, return_dtype=pl.UInt32)
        # Get the Harmony embeddings for self and other
        self_Harmony_embeddings = self._obsm[Harmony_key] \
            if QC_column is None else \
            self._obsm[Harmony_key][QC_column.to_numpy()]
        other_Harmony_embeddings = other._obsm[Harmony_key] \
            if other_QC_column is None else \
            other._obsm[Harmony_key][other_QC_column.to_numpy()]
        dim = self_Harmony_embeddings.shape[1]
        if other_Harmony_embeddings.shape[1] != dim:
            error_message = (
                f"the two datasets' Harmony embeddings have different numbers "
                f"of dimensions ({dim:,} vs "
                f"{other_Harmony_embeddings.shape[1]:,}")
            raise ValueError(error_message)
        # Use faiss to get the indices of the num_neighbors nearest
        # neighbors in other for each cell in self
        quantizer = faiss.IndexFlatL2(dim)
        quantizer.verbose = verbose
        index = faiss.IndexIVFFlat(quantizer, dim, num_clusters)
        index.cp.seed = seed
        index.verbose = verbose
        index.cp.verbose = verbose
        index.cp.niter = num_clustering_iterations
        # noinspection PyArgumentList
        index.train(other_Harmony_embeddings)
        # noinspection PyArgumentList
        index.add(other_Harmony_embeddings)
        index.nprobe = num_probes
        # noinspection PyArgumentList
        nearest_neighbor_indices = \
            index.search(self_Harmony_embeddings, num_neighbors)[1]
        # Sometimes there aren't enough nearest neighbors for certain cells
        # with `num_probes` probes; if so, double `num_probes` (and threshold
        # to at most `num_clusters`), then re-run nearest-neighbor finding for
        # those cells
        needs_update = nearest_neighbor_indices[:, -1] == -1
        # noinspection PyUnresolvedReferences
        if needs_update.any():
            needs_update_X = self_Harmony_embeddings[needs_update]
            while True:
                num_probes = min(num_probes * 2, num_clusters)
                if verbose:
                    print(f'{len(needs_update_X):,} '
                          f'{plural("cell", len(needs_update_X))} '
                          f'({len(needs_update_X) / len(self._obs):.2f}%) '
                          f'did not have enough neighbors with '
                          f'{index.nprobe:,} probes; re-running '
                          f'nearest-neighbors finding for these cells with '
                          f'{num_probes:,} probes')
                index.nprobe = num_probes
                # noinspection PyArgumentList
                new_indices = index.search(needs_update_X, num_neighbors)[1]
                nearest_neighbor_indices[needs_update] = new_indices
                still_needs_update = new_indices[:, -1] == -1
                # noinspection PyUnresolvedReferences
                if not still_needs_update.any():
                    break
                # noinspection PyUnresolvedReferences
                needs_update[needs_update] = still_needs_update
                needs_update_X = needs_update_X[still_needs_update]
        # Get the cell-type labels of these nearest neighbors (using our
        # integer encoding where the most common cell type is 0, the next-most
        # common 1, etc.)
        nearest_neighbor_cell_types = \
            original_cell_type_column.to_numpy()[nearest_neighbor_indices]
        # Get the two most common cell types for each cell in `self` among its
        # `num_neighbors` nearest neighbors in `other`. Pick the first element
        # in case of ties, which according to our encoding is the most common
        # cell type. Also get the cell-type confidence of these two cell types,
        # i.e. their frequency among the nearest neighbors.
        cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        confidences = np.empty(num_cells_in_self, dtype=np.float32)
        next_best_cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        next_best_confidences = np.empty(num_cells_in_self, dtype=np.float32)
        cython_inline(_uninitialized_vector_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.algorithm cimport fill
        from libcpp.vector cimport vector
        
        def label_transfer(const unsigned[:, ::1] nearest_neighbor_cell_types,
                           const unsigned num_cell_types,
                           unsigned[::1] cell_types,
                           float[::1] confidences,
                           unsigned[::1] next_best_cell_types,
                           float[::1] next_best_confidences,
                           const unsigned num_threads):
            cdef unsigned i, j, thread_index, cell_type, count, \
                most_common_cell_type, second_most_common_cell_type, \
                max_count, second_max_count, \
                num_cells = nearest_neighbor_cell_types.shape[0], \
                num_neighbors = nearest_neighbor_cell_types.shape[1]
            cdef float inv_num_neighbors = 1.0 / num_neighbors
            cdef uninitialized_vector[unsigned] counts
            cdef vector[uninitialized_vector[unsigned]] thread_counts
            
            if num_threads == 1:
                counts.resize(num_cell_types)
                for i in range(num_cells):
                    fill(counts.begin(), counts.end(), 0)
                    for j in range(num_neighbors):
                        cell_type = nearest_neighbor_cell_types[i, j]
                        counts[cell_type] += 1
                    if counts[0] >= counts[1]:
                        max_count = counts[0]
                        second_max_count = counts[1]
                        most_common_cell_type = 0
                        second_most_common_cell_type = 1
                    else:
                        max_count = counts[1]
                        second_max_count = counts[0]
                        most_common_cell_type = 1
                        second_most_common_cell_type = 0
                    for cell_type in range(2, num_cell_types):
                        count = counts[cell_type]
                        if count > max_count:
                            second_max_count = max_count
                            second_most_common_cell_type = \
                                most_common_cell_type
                            max_count = count
                            most_common_cell_type = cell_type
                        elif count > second_max_count:
                            second_max_count = count
                            second_most_common_cell_type = cell_type
                    cell_types[i] = most_common_cell_type
                    confidences[i] = max_count * inv_num_neighbors
                    next_best_cell_types[i] = \
                        second_most_common_cell_type
                    next_best_confidences[i] = \
                        second_max_count * inv_num_neighbors
            else:
                thread_counts.resize(num_threads)
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    thread_counts[thread_index].resize(num_cell_types)
                    for i in prange(num_cells):
                        fill(thread_counts[thread_index].begin(),
                             thread_counts[thread_index].end(), 0)
                        for j in range(num_neighbors):
                            cell_type = nearest_neighbor_cell_types[i, j]
                            thread_counts[thread_index][cell_type] += 1
                        if thread_counts[thread_index][0] >= \
                                thread_counts[thread_index][1]:
                            max_count = thread_counts[thread_index][0]
                            second_max_count = thread_counts[thread_index][1]
                            most_common_cell_type = 0
                            second_most_common_cell_type = 1
                        else:
                            max_count = thread_counts[thread_index][1]
                            second_max_count = thread_counts[thread_index][0]
                            most_common_cell_type = 1
                            second_most_common_cell_type = 0
                        for cell_type in range(2, num_cell_types):
                            count = thread_counts[thread_index][cell_type]
                            if count > max_count:
                                second_max_count = max_count
                                second_most_common_cell_type = \
                                    most_common_cell_type
                                max_count = count
                                most_common_cell_type = cell_type
                            elif count > second_max_count:
                                second_max_count = count
                                second_most_common_cell_type = cell_type
                        cell_types[i] = most_common_cell_type
                        confidences[i] = max_count * inv_num_neighbors
                        next_best_cell_types[i] = \
                            second_most_common_cell_type
                        next_best_confidences[i] = \
                            second_max_count * inv_num_neighbors
            ''')['label_transfer'](
                nearest_neighbor_cell_types=nearest_neighbor_cell_types,
                num_cell_types=len(cell_type_to_code), cell_types=cell_types,
                confidences=confidences,
                next_best_cell_types=next_best_cell_types,
                next_best_confidences=next_best_confidences,
                num_threads=num_threads)
        # Map the cell-type codes back to their labels by constructing a polars
        # Series from the codes, then casting it to an Enum. Also convert
        # cell-type confidences to Series.
        cell_types = pl.Series(cell_type_column, cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        confidences = pl.Series(confidence_column, confidences)
        next_best_cell_types = \
            pl.Series(next_best_cell_type_column, next_best_cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        next_best_confidences = \
            pl.Series(next_best_confidence_column, next_best_confidences)
        # Add the four columns to `self.obs`. If `QC_column` was specified,
        # back-project from QCed cells to all cells, filling with `null`.
        columns = cell_types, confidences, next_best_cell_types, \
            next_best_confidences
        if QC_column is None:
            obs = self._obs.with_columns(columns)
        else:
            # noinspection PyTypeChecker
            expand = lambda series: pl.when(QC_column.name)\
                .then(pl.lit(series).gather(pl.col(QC_column.name).cum_sum()
                                            .cast(pl.Int32) - 1))
            obs = self._obs.with_columns(map(expand, columns))
        # Return a new SingleCell dataset containing the cell-type labels and
        # confidences
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def label_transfer_from(
            self,
            other: SingleCell,
            original_cell_type_column: SingleCellColumn,
            *,
            QC_column: SingleCellColumn | None = 'passed_QC',
            other_QC_column: SingleCellColumn | None = 'passed_QC',
            Harmony_key: str = 'Harmony_PCs',
            cell_type_column: str = 'cell_type',
            confidence_column: str | None = None,
            next_best: bool = False,
            next_best_cell_type_column: str | None = None,
            next_best_confidence_column: str | None = None,
            num_neighbors: int | np.integer = 20,
            num_clusters: int | np.integer | None = None,
            min_clusters_searched: int | np.integer | None = None,
            max_clusters_searched: int | np.integer | None = None,
            num_candidates_per_neighbor: int | np.integer = 10,
            num_kmeans_iterations: int | np.integer = 10,
            kmeans_barbar: bool = False,
            num_init_iterations: int | np.integer = 5,
            oversampling_factor: int | np.integer | float | np.floating = 1,
            chunk_size: int | np.integer | None = None,
            seed: int | np.integer = 0,
            overwrite: bool = False,
            verbose: bool = True,
            num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Transfer cell-type labels from another dataset to this one, using the
        two datasets' Harmony embeddings from `harmonize()`.
        
        For each cell in `self`, the transferred cell-type label is the most
        common cell-type label among the `num_neighbors` cells in `other` with
        the nearest Harmony embeddings. The cell-type confidence is the
        fraction of these neighbors that share this most common cell-type
        label.
        
        The nearest-neighbor search is conducted using the same method as
        `neighbors()`, with one crucial difference: whereas `neighbors()`
        searches for a cell's nearest neighbors in its own dataset, this
        function searches for a cell's nearest neighbors in another dataset,
        `other`.
        
        Args:
            other: the dataset to transfer cell-type labels from
            original_cell_type_column: a column of `other.obs` containing
                                       cell-type labels. Can be a column name,
                                       a polars expression, a polars Series, a
                                       1D NumPy array, or a function that takes
                                       in `other` and returns a polars Series
                                       or 1D NumPy array.
            QC_column: an optional Boolean column of `self.obs` indicating
                       which cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in `self` and returns a polars
                       Series or 1D NumPy array. Set to `None` to include all
                       cells. Cells failing QC will have their cell-type labels
                       and confidences set to `null`.
            other_QC_column: an optional Boolean column of `other.obs`
                             indicating which cells passed QC. Can be a column
                             name, a polars expression, a polars Series, a 1D
                             NumPy array, or a function that takes in `other`
                             and returns a polars Series or 1D NumPy array. Set
                             to `None` to include all cells. Cells failing QC
                             will be ignored during the label transfer.
            Harmony_key: the key of `self.obsm` and `other.obsm` containing the
                         Harmony embeddings for each dataset
            cell_type_column: the name of a column to be added to `self.obs`
                              indicating each cell's most likely cell type,
                              i.e. the most common cell-type label among the
                              cell's `num_neighbors` nearest neighbors in
                              `other`
            confidence_column: the name of a column to be added to `self.obs`
                               indicating each cell's cell-type confidence,
                               i.e. the fraction of the cell's `num_neighbors`
                               nearest neighbors in `other` that share the most
                               common cell-type label. If multiple cell types
                               are equally common among the nearest neighbors,
                               tiebreak based on which of them is most common
                               in `original_cell_type_column`. If `None`,
                               defaults to `f'{cell_type_column}_confidence'`.
            next_best: whether to also compute each cell's second-most likely
                       cell type and confidence, or just its most likely
            next_best_cell_type_column: the name of a column to be added to
                                        `self.obs` indicating each cell's
                                        second-most likely cell type, i.e. the
                                        second-most common cell-type label
                                        among the cell's `num_neighbors`
                                        nearest neighbors in
                                        `original_cell_type_column`. If `None`,
                                        defaults to
                                        `f'next_best_{cell_type_column}'`. Can
                                        only be specified when
                                        `next_best=True`.
            next_best_confidence_column: the name of a column to be added to
                                         `self.obs` indicating each cell's
                                         cell-type confidence, i.e. the
                                         fraction of the cell's `num_neighbors`
                                         nearest neighbors in
                                         `original_cell_type_column` that share
                                         the most common cell-type label. If
                                         multiple cell types are equally common
                                         among the nearest neighbors, tiebreak
                                         based on which of them is most common
                                         in `other`. If `None`, defaults to
                                         `f'next_best_{cell_type_column}_
                                         confidence'`. Can only be specified
                                         when `next_best=True`.
            num_neighbors: the number of nearest neighbors to use when
                           determining a cell's label. All cell-type
                           confidences will be multiples of
                           `1 / num_neighbors`.
            num_clusters: the number of k-means clusters to use during the
                          nearest-neighbor search. Must be less than the number
                          of cells. If `None`, will be set to
                          `ceil(min(sqrt(num_cells), num_cells / 100))`
                          clusters, i.e. the minimum of the square root of the
                          number of cells in `other` and 1% of the number of
                          cells in `other`, rounding up. The core of the
                          heuristic, `sqrt(num_cells)`, is on the order of the
                          range recommended by faiss, 4 to 16 times the square
                          root (github.com/facebookresearch/faiss/wiki/
                          Guidelines-to-choose-an-index). However, faiss also
                          recommends using between 39 and 256 data points per
                          centroid when training the k-means clustering used
                          in the k-nearest neighbors search. If there are more
                          than 256, the dataset is automatically subsampled
                          for the k-means step, but if there are fewer than 39,
                          faiss gives a warning. To avoid going below 39, we
                          switch to using `num_cells / 100` centroids for small
                          datasets, since 100 is the midpoint of 39 and 256 in
                          log space.
            min_clusters_searched: the minimum number of a cell's nearest
                                   clusters to search; must be between 1 and
                                   `max_clusters_searched`. Defaults to
                                   `min(10, num_neighbors, num_clusters)`.
            max_clusters_searched: the maximum number of a cell's nearest
                                   clusters to search; must be at least
                                   `num_neighbors` (to guarantee that each
                                   cell has enough nearest-neighbor candidates
                                   even in the worst-case scenario where all
                                   the nearest clusters contain just one cell)
                                   and at most `num_clusters`. Defaults to
                                   `min(num_neighbors, num_clusters)`.
            num_candidates_per_neighbor: the target number of nearest-neighbor
                                         candidates (cells) to search per
                                         neighbor requested. The true number of
                                         candidates searched may be either
                                         lower or higher than `num_neighbors *
                                         num_candidates_per_neighbor`: lower
                                         when the `num_neighbors` nearest
                                         clusters do not have enough candidates
                                         (in the worst-case scenario where all
                                         the nearest clusters contain just one
                                         cell, only `num_neighbors` cells will
                                         be searched and every searched cell
                                         will be a nearest neighbor), and
                                         higher when the final cluster searched
                                         puts the total number of candidates
                                         slightly over this value, because
                                         clusters are always searched in their
                                         entirety.
            num_kmeans_iterations: the maximum number of iterations of
                                   k-means clustering to perform before
                                   starting the nearest-neighbor search,
                                   stopping early if convergence is reached
            kmeans_barbar: whether to use k-means|| initialization (a parallel
                           version of k-means++ from arxiv.org/abs/1203.6402)
                           to initialize the k-means clustering centroids,
                           instead of random initialization
            num_init_iterations: the number of k-means|| iterations used to
                                 initialize the k-means clustering that
                                 constitutes the first step of Harmony.
                                 k-means|| is a parallel version of the
                                 widely used k-means++ initialization scheme
                                 for k-means clustering. The default value of 5
                                 is recommended by the k-means|| paper
                                 (arxiv.org/abs/1203.6402). Only used when
                                 `kmeans_barbar=True`.
            oversampling_factor: the number of candidate centroids selected, on
                                 average, at each of the `num_init_iterations`
                                 iterations of k-means||, as a multiple of
                                 `num_clusters`. The default value of 1 is the
                                 midpoint (in log space) of the values explored
                                 by the k-means|| paper
                                 (arxiv.org/abs/1203.6402), namely 0.1 to 10.
                                 The total number of candidate centroids
                                 selected, on average, will be
                                 `oversampling_factor * num_clusters + 1`, from
                                 which the final `num_clusters` centroids will
                                 then be selected via k-means++. Only used when
                                 `kmeans_barbar=True`.
            chunk_size: the chunk size to use for distance calculations in the
                        initial k-means clustering and in the nearest-neighbor
                        search itself. Setting this to a power of 2 is
                        recommended; 256 was determined to be the optimal chunk
                        size by scikit-learn (for k-means clustering) and in
                        our tests (for both steps). Defaults to
                        `min(256, number of cells in other)`.
            seed: the random seed to use when finding nearest neighbors
            overwrite: if `True`, overwrite `cell_type_column` and/or
                       `confidence_column` if already present in this dataset's
                       obs, instead of raising an error
            verbose: whether to print details of the nearest-neighbor search
            num_threads: the number of threads to use for the nearest-neighbor
                         search and label transfer. Set `num_threads=-1` to use
                         all available cores, as determined by
                         `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the returned
                         SingleCell dataset's `num_threads`; this will always
                         be the same as the original dataset's `num_threads`.
        
        Returns:
            `self`, but with two columns added to `obs`: `cell_type_column`,
            containing the transferred cell-type labels, and
            `confidence_column`, containing the cell-type confidences. If
            `next_best=True`, also adds the columns
            `next_best_cell_type_column` and `next_best_confidence_column`,
            containing the second-most likely cell type and its confidence.
        """
        # Check that `cell_type_column` is a string
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        
        # Check that `confidence_column` is a string or `None`; if `None`, set 
        # to `f'{cell_type_column}_confidence'`. 
        if confidence_column is None:
            confidence_column = f'{cell_type_column}_confidence'
        else:
            check_type(confidence_column, 'confidence_column', str,
                       'a string or None')

        # Check that `next_best` is Boolean
        check_type(next_best, 'next_best', bool, 'Boolean')
        
        # If `next_best=False`, check that `next_best_cell_type_column` and 
        # `next_best_confidence_column` are `None`. If `next_best=True`, check
        # that they are strings or `None`; if `None, set to 
        # `f'next_best_{cell_type_column}'` and 
        # `f'next_best_{cell_type_column}_confidence'` respectively. 
        if next_best:
            if next_best_cell_type_column is None:
                next_best_cell_type_column = f'next_best_{cell_type_column}'
            else:
                check_type(next_best_cell_type_column,
                           'next_best_cell_type_column', str,
                           'a string or None')
            if next_best_confidence_column is None:
                next_best_confidence_column = \
                    f'next_best_{cell_type_column}_confidence'
            else:
                check_type(next_best_confidence_column, 
                           'next_best_confidence_column', str, 
                           'a string or None')
        elif next_best_cell_type_column is not None:
            error_message = \
                'next_best_cell_type_column must be None unless next_best=True'
            raise ValueError(error_message)
        elif next_best_confidence_column is not None:
            error_message = (
                'next_best_confidence_column must be None unless '
                'next_best=True')
            raise ValueError(error_message)
            
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # If `overwrite=False`, check that `cell_type_column` and 
        # `confidence_column` (and `next_best_cell_type_column` and 
        # `next_best_confidence_column`, if `next_best=True`) are not already 
        # columns of `self.obs`
        if not overwrite:
            for column, column_name in (
                    (cell_type_column, 'cell_type_column'),
                    (confidence_column, 'confidence_column')):
                if column in self._obs:
                    error_message = (
                        f'{column_name} {column!r} is already a column '
                        f'of obs; did you already run label_transfer_from()? '
                        f'Set overwrite=True to overwrite.')
                    raise ValueError(error_message)
            for column, column_name in (
                    (next_best_cell_type_column, 'next_best_cell_type_column'),
                    (next_best_confidence_column, 
                     'next_best_confidence_column')):
                if column in self._obs:
                    error_message = (
                        f'{column_name} {column!r} is already a column '
                        f'of obs; did you already run label_transfer_from()? '
                        f'Set overwrite=True to overwrite, or next_best=False '
                        f'to not compute {column_name}.')
                    raise ValueError(error_message)
        
        # Check that `other` is a SingleCell dataset
        check_type(other, 'other', SingleCell, 'a SingleCell dataset')
        
        # Get `QC_column` from `self` and `other_QC_column` from `other`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        if other_QC_column is not None:
            other_QC_column = other._get_column(
                'obs', other_QC_column, 'other_QC_column', pl.Boolean,
                allow_missing=other_QC_column == 'passed_QC')
        
        # Get the number of cells in `self` and `other`
        num_cells_in_self = len(self._obs) if QC_column is None else \
            QC_column.sum()
        num_cells_in_other = len(other._obs) if other_QC_column is None else \
            other_QC_column.sum()
        
        # Get `original_cell_type_column` from `other`
        original_original_cell_type_column = original_cell_type_column
        original_cell_type_column = other._get_column(
            'obs', original_cell_type_column, 'original_cell_type_column',
            (pl.Categorical, pl.Enum, pl.String), QC_column=other_QC_column)
        
        # If `other_QC_column` was specified, filter the cell type labels in
        # `original_cell_type_column` to cells passing QC
        if other_QC_column is not None:
            original_cell_type_column = \
                original_cell_type_column.filter(other_QC_column)
        
        # Check that `original_cell_type_column` has at least two distinct cell
        # types
        most_common_cell_types = \
            original_cell_type_column.value_counts(sort=True).to_series()
        if len(most_common_cell_types) == 1:
            original_cell_type_column_description = \
                SingleCell._describe_column('original_cell_type_column',
                                            original_original_cell_type_column)
            error_message = (
                f'{original_cell_type_column_description} must have at least '
                f'two distinct cell types')
            if other_QC_column is not None:
                error_message += ' after filtering to cells passing QC'
            raise ValueError(error_message)
        
        # Check that `Harmony_key` is a string and in both `self.obsm` and
        # `other.obsm`
        check_type(Harmony_key, 'Harmony_key', str, 'a string')
        datasets = (self, 'self'), (other, 'other')
        for dataset, dataset_name in datasets:
            if Harmony_key not in dataset._obsm:
                error_message = (
                    f'Harmony_key {Harmony_key!r} is not a column of '
                    f'{dataset_name}.obs; did you forget to run harmonize() '
                    f'before label_transfer_from()?')
                raise ValueError(error_message)
        
        # Check that `num_neighbors` is between 1 and `num_cells_in_other - 1`
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        if not 1 <= num_neighbors < num_cells_in_other:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be ≥ 1 and '
                f'less than the number of cells in other '
                f'({num_cells_in_other:,})')
            raise ValueError(error_message)
        
        # Check that `num_clusters` is between 1 and `num_cells_in_other`; if
        # `None`, set to `ceil(min(sqrt(num_cells_in_other),
        # num_cells_in_other / 100)))`
        if num_clusters is None:
            num_clusters = int(np.ceil(min(np.sqrt(num_cells_in_other),
                                           num_cells_in_other / 100)))
        else:
            check_type(num_clusters, 'num_clusters', int, 'a positive integer')
            if not 1 <= num_clusters <= num_cells_in_other:
                error_message = (
                    f'num_clusters is {num_clusters:,}, but must be ≥ 1 and ≤ '
                    f'the number of cells in other ({num_cells_in_other:,})')
                raise ValueError(error_message)
        
        # Check that `max_clusters_searched` is between `num_neighbors` and
        # `num_clusters`; if `None`, set to `min(num_neighbors, num_clusters)`
        if max_clusters_searched is None:
            max_clusters_searched = min(num_neighbors, num_clusters)
        else:
            check_type(max_clusters_searched, 'max_clusters_searched', int,
                       'a positive integer')
            if not num_neighbors <= max_clusters_searched <= num_clusters:
                error_message = (
                    f'max_clusters_searched is {max_clusters_searched:,}, but '
                    f'must be ≥ num_neighbors ({num_neighbors:,}) and ≤ '
                    f'num_clusters ({num_clusters:,})')
                raise ValueError(error_message)
        
        # Check that `min_clusters_searched` is between 1 and
        # `max_clusters_searched`; if `None`, set to
        # `min(10, num_neighbors, num_clusters)`
        if min_clusters_searched is None:
            min_clusters_searched = min(10, num_neighbors, num_clusters)
        else:
            check_type(min_clusters_searched, 'min_clusters_searched', int,
                       'a positive integer')
            if not 1 <= min_clusters_searched <= max_clusters_searched:
                error_message = (
                    f'min_clusters_searched is {min_clusters_searched:,}, but '
                    f'must be ≥ 1 and ≤ max_clusters_searched '
                    f'({max_clusters_searched:,})')
                raise ValueError(error_message)
        
        # Check that `num_candidates_per_neighbor` is between 1 and
        # `num_cells_in_other - 1`
        check_type(num_candidates_per_neighbor, 'num_candidates_per_neighbor',
                   int, 'a positive integer')
        if not 1 <= num_candidates_per_neighbor < num_cells_in_other:
            error_message = (
                f'num_candidates_per_neighbor is '
                f'{num_candidates_per_neighbor:,}, but must be ≥ 1 and less '
                f'than the number of cells in other ({num_cells_in_other:,})')
            raise ValueError(error_message)
        
        # Check that `num_kmeans_iterations` is a positive integer
        check_type(num_kmeans_iterations, 'num_kmeans_iterations', int,
                   'a positive integer')
        
        # Check that `kmeans_barbar` is Boolean
        check_type(kmeans_barbar, 'kmeans_barbar', bool, 'Boolean')
        
        # Check that `num_init_iterations` is a positive integer
        check_type(num_init_iterations, 'num_init_iterations', int,
                   'a positive integer')
        check_bounds(num_init_iterations, 'num_init_iterations', 1)
        
        # Check that `oversampling_factor` is a positive number
        check_type(oversampling_factor, 'oversampling_factor', (int, float),
                   'a positive number')
        check_bounds(oversampling_factor, 'oversampling_factor', 0,
                     left_open=True)
        
        # If `kmeans_barbar=False`, check that `num_init_iterations` and
        # `oversampling_factor` have their default values
        if not kmeans_barbar:
            if num_init_iterations != 5:
                error_message = (
                    'num_init_iterations can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
            if oversampling_factor != 1:
                error_message = (
                    'oversampling_factor can only be specified when '
                    'kmeans_barbar=True')
                raise ValueError(error_message)
        
        # If `chunk_size` is `None`, set it to `min(256, num_cells_in_other)`.
        # Otherwise, check that it is between 1 and `num_cells_in_other`.
        if chunk_size is None:
            chunk_size = min(256, num_cells_in_other)
        else:
            check_type(chunk_size, 'chunk_size', int, 'a positive integer')
            if not 1 <= chunk_size <= num_cells_in_other:
                error_message = (
                    f'chunk_size is {chunk_size:,}, but must be ≥ 1 and ≤ the '
                    f'number of cells in other ({num_cells_in_other:,})')
                raise ValueError(error_message)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Recode cell types so the most common is 0, the next-most common 1,
        # etc. This has the effect of breaking ties by taking the most common
        # cell type: we pick the first element in case of ties.
        cell_type_to_code = dict(zip(most_common_cell_types, range(
            len(most_common_cell_types))))
        original_cell_type_column = original_cell_type_column\
            .replace_strict(cell_type_to_code, return_dtype=pl.UInt32)
        
        # Get the Harmony embeddings for self and other; check that they are
        # float32 and C-contiguous and have the same width
        self_Harmony_embeddings = self._obsm[Harmony_key] \
            if QC_column is None else \
            self._obsm[Harmony_key][QC_column.to_numpy()]
        other_Harmony_embeddings = other._obsm[Harmony_key] \
            if other_QC_column is None else \
            other._obsm[Harmony_key][other_QC_column.to_numpy()]
        if self_Harmony_embeddings.dtype != np.float32:
            error_message = (
                f'obsm[{Harmony_key!r}].dtype is '
                f'{self_Harmony_embeddings.dtype!r}, but must be float32')
            raise TypeError(error_message)
        if not self_Harmony_embeddings.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{Harmony_key!r}] is not C-contiguous; make it '
                f'C-contiguous with '
                f'pipe_obsm_key({Harmony_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        if other_Harmony_embeddings.dtype != np.float32:
            error_message = (
                f'other.obsm[{Harmony_key!r}].dtype is '
                f'{other_Harmony_embeddings.dtype!r}, but must be float32')
            raise TypeError(error_message)
        if not other_Harmony_embeddings.flags['C_CONTIGUOUS']:
            error_message = (
                f'other.obsm[{Harmony_key!r}] is not C-contiguous; make it '
                f'C-contiguous with '
                f'pipe_obsm_key({Harmony_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        num_dimensions = self_Harmony_embeddings.shape[1]
        if other_Harmony_embeddings.shape[1] != num_dimensions:
            error_message = (
                f"the two datasets' Harmony embeddings have different numbers "
                f"of dimensions ({num_dimensions:,} vs "
                f"{other_Harmony_embeddings.shape[1]:,}")
            raise ValueError(error_message)
        
        # Run k-means clustering on `other_Harmony_embeddings`. Since
        # `centroids` and `centroids_new` are swapped every iteration,
        # `centroids_new` will contain the final centroids when doing an odd
        # number of k-means iterations; if so, swap them at the end.
        num_other = len(other_Harmony_embeddings)
        cluster_labels = np.empty(num_other, dtype=np.uint32)
        centroids = np.empty((num_clusters, num_dimensions), dtype=np.float32)
        centroids_new = np.empty((num_clusters, num_dimensions),
                                 dtype=np.float32)
        num_cells_per_cluster = np.empty(num_clusters, dtype=np.uint32)
        with threadpool_limits(1, user_api='blas'):
            kmeans = _kmeans_and_knn_functions['kmeans']
            kmeans(X=other_Harmony_embeddings, cluster_labels=cluster_labels,
                   centroids=centroids, centroids_new=centroids_new,
                   num_cells_per_cluster=num_cells_per_cluster,
                   kmeans_barbar=kmeans_barbar,
                   num_init_iterations=num_init_iterations,
                   oversampling_factor=oversampling_factor,
                   num_kmeans_iterations=num_kmeans_iterations, seed=seed,
                   chunk_size=chunk_size, num_threads=num_threads)
        if num_kmeans_iterations & 1:
            centroids = centroids_new
        del centroids_new
        
        # Find the `num_neighbors` nearest neighbors in `other` of each cell in
        # `self`, according to `self_Harmony_embeddings` and
        # `other_Harmony_embeddings`
        num_self = len(self_Harmony_embeddings)
        neighbors = np.empty((num_self, num_neighbors), dtype=np.uint32)
        distances = np.empty((num_self, num_neighbors), dtype=np.float32)
        with threadpool_limits(1, user_api='blas'):
            knn_cross = _kmeans_and_knn_functions['knn_cross']
            knn_cross(Y=self_Harmony_embeddings, X=other_Harmony_embeddings,
                      cluster_labels=cluster_labels, centroids=centroids,
                      num_cells_per_cluster=num_cells_per_cluster,
                      neighbors=neighbors, distances=distances,
                      num_neighbors=num_neighbors,
                      min_clusters_searched=min_clusters_searched,
                      max_clusters_searched=max_clusters_searched,
                      num_candidates_per_neighbor=num_candidates_per_neighbor,
                      chunk_size=chunk_size, num_threads=num_threads)
        
        # Get the (two) most common cell type(s) for each cell in `self` among
        # its `num_neighbors` nearest neighbors in `other`. Pick the first
        # element in case of ties, which according to our encoding is the most
        # common cell type. Also get the cell-type confidence of each cell's
        # type(s), i.e. the frequency of the cell type among the cell's
        # `num_neighbors` nearest neighbors.
        cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
        confidences = np.empty(num_cells_in_self, dtype=np.float32)
        if next_best:
            next_best_cell_types = np.empty(num_cells_in_self, dtype=np.uint32)
            next_best_confidences = \
                np.empty(num_cells_in_self, dtype=np.float32)
        else:
            next_best_cell_types = np.array([], dtype=np.uint32)
            next_best_confidences = np.array([], dtype=np.float32)
        cython_inline(_uninitialized_vector_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libcpp.algorithm cimport fill
        from libcpp.vector cimport vector
        
        def label_transfer(const unsigned[:, ::1] neighbors,
                           const unsigned[::1] original_cell_type_column,
                           const unsigned num_cell_types,
                           unsigned[::1] cell_types,
                           float[::1] confidences,
                           unsigned[::1] next_best_cell_types,
                           float[::1] next_best_confidences,
                           unsigned num_threads):
            cdef unsigned i, j, thread_index, cell_type, count, \
                most_common_cell_type, second_most_common_cell_type, \
                max_count, second_max_count, \
                num_cells = neighbors.shape[0], \
                num_neighbors = neighbors.shape[1]
            cdef float inv_num_neighbors = 1.0 / num_neighbors
            cdef uninitialized_vector[unsigned] counts_buffer
            cdef vector[uninitialized_vector[unsigned]] thread_counts
            cdef unsigned[::1] counts
            
            num_threads = min(num_threads, num_cells)
            if next_best_cell_types.shape[0] == 0:  # `next_best=False`
                if num_threads == 1:
                    counts_buffer.resize(num_cell_types)
                    counts = <unsigned[:num_cell_types]> counts_buffer.data()
                    for i in range(num_cells):
                        counts[:] = 0
                        for j in range(num_neighbors):
                            # Get the cell-type label of this nearest neighbor (using
                            # our integer encoding where the most common cell type is
                            # 0, the next-most common 1, etc.)
                            cell_type = original_cell_type_column[neighbors[i, j]]
                            counts[cell_type] += 1
                        if counts[0] >= counts[1]:
                            max_count = counts[0]
                            most_common_cell_type = 0
                        else:
                            max_count = counts[1]
                            most_common_cell_type = 1
                        for cell_type in range(2, num_cell_types):
                            count = counts[cell_type]
                            if count > max_count:
                                max_count = count
                                most_common_cell_type = cell_type
                        cell_types[i] = most_common_cell_type
                        confidences[i] = max_count * inv_num_neighbors
                else:
                    thread_counts.resize(num_threads)
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_counts[thread_index].resize(num_cell_types)
                        for i in prange(num_cells):
                            fill(thread_counts[thread_index].begin(),
                                 thread_counts[thread_index].end(), 0)
                            for j in range(num_neighbors):
                                cell_type = original_cell_type_column[neighbors[i, j]]
                                thread_counts[thread_index][cell_type] += 1
                            if thread_counts[thread_index][0] >= \
                                    thread_counts[thread_index][1]:
                                max_count = thread_counts[thread_index][0]
                                most_common_cell_type = 0
                            else:
                                max_count = thread_counts[thread_index][1]
                                most_common_cell_type = 1
                            for cell_type in range(2, num_cell_types):
                                count = thread_counts[thread_index][cell_type]
                                if count > max_count:
                                    max_count = count
                                    most_common_cell_type = cell_type
                            cell_types[i] = most_common_cell_type
                            confidences[i] = max_count * inv_num_neighbors
            else:  # `next_best=True`; also compute the next-best cell type/confidence
                if num_threads == 1:
                    counts_buffer.resize(num_cell_types)
                    counts = <unsigned[:num_cell_types]> counts_buffer.data()
                    for i in range(num_cells):
                        counts[:] = 0
                        for j in range(num_neighbors):
                            # Get the cell-type label of this nearest neighbor (using
                            # our integer encoding where the most common cell type is
                            # 0, the next-most common 1, etc.)
                            cell_type = original_cell_type_column[neighbors[i, j]]
                            counts[cell_type] += 1
                        if counts[0] >= counts[1]:
                            max_count = counts[0]
                            second_max_count = counts[1]
                            most_common_cell_type = 0
                            second_most_common_cell_type = 1
                        else:
                            max_count = counts[1]
                            second_max_count = counts[0]
                            most_common_cell_type = 1
                            second_most_common_cell_type = 0
                        for cell_type in range(2, num_cell_types):
                            count = counts[cell_type]
                            if count > max_count:
                                second_max_count = max_count
                                second_most_common_cell_type = \
                                    most_common_cell_type
                                max_count = count
                                most_common_cell_type = cell_type
                            elif count > second_max_count:
                                second_max_count = count
                                second_most_common_cell_type = cell_type
                        cell_types[i] = most_common_cell_type
                        confidences[i] = max_count * inv_num_neighbors
                        next_best_cell_types[i] = \
                            second_most_common_cell_type
                        next_best_confidences[i] = \
                            second_max_count * inv_num_neighbors
                else:
                    thread_counts.resize(num_threads)
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_counts[thread_index].resize(num_cell_types)
                        for i in prange(num_cells):
                            fill(thread_counts[thread_index].begin(),
                                 thread_counts[thread_index].end(), 0)
                            for j in range(num_neighbors):
                                cell_type = original_cell_type_column[neighbors[i, j]]
                                thread_counts[thread_index][cell_type] += 1
                            if thread_counts[thread_index][0] >= \
                                    thread_counts[thread_index][1]:
                                max_count = thread_counts[thread_index][0]
                                second_max_count = thread_counts[thread_index][1]
                                most_common_cell_type = 0
                                second_most_common_cell_type = 1
                            else:
                                max_count = thread_counts[thread_index][1]
                                second_max_count = thread_counts[thread_index][0]
                                most_common_cell_type = 1
                                second_most_common_cell_type = 0
                            for cell_type in range(2, num_cell_types):
                                count = thread_counts[thread_index][cell_type]
                                if count > max_count:
                                    second_max_count = max_count
                                    second_most_common_cell_type = \
                                        most_common_cell_type
                                    max_count = count
                                    most_common_cell_type = cell_type
                                elif count > second_max_count:
                                    second_max_count = count
                                    second_most_common_cell_type = cell_type
                            cell_types[i] = most_common_cell_type
                            confidences[i] = max_count * inv_num_neighbors
                            next_best_cell_types[i] = \
                                second_most_common_cell_type
                            next_best_confidences[i] = \
                                second_max_count * inv_num_neighbors
            ''')['label_transfer'](
                neighbors=neighbors,
                original_cell_type_column=original_cell_type_column.to_numpy(),
                num_cell_types=len(cell_type_to_code), cell_types=cell_types,
                confidences=confidences,
                next_best_cell_types=next_best_cell_types,
                next_best_confidences=next_best_confidences,
                num_threads=num_threads)
       
        # Map the cell-type codes back to their labels by constructing a polars
        # Series from the codes, then casting it to an Enum. Also convert
        # cell-type confidences to Series.
        cell_types = pl.Series(cell_type_column, cell_types)\
            .cast(pl.Enum(most_common_cell_types.to_list()))
        confidences = pl.Series(confidence_column, confidences)
        if next_best:
            next_best_cell_types = \
                pl.Series(next_best_cell_type_column, next_best_cell_types)\
                .cast(pl.Enum(most_common_cell_types.to_list()))
            next_best_confidences = \
                pl.Series(next_best_confidence_column, next_best_confidences)
            columns = cell_types, confidences, next_best_cell_types, \
                next_best_confidences
        else:
            columns = cell_types, confidences
        
        # Add the cell-type labels and confidences to `self.obs`. If
        # `QC_column` was specified, back-project from QCed cells to all cells,
        # filling with `null`.
        if QC_column is None:
            obs = self._obs.with_columns(columns)
        else:
            # noinspection PyTypeChecker
            expand = lambda series: pl.when(QC_column.name)\
                .then(pl.lit(series).gather(pl.col(QC_column.name).cum_sum()
                                            .cast(pl.Int32) - 1))
            obs = self._obs.with_columns(map(expand, columns))
        
        # Return a new SingleCell dataset containing the cell-type labels and
        # confidences
        return SingleCell(X=self._X, obs=obs, var=self._var, obsm=self._obsm,
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    # noinspection PyUnresolvedReferences
    @staticmethod
    def _get_rocket_r() -> 'LinearSegmentedColormap':
        """
        Define Seaborn's rocket_r colormap using base Matplotlib.
        
        Returns:

        """
        import matplotlib.pyplot as plt
        rocket_colors = [
            [0.01060815, 0.01808215, 0.10018654],
            [0.01428972, 0.02048237, 0.10374486],
            [0.01831941, 0.0229766, 0.10738511],
            [0.02275049, 0.02554464, 0.11108639],
            [0.02759119, 0.02818316, 0.11483751],
            [0.03285175, 0.03088792, 0.11863035],
            [0.03853466, 0.03365771, 0.12245873],
            [0.04447016, 0.03648425, 0.12631831],
            [0.05032105, 0.03936808, 0.13020508],
            [0.05611171, 0.04224835, 0.13411624],
            [0.0618531, 0.04504866, 0.13804929],
            [0.06755457, 0.04778179, 0.14200206],
            [0.0732236, 0.05045047, 0.14597263],
            [0.0788708, 0.05305461, 0.14995981],
            [0.08450105, 0.05559631, 0.15396203],
            [0.09011319, 0.05808059, 0.15797687],
            [0.09572396, 0.06050127, 0.16200507],
            [0.10132312, 0.06286782, 0.16604287],
            [0.10692823, 0.06517224, 0.17009175],
            [0.1125315, 0.06742194, 0.17414848],
            [0.11813947, 0.06961499, 0.17821272],
            [0.12375803, 0.07174938, 0.18228425],
            [0.12938228, 0.07383015, 0.18636053],
            [0.13501631, 0.07585609, 0.19044109],
            [0.14066867, 0.0778224, 0.19452676],
            [0.14633406, 0.07973393, 0.1986151],
            [0.15201338, 0.08159108, 0.20270523],
            [0.15770877, 0.08339312, 0.20679668],
            [0.16342174, 0.0851396, 0.21088893],
            [0.16915387, 0.08682996, 0.21498104],
            [0.17489524, 0.08848235, 0.2190294],
            [0.18065495, 0.09009031, 0.22303512],
            [0.18643324, 0.09165431, 0.22699705],
            [0.19223028, 0.09317479, 0.23091409],
            [0.19804623, 0.09465217, 0.23478512],
            [0.20388117, 0.09608689, 0.23860907],
            [0.20973515, 0.09747934, 0.24238489],
            [0.21560818, 0.09882993, 0.24611154],
            [0.22150014, 0.10013944, 0.2497868],
            [0.22741085, 0.10140876, 0.25340813],
            [0.23334047, 0.10263737, 0.25697736],
            [0.23928891, 0.10382562, 0.2604936],
            [0.24525608, 0.10497384, 0.26395596],
            [0.25124182, 0.10608236, 0.26736359],
            [0.25724602, 0.10715148, 0.27071569],
            [0.26326851, 0.1081815, 0.27401148],
            [0.26930915, 0.1091727, 0.2772502],
            [0.27536766, 0.11012568, 0.28043021],
            [0.28144375, 0.11104133, 0.2835489],
            [0.2875374, 0.11191896, 0.28660853],
            [0.29364846, 0.11275876, 0.2896085],
            [0.29977678, 0.11356089, 0.29254823],
            [0.30592213, 0.11432553, 0.29542718],
            [0.31208435, 0.11505284, 0.29824485],
            [0.31826327, 0.1157429, 0.30100076],
            [0.32445869, 0.11639585, 0.30369448],
            [0.33067031, 0.11701189, 0.30632563],
            [0.33689808, 0.11759095, 0.3088938],
            [0.34314168, 0.11813362, 0.31139721],
            [0.34940101, 0.11863987, 0.3138355],
            [0.355676, 0.11910909, 0.31620996],
            [0.36196644, 0.1195413, 0.31852037],
            [0.36827206, 0.11993653, 0.32076656],
            [0.37459292, 0.12029443, 0.32294825],
            [0.38092887, 0.12061482, 0.32506528],
            [0.38727975, 0.12089756, 0.3271175],
            [0.39364518, 0.12114272, 0.32910494],
            [0.40002537, 0.12134964, 0.33102734],
            [0.40642019, 0.12151801, 0.33288464],
            [0.41282936, 0.12164769, 0.33467689],
            [0.41925278, 0.12173833, 0.33640407],
            [0.42569057, 0.12178916, 0.33806605],
            [0.43214263, 0.12179973, 0.33966284],
            [0.43860848, 0.12177004, 0.34119475],
            [0.44508855, 0.12169883, 0.34266151],
            [0.45158266, 0.12158557, 0.34406324],
            [0.45809049, 0.12142996, 0.34540024],
            [0.46461238, 0.12123063, 0.34667231],
            [0.47114798, 0.12098721, 0.34787978],
            [0.47769736, 0.12069864, 0.34902273],
            [0.48426077, 0.12036349, 0.35010104],
            [0.49083761, 0.11998161, 0.35111537],
            [0.49742847, 0.11955087, 0.35206533],
            [0.50403286, 0.11907081, 0.35295152],
            [0.51065109, 0.11853959, 0.35377385],
            [0.51728314, 0.1179558, 0.35453252],
            [0.52392883, 0.11731817, 0.35522789],
            [0.53058853, 0.11662445, 0.35585982],
            [0.53726173, 0.11587369, 0.35642903],
            [0.54394898, 0.11506307, 0.35693521],
            [0.5506426, 0.11420757, 0.35737863],
            [0.55734473, 0.11330456, 0.35775059],
            [0.56405586, 0.11235265, 0.35804813],
            [0.57077365, 0.11135597, 0.35827146],
            [0.5774991, 0.11031233, 0.35841679],
            [0.58422945, 0.10922707, 0.35848469],
            [0.59096382, 0.10810205, 0.35847347],
            [0.59770215, 0.10693774, 0.35838029],
            [0.60444226, 0.10573912, 0.35820487],
            [0.61118304, 0.10450943, 0.35794557],
            [0.61792306, 0.10325288, 0.35760108],
            [0.62466162, 0.10197244, 0.35716891],
            [0.63139686, 0.10067417, 0.35664819],
            [0.63812122, 0.09938212, 0.35603757],
            [0.64483795, 0.0980891, 0.35533555],
            [0.65154562, 0.09680192, 0.35454107],
            [0.65824241, 0.09552918, 0.3536529],
            [0.66492652, 0.09428017, 0.3526697],
            [0.67159578, 0.09306598, 0.35159077],
            [0.67824099, 0.09192342, 0.3504148],
            [0.684863, 0.09085633, 0.34914061],
            [0.69146268, 0.0898675, 0.34776864],
            [0.69803757, 0.08897226, 0.3462986],
            [0.70457834, 0.0882129, 0.34473046],
            [0.71108138, 0.08761223, 0.3430635],
            [0.7175507, 0.08716212, 0.34129974],
            [0.72398193, 0.08688725, 0.33943958],
            [0.73035829, 0.0868623, 0.33748452],
            [0.73669146, 0.08704683, 0.33543669],
            [0.74297501, 0.08747196, 0.33329799],
            [0.74919318, 0.08820542, 0.33107204],
            [0.75535825, 0.08919792, 0.32876184],
            [0.76145589, 0.09050716, 0.32637117],
            [0.76748424, 0.09213602, 0.32390525],
            [0.77344838, 0.09405684, 0.32136808],
            [0.77932641, 0.09634794, 0.31876642],
            [0.78513609, 0.09892473, 0.31610488],
            [0.79085854, 0.10184672, 0.313391],
            [0.7965014, 0.10506637, 0.31063031],
            [0.80205987, 0.10858333, 0.30783],
            [0.80752799, 0.11239964, 0.30499738],
            [0.81291606, 0.11645784, 0.30213802],
            [0.81820481, 0.12080606, 0.29926105],
            [0.82341472, 0.12535343, 0.2963705],
            [0.82852822, 0.13014118, 0.29347474],
            [0.83355779, 0.13511035, 0.29057852],
            [0.83850183, 0.14025098, 0.2876878],
            [0.84335441, 0.14556683, 0.28480819],
            [0.84813096, 0.15099892, 0.281943],
            [0.85281737, 0.15657772, 0.27909826],
            [0.85742602, 0.1622583, 0.27627462],
            [0.86196552, 0.16801239, 0.27346473],
            [0.86641628, 0.17387796, 0.27070818],
            [0.87079129, 0.17982114, 0.26797378],
            [0.87507281, 0.18587368, 0.26529697],
            [0.87925878, 0.19203259, 0.26268136],
            [0.8833417, 0.19830556, 0.26014181],
            [0.88731387, 0.20469941, 0.25769539],
            [0.89116859, 0.21121788, 0.2553592],
            [0.89490337, 0.21785614, 0.25314362],
            [0.8985026, 0.22463251, 0.25108745],
            [0.90197527, 0.23152063, 0.24918223],
            [0.90530097, 0.23854541, 0.24748098],
            [0.90848638, 0.24568473, 0.24598324],
            [0.911533, 0.25292623, 0.24470258],
            [0.9144225, 0.26028902, 0.24369359],
            [0.91717106, 0.26773821, 0.24294137],
            [0.91978131, 0.27526191, 0.24245973],
            [0.92223947, 0.28287251, 0.24229568],
            [0.92456587, 0.29053388, 0.24242622],
            [0.92676657, 0.29823282, 0.24285536],
            [0.92882964, 0.30598085, 0.24362274],
            [0.93078135, 0.31373977, 0.24468803],
            [0.93262051, 0.3215093, 0.24606461],
            [0.93435067, 0.32928362, 0.24775328],
            [0.93599076, 0.33703942, 0.24972157],
            [0.93752831, 0.34479177, 0.25199928],
            [0.93899289, 0.35250734, 0.25452808],
            [0.94036561, 0.36020899, 0.25734661],
            [0.94167588, 0.36786594, 0.2603949],
            [0.94291042, 0.37549479, 0.26369821],
            [0.94408513, 0.3830811, 0.26722004],
            [0.94520419, 0.39062329, 0.27094924],
            [0.94625977, 0.39813168, 0.27489742],
            [0.94727016, 0.4055909, 0.27902322],
            [0.94823505, 0.41300424, 0.28332283],
            [0.94914549, 0.42038251, 0.28780969],
            [0.95001704, 0.42771398, 0.29244728],
            [0.95085121, 0.43500005, 0.29722817],
            [0.95165009, 0.44224144, 0.30214494],
            [0.9524044, 0.44944853, 0.3072105],
            [0.95312556, 0.45661389, 0.31239776],
            [0.95381595, 0.46373781, 0.31769923],
            [0.95447591, 0.47082238, 0.32310953],
            [0.95510255, 0.47787236, 0.32862553],
            [0.95569679, 0.48489115, 0.33421404],
            [0.95626788, 0.49187351, 0.33985601],
            [0.95681685, 0.49882008, 0.34555431],
            [0.9573439, 0.50573243, 0.35130912],
            [0.95784842, 0.51261283, 0.35711942],
            [0.95833051, 0.51946267, 0.36298589],
            [0.95879054, 0.52628305, 0.36890904],
            [0.95922872, 0.53307513, 0.3748895],
            [0.95964538, 0.53983991, 0.38092784],
            [0.96004345, 0.54657593, 0.3870292],
            [0.96042097, 0.55328624, 0.39319057],
            [0.96077819, 0.55997184, 0.39941173],
            [0.9611152, 0.5666337, 0.40569343],
            [0.96143273, 0.57327231, 0.41203603],
            [0.96173392, 0.57988594, 0.41844491],
            [0.96201757, 0.58647675, 0.42491751],
            [0.96228344, 0.59304598, 0.43145271],
            [0.96253168, 0.5995944, 0.43805131],
            [0.96276513, 0.60612062, 0.44471698],
            [0.96298491, 0.6126247, 0.45145074],
            [0.96318967, 0.61910879, 0.45824902],
            [0.96337949, 0.6255736, 0.46511271],
            [0.96355923, 0.63201624, 0.47204746],
            [0.96372785, 0.63843852, 0.47905028],
            [0.96388426, 0.64484214, 0.4861196],
            [0.96403203, 0.65122535, 0.4932578],
            [0.96417332, 0.65758729, 0.50046894],
            [0.9643063, 0.66393045, 0.5077467],
            [0.96443322, 0.67025402, 0.51509334],
            [0.96455845, 0.67655564, 0.52251447],
            [0.96467922, 0.68283846, 0.53000231],
            [0.96479861, 0.68910113, 0.53756026],
            [0.96492035, 0.69534192, 0.5451917],
            [0.96504223, 0.7015636, 0.5528892],
            [0.96516917, 0.70776351, 0.5606593],
            [0.96530224, 0.71394212, 0.56849894],
            [0.96544032, 0.72010124, 0.57640375],
            [0.96559206, 0.72623592, 0.58438387],
            [0.96575293, 0.73235058, 0.59242739],
            [0.96592829, 0.73844258, 0.60053991],
            [0.96612013, 0.74451182, 0.60871954],
            [0.96632832, 0.75055966, 0.61696136],
            [0.96656022, 0.75658231, 0.62527295],
            [0.96681185, 0.76258381, 0.63364277],
            [0.96709183, 0.76855969, 0.64207921],
            [0.96739773, 0.77451297, 0.65057302],
            [0.96773482, 0.78044149, 0.65912731],
            [0.96810471, 0.78634563, 0.66773889],
            [0.96850919, 0.79222565, 0.6764046],
            [0.96893132, 0.79809112, 0.68512266],
            [0.96935926, 0.80395415, 0.69383201],
            [0.9698028, 0.80981139, 0.70252255],
            [0.97025511, 0.81566605, 0.71120296],
            [0.97071849, 0.82151775, 0.71987163],
            [0.97120159, 0.82736371, 0.72851999],
            [0.97169389, 0.83320847, 0.73716071],
            [0.97220061, 0.83905052, 0.74578903],
            [0.97272597, 0.84488881, 0.75440141],
            [0.97327085, 0.85072354, 0.76299805],
            [0.97383206, 0.85655639, 0.77158353],
            [0.97441222, 0.86238689, 0.78015619],
            [0.97501782, 0.86821321, 0.78871034],
            [0.97564391, 0.87403763, 0.79725261],
            [0.97628674, 0.87986189, 0.8057883],
            [0.97696114, 0.88568129, 0.81430324],
            [0.97765722, 0.89149971, 0.82280948],
            [0.97837585, 0.89731727, 0.83130786],
            [0.97912374, 0.90313207, 0.83979337],
            [0.979891, 0.90894778, 0.84827858],
            [0.98067764, 0.91476465, 0.85676611],
            [0.98137749, 0.92061729, 0.86536915]]
        return plt.matplotlib.colors.LinearSegmentedColormap.from_list(
            'rocket_r', rocket_colors[::-1])
    
    # noinspection PyUnresolvedReferences
    def plot_heatmap(self,
                     x: SingleCellColumn,
                     y: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     cells_to_plot_column: SingleCellColumn |
                                           None = 'passed_QC',
                     normalize_rows: bool = False,
                     normalize_columns: bool = False,
                     ax: 'Axes' | None = None,
                     figure_kwargs: dict[str, Any] | None = None,
                     colormap: str | 'Colormap' | None = None,
                     heatmap_kwargs: dict[str, Any] | None = None,
                     label: bool = False,
                     label_format: str | None = None,
                     label_kwargs: dict[str, Any] | None = None,
                     colorbar: bool = True,
                     colorbar_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | Literal[True] | None = True,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | Literal[True] | None = True,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot a heatmap of the count of each combination of two categorical
        columns, `x` and `y`. If `normalize_rows` or `normalize_columns` is
        specified, plot percentages instead of counts, so that each row or
        column sums to 100%.
        
        Args:
            x: the first column; must be String, Enum or Categorical
            y: the second column; must be String, Enum or Categorical
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            normalize_rows: whether to plot percentages instead of counts, so
                            that each row sums to 100%. Mutually exclusive with
                            `normalize_columns`.
            normalize_columns: whether to plot percentages instead of counts,
                               so that each column sums to 100%. Mutually
                               exclusive with `normalize_rows`.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. The default is a
                             complicated formula based on the number of genes
                             and cell types being plotted, unlike Matplotlib's
                             default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in the heatmap, or `None` to use
                      Seaborn's `'rocket_r'` colormap.
            heatmap_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.pcolormesh()` when generating the heatmap, such
                            as:
                            - `rasterized`: whether to convert the heatmap
                              cells to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `norm`, `vmin`, and `vmax`: control how the
                              colormap maps counts or percentages to heatmap
                              colors
                            - `edgecolors`: the border color of each heatmap
                              cell; defaults to `'none'`, meaning no borders.
                              Specifying `cmap` will raise an error, since it
                              conflicts with the `colormap` argument.
            label: whether to label each cell of the heatmap with its count
                   (or percentage, if `normalize_rows=True` or
                    `normalize_columns=True`)
            label_format: a format string to apply to the label for each count
                          or percentage. If `None`, use `'{:,}'` for counts and
                          `'{:.2f}%'` for percentages. Can only be specified
                          when `label=True`.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size.
                             By default, the color is dark gray for
                             light-colored cells, and white for dark-colored
                             ones.
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                          Can only be specified when `label=True`.
            colorbar: whether to add a colorbar
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, `True` to use the name of `x` as the
                    x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, `True` to use the name of `y` as the
                    y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the spines (borders of the plot area)
                     from the plot; unlike the other plotting functions in this
                     library, this also removes the left and bottom spines
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        
        # Get `cells_to_plot_column`, if not `None`
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        
        # Get `x` and `y`, and check that they are String, Enum, or Categorical
        x = self._get_column('obs', x, 'x',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        y = self._get_column('obs', y, 'y',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        if cells_to_plot_column is not None:
            x = x.filter(cells_to_plot_column)
            y = y.filter(cells_to_plot_column)
        
        # If `filename` was specified, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        
        # Check that `normalize_rows`, `normalize_columns`, `label`,
        # `colorbar`, and `despine` are Boolean
        check_type(normalize_rows, 'normalize_rows', bool, 'Boolean')
        check_type(normalize_columns, 'normalize_columns', bool, 'Boolean')
        check_type(label, 'label', bool, 'Boolean')
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        
        # Check that `normalize_rows` and `normalize_columns` are mutually
        # exclusive
        if normalize_rows and normalize_columns:
            error_message = \
                'only one of normalize_rows and normalize_columns can be True'
            raise ValueError(error_message)
        
        # If `figure_kwargs` was specified, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        
        # Check that `colormap` is a string in `plt.colormaps`, a Colormap
        # object, or `None`; if `None`, default to Seaborn's rocket_r colormap
        if colormap is None:
            colormap = SingleCell._get_rocket_r()
        else:
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap),
                       'a string or matplotlib Colormap object')
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
        
        # If `label=False`, check that `label_format` and `label_kwargs` are
        # `None`.
        if not label:
            if label_format is not None:
                error_message = 'label_format must be None when label=False'
                raise ValueError(error_message)
            if label_kwargs is not None:
                error_message = 'label_kwargs must be None when label=False'
                raise ValueError(error_message)
        
        # If not `None`, check that `label_format` is a valid format string.
        # For simplicity, just check that it has curly braces and that all
        # braces are matched. If `label_format` is `None`, use `'{:,}'` for
        # counts or, if `normalize_rows=True` or `normalize_columns=True`,
        # `'{:.2f}%'` for percentages.
        if label_format is None:
            label_format = \
                '{:.2f}%' if normalize_rows or normalize_columns else '{:,}'
        else:
            check_type(label_format, 'label_format', str, 'a string')
            open_braces = 0
            has_braces = False
            for char in label_format:
                if char == '{':
                    has_braces = True
                    open_braces += 1
                elif char == '}':
                    open_braces -= 1
                if open_braces < 0:
                    error_message = \
                        'label_format contains mismatched curly braces'
                    raise ValueError(error_message)
            if open_braces == 0:
                error_message = 'label_format contains mismatched curly braces'
                raise ValueError(error_message)
            if not has_braces:
                error_message = 'label_format must contain curly braces'
                raise ValueError(error_message)
        
        # If `colorbar=False`, check that `colorbar_kwargs` is None
        if not colorbar and colorbar_kwargs is not None:
            error_message = 'colorbar_kwargs must be None when colorbar=False'
            raise ValueError(error_message)
        
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well.
        if title is not None:
            check_type(title, 'title', str, 'a string')
        elif title_kwargs is not None:
            error_message = 'title_kwargs must be None when title is None'
            raise ValueError(error_message)
        
        # Check that `xlabel` is a string, `True` (in which case set it to
        # `x.name`), or `None`; if `None`, check that `xlabel_kwargs` is `None`
        # as well. Ditto for `ylabel`.
        if xlabel is not None:
            if xlabel is True:
                xlabel = x.name
            else:
                check_type(xlabel, 'xlabel', str, 'a string')
        elif xlabel_kwargs is not None:
            error_message = 'xlabel_kwargs must be None when xlabel is None'
            raise ValueError(error_message)
        if ylabel is not None:
            if ylabel is True:
                ylabel = y.name
            else:
                check_type(ylabel, 'ylabel', str, 'a string')
        elif ylabel_kwargs is not None:
            error_message = 'ylabel_kwargs must be None when ylabel is None'
            raise ValueError(error_message)
        
        # For each of the kwargs arguments, if the argument was specified,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (heatmap_kwargs, 'heatmap_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        
        # Override the defaults for certain values of `heatmap_kwargs`; if
        # specified, check that `heatmap_kwargs` does not contain the `cmap`
        # argument
        default_heatmap_kwargs = dict(rasterized=True)
        if heatmap_kwargs is None:
            heatmap_kwargs = default_heatmap_kwargs
        else:
            if 'cmap' in heatmap_kwargs:
                error_message = (
                    f"'cmap' cannot be specified as a key in heatmap_kwargs; "
                    f"specify the colormap argument instead")
                raise ValueError(error_message)
            heatmap_kwargs = heatmap_kwargs | default_heatmap_kwargs
        
        # Get the heatmap data
        count = pl.DataFrame((x, y))\
            .group_by(pl.all(), maintain_order=True)\
            .len(name='_SingleCell_count')\
            .pivot(index=y.name, columns=x.name, values='_SingleCell_count')\
            .fill_null(0)
        heatmap_data = count[:, 1:].to_numpy()
        
        # Normalize, if `normalize_rows=True` or `normalize_columns=True`
        if normalize_rows:
            heatmap_data = \
                heatmap_data / heatmap_data.sum(axis=1, keepdims=True)
        elif normalize_columns:
            heatmap_data = \
                heatmap_data / heatmap_data.sum(axis=0, keepdims=True)
        
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            num_rows, num_columns = heatmap_data.shape
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                if figure_kwargs is None or 'figsize' not in figure_kwargs:
                    if colorbar:
                        width_ratio = max(4, 0.2 * num_columns)
                        width = 6.4 / (1 + width_ratio) + \
                                6.4 * width_ratio / (1 + width_ratio) * \
                                max(num_columns, 5) / 20
                    else:
                        width = 6.4 * max(num_columns, 5) / 20
                    height = max(4.8, 1 + 3.8 * num_rows / 20)
                    default_figure_kwargs['figsize'] = width, height
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            
            # Make the heatmap
            xticks = np.arange(0.5, num_columns)
            yticks = np.arange(0.5, num_rows)
            heatmap = \
                ax.pcolormesh(heatmap_data, cmap=colormap, **heatmap_kwargs)
            ax.set_xticks(xticks, count.columns[1:], rotation=90)
            ax.set_yticks(yticks, count[:, 0].to_numpy())
            ax.set_aspect('equal')
            
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`. If normalizing rows or columns, make the
            # colorbar ticks percentages.
            if colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(heatmap, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
                if normalize_rows or normalize_columns:
                    cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(
                        lambda x, pos: f'{100 * x:.0f}%'))
            
            # Add labels; this code is edited from `_annotate_heatmap()` at
            # github.com/mwaskom/seaborn/blob/master/seaborn/matrix.py
            if label:
                heatmap.update_scalarmappable()
                xpos, ypos = np.meshgrid(xticks, yticks)
                if label_kwargs is None:
                    label_kwargs = {}
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')))
                if 'c' in label_kwargs or 'color' in label_kwargs:
                    # Use the same color for all labels
                    for x, y, val in zip(xpos.ravel(), ypos.ravel(),
                                         heatmap_data.ravel()):
                        ax.text(x, y, s=label_format.format(val),
                                **label_kwargs)
                else:
                    # Use either dark gray or white for the label, depending on
                    # the cell's luminance
                    rgb_weights = np.array([0.2126, 0.7152, 0.0722])
                    for x, y, color, val in zip(
                            xpos.ravel(), ypos.ravel(),
                            heatmap.get_facecolors(), heatmap_data.ravel()):
                        rgb = plt.matplotlib.colors.colorConverter\
                            .to_rgba_array(color)[:, :3]
                        rgb = np.where(rgb <= 0.03928, rgb / 12.92,
                                       ((rgb + 0.055) / 1.055) ** 2.4)
                        lum = rgb.dot(rgb_weights).item()
                        ax.text(x, y, s=label_format.format(val),
                                c='.15' if lum > .408 else 'w', **label_kwargs)
            
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
            
            # Despine, if specified
            if despine:
                spines = ax.spines
                for direction in 'top', 'bottom', 'left', 'right':
                    spines[direction].set_visible(False)
            
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    def find_markers(self,
                     cell_type_column: SingleCellColumn,
                     *,
                     QC_column: SingleCellColumn | None = 'passed_QC',
                     min_detection_rate: int | float | np.integer |
                                         np.floating = 0.25,
                     min_fold_change: int | float | np.integer |
                                      np.floating = 2,
                     pareto: bool = True,
                     all_genes: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            pl.DataFrame:
        """
        Find "marker genes" that distinguish each cell type from all other cell
        types. This function gives the same result regardless of whether it is
        run before or after normalization.
        
        Marker genes are chosen via an adaptation of the strategy of Fischer
        and Gillis 2021 (ncbi.nlm.nih.gov/pmc/articles/PMC8571500). For a given
        cell type, genes are scored based on a) their "detection rate" in that
        cell type (the fraction of cells of that type that have non-zero count
        for that gene), as well as b) the fold change in detection rate between
        that cell type and every other cell type. Genes must also have a
        detection rate of at least `min_detection_rate` (25% by default) and a
        minimum fold change of at least `min_fold_change` (2-fold by default)
        to be considered as markers.
        
        There is an inherent tradeoff between these two metrics. For instance,
        candidate marker genes with high enough expression to be expressed in
        every cell of a given type (i.e. to have a high detection rate) tend to
        also have at least some expression in other cell types (i.e. a low fold
        change in detection rate).
        
        Thus, marker genes are selected to optimally trade off between these
        two metrics: all genes on the Pareto front of the two metrics (i.e.
        genes for which there is no other gene that does better on both metrics
        simultaneously) are selected as marker genes.
        
        Note that Fischer and Gillis use AUROC versus log2 fold change in
        detection rate, instead of detection rate versus fold change in
        detection rate. However, detection rate is much faster to compute than
        AUROC, and is a very accurate proxy for AUROC: as Figure 1D in their
        paper shows, AUROC is almost perfectly correlated with detection rate
        across marker genes.
        
        Args:
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored.
            min_detection_rate: the minimum detection rate required to select
                                a gene as a marker gene; must be greater than 0
                                and less than or equal to 1
            min_fold_change: the minimum fold change in detection rate required
                             to select a gene as a marker gene; must be greater
                             than 1
            pareto: if `True`, include only genes on the Pareto front of
                    detection rate and fold change as markers; if `False`,
                    include all genes that pass the `min_detection_rate` and
                    `min_fold_change` thresholds as markers
            all_genes: if `True`, include all genes in the output, not just
                       marker genes. An additional Boolean column will be
                       included to specify which genes are the marker genes.
                       Note that this option does not change which marker genes
                       are selected, only which information is returned.
            num_threads: the number of threads to use for marker-gene finding.
                         Set `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. For count matrices stored in
                         the usual CSR format, the most time-consuming step
                         (calculating detection counts of each gene in each
                         cell type) is parallelized across cell types, so
                         specifying more cores than the number of cell types
                         may not improve performance.
        
        Returns:
            By default, a DataFrame with one row per marker gene, with columns:
            - `'cell_type'`: a cell-type name from `cell_type_column`
            - `'gene'`: a gene symbol from `var_names`
            - `'detection_rate'`: the gene's detection rate in that cell type
            - `'fold_change'`, the gene's fold change in detection rate
              between that cell type and all other cell types
            If `all_genes=True`, a DataFrame with one row per cell type-gene
            pair, with those four columns plus one other:
            - `'marker'`, a Boolean column listing whether the gene is a marker
              for that cell type
            If `all_genes=False`, marker genes within each cell type will be
            sorted in increasing order of detection rate, and decreasing order
            of fold change.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        # Check that `X` is present
        X = self._X
        if X is None:
            error_message = 'X is None, so marker gene finding is not possible'
            raise ValueError(error_message)
        
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "find_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        
        # Get the QC column, if not `None`
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=QC_column)
        cell_type_column_name = cell_type_column.name
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `min_detection_rate` and `min_fold_change` are numeric and
        # have the correct ranges: 0 < min_detection_rate <= 1,
        # min_fold_change > 1
        check_type(min_detection_rate, 'min_detection_rate',
                   (int, float), 'a positive number less than or equal to 1')
        check_bounds(min_detection_rate, 'min_detection_rate', 0, 1,
                     left_open=True)
        check_type(min_fold_change, 'min_fold_change', (int, float),
                   'a number greater than 1')
        check_bounds(min_fold_change, 'min_fold_change', 1, left_open=True)
        
        # Check that `all_genes` is Boolean
        check_type(all_genes, 'all_genes', bool, 'Boolean')
        
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  if QC_column is None else
                  pl.LazyFrame((cell_type_column, QC_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  .filter(QC_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene, i.e. the gene's detection
        # count in that cell type
        num_cell_types = len(groups)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = X.shape[1]
        detection_count = np.empty((num_cell_types, num_genes),
                                   dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csr(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const unsigned[::1] group_indices,
                        const unsigned[::1] group_ends,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned group, cell, row, num_groups = group_ends.shape[0], \
                        num_genes = result.shape[1]
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # Initialize all elements of the group to 0
                            result[group, :] = 0
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse array
                                row = group_indices[cell]
                                
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    # Add 1 to the total for this group and gene
                                    result[group, indices[gene]] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            result[group, :] = 0
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    result[group, indices[gene]] += 1
            ''', warn_undeclared=False)['groupby_getnnz_csr'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                result=detection_count, num_threads=num_threads)
        else:
            # noinspection PyUnresolvedReferences
            group_map = pl.int_range(X.shape[0], dtype=pl.UInt32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = group_map.null_count() > 0
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            # noinspection PyUnresolvedReferences
            cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, threadid
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def groupby_getnnz_csc(
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const int[::1] group_map,
                const bint has_missing,
                unsigned[:, ::1] result,
                unsigned num_threads):
            cdef unsigned column, thread_index, num_columns = result.shape[1]
            cdef int group
            cdef unsigned long row
            cdef pair[unsigned, unsigned] col_range
            
            num_threads = min(num_threads, num_columns)
            if num_threads == 1:
                if has_missing:
                    # For each gene (column of the sparse array)...
                    for column in range(num_columns):
                        # For each cell (row) that's non-zero for this gene...
                        for row in range(<unsigned long> indptr[column],
                                         <unsigned long> indptr[column + 1]):
                            # Get the group index for this row (-1 if it failed
                            # QC)
                            group = group_map[indices[row]]
                            if group == -1:
                                continue
                            
                            # Add 1 to the total for this group and column
                            result[group, column] += 1
                else:
                    for column in range(num_columns):
                        for row in range(<unsigned long> indptr[column],
                                         <unsigned long> indptr[column + 1]):
                            group = group_map[indices[row]]
                            result[group, column] += 1
            else:
                if has_missing:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for column in range(col_range.first, col_range.second):
                            for row in range(<unsigned long> indptr[column],
                                             <unsigned long> indptr[column + 1]):
                                group = group_map[indices[row]]
                                if group == -1:
                                    continue
                                result[group, column] += 1
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for column in range(col_range.first, col_range.second):
                            for row in range(<unsigned long> indptr[column],
                                             <unsigned long> indptr[column + 1]):
                                group = group_map[indices[row]]
                                result[group, column] += 1
                    ''', warn_undeclared=False)['groupby_getnnz_csc'](
                        indices=X.indices, indptr=X.indptr,
                        group_map=group_map, has_missing=has_missing,
                        result=detection_count, num_threads=num_threads)
        
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.uint32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes),
                                  dtype=np.float32)
        fold_change = np.empty((num_cell_types, num_genes), dtype=np.float32)
        is_pareto = np.empty((num_cell_types, num_genes), dtype=bool)
        cython_inline(r'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const unsigned[:, ::1] detection_count,
                    const unsigned[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    const float min_detection_rate,
                    const float min_fold_change,
                    float[:, ::1] detection_rate,
                    float[:, ::1] fold_change,
                    char[:, ::1] is_pareto,
                    const unsigned num_threads):
                cdef unsigned cell_type, gene, count, background_count, \
                    num_cells, background_num_cells, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef float pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <float> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <float> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                            is_pareto[cell_type, gene] = \
                                (pair_detection_rate >= min_detection_rate) & \
                                (pair_fold_change >= min_fold_change)
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells,
                min_detection_rate=min_detection_rate,
                min_fold_change=min_fold_change, detection_rate=detection_rate,
                fold_change=fold_change, is_pareto=is_pareto,
                num_threads=num_threads)
        
        # If `pareto=True`, get the set of genes on the Pareto front of the two
        # metrics for each cell type; these are the marker genes.
        # If `pareto=False`, just include the genes we've initialized
        # `is_pareto` to `True` for: all those passing the `min_detection_rate`
        # and `min_fold_change` thresholds.
        if pareto:
            cython_inline(r'''
                from cython.parallel cimport prange
                
                def pareto_front(float[:, ::1] detection_rate,
                                 float[:, ::1] fold_change,
                                 char[:, ::1] is_pareto,
                                 unsigned num_threads):
                    cdef unsigned gene, other_gene, cell_type, \
                        num_cell_types = detection_rate.shape[0], \
                        num_genes = detection_rate.shape[1]
                    cdef float gene_detection_rate, gene_fold_change
                    
                    if num_threads == 1:
                        for cell_type in range(num_cell_types):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                    else:
                        for cell_type in prange(num_cell_types, nogil=True,
                                                num_threads=num_threads):
                            for gene in range(num_genes):
                                if not is_pareto[cell_type, gene]:
                                    continue
                                gene_detection_rate = \
                                    detection_rate[cell_type, gene]
                                gene_fold_change = fold_change[cell_type, gene]
                                for other_gene in range(num_genes):
                                    if gene == other_gene or \
                                            not is_pareto[cell_type, other_gene]:
                                        continue
                                    if gene_detection_rate <= \
                                            detection_rate[cell_type, other_gene] \
                                            and gene_fold_change <= \
                                            fold_change[cell_type, other_gene]:
                                        is_pareto[cell_type, gene] = 0
                                        break
                ''')['pareto_front'](
                    detection_rate=detection_rate, fold_change=fold_change,
                    is_pareto=is_pareto, num_threads=num_threads)
        
        # Return a DataFrame of the selected marker genes, or all genes if
        # `all_genes=True`
        cell_types = groups[cell_type_column_name].rename('cell_type')
        genes = self._var[:, 0].rename('gene')
        if all_genes:
            cell_types = pl.select(pl.lit(cell_types).repeat_by(num_genes))\
                .explode('cell_type')\
                .to_series()
            genes = pl.concat([genes] * num_cell_types)
            return pl.DataFrame((
                cell_types, genes,
                pl.Series('marker', is_pareto.ravel()),
                pl.Series('detection_rate', detection_rate.ravel()),
                pl.Series('fold_change', fold_change.ravel())))
        else:
            cell_type_indices, gene_indices = is_pareto.nonzero()
            return pl.DataFrame((
                cell_types[cell_type_indices], genes[gene_indices],
                pl.Series('detection_rate', detection_rate[
                    cell_type_indices, gene_indices].ravel()),
                pl.Series('fold_change', fold_change[
                    cell_type_indices, gene_indices].ravel())))\
                .select(pl.all().sort_by('detection_rate').over('cell_type'))
    
    # noinspection PyUnresolvedReferences
    def plot_markers(self,
                     genes: str | Iterable[str],
                     cell_type_column: SingleCellColumn,
                     filename: str | Path | None = None,
                     *,
                     cells_to_plot_column: SingleCellColumn |
                                           None = 'passed_QC',
                     cell_types: str | Iterable[str] | None = None,
                     figure_kwargs: dict[str, Any] | None = None,
                     colormap: str | 'Colormap' = 'RdBu_r',
                     colorbar: bool = True,
                     colorbar_kwargs: dict[str, Any] | None = None,
                     swap_axes: bool = False,
                     scatter_kwargs: dict[str, Any] | None = None,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = None,
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = None,
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None,
                     num_threads: int | np.integer | None = None) -> None:
        """
        Make a dot plot of the detection rate and fold change of a set of genes
        across cell types - the same metrics used to select marker genes in
        `find_markers()`. This function gives the same result regardless of
        whether it is run before or after normalization.
        
        The size of a gene's dot for a cell type represents its
        "detection rate" in that cell type (the fraction of cells of that type
        that have non-zero count for that gene), while its color represents the
        gene's fold change in detection rate between that cell type and every
        other cell type (by default: more positive fold changes are redder,
        while more negative fold changes are bluer).
        
        Unlike the other plotting functions, this is a figure-level rather than
        an axis-level function, and does not take an `axis` argument.
        
        Args:
            genes: a list of genes to plot: for instance, marker genes found by
                   `find_markers()`, or marker genes from the literature
            cell_type_column: a column of `obs` containing cell-type labels.
                              Can be a column name, a polars expression, a
                              polars Series, a 1D NumPy array, or a function
                              that takes in this SingleCell dataset and returns
                              a polars Series or 1D NumPy array.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            cell_types: a list of cell types to plot; by default, plots all of
                        them. Can also be used to change the order in which
                        cell types are displayed, even if plotting all cell
                        types.
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. The default is a
                             complicated formula based on the number of genes
                             and cell types being plotted, unlike Matplotlib's
                             default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use in `ax.scatter()` for representing fold
                      changes
            colorbar: whether to add a colorbar
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `colorbar=True`.
            swap_axes: if `True`, plot genes on the y-axis and cell types on
                       the x-axis, instead of the other way around
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since the size and color of each point
                            are set automatically, and `cmap` conflicts with
                            the `colormap` argument.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. The legend will be placed in its
                             own axis in the top right of the plot, and by
                             default, `loc` is set to `'center'`.
                           - `ncols` to set its number of columns
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border. `frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`.
                           - `title` to modify the legend title. Defaults to
                             `'Detection rate'`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`), instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
            num_threads: the number of threads to use when tabulating each
                         gene's detection rate and fold change. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. For count matrices stored in
                         the usual CSR format, the most time-consuming step
                         (calculating detection counts of each gene in each
                         cell type) is parallelized across cell types, so
                         specifying more cores than the number of cell types
                         may not improve performance.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains explicit zeros (i.e. if `(X.data == 0).any()`): this is
            not checked for, due to speed considerations. In the unlikely event
            that your dataset contains explicit zeros, remove them by running
            `X.eliminate_zeros()` (an in-place operation) first.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        # Check that `X` is present
        X = self._X
        if X is None:
            error_message = \
                'X is None, so marker gene plotting is not possible'
            raise ValueError(error_message)
        import matplotlib.pyplot as plt
        
        # Check that `self` is QCed
        if not self._uns['QCed']:
            error_message = (
                "uns['QCed'] is False; did you forget to run qc() before "
                "plot_markers()? Set uns['QCed'] = True or run "
                "with_uns(QCed=True) to bypass this check.")
            raise ValueError(error_message)
        
        # Get `genes` as a polars Series of the same dtype as `var_names`; make
        # sure all its entries are unique and present in `var_names`
        genes = to_tuple_checked(genes, 'genes', str, 'strings')
        genes = pl.Series(genes)
        if genes.n_unique() < len(genes):
            error_message = 'genes contains duplicates'
            raise ValueError(error_message)
        var_names = self._var[:, 0]
        if not genes.is_in(var_names).all():
            if not genes.is_in(var_names).any():
                error_message = \
                    'none of the specified genes were found in var_names'
                raise ValueError(error_message)
            else:
                for gene in genes:
                    if gene not in var_names:
                        error_message = (
                            f'one of the specified genes, {gene!r}, was not '
                            f'found in var_names')
                        raise ValueError(error_message)
        if var_names.dtype != pl.String:
            genes = genes.cast(var_names.dtype)
        
        # Get `cells_to_plot_column`, if not `None`
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        
        # Get the cell-type column
        original_cell_type_column = cell_type_column
        cell_type_column = \
            self._get_column('obs', cell_type_column, 'cell_type_column',
                             (pl.String, pl.Categorical, pl.Enum),
                             QC_column=cells_to_plot_column)
        cell_type_column_name = cell_type_column.name
        
        # If `cell_types` is not `None`, check that `cell_types` contains only
        # cell type names present in `cell_type_column`
        if cell_types is not None:
            cell_types = to_tuple_checked(cell_types, 'cell_types', str,
                                          'strings')
            unique_cell_types = cell_type_column.unique(maintain_order=True)
            for cell_type in cell_types:
                if cell_type not in unique_cell_types:
                    error_message = (
                        f'cell_types contains a cell type, {cell_type!r}, not '
                        f'present in cell_type_column')
                    raise ValueError(error_message)
        
        # If `filename` was specified, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        
        # Check that `colormap` is a string in `plt.colormaps` or a Colormap
        # object
        check_type(colormap, 'colormap', (str, plt.matplotlib.colors.Colormap),
                   'a string or matplotlib Colormap object')
        if isinstance(colormap, str):
            colormap = plt.colormaps[colormap]
        
        # Check that `colorbar` is Boolean
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        
        # If `colorbar=False`, check that `colorbar_kwargs` is None
        if not colorbar and colorbar_kwargs is not None:
            error_message = 'colorbar_kwargs must be None when colorbar=False'
            raise ValueError(error_message)
        
        # Check that `swap_axes` and `despine` are Boolean
        check_type(swap_axes, 'swap_axes', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
        
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
        
        # For each of the kwargs arguments, if the argument was specified,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Get the indices corresponding to the cells of each cell type,
        # ignoring cells failing QC when `QC_column` is present in `obs`
        # noinspection PyUnboundLocalVariable
        groups = (cell_type_column.to_frame().lazy()
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  if cells_to_plot_column is None else
                  pl.LazyFrame((cell_type_column, cells_to_plot_column))
                  .with_columns(
                      _SingleCell_group_indices=pl.int_range(pl.len(),
                                                             dtype=pl.UInt32))
                  .filter(cells_to_plot_column.name))\
            .group_by(cell_type_column_name)\
            .agg('_SingleCell_group_indices', _SingleCell_num_cells=pl.len())\
            .sort(cell_type_column_name)\
            .collect()
        
        # If `cell_types` is not `None`, reindex `groups` to contain only the
        # cell types in `cell_types`, in the same order. Otherwise, get the
        # list of cell types from `groups`.
        if cell_types is not None:
            groups = pl.DataFrame({cell_type_column_name: cell_types})\
                .join(groups, on=cell_type_column_name, how='left')
        else:
            cell_types = groups[cell_type_column_name]
        
        # Get a cell-type-by-gene matrix of the number of cells of each type
        # with non-zero expression of each gene in `genes`, i.e. the gene's
        # detection count in that cell type
        num_cell_types = len(cell_types)
        if num_cell_types == 1:
            cell_type_column_description = \
                SingleCell._describe_column('cell_type_column',
                                            original_cell_type_column)
            error_message = (
                f'{cell_type_column_description} only contains one unique '
                f'value')
            raise ValueError(error_message)
        num_genes = len(genes)
        detection_count = np.empty((num_cell_types, num_genes),
                                   dtype=np.uint32)
        if isinstance(X, csr_array):
            group_indices = \
                groups['_SingleCell_group_indices'].explode().to_numpy()
            group_ends = \
                groups['_SingleCell_num_cells'].cum_sum().to_numpy()
            
            # Get an array mapping each gene in `var_names` to its position in
            # `genes` (-1 if missing from `genes`)
            gene_map = var_names\
                .to_frame()\
                .join(genes
                      .to_frame(var_names.name)
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.Int32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()
            gene_map = gene_map.fill_null(-1).to_numpy()
            cython_inline(r'''
                from cython.parallel cimport prange
                
                ctypedef fused numeric:
                    int
                    unsigned
                    long
                    unsigned long
                    float
                    double
                
                ctypedef fused signed_integer:
                    int
                    long
                
                def groupby_getnnz_csr_for_gene_subset(
                        const signed_integer[::1] indices,
                        const signed_integer[::1] indptr,
                        const unsigned[::1] group_indices,
                        const unsigned[::1] group_ends,
                        const int[::1] gene_map,
                        unsigned[:, ::1] result,
                        const unsigned num_threads):
                    cdef unsigned group, cell, row, num_groups = group_ends.shape[0], \
                        num_genes = result.shape[1]
                    cdef int column
                    cdef unsigned long gene
                    
                    if num_threads == 1:
                        # For each group (cell type)...
                        for group in range(num_groups):
                            # Initialize all elements of the group to 0
                            result[group, :] = 0
                            # For each cell within this group...
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                # Get this cell's row index in the sparse array
                                row = group_indices[cell]
                                # For each gene (column) that's non-zero for this
                                # cell...
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    # Get this gene's column index in `result`
                                    # (-1 if the gene is not in `genes`)
                                    column = gene_map[indices[gene]]
                                    if column == -1:
                                        continue
                                    
                                    # Add 1 to the total for this group and gene
                                    result[group, column] += 1
                    else:
                        for group in prange(num_groups, nogil=True,
                                            num_threads=num_threads):
                            result[group, :] = 0
                            for cell in range(
                                    0 if group == 0 else group_ends[group - 1],
                                    group_ends[group]):
                                row = group_indices[cell]
                                for gene in range(<unsigned long> indptr[row],
                                                  <unsigned long> indptr[row + 1]):
                                    column = gene_map[indices[gene]]
                                    if column == -1:
                                        continue
                                    result[group, column] += 1
            ''', warn_undeclared=False)['groupby_getnnz_csr_for_gene_subset'](
                indices=X.indices, indptr=X.indptr,
                group_indices=group_indices, group_ends=group_ends,
                gene_map=gene_map, result=detection_count,
                num_threads=num_threads)
        else:
            group_map = pl.int_range(X.shape[0], dtype=pl.UInt32, eager=True)\
                .to_frame('_SingleCell_group_indices')\
                .join(groups
                      .select('_SingleCell_group_indices',
                              _SingleCell_index=pl.int_range(pl.len(),
                                                             dtype=pl.Int32))
                      .explode('_SingleCell_group_indices'),
                      on='_SingleCell_group_indices', how='left')\
                ['_SingleCell_index']
            has_missing = group_map.null_count() > 0
            if has_missing:
                group_map = group_map.fill_null(-1)
            group_map = group_map.to_numpy()
            
            # Get an array mapping each gene in `genes` to its position in
            # `var_names`
            gene_map = genes\
                .to_frame(var_names.name)\
                .join(var_names
                      .to_frame()
                      .with_columns(index=pl.int_range(pl.len(),
                                                       dtype=pl.UInt32)),
                      on=var_names.name, how='left')\
                .select('index')\
                .to_series()\
                .to_numpy()
            cython_inline(_thread_offset_import + r'''
        from cython.parallel cimport parallel, threadid
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        def groupby_getnnz_csc_for_gene_subset(
                const signed_integer[::1] indices,
                const signed_integer[::1] indptr,
                const int[::1] group_map,
                const unsigned[::1] gene_map,
                const bint has_missing,
                unsigned[:, ::1] result,
                unsigned num_threads):
            cdef unsigned column, gene, thread_index, num_columns = result.shape[1]
            cdef int group
            cdef unsigned long cell
            cdef pair[unsigned, unsigned] col_range
            
            num_threads = min(num_threads, num_columns)
            if num_threads == 1:
                if has_missing:
                    # For each gene...
                    for column in range(num_columns):
                        # Get the index of this gene in the count matrix
                        gene = gene_map[column]
                        # For each cell (row) that's non-zero for this gene...
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            # Get the group index for this cell (-1 if it
                            # failed QC)
                            group = group_map[indices[cell]]
                            if group == -1:
                                continue
                            
                            # Add 1 to the total for this group and gene
                            result[group, column] += 1
                else:
                    for column in range(num_columns):
                        gene = gene_map[column]
                        for cell in range(<unsigned long> indptr[gene],
                                          <unsigned long> indptr[gene + 1]):
                            group = group_map[indices[cell]]
                            result[group, column] += 1
            else:
                if has_missing:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for column in range(col_range.first, col_range.second):
                            gene = gene_map[column]
                            for cell in range(<unsigned long> indptr[gene],
                                              <unsigned long> indptr[gene + 1]):
                                group = group_map[indices[cell]]
                                if group == -1:
                                    continue
                                result[group, column] += 1
                else:
                    with nogil, parallel(num_threads=num_threads):
                        thread_index = threadid()
                        col_range = get_thread_offset(indptr, thread_index, num_threads)
                        for column in range(col_range.first, col_range.second):
                            gene = gene_map[column]
                            for cell in range(<unsigned long> indptr[gene],
                                              <unsigned long> indptr[gene + 1]):
                                group = group_map[indices[cell]]
                                result[group, column] += 1
                    ''', warn_undeclared=False)\
                ['groupby_getnnz_csc_for_gene_subset'](
                    indices=X.indices, indptr=X.indptr,
                    group_map=group_map, gene_map=gene_map,
                    has_missing=has_missing, result=detection_count,
                    num_threads=num_threads)
        
        # For each cell type, calculate the detection rate and the fold change
        # of the detection rate. Also, initialize the candidate set of points
        # on the Pareto front to those with detection rate of at least
        # `min_detection_rate` and fold change of at least `min_fold_change`
        total_detection_count = detection_count.sum(axis=0, dtype=np.uint32)
        num_cells_per_cell_type = groups['_SingleCell_num_cells'].to_numpy()
        total_num_cells = num_cells_per_cell_type.sum()
        detection_rate = np.empty((num_cell_types, num_genes),
                                  dtype=np.float32)
        fold_change = np.empty((num_cell_types, num_genes), dtype=np.float32)
        cython_inline(r'''
            from cython.parallel cimport prange
            
            def get_detection_rate_and_fold_change(
                    const unsigned[:, ::1] detection_count,
                    const unsigned[::1] total_detection_count,
                    const unsigned[::1] num_cells_per_cell_type,
                    const unsigned total_num_cells,
                    float[:, ::1] detection_rate,
                    float[:, ::1] fold_change,
                    const unsigned num_threads):
                cdef unsigned cell_type, gene, count, background_count, \
                    num_cells, background_num_cells, \
                    num_cell_types = detection_count.shape[0], \
                    num_genes = detection_count.shape[1]
                cdef float pair_detection_rate, pair_fold_change
                
                if num_threads == 1:
                    for cell_type in range(num_cell_types):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <float> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
                else:
                    for cell_type in prange(num_cell_types, nogil=True,
                                            num_threads=num_threads):
                        num_cells = num_cells_per_cell_type[cell_type]
                        background_num_cells = total_num_cells - num_cells
                        for gene in range(num_genes):
                            count = detection_count[cell_type, gene]
                            pair_detection_rate = <float> count / num_cells
                            background_count = total_detection_count[gene] - count
                            pair_fold_change = pair_detection_rate * \
                                background_num_cells / background_count
                            detection_rate[cell_type, gene] = pair_detection_rate
                            fold_change[cell_type, gene] = pair_fold_change
            ''')['get_detection_rate_and_fold_change'](
                detection_count=detection_count,
                total_detection_count=total_detection_count,
                num_cells_per_cell_type=num_cells_per_cell_type,
                total_num_cells=total_num_cells, detection_rate=detection_rate,
                fold_change=fold_change, num_threads=num_threads)
        
        # If `swap_axes=True`, swap cell types and genes
        if swap_axes:
            cell_types, genes = genes, cell_types
            num_cell_types, num_genes = num_genes, num_cell_types
            detection_rate = detection_rate.T
            fold_change = fold_change.T
        
        # Calculate the range of the legend, and the multiplier to multiply
        # each point's size by
        max_detection_rate = detection_rate.max()
        interval = 0.2 if max_detection_rate > 0.5 else \
            0.1 if max_detection_rate > 0.2 else 0.05
        max_detection_rate = \
            np.ceil(max_detection_rate / interval) * interval
        legend_point_sizes = \
            np.arange(interval, max_detection_rate + interval / 2, interval)
        point_size_multiplier = 180 / max_detection_rate
        
        try:
            # Make the figure, including separate portions on the left for the
            # legend and colorbar (if `colorbar=True`)
            default_figure_kwargs = dict(layout='constrained')
            # noinspection PyTypeChecker
            width_ratio = max(4, 0.2 * num_genes)
            if figure_kwargs is None or 'figsize' not in figure_kwargs:
                if colorbar:
                    width = 6.4 / (1 + width_ratio) + \
                            6.4 * width_ratio / (1 + width_ratio) * \
                            max(num_genes, 5) / 20
                else:
                    width = 6.4 * max(num_genes, 5) / 20
                height = max(4.8, 1 + 3.8 * num_cell_types / 20)
                default_figure_kwargs['figsize'] = width, height
            figure_kwargs = default_figure_kwargs | figure_kwargs \
                if figure_kwargs is not None else default_figure_kwargs
            fig = plt.figure(**figure_kwargs)
            if colorbar:
                gs = fig.add_gridspec(2, 2, width_ratios=[width_ratio, 1],
                                      height_ratios=[1, 1])
            else:
                gs = fig.add_gridspec(2, 1, height_ratios=[1, 1])
            
            # Plot the circles; override the defaults for certain keys of
            # `scatter_kwargs`
            ax_main = fig.add_subplot(gs[:, 0])  # the main plot spans all rows
            point_size = detection_rate * point_size_multiplier
            x, y = np.meshgrid(range(len(genes)), range(len(cell_types)))
            default_scatter_kwargs = dict(linewidths=0)
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
            scatter = ax_main.scatter(
                x.ravel(), y.ravel(), s=point_size.ravel(),
                c=np.log2(fold_change), cmap=colormap, **scatter_kwargs)
            ax_main.set_aspect('equal')
            ax_main.invert_yaxis()
            
            # Set x and y limits
            padding = 0.6
            ax_main.set_xlim([-padding, len(genes) - 1 + padding])
            ax_main.set_ylim([-padding, len(cell_types) - 1 + padding])
            
            # Add x and y ticks and tick labels
            ax_main.set_xticks(range(len(genes)), genes, rotation=90)
            ax_main.set_yticks(range(len(cell_types)), cell_types)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax_main.set_xlabel(xlabel)
                else:
                    ax_main.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax_main.set_ylabel(ylabel)
                else:
                    ax_main.set_ylabel(ylabel, **ylabel_kwargs)
            
            # Add a legend for detection rate; markers should be at intervals
            # of `X`% (`X`%, `2X`%, `3X`%, ...) up to the maximum detection
            # rate (rounded up to the nearest `X`%). Override the defaults for
            # certain keys of `legend_kwargs`.
            ax_legend = fig.add_subplot(gs[0, 1])
            ax_legend.axis('off')
            legend_elements = [
                plt.Line2D([0], [0], label=f'{100 * size:.0f}%',
                           markersize=np.sqrt(size * point_size_multiplier),
                           marker='o', linestyle='None',
                           markerfacecolor='black', markeredgecolor='None')
                for size in legend_point_sizes]
            default_legend_kwargs = dict(title='Detection rate', loc='center',
                                         frameon=False)
            legend_kwargs = default_legend_kwargs | legend_kwargs \
                if legend_kwargs is not None else default_legend_kwargs
            ax_legend.legend(handles=legend_elements, **legend_kwargs)
            
            # Add a colorbar for fold change, with labels at powers of 2
            if colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else \
                    default_colorbar_kwargs
                ax_colorbar = fig.add_subplot(gs[1, 1])
                cbar = plt.colorbar(scatter, cax=ax_colorbar,
                                    **colorbar_kwargs)
                cbar.outline.set_visible(False)
                cbar.ax.set_box_aspect(12)
                cbar.ax.set_title('Fold change', size='medium')
                cbar.ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))
                cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(
                    lambda x, pos: f'{2 ** x:.4f}'.rstrip('0').rstrip('.')))
                
            # Add the title
            if title is not None:
                if title_kwargs is None:
                    ax_main.set_title(title)
                else:
                    ax_main.set_title(title, **title_kwargs)
        
            # Despine, if specified
            if despine:
                spines = ax_main.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                plt.close()
        except:
            # Since we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            plt.close()
            raise
    
    def embed_old(self,
              *,
              QC_column: SingleCellColumn | None = 'passed_QC',
              PC_key: str = 'PCs',
              neighbors_key: str = 'neighbors',
              embedding_key: str = 'PaCMAP',
              num_neighbors: int | np.integer = 10,
              num_extra_neighbors: int | np.integer = 10,
              num_mid_near_pairs: int | np.integer = 5,
              num_further_pairs: int | np.integer = 20,
              num_iterations: int | np.integer |
                              tuple[int | np.integer, int | np.integer,
                                    int | np.integer] | None = None,
              learning_rate: int | float | np.integer | np.floating = 1.0,
              seed: int | np.integer = 0,
              faster_single_threading: bool = False,
              overwrite: bool = False,
              verbose: bool = True,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate a two-dimensional embedding of this SingleCell dataset
        suitable for plotting with `plot_embedding()`.
        
        Uses PaCMAP (Pairwise Controlled Manifold Approximation;
        github.com/YingfanWang/PaCMAP; arxiv.org/pdf/2012.04456), a faster
        alternative to UMAP that also captures global structure better.
        
        This function is intended to be run after `PCA()` and `neighbors()`; by
        default, it uses `obsm['PCs']` and `obsm['neighbors']` as the inputs to
        PaCMAP, and stores the output in `obsm['PaCMAP']` as a `len(obs)` × 2
        NumPy array. It can also be run on Harmony embeddings by running
        `harmonize()` and then specifying `PC_key='Harmony_PCs'`.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their embeddings set to `NaN`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as an input for the
                    embedding calculation. Can also be set to the Harmony
                    embeddings calculated by `harmonize()`, by specifying
                    `PC_key='Harmony_PCs'`.
            neighbors_key: the key of `obsm` containing the nearest-neighbor
                           indices for each cell, to use as an input for the
                           embedding calculation
            embedding_key: the key of `obsm` where the embeddings will be
                           stored
            num_neighbors: the number of nearest neighbors to consider for
                           local structure preservation. `neighbors_key` must
                           contain at least
                           `num_neighbors + num_extra_neighbors` nearest
                           neighbors.
            num_extra_neighbors: the number of extra nearest neighbors (on top
                                 of `num_neighbors`) to search for initially,
                                 before pruning to the `num_neighbors` of these
                                 `num_neighbors + num_extra_neighbors` cells
                                 with the smallest scaled distances. For a pair
                                 of cells `i` and `j`, the scaled distance
                                 between `i` and `j` is its squared Euclidean
                                 distance, divided by `i`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors, divided by `j`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors. Must be a non-negative integer.
                                 Defaults to 10, instead of PaCMAP's original
                                 default of 50. `neighbors_key` must contain at
                                 least `num_neighbors + num_extra_neighbors`
                                 nearest neighbors.
            num_mid_near_pairs: the number of mid-near pairs to consider for
                                global structure preservation
            num_further_pairs: the number of further pairs to consider for
                               local and global structure preservation
            num_iterations: the number of iterations to run PaCMAP for. Can be
                            a length-3 tuple of the number of iterations for
                            each of the 3 stages of PaCMAP, or a single integer
                            of the number of iterations for the third stage (in
                            which case the number of iterations for the first
                            two stages will be set to 100).
            learning_rate: the learning rate of the Adam optimizer for PaCMAP
            seed: the random seed toer use for PaCMAP
            faster_single_threading: if `True`, use a different order of
                                     operations for single-threaded PaCMAP.
                                     This gives a modest (~15%) boost in
                                     single-threaded performance at the cost of
                                     no longer exactly matching the embedding
                                     produced by the multithreaded version (due
                                     to differences in floating-point error
                                     arising from the different order of
                                     operations). Must be `False` unless
                                     `num_threads=1`.
            overwrite: if `True`, overwrite `embedding_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the PaCMAP construction
            num_threads: the number of threads to use when running PaCMAP. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores.
        
        Returns:
            A new SingleCell dataset with the PaCMAP embedding stored in
            `obsm[embedding_key]`.
        
        Note:
            PaCMAP's original implementation assumes generic input data, so it
            initializes the embedding by standardizing the input data, running
            PCA on it, and taking the first two PCs. Because our input data is
            already PCs (or harmonized PCs), we avoid redundancy by omitting
            this step and initializing the embedding with the first two columns
            of our input data, i.e. the first two PCs.
        """
        # Check that `embedding_key` is a string
        check_type(embedding_key, 'embedding_key', str, 'a string')
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        # Check that `embedding_key` is not already a key in `obsm`, unless
        # `overwrite=True`
        if not overwrite and embedding_key in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is already a key of obsm; '
                f'did you already run embed()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        # Get PCs, and check that they are float32 and C-contiguous
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = \
                f'PC_key {PC_key!r} is not a key of obsm'
            if neighbors_key == 'PCs':
                error_message += (
                    '; did you forget to run PCA() (and possibly neighbors()) '
                    'before embed()?')
            raise ValueError(error_message)
        PCs = self._obsm[PC_key]
        if PCs.dtype != np.float32:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float32'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        # Get the nearest-neighbor indices, and check that they have integer
        # dtype and are C-contiguous
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += \
                    '; did you forget to run neighbors() before embed()?'
            raise ValueError(error_message)
        nearest_neighbor_indices = self._obsm[neighbors_key]
        if not np.issubdtype(nearest_neighbor_indices.dtype, np.integer):
            error_message = (
                f'obsm[{neighbors_key!r}] must have integer data type, but '
                f'has data type {str(nearest_neighbor_indices.dtype)!r}')
            raise TypeError(error_message)
        # Subset PCs and nearest-neighbor indices to QCed cells only, if
        # `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
            nearest_neighbor_indices = nearest_neighbor_indices[QCed_NumPy]
        # Check that `num_neighbors`, `num_mid_near_pairs` and
        # `num_further_pairs` are positive integers
        for variable, variable_name in (
                (num_neighbors, 'num_neighbors'),
                (num_mid_near_pairs, 'num_mid_near_pairs'),
                (num_further_pairs, 'num_further_pairs')):
            check_type(variable, variable_name, int, 'a positive integer')
            check_bounds(variable, variable_name, 1)
        # Check that `num_extra_neighbors` is a non-negative integer
        check_type(num_extra_neighbors, 'num_extra_neighbors', int,
                   'a non-negative integer')
        check_bounds(num_extra_neighbors, 'num_extra_neighbors', 0)
        # Check that `num_iterations` is an integer or length-3 tuple of
        # integers, or `None`
        if num_iterations is not None:
            check_type(num_iterations, 'num_iterations', (int, tuple),
                       'a positive integer or length-3 tuple of positive '
                       'integers')
            if isinstance(num_iterations, tuple):
                if len(num_iterations) != 3:
                    error_message = (
                        f'num_iterations must be a positive integer or '
                        f'length-3 tuple of positive integers, but has length '
                        f'{len(num_iterations):,}')
                    raise ValueError(error_message)
                for step, step_num_iterations in enumerate(num_iterations):
                    check_type(step_num_iterations,
                               f'num_iterations[{step!r}]', int,
                               'a positive integer')
                    check_bounds(step_num_iterations,
                                 f'num_iterations[{step!r}]', 1)
            else:
                check_bounds(num_iterations, 'num_iterations', 1)
                num_iterations = 100, 100, num_iterations
        else:
            num_iterations = 100, 100, 250
        # Check that `learning_rate` is a positive floating-point number
        check_type(learning_rate, 'learning_rate', (int, float),
                   'a positive number')
        check_bounds(learning_rate, 'learning_rate', 0, left_open=True)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        # Check that `faster_single_threading` is Boolean, and `False` unless
        # `num_threads=1`
        check_type(faster_single_threading, 'faster_single_threading', bool,
                   'Boolean')
        if faster_single_threading and num_threads != 1:
            error_message = \
                'faster_single_threading must be False unless num_threads is 1'
            raise ValueError(error_message)
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        # Define Cython functions
        cython_functions = cython_inline(_uninitialized_vector_import + r'''
        from cython.parallel cimport parallel, prange, threadid
        from libc.limits cimport UINT_MAX
        from libc.string cimport memcpy
        from libcpp.algorithm cimport sort
        from libcpp.cmath cimport sqrt
        from libcpp.vector cimport vector
        
        cdef extern from * nogil:
            """
            #define atomic_add(x, y) _Pragma("omp atomic") x += y
            """
            void atomic_add(unsigned &x, unsigned y)
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef extern from * nogil:
            """
            struct Compare {
                const float* data;
                Compare() noexcept {}
                Compare(const float* d) noexcept : data(d) {}
                bool operator()(unsigned a, unsigned b) const noexcept {
                    return data[a] < data[b];
                }
            };
            """
            cdef cppclass Compare:
                Compare(const float*) noexcept
                bint operator()(unsigned, unsigned) noexcept
        
        cdef inline void argsort(const float* arr, unsigned* indices,
                                 const unsigned n) noexcept nogil:
            cdef unsigned i
            for i in range(n):
                indices[i] = i
            sort(indices, indices + n, Compare(arr))
            
        def get_scaled_distances(const float[:, ::1] X,
                                 const long[:, :] neighbors,
                                 float[:, ::1] scaled_distances,
                                 const unsigned num_threads):
            cdef unsigned i, j, k, num_cells = scaled_distances.shape[0], \
                num_total_neighbors = scaled_distances.shape[1], \
                num_PCs = X.shape[1]
            cdef long neighbor
            cdef unsigned too_small = 0, too_large = 0
            cdef uninitialized_vector[float] sig
            sig.resize(num_cells)
            
            if num_threads == 1:
                for i in range(num_cells):
                    for j in range(num_total_neighbors):
                        scaled_distances[i, j] = 0
                        for k in range(num_PCs):
                            scaled_distances[i, j] = \
                                scaled_distances[i, j] + \
                                (X[i, k] - X[j, k]) ** 2
                    sig[i] = (sqrt(scaled_distances[i, 3]) +
                              sqrt(scaled_distances[i, 4]) +
                              sqrt(scaled_distances[i, 5])) / 3
                    if sig[i] < 1e-10:
                        sig[i] = 1e-10
                
                for i in range(num_cells):
                    for j in range(num_total_neighbors):
                        neighbor = neighbors[i, j]
                        if neighbor < 0:
                            return True, False
                        if neighbor >= <long> num_cells:
                            return False, True
                        scaled_distances[i, j] = scaled_distances[i, j] / \
                            sig[i] / sig[neighbor]
            else:
                with nogil:
                    for i in prange(num_cells, num_threads=num_threads):
                        for j in range(num_total_neighbors):
                            scaled_distances[i, j] = 0
                            for k in range(num_PCs):
                                scaled_distances[i, j] = \
                                    scaled_distances[i, j] + \
                                    (X[i, k] - X[j, k]) ** 2
                        sig[i] = (sqrt(scaled_distances[i, 3]) +
                                  sqrt(scaled_distances[i, 4]) +
                                  sqrt(scaled_distances[i, 5])) / 3
                        if sig[i] < 1e-10:
                            sig[i] = 1e-10
                    
                    for i in prange(num_cells, num_threads=num_threads):
                        for j in range(num_total_neighbors):
                            neighbor = neighbors[i, j]
                            if neighbor < 0:
                                atomic_add(too_small, 1)
                                with gil:
                                    return too_small, too_large
                            if neighbor >= <long> num_cells:
                                atomic_add(too_large, 1)
                                with gil:
                                    return too_small, too_large
                            scaled_distances[i, j] = scaled_distances[i, j] / \
                                sig[i] / sig[neighbor]
            
            return too_small, too_large
    
        def get_neighbor_pairs(const float[:, ::1] X,
                               const float[:, ::1] scaled_distances,
                               const long[:, :] neighbors,
                               unsigned[:, ::1] neighbor_pairs,
                               const unsigned num_threads):
            cdef unsigned i, j, thread_index, num_cells = X.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_total_neighbors = scaled_distances.shape[1]
            cdef uninitialized_vector[unsigned] indices
            cdef vector[uninitialized_vector[unsigned]] thread_indices
            
            if num_threads == 1:
                indices.resize(num_total_neighbors)
                for i in range(num_cells):
                    argsort(&scaled_distances[i, 0], indices.data(),
                            num_total_neighbors)
                    for j in range(num_neighbors):
                        neighbor_pairs[i, j] = \
                            <unsigned> neighbors[i, indices[j]]
            else:
                thread_indices.resize(num_threads)
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    thread_indices[thread_index].resize(num_total_neighbors)
                    for i in prange(num_cells):
                        argsort(&scaled_distances[i, 0],
                                thread_indices[thread_index].data(), num_total_neighbors)
                        for j in range(num_neighbors):
                            neighbor_pairs[i, j] = \
                                <unsigned> neighbors[i, thread_indices[thread_index][j]]
        
        def sample_mid_near_pairs(const float[:, ::1] X,
                                  unsigned[:, ::1] mid_near_pairs,
                                  const unsigned long seed,
                                  const unsigned num_threads):
            cdef unsigned i, j, k, l, sampled_k, thread_index, \
                n = X.shape[0], closest_cell = UINT_MAX, \
                second_closest_cell = UINT_MAX, \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_PCs = X.shape[1]
            cdef float difference, distance, smallest, second_smallest
            cdef unsigned long state
            cdef uninitialized_vector[unsigned] sampled
            cdef vector[uninitialized_vector[unsigned]] thread_sampled
            
            if num_threads == 1:
                sampled.resize(6)
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_mid_near_pairs):
                        # Randomly sample 6 cells (which are not the
                        # current cell) and select the 2nd-closest
                        smallest = UINT_MAX
                        second_smallest = UINT_MAX
                        for k in range(6):
                            while True:
                                # Sample a random cell...
                                sampled[k] = randint(n, &state)
                                # ...that is not this cell...
                                if sampled[k] == i:
                                    continue
                                # ...nor a previously sampled cell
                                for l in range(k):
                                    if sampled[k] == sampled[l]:
                                        break
                                else:
                                    break
                        for k in range(6):
                            sampled_k = sampled[k]
                            difference = X[i, 0] - X[sampled_k, 0]
                            distance = difference * difference
                            for l in range(1, num_PCs):
                                difference = X[i, l] - X[sampled_k, l]
                                distance += difference * difference
                            if distance < smallest:
                                second_smallest = smallest
                                second_closest_cell = closest_cell
                                smallest = distance
                                closest_cell = sampled[k]
                            elif distance < second_smallest:
                                second_smallest = distance
                                second_closest_cell = sampled[k]
                        mid_near_pairs[i, j] = second_closest_cell
            else:
                thread_sampled.resize(num_threads)
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    thread_sampled[thread_index].resize(6)
                    for i in prange(n):
                        state = srand(seed + i)
                        for j in range(num_mid_near_pairs):
                            smallest = UINT_MAX
                            second_smallest = UINT_MAX
                            for k in range(6):
                                while True:
                                    thread_sampled[thread_index][k] = \
                                        randint(n, &state)
                                    if thread_sampled[thread_index][k] == i:
                                        continue
                                    for l in range(k):
                                        if thread_sampled[thread_index][k] == \
                                                thread_sampled[thread_index][l]:
                                            break
                                    else:
                                        break
                            for k in range(6):
                                sampled_k = thread_sampled[thread_index][k]
                                difference = X[i, 0] - X[sampled_k, 0]
                                distance = difference * difference
                                for l in range(1, num_PCs):
                                    difference = X[i, l] - X[sampled_k, l]
                                    distance = distance + difference * difference
                                if distance < smallest:
                                    second_smallest = smallest
                                    second_closest_cell = closest_cell
                                    smallest = distance
                                    closest_cell = thread_sampled[thread_index][k]
                                elif distance < second_smallest:
                                    second_smallest = distance
                                    second_closest_cell = thread_sampled[thread_index][k]
                            mid_near_pairs[i, j] = second_closest_cell
        
        def sample_further_pairs(const float[:, ::1] X,
                                 const unsigned[:, ::1] neighbor_pairs,
                                 unsigned[:, ::1] further_pairs,
                                 const unsigned long seed,
                                 const unsigned num_threads):
            """Sample Further pairs using the given seed."""
            cdef unsigned i, j, k, further_pair_index, n = X.shape[0], \
                num_further_pairs = further_pairs.shape[1], \
                num_neighbors = neighbor_pairs.shape[1]
            cdef unsigned long state
            
            if num_threads == 1:
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            # Sample a random cell...
                            further_pair_index = randint(n, &state)
                            # ...that is not this cell...
                            if further_pair_index == i:
                                continue
                            # ...nor one of its nearest neighbors...
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                # ...nor a previously sampled cell
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
            else:
                for i in prange(n, nogil=True, num_threads=num_threads):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            further_pair_index = randint(n, &state)
                            if further_pair_index == i:
                                continue
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
        
        def reformat_for_parallel(const unsigned[:, ::1] pairs,
                                  unsigned[::1] pair_indices,
                                  unsigned[::1] pair_indptr):
            cdef unsigned i, j, k, dest_index, num_cells = pairs.shape[0], \
                num_pairs_per_cell = pairs.shape[1]
            cdef uninitialized_vector[unsigned] dest_indices
            
            # Tabulate how often each cell appears in pairs; at a minimum, it
            # will appear `pairs.shape[1]` times (i.e. the number of
            # neighbors), as the `i` in the pair, but it will also appear a
            # variable number of times as the `j` in the pair.
            
            pair_indptr[0] = 0
            pair_indptr[1:] = pairs.shape[1]
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indptr[j + 1] += 1
                    
            # Take the cumulative sum of the values in `pair_indptr`
            
            for i in range(2, pair_indptr.shape[0]):
                pair_indptr[i] += pair_indptr[i - 1]
                
            # Now that we know how many pairs each cell is a part of, do a
            # second pass over `pairs` to populate `pair_indices` with the
            # pairs' indices. Use a temporary buffer, `dest_indices`, to keep
            # track of the index within `pair_indptr` to write each cell's next
            # pair to.
            
            dest_indices.resize(num_cells)
            memcpy(dest_indices.data(), &pair_indptr[0],
                   num_cells * sizeof(unsigned))
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indices[dest_indices[i]] = j
                    pair_indices[dest_indices[j]] = i
                    dest_indices[i] += 1
                    dest_indices[j] += 1

        def get_gradients(const float[:, ::1] embedding,
                          const unsigned[:, ::1] neighbor_pairs,
                          const unsigned[:, ::1] mid_near_pairs,
                          const unsigned[:, ::1] further_pairs,
                          const float w_neighbors,
                          const float w_mid_near,
                          float[:, ::1] gradients):
            cdef unsigned i, j, k, num_cells = neighbor_pairs.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_further_pairs = further_pairs.shape[1]
            cdef float distance_ij, embedding_ij_0, embedding_ij_1, w
            gradients[:] = 0
            
            # Nearest-neighbor pairs
            
            for i in range(num_cells):
                for k in range(num_neighbors):
                    j = neighbor_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                    
            # Mid-near pairs
            
            for i in range(num_cells):
                for k in range(num_mid_near_pairs):
                    j = mid_near_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                    gradients[i, 0] += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients[i, 1] += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                    
            # Further pairs
            
            for i in range(num_cells):
                for k in range(num_further_pairs):
                    j = further_pairs[i, k]
                    embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                    embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                    w = 2 / (1 + distance_ij) ** 2
                    gradients[i, 0] -= w * embedding_ij_0
                    gradients[j, 0] += w * embedding_ij_0
                    gradients[i, 1] -= w * embedding_ij_1
                    gradients[j, 1] += w * embedding_ij_1
        
        def get_gradients_parallel(const float[:, ::1] embedding,
                                   const unsigned[::1] neighbor_pair_indices,
                                   const unsigned[::1] neighbor_pair_indptr,
                                   const unsigned[::1] mid_near_pair_indices,
                                   const unsigned[::1] mid_near_pair_indptr,
                                   const unsigned[::1] further_pair_indices,
                                   const unsigned[::1] further_pair_indptr,
                                   const float w_neighbors,
                                   const float w_mid_near,
                                   float[:, ::1] gradients,
                                   const unsigned num_threads):
            cdef unsigned i, j, k, num_cells = embedding.shape[0]
            cdef float distance_ij, embedding_ij_0, embedding_ij_1, w
            
            if num_threads == 1:
                for i in range(num_cells):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    
                    # Nearest-neighbor pairs
                    
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] += w * embedding_ij_0
                        gradients[i, 1] += w * embedding_ij_1
                        
                    # Mid-near pairs
                    
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] += w * embedding_ij_0
                        gradients[i, 1] += w * embedding_ij_1
                        
                    # Further pairs
                    
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] -= w * embedding_ij_0
                        gradients[i, 1] -= w * embedding_ij_1
            else:
                for i in prange(num_cells, nogil=True, num_threads=num_threads):
                    gradients[i, 0] = 0
                    gradients[i, 1] = 0
                    
                    # Nearest-neighbor pairs
                    
                    for k in range(neighbor_pair_indptr[i],
                                   neighbor_pair_indptr[i + 1]):
                        j = neighbor_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_neighbors * (20 / (10 + distance_ij) ** 2)
                        gradients[i, 0] += w * embedding_ij_0
                        gradients[i, 1] += w * embedding_ij_1
                        
                    # Mid-near pairs
                    
                    for k in range(mid_near_pair_indptr[i],
                                   mid_near_pair_indptr[i + 1]):
                        j = mid_near_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = w_mid_near * 20000 / (10000 + distance_ij) ** 2
                        gradients[i, 0] += w * embedding_ij_0
                        gradients[i, 1] += w * embedding_ij_1
                        
                    # Further pairs
                    
                    for k in range(further_pair_indptr[i],
                                   further_pair_indptr[i + 1]):
                        j = further_pair_indices[k]
                        embedding_ij_0 = embedding[i, 0] - embedding[j, 0]
                        embedding_ij_1 = embedding[i, 1] - embedding[j, 1]
                        distance_ij = 1 + embedding_ij_0 ** 2 + embedding_ij_1 ** 2
                        w = 2 / (1 + distance_ij) ** 2
                        gradients[i, 0] -= w * embedding_ij_0
                        gradients[i, 1] -= w * embedding_ij_1
        
        def update_embedding_adam(float[:, ::1] embedding,
                                  const float[:, ::1] gradients,
                                  float[:, ::1] momentum,
                                  float[:, ::1] velocity,
                                  const float beta1,
                                  const float beta2,
                                  float learning_rate,
                                  const unsigned iteration,
                                  const unsigned num_threads):
            cdef unsigned i, num_cells = embedding.shape[0]
            
            learning_rate = \
                learning_rate * sqrt(1 - beta2 ** (iteration + 1)) / \
                (1 - beta1 ** (iteration + 1))
            if num_threads == 1:
                for i in range(num_cells):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            else:
                for i in prange(num_cells, nogil=True,
                                num_threads=num_threads):
                    momentum[i, 0] += \
                        (1 - beta1) * (gradients[i, 0] - momentum[i, 0])
                    velocity[i, 0] += \
                        (1 - beta2) * (gradients[i, 0] ** 2 - velocity[i, 0])
                    embedding[i, 0] -= learning_rate * momentum[i, 0] / \
                                       (sqrt(velocity[i, 0]) + 1e-7)
                    momentum[i, 1] += \
                        (1 - beta1) * (gradients[i, 1] - momentum[i, 1])
                    velocity[i, 1] += \
                        (1 - beta2) * (gradients[i, 1] ** 2 - velocity[i, 1])
                    embedding[i, 1] -= learning_rate * momentum[i, 1] / \
                                       (sqrt(velocity[i, 1]) + 1e-7)
            ''')
        get_scaled_distances = cython_functions['get_scaled_distances']
        get_neighbor_pairs = cython_functions['get_neighbor_pairs']
        sample_mid_near_pairs = cython_functions['sample_mid_near_pairs']
        sample_further_pairs = cython_functions['sample_further_pairs']
        update_embedding_adam = cython_functions['update_embedding_adam']
        # Get scaled distances between each cell and its nearest neighbors
        scaled_distances = np.empty_like(nearest_neighbor_indices,
                                         dtype=np.float32)
        too_small, too_large = get_scaled_distances(
            PCs, nearest_neighbor_indices, scaled_distances, num_threads)
        # If any nearest-neighbor indices were out of range, raise an error
        if too_small:
            error_message = (
                f'some nearest-neighbor indices in obsm[{neighbors_key!r}] '
                f'are negative')
            raise ValueError(error_message)
        if too_large:
            error_message = (
                f'some nearest-neighbor indices in obsm[{neighbors_key!r}] '
                f'are >= the total number of cells, '
                f'{nearest_neighbor_indices.shape[0]:,}. This may happen if '
                f'you subset this SingleCell dataset between neighbors() and '
                f'embed(); if so, make sure to run neighbors() after, not '
                f'before, subsetting.')
            raise ValueError(error_message)
        # Select the `num_neighbors` of the
        # `num_neighbors + num_extra_neighbors` nearest-neighbor pairs with the
        # lowest scaled distances
        num_cells = PCs.shape[0]
        neighbor_pairs = np.empty((num_cells, num_neighbors), dtype=np.uint32)
        get_neighbor_pairs(PCs, scaled_distances, nearest_neighbor_indices,
                           neighbor_pairs, num_threads)
        del nearest_neighbor_indices, scaled_distances
        # Sample mid-near pairs
        mid_near_pairs = np.empty((num_cells, num_mid_near_pairs),
                                  dtype=np.uint32)
        sample_mid_near_pairs(PCs, mid_near_pairs, seed, num_threads)
        # Sample further pairs
        further_pairs = np.empty((num_cells, num_further_pairs),
                                 dtype=np.uint32)
        sample_further_pairs(PCs, neighbor_pairs, further_pairs,
                             seed + mid_near_pairs.size, num_threads)
        # If multithreaded, or single-threaded with
        # `faster_single_threading=False`, reformat the three lists of pairs to
        # ensure deterministic parallelism. Specifically, transform pairs of
        # cell indices from the original format of a 2D array `pairs` where
        # `pairs[i]` contains all js for which (i, j) is a pair, to a pair of
        # 1D arrays `pair_indices` and `pair_indptr` forming a sparse array,
        # where `pair_indices[pair_indptr[i]:pair_indptr[i + 1]]` contains all
        # js for which (i, j) is a pair or (j, i) is a pair. `pair_indices`
        # must have length `2 * pairs.size`, since each pair will appear twice,
        # once for (i, j) and once for (j, i). `pair_indptr` must have length
        # equal to the number of cells plus one, just like for scipy sparse
        # matrices.
        if faster_single_threading:
            get_gradients = cython_functions['get_gradients']
        else:
            reformat_for_parallel = cython_functions['reformat_for_parallel']
            
            neighbor_pair_indices = np.empty(2 * neighbor_pairs.size,
                                             dtype=np.uint32)
            neighbor_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(neighbor_pairs, neighbor_pair_indices,
                                  neighbor_pair_indptr)
            del neighbor_pairs
            mid_near_pair_indices = \
                np.empty(2 * mid_near_pairs.size, dtype=np.uint32)
            mid_near_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(mid_near_pairs, mid_near_pair_indices,
                                  mid_near_pair_indptr)
            del mid_near_pairs
            further_pair_indices = \
                np.empty(2 * further_pairs.size, dtype=np.uint32)
            further_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(further_pairs, further_pair_indices,
                                  further_pair_indptr)
            del further_pairs
            get_gradients = cython_functions['get_gradients_parallel']
        # Initialize the embedding, gradients, and other optimizer parameters
        embedding = 0.01 * PCs[:, :2]
        gradients = np.zeros_like(embedding, dtype=np.float32)
        momentum = np.zeros_like(embedding, dtype=np.float32)
        velocity = np.zeros_like(embedding, dtype=np.float32)
        w_mid_near_init = 1000
        beta1 = 0.9
        beta2 = 0.999
        # Optimize the embedding
        num_phase_1_iterations, num_phase_2_iterations, \
            num_phase_3_iterations = num_iterations
        for iteration in range(num_phase_1_iterations +
                               num_phase_2_iterations +
                               num_phase_3_iterations):
            if iteration < num_phase_1_iterations:
                w_mid_near = \
                    (1 - iteration / num_phase_1_iterations) * \
                    w_mid_near_init + iteration / num_phase_1_iterations * 3
                w_neighbors = 2
            elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                w_mid_near = 3
                w_neighbors = 3
            else:
                w_mid_near = 0
                w_neighbors = 1
            # Calculate gradients
            if faster_single_threading:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pairs, mid_near_pairs,
                              further_pairs, w_neighbors, w_mid_near,
                              gradients)
            else:
                # noinspection PyUnboundLocalVariable
                get_gradients(embedding, neighbor_pair_indices,
                              neighbor_pair_indptr, mid_near_pair_indices,
                              mid_near_pair_indptr, further_pair_indices,
                              further_pair_indptr, w_neighbors, w_mid_near,
                              gradients, num_threads)
            # Update the embedding based on the gradients, via the Adam
            # optimizer
            update_embedding_adam(embedding, gradients, momentum, velocity,
                                  beta1, beta2, learning_rate, iteration,
                                  num_threads)
        # If `QC_column` was specified, back-project from QCed cells to all
        # cells, filling with `NaN`
        if QC_column is not None:
            embedding_QCed = embedding
            embedding = np.full((len(self), embedding_QCed.shape[1]), np.nan,
                                dtype=np.float32)
            # noinspection PyUnboundLocalVariable
            embedding[QCed_NumPy] = embedding_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {embedding_key: embedding},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    def embed(self,
              *,
              QC_column: SingleCellColumn | None = 'passed_QC',
              PC_key: str = 'PCs',
              neighbors_key: str = 'neighbors',
              distances_key: str = 'distances',
              embedding_key: str = 'PaCMAP',
              num_neighbors: int | np.integer = 10,
              num_extra_neighbors: int | np.integer = 10,
              num_mid_near_pairs: int | np.integer = 5,
              num_further_pairs: int | np.integer = 20,
              num_iterations: int | np.integer |
                              tuple[int | np.integer, int | np.integer,
                                    int | np.integer] | None = None,
              learning_rate: int | float | np.integer | np.floating = 1.0,
              seed: int | np.integer = 0,
              match_parallel: bool = False,
              overwrite: bool = False,
              verbose: bool = True,
              num_threads: int | np.integer | None = None) -> SingleCell:
        """
        Calculate a two-dimensional embedding of this SingleCell dataset
        suitable for plotting with `plot_embedding()`.
        
        Uses PaCMAP (Pairwise Controlled Manifold Approximation;
        github.com/YingfanWang/PaCMAP; arxiv.org/pdf/2012.04456), a faster
        alternative to UMAP that also captures global structure better.
        
        This function is intended to be run after `PCA()` and `neighbors()`; by
        default, it uses `obsm['PCs']` and `obsm['neighbors']` as the inputs to
        PaCMAP, and stores the output in `obsm['PaCMAP']` as a `len(obs)` × 2
        NumPy array. It can also be run on Harmony embeddings by running
        `harmonize()` and then specifying `PC_key='Harmony_PCs'`.
        
        Args:
            QC_column: an optional Boolean column of `obs` indicating which
                       cells passed QC. Can be a column name, a polars
                       expression, a polars Series, a 1D NumPy array, or a
                       function that takes in this SingleCell dataset and
                       returns a polars Series or 1D NumPy array. Set to `None`
                       to include all cells. Cells failing QC will be ignored
                       and have their embeddings set to `NaN`.
            PC_key: the key of `obsm` containing the principal components
                    calculated with `PCA()`, to use as an input for the
                    embedding calculation. Can also be set to the Harmony
                    embeddings calculated by `harmonize()`, by specifying
                    `PC_key='Harmony_PCs'`.
            neighbors_key: the key of `obsm` containing the nearest-neighbor
                           indices for each cell, to use as an input for the
                           embedding calculation
            distances_key: the key of `obsm` containing the squared Euclidean
                           distance to each nearest neighbor in
                           `neighbors_key`, to use as an input for the
                           embedding calculation
            embedding_key: the key of `obsm` where the embeddings will be
                           stored
            num_neighbors: the number of nearest neighbors to consider for
                           local structure preservation. `neighbors_key` must
                           contain at least
                           `num_neighbors + num_extra_neighbors` nearest
                           neighbors.
            num_extra_neighbors: the number of extra nearest neighbors (on top
                                 of `num_neighbors`) to search for initially,
                                 before pruning to the `num_neighbors` of these
                                 `num_neighbors + num_extra_neighbors` cells
                                 with the smallest scaled distances. For a pair
                                 of cells `i` and `j`, the scaled distance
                                 between `i` and `j` is its squared Euclidean
                                 distance, divided by `i`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors, divided by `j`'s average Euclidean
                                 distance to its 3rd, 4th, and 5th nearest
                                 neighbors. Must be a non-negative integer.
                                 Defaults to 10, instead of PaCMAP's original
                                 default of 50. `neighbors_key` must contain at
                                 least `num_neighbors + num_extra_neighbors`
                                 nearest neighbors.
            num_mid_near_pairs: the number of mid-near pairs to consider for
                                global structure preservation
            num_further_pairs: the number of further pairs to consider for
                               local and global structure preservation
            num_iterations: the number of iterations to run PaCMAP for. Can be
                            a length-3 tuple of the number of iterations for
                            each of the 3 stages of PaCMAP, or a single integer
                            of the number of iterations for the third stage (in
                            which case the number of iterations for the first
                            two stages will be set to 100).
            learning_rate: the learning rate of the Adam optimizer for PaCMAP
            seed: the random seed toer use for PaCMAP
            match_parallel: if `False`, use a different order of operations for
                            single-threaded PaCMAP. This gives a modest (~15%)
                            boost in single-threaded performance at the cost of
                            no longer exactly matching the embedding produced
                            by the multithreaded version (due to differences in
                            floating-point error arising from the different
                            order of operations). Must be `False` unless
                            `num_threads=1`.
            overwrite: if `True`, overwrite `embedding_key` if already present
                       in `obsm`, instead of raising an error
            verbose: whether to print details of the PaCMAP construction
            num_threads: the number of threads to use when running PaCMAP. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Does not affect the returned
                         SingleCell dataset's `num_threads`; this will always
                         be the same as the original dataset's `num_threads`.
        
        Returns:
            A new SingleCell dataset with the PaCMAP embedding stored in
            `obsm[embedding_key]`.
        
        Note:
            PaCMAP's original implementation assumes generic input data, so it
            initializes the embedding by standardizing the input data, running
            PCA on it, and taking the first two PCs. Because our input data is
            already PCs (or harmonized PCs), we avoid redundancy by omitting
            this step and initializing the embedding with the first two columns
            of our input data, i.e. the first two PCs.
        """
        # Check that `embedding_key` is a string
        check_type(embedding_key, 'embedding_key', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `embedding_key` is not already a key in `obsm`, unless
        # `overwrite=True`
        if not overwrite and embedding_key in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is already a key of obsm; '
                f'did you already run embed()? Set overwrite=True to '
                f'overwrite.')
            raise ValueError(error_message)
        
        # Get the QC column, if not None
        if QC_column is not None:
            QC_column = self._get_column(
                'obs', QC_column, 'QC_column', pl.Boolean,
                allow_missing=QC_column == 'passed_QC')
        
        # Get PCs, and check that they are float32 and C-contiguous
        check_type(PC_key, 'PC_key', str, 'a string')
        if PC_key not in self._obsm:
            error_message = f'PC_key {PC_key!r} is not a key of obsm'
            if PC_key == 'PCs':
                error_message += (
                    '; did you forget to run PCA() (and possibly neighbors()) '
                    'before embed()?')
            raise ValueError(error_message)
        PCs = self._obsm[PC_key]
        if PCs.dtype != np.float32:
            error_message = \
                f'obsm[{PC_key!r}].dtype is {PCs.dtype!r}, but must be float32'
            raise TypeError(error_message)
        if not PCs.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{PC_key!r}] is not C-contiguous; make it C-contiguous '
                f'with pipe_obsm_key({PC_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        
        # Get the nearest-neighbor indices and distances, and check that they
        # are uint32 and float32, respectively, and C-contiguous
        check_type(neighbors_key, 'neighbors_key', str, 'a string')
        if neighbors_key not in self._obsm:
            error_message = \
                f'neighbors_key {neighbors_key!r} is not a key of obsm'
            if neighbors_key == 'neighbors':
                error_message += (
                    '; did you forget to run neighbors() before embed()?')
            raise ValueError(error_message)
        neighbors = self._obsm[neighbors_key]
        if neighbors.dtype != np.uint32:
            error_message = (
                f'obsm[{neighbors_key!r}] must have uint32 data type, but '
                f'has data type {str(neighbors.dtype)!r}')
            raise TypeError(error_message)
        if not neighbors.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{neighbors_key!r}] is not C-contiguous; make it '
                f'C-contiguous with '
                f'pipe_obsm_key({neighbors_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        check_type(distances_key, 'distances_key', str, 'a string')
        if distances_key not in self._obsm:
            error_message = \
                f'distances_key {distances_key!r} is not a key of obsm'
            if distances_key == 'distances':
                error_message += (
                    '; did you forget to run neighbors() before embed()?')
            raise ValueError(error_message)
        distances = self._obsm[distances_key]
        if distances.dtype != np.float32:
            error_message = (
                f'obsm[{distances_key!r}] must have float32 data type, but '
                f'has data type {str(distances.dtype)!r}')
            raise TypeError(error_message)
        if not distances.flags['C_CONTIGUOUS']:
            error_message = (
                f'obsm[{distances_key!r}] is not C-contiguous; make it '
                f'C-contiguous with '
                f'pipe_obsm_key({distances_key!r}, np.ascontiguousarray)')
            raise ValueError(error_message)
        
        # Subset PCs and nearest-neighbor indices to QCed cells only, if
        # `QC_column` is not `None`
        if QC_column is not None:
            QCed_NumPy = QC_column.to_numpy()
            PCs = PCs[QCed_NumPy]
            neighbors = neighbors[QCed_NumPy]
            distances = distances[QCed_NumPy]
        
        # Check that there are at least 7 cells (since
        # `sample_mid_near_pairs() requires 6 other cells)
        num_cells = PCs.shape[0]
        if num_cells < 7:
            error_message = (
                f'there are fewer than 7 cells, so the embedding cannot be '
                f'calculated')
            raise ValueError(error_message)
        
        # Check that `num_neighbors` is between 1 and `num_cells - 1`
        check_type(num_neighbors, 'num_neighbors', int, 'a positive integer')
        if not 1 <= num_neighbors < num_cells:
            error_message = (
                f'num_neighbors is {num_neighbors:,}, but must be ≥ 1 and '
                f'less than the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        
        # Check that `num_mid_near_pairs` and `num_further_pairs` are between 1
        # and `num_cells`
        for variable, variable_name in (
                (num_mid_near_pairs, 'num_mid_near_pairs'),
                (num_further_pairs, 'num_further_pairs')):
            check_type(variable, variable_name, int, 'a positive integer')
            if not 1 <= variable <= num_cells:
                error_message = (
                    f'{variable_name} is {variable:,}, but must be ≥ 1 and ≤ '
                    f'the number of cells ({num_cells:,})')
                raise ValueError(error_message)
        
        # Check that there are at least `num_neighbors + num_further_pairs + 1`
        # cells (since `sample_further_pairs()` requires
        # `num_neighbors + num_further_pairs` other cells)
        if num_cells < num_neighbors + num_further_pairs + 1:
            error_message = (
                f'there are fewer than '
                f'{num_neighbors + num_further_pairs + 1} (num_neighbors + '
                f'num_further_pairs + 1) cells, so the embedding cannot be '
                f'calculated')
            raise ValueError(error_message)
        
        # Check that `num_extra_neighbors` is ≥ 0 and that
        # `num_neighbors + num_extra_neighbors` is less than `num_cells`
        check_type(num_extra_neighbors, 'num_extra_neighbors', int,
                   'a non-negative integer')
        check_bounds(num_extra_neighbors, 'num_extra_neighbors', 0)
        if num_neighbors + num_extra_neighbors >= num_cells:
            error_message = (
                f'num_neighbors ({num_neighbors:,}) + num_extra_neighbors '
                f'({num_extra_neighbors:,}) is '
                f'{num_neighbors + num_extra_neighbors:,}, but must be less '
                f'than the number of cells ({num_cells:,})')
            raise ValueError(error_message)
        
        # Check that `num_iterations` is an integer or length-3 tuple of
        # integers, or `None`
        if num_iterations is not None:
            check_type(num_iterations, 'num_iterations', (int, tuple),
                       'a positive integer or length-3 tuple of positive '
                       'integers')
            if isinstance(num_iterations, tuple):
                if len(num_iterations) != 3:
                    error_message = (
                        f'num_iterations must be a positive integer or '
                        f'length-3 tuple of positive integers, but has length '
                        f'{len(num_iterations):,}')
                    raise ValueError(error_message)
                for step, step_num_iterations in enumerate(num_iterations):
                    check_type(step_num_iterations,
                               f'num_iterations[{step!r}]', int,
                               'a positive integer')
                    check_bounds(step_num_iterations,
                                 f'num_iterations[{step!r}]', 1)
            else:
                check_bounds(num_iterations, 'num_iterations', 1)
                num_iterations = 100, 100, num_iterations
        else:
            num_iterations = 100, 100, 250
        
        # Check that `learning_rate` is a positive floating-point number
        check_type(learning_rate, 'learning_rate', (int, float),
                   'a positive number')
        check_bounds(learning_rate, 'learning_rate', 0, left_open=True)
        
        # Check that `seed` is an integer
        check_type(seed, 'seed', int, 'an integer')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads`, and if -1, set to `os.cpu_count()`
        num_threads = self._process_num_threads(num_threads)
        
        # Check that `match_parallel` is Boolean, and `False` unless
        # `num_threads=1`
        check_type(match_parallel, 'match_parallel', bool, 'Boolean')
        if match_parallel and num_threads != 1:
            error_message = \
                'match_parallel must be False unless num_threads is 1'
            raise ValueError(error_message)
        
        # Check that `verbose` is Boolean
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Define Cython functions
        cython_functions = cython_inline(_uninitialized_vector_import + _heap_functions + r'''
        from cpython.exc cimport PyErr_CheckSignals
        from cython.parallel cimport parallel, prange, threadid
        from libc.float cimport FLT_MAX
        from libcpp.algorithm cimport sort
        from libcpp.cmath cimport sqrt
        from libcpp.vector cimport vector
        
        cdef extern from * nogil:
            """
            #define atomic_or(x, y) _Pragma("omp atomic") x |= y
            """
            void atomic_or(bint &x, bint y)
        
        cdef inline unsigned rand(unsigned long* state) noexcept nogil:
            cdef unsigned long x = state[0]
            state[0] = x * 6364136223846793005UL + 1442695040888963407UL
            cdef unsigned s = (x ^ (x >> 18)) >> 27
            cdef unsigned rot = x >> 59
            return (s >> rot) | (s << ((-rot) & 31))
        
        cdef inline unsigned long srand(const unsigned long seed) noexcept nogil:
            cdef unsigned long state = seed + 1442695040888963407UL
            rand(&state)
            return state
        
        cdef inline unsigned randint(const unsigned bound, unsigned long* state) noexcept nogil:
            cdef unsigned r, threshold = -bound % bound
            while True:
                r = rand(state)
                if r >= threshold:
                    return r % bound
        
        cdef extern from * nogil:
            """
            struct Compare {
                const float* data;
                Compare() noexcept {}
                Compare(const float* d) noexcept : data(d) {}
                bool operator()(unsigned a, unsigned b) const noexcept {
                    return data[a] < data[b];
                }
            };
            """
            cdef cppclass Compare:
                Compare(const float*) noexcept
                bint operator()(unsigned, unsigned) noexcept
        
        cdef inline void argsort(const float* arr, unsigned* indices,
                                 const unsigned n) noexcept nogil:
            cdef unsigned i
            for i in range(n):
                indices[i] = i
            sort(indices, indices + n, Compare(arr))
        
        def get_neighbor_pairs(const float[:, ::1] X,
                               const unsigned[:, ::1] neighbors,
                               const float[:, ::1] distances,
                               unsigned[:, ::1] neighbor_pairs,
                               unsigned num_threads):
            cdef unsigned i, j, k, neighbor, thread_index, num_cells = X.shape[0], \
                num_total_neighbors = distances.shape[1], \
                num_neighbors = neighbor_pairs.shape[1], num_PCs = X.shape[1], \
                too_large = 0
            cdef float scaled_distance
            cdef uninitialized_vector[float] average_distances_buffer, \
                scaled_distances_i_buffer
            cdef vector[uninitialized_vector[float]] thread_scaled_distances_i
            average_distances_buffer.resize(num_cells)
            cdef float[::1] scaled_distances_i, average_distances = \
                <float[:num_cells]> average_distances_buffer.data()
            
            num_threads = min(num_threads, num_cells)
            if num_threads == 1:
                # Calculate the average Euclidean distance from each cell to its 4th-,
                # 5th-, and 6th-nearest neighbors
                for i in range(num_cells):
                    average_distances[i] = (
                        sqrt(distances[i, 3]) + sqrt(distances[i, 4]) +
                        sqrt(distances[i, 5])) / 3
                    if average_distances[i] < 1e-10:
                        average_distances[i] = 1e-10
                
                # Select the `num_neighbors` of each cell's
                # `num_total_neighbors`-nearest neighbors with the lowest scaled
                # distances. PaCMAP defines the scaled distance between cells `i` and
                # `j` as the squared Euclidean distance from `i` to `j`, divided by
                # `average_distance[i] * average_distance[j]`. However, when ranking
                # cells by their scaled distance to cell `i`, we can ignore the
                # normalization by `average_distance[i]` since it is constant for all
                # neighbors `j`.
                scaled_distances_i_buffer.resize(num_neighbors)
                scaled_distances_i = \
                    <float[:num_neighbors]> scaled_distances_i_buffer.data()
                for i in range(num_cells):
                    for j in range(num_neighbors):
                        scaled_distances_i[j] = FLT_MAX
                    for j in range(num_total_neighbors):
                        neighbor = neighbors[i, j]
                        if neighbor >= num_cells:  # check if out-of-range
                            return True
                        scaled_distance = distances[i, j] / average_distances[neighbor]
                        if scaled_distance < scaled_distances_i[0]:
                            max_heap_replace_top(&neighbor_pairs[i, 0],
                                                 &scaled_distances_i[0],
                                                 neighbor, scaled_distance,
                                                 num_neighbors)
                   
                    # Sort the heap to get nearest neighbors in ascending order of
                    # scaled distance
                    max_heap_sort(&neighbor_pairs[i, 0], &scaled_distances_i[0],
                                  num_neighbors)
            else:
                # Same as the single-threaded version, but use
                # `thread_scaled_distances_i` instead of `scaled_distances_i`. Also,
                # use hardware-level atomics to thread-safely flag out-of-range
                # neighbors.
                
                with nogil:
                    for i in prange(num_cells, num_threads=num_threads):
                        average_distances[i] = (
                            sqrt(distances[i, 3]) + sqrt(distances[i, 4]) +
                            sqrt(distances[i, 5])) / 3
                        if average_distances[i] < 1e-10:
                            average_distances[i] = 1e-10
                    
                    thread_scaled_distances_i.resize(num_threads)
                    with parallel(num_threads=num_threads):
                        thread_index = threadid()
                        thread_scaled_distances_i[thread_index].resize(num_neighbors)
                        for i in prange(num_cells):
                            for j in range(num_neighbors):
                                thread_scaled_distances_i[thread_index][j] = FLT_MAX
                            for j in range(num_total_neighbors):
                                neighbor = neighbors[i, j]
                                if neighbor >= num_cells:  # check if out-of-range
                                    atomic_or(too_large, True)
                                    with gil:
                                        return too_large
                                scaled_distance = \
                                    distances[i, j] / average_distances[neighbor]
                                if scaled_distance < \
                                        thread_scaled_distances_i[thread_index][0]:
                                    max_heap_replace_top(
                                        &neighbor_pairs[i, 0],
                                        thread_scaled_distances_i[thread_index].data(),
                                        neighbor, scaled_distance, num_neighbors)
                            max_heap_sort(
                                &neighbor_pairs[i, 0],
                                thread_scaled_distances_i[thread_index].data(),
                                num_neighbors)
            return too_large
        
        def sample_mid_near_pairs(const float[:, ::1] X,
                                  unsigned[:, ::1] mid_near_pairs,
                                  const unsigned long seed,
                                  const unsigned num_threads):
            cdef unsigned i, j, k, l, sampled_k, closest_cell, second_closest_cell, \
                thread_index, n = X.shape[0], \
                num_mid_near_pairs = mid_near_pairs.shape[1], num_PCs = X.shape[1]
            cdef float difference, distance, smallest, second_smallest
            cdef unsigned long state
            cdef uninitialized_vector[unsigned] sampled_buffer
            cdef vector[uninitialized_vector[unsigned]] thread_sampled
            cdef unsigned[::1] sampled
            
            if num_threads == 1:
                sampled_buffer.resize(6)
                sampled = <unsigned[:6]> sampled_buffer.data()
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_mid_near_pairs):
                        # Randomly sample 6 cells (which are not the
                        # current cell) and select the 2nd-closest
                        smallest = FLT_MAX
                        second_smallest = FLT_MAX
                        for k in range(6):
                            while True:
                                # Sample a random cell...
                                sampled_k = randint(n, &state)
                                
                                # ...that is not this cell...
                                if sampled_k == i:
                                    continue
                                
                                # ...nor a previously sampled cell
                                for l in range(k):
                                    if sampled_k == sampled[l]:
                                        break
                                else:
                                    sampled[k] = sampled_k
                                    break
                        for k in range(6):
                            sampled_k = sampled[k]
                            difference = X[i, 0] - X[sampled_k, 0]
                            distance = difference * difference
                            for l in range(1, num_PCs):
                                difference = X[i, l] - X[sampled_k, l]
                                distance += difference * difference
                            if distance < smallest:
                                second_smallest = smallest
                                second_closest_cell = closest_cell
                                smallest = distance
                                closest_cell = sampled_k
                            elif distance < second_smallest:
                                second_smallest = distance
                                second_closest_cell = sampled_k
                        mid_near_pairs[i, j] = second_closest_cell
            else:
                thread_sampled.resize(num_threads)
                with nogil, parallel(num_threads=num_threads):
                    thread_index = threadid()
                    thread_sampled[thread_index].resize(6)
                    for i in prange(n):
                        state = srand(seed + i)
                        for j in range(num_mid_near_pairs):
                            smallest = FLT_MAX
                            second_smallest = FLT_MAX
                            for k in range(6):
                                while True:
                                    sampled_k = randint(n, &state)
                                    if sampled_k == i:
                                        continue
                                    for l in range(k):
                                        if sampled_k == \
                                                thread_sampled[thread_index][l]:
                                            break
                                    else:
                                        thread_sampled[thread_index][k] = sampled_k
                                        break
                            for k in range(6):
                                sampled_k = thread_sampled[thread_index][k]
                                difference = X[i, 0] - X[sampled_k, 0]
                                distance = difference * difference
                                for l in range(1, num_PCs):
                                    difference = X[i, l] - X[sampled_k, l]
                                    distance = distance + difference * difference
                                if distance < smallest:
                                    second_smallest = smallest
                                    second_closest_cell = closest_cell
                                    smallest = distance
                                    closest_cell = sampled_k
                                elif distance < second_smallest:
                                    second_smallest = distance
                                    second_closest_cell = sampled_k
                            mid_near_pairs[i, j] = second_closest_cell
        
        def sample_further_pairs(const float[:, ::1] X,
                                 const unsigned[:, ::1] neighbor_pairs,
                                 unsigned[:, ::1] further_pairs,
                                 const unsigned long seed,
                                 const unsigned num_threads):
            """Sample Further pairs using the given seed."""
            cdef unsigned i, j, k, further_pair_index, n = X.shape[0], \
                num_further_pairs = further_pairs.shape[1], \
                num_neighbors = neighbor_pairs.shape[1]
            cdef unsigned long state
            
            if num_threads == 1:
                for i in range(n):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            # Sample a random cell...
                            further_pair_index = randint(n, &state)
                            
                            # ...that is not this cell...
                            if further_pair_index == i:
                                continue
                            
                            # ...nor one of its nearest neighbors...
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                # ...nor a previously sampled cell
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
            else:
                for i in prange(n, nogil=True, num_threads=num_threads):
                    state = srand(seed + i)
                    for j in range(num_further_pairs):
                        while True:
                            further_pair_index = randint(n, &state)
                            if further_pair_index == i:
                                continue
                            for k in range(num_neighbors):
                                if further_pair_index == neighbor_pairs[i, k]:
                                    break
                            else:
                                for k in range(j):
                                    if further_pair_index == further_pairs[i, k]:
                                        break
                                else:
                                    break
                        further_pairs[i, j] = further_pair_index
        
        def reformat_for_parallel(const unsigned[:, ::1] pairs,
                                  unsigned[::1] pair_indices,
                                  unsigned[::1] pair_indptr):
            cdef unsigned i, j, k, dest_index, num_cells = pairs.shape[0], \
                num_pairs_per_cell = pairs.shape[1]
            cdef uninitialized_vector[unsigned] dest_indices_buffer
            dest_indices_buffer.resize(num_cells)
            cdef unsigned[::1] dest_indices = \
                <unsigned[:num_cells]> dest_indices_buffer.data()
            
            # Tabulate how often each cell appears in pairs; at a minimum, it
            # will appear `pairs.shape[1]` times (i.e. the number of
            # neighbors), as the `i` in the pair, but it will also appear a
            # variable number of times as the `j` in the pair.
            pair_indptr[0] = 0
            pair_indptr[1:] = pairs.shape[1]
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indptr[j + 1] += 1
                    
            # Take the cumulative sum of the values in `pair_indptr`
            for i in range(2, pair_indptr.shape[0]):
                pair_indptr[i] += pair_indptr[i - 1]
                
            # Now that we know how many pairs each cell is a part of, do a
            # second pass over `pairs` to populate `pair_indices` with the
            # pairs' indices. Use a temporary buffer, `dest_indices`, to keep
            # track of the index within `pair_indptr` to write each cell's next
            # pair to.
            dest_indices[:] = pair_indptr[:num_cells]
            for i in range(num_cells):
                for k in range(num_pairs_per_cell):
                    j = pairs[i, k]
                    pair_indices[dest_indices[i]] = j
                    pair_indices[dest_indices[j]] = i
                    dest_indices[i] += 1
                    dest_indices[j] += 1

        cdef inline void get_gradients_fast(const float[:, ::1] embedding,
                                            const unsigned[:, ::1] neighbor_pairs,
                                            const unsigned[:, ::1] mid_near_pairs,
                                            const unsigned[:, ::1] further_pairs,
                                            const float w_neighbors,
                                            const float w_mid_near,
                                            float[:, ::1] gradients):
            cdef unsigned i, j, k, num_cells = neighbor_pairs.shape[0], \
                num_neighbors = neighbor_pairs.shape[1], \
                num_mid_near_pairs = mid_near_pairs.shape[1], \
                num_further_pairs = further_pairs.shape[1]
            cdef float embedding_i0, embedding_i1, gradients_i0, gradients_i1, \
                embedding_ij_0, embedding_ij_1, distance_ij, w
            
            for i in range(num_cells):
                embedding_i0 = embedding[i, 0]
                embedding_i1 = embedding[i, 1]
                gradients_i0 = 0
                gradients_i1 = 0
                
                # Nearest-neighbor pairs
                for k in range(num_neighbors):
                    j = neighbor_pairs[i, k]
                    embedding_ij_0 = embedding_i0 - embedding[j, 0]
                    embedding_ij_1 = embedding_i1 - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                        embedding_ij_1 * embedding_ij_1
                    w = w_neighbors * (20 / ((10 + distance_ij) * (10 + distance_ij)))
                    gradients_i0 += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients_i1 += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                
                # Mid-near pairs
                for k in range(num_mid_near_pairs):
                    j = mid_near_pairs[i, k]
                    embedding_ij_0 = embedding_i0 - embedding[j, 0]
                    embedding_ij_1 = embedding_i1 - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                        embedding_ij_1 * embedding_ij_1
                    w = w_mid_near * (20000 / ((10000 + distance_ij) *
                                               (10000 + distance_ij)))
                    gradients_i0 += w * embedding_ij_0
                    gradients[j, 0] -= w * embedding_ij_0
                    gradients_i1 += w * embedding_ij_1
                    gradients[j, 1] -= w * embedding_ij_1
                
                # Further pairs
                for k in range(num_further_pairs):
                    j = further_pairs[i, k]
                    embedding_ij_0 = embedding_i0 - embedding[j, 0]
                    embedding_ij_1 = embedding_i1 - embedding[j, 1]
                    distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                        embedding_ij_1 * embedding_ij_1
                    w = 2 / ((1 + distance_ij) * (1 + distance_ij))
                    gradients_i0 -= w * embedding_ij_0
                    gradients[j, 0] += w * embedding_ij_0
                    gradients_i1 -= w * embedding_ij_1
                    gradients[j, 1] += w * embedding_ij_1
        
                gradients[i, 0] += gradients_i0
                gradients[i, 1] += gradients_i1
        
        cdef inline void get_gradient(const float[:, ::1] embedding,
                                      const unsigned[::1] neighbor_pair_indices,
                                      const unsigned[::1] neighbor_pair_indptr,
                                      const unsigned[::1] mid_near_pair_indices,
                                      const unsigned[::1] mid_near_pair_indptr,
                                      const unsigned[::1] further_pair_indices,
                                      const unsigned[::1] further_pair_indptr,
                                      const float w_neighbors,
                                      const float w_mid_near,
                                      float[:, ::1] gradients,
                                      const unsigned i) noexcept nogil:
            cdef unsigned j, k, num_cells = embedding.shape[0]
            cdef float embedding_ij_0, embedding_ij_1, distance_ij, w, \
                embedding_i0 = embedding[i, 0], embedding_i1 = embedding[i, 1], \
                gradient_i0 = 0, gradient_i1 = 0
            
            # Nearest-neighbor pairs
            for k in range(neighbor_pair_indptr[i],
                           neighbor_pair_indptr[i + 1]):
                j = neighbor_pair_indices[k]
                embedding_ij_0 = embedding_i0 - embedding[j, 0]
                embedding_ij_1 = embedding_i1 - embedding[j, 1]
                distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                    embedding_ij_1 * embedding_ij_1
                w = w_neighbors * (20 / ((10 + distance_ij) * (10 + distance_ij)))
                gradient_i0 = gradient_i0 + w * embedding_ij_0
                gradient_i1 = gradient_i1 + w * embedding_ij_1
                
            # Mid-near pairs
            for k in range(mid_near_pair_indptr[i],
                           mid_near_pair_indptr[i + 1]):
                j = mid_near_pair_indices[k]
                embedding_ij_0 = embedding_i0 - embedding[j, 0]
                embedding_ij_1 = embedding_i1 - embedding[j, 1]
                distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                    embedding_ij_1 * embedding_ij_1
                w = w_mid_near * (20000 / ((10000 + distance_ij) *
                                           (10000 + distance_ij)))
                gradient_i0 = gradient_i0 + w * embedding_ij_0
                gradient_i1 = gradient_i1 + w * embedding_ij_1
                
            # Further pairs
            for k in range(further_pair_indptr[i],
                           further_pair_indptr[i + 1]):
                j = further_pair_indices[k]
                embedding_ij_0 = embedding_i0 - embedding[j, 0]
                embedding_ij_1 = embedding_i1 - embedding[j, 1]
                distance_ij = 1 + embedding_ij_0 * embedding_ij_0 + \
                    embedding_ij_1 * embedding_ij_1
                w = 2 / ((1 + distance_ij) * (1 + distance_ij))
                gradient_i0 = gradient_i0 - w * embedding_ij_0
                gradient_i1 = gradient_i1 - w * embedding_ij_1
            
            gradients[i, 0] = gradient_i0
            gradients[i, 1] = gradient_i1
        
        cdef inline void get_gradients(const float[:, ::1] embedding,
                                       const unsigned[::1] neighbor_pair_indices,
                                       const unsigned[::1] neighbor_pair_indptr,
                                       const unsigned[::1] mid_near_pair_indices,
                                       const unsigned[::1] mid_near_pair_indptr,
                                       const unsigned[::1] further_pair_indices,
                                       const unsigned[::1] further_pair_indptr,
                                       const float w_neighbors,
                                       const float w_mid_near,
                                       float[:, ::1] gradients):
            cdef unsigned i, num_cells = embedding.shape[0]
            
            for i in range(num_cells):
                get_gradient(embedding, neighbor_pair_indices, neighbor_pair_indptr,
                             mid_near_pair_indices, mid_near_pair_indptr,
                             further_pair_indices, further_pair_indptr, w_neighbors,
                             w_mid_near, gradients, i)
        
        cdef inline void get_gradients_parallel(
                const float[:, ::1] embedding,
                const unsigned[::1] neighbor_pair_indices,
                const unsigned[::1] neighbor_pair_indptr,
                const unsigned[::1] mid_near_pair_indices,
                const unsigned[::1] mid_near_pair_indptr,
                const unsigned[::1] further_pair_indices,
                const unsigned[::1] further_pair_indptr,
                const float w_neighbors,
                const float w_mid_near,
                float[:, ::1] gradients,
                const unsigned num_threads) noexcept nogil:
            cdef unsigned i, num_cells = embedding.shape[0]
            
            for i in prange(num_cells, num_threads=num_threads):
                get_gradient(embedding, neighbor_pair_indices, neighbor_pair_indptr,
                             mid_near_pair_indices, mid_near_pair_indptr,
                             further_pair_indices, further_pair_indptr, w_neighbors,
                             w_mid_near, gradients, i)
        
        cdef inline void update_cell_embedding_adam(
                float& embedding,
                const float gradient,
                float& momentum,
                float& velocity,
                const unsigned num_cells,
                const float beta1,
                const float beta2,
                const float learning_rate) noexcept nogil:
            momentum += (1 - beta1) * (gradient - momentum)
            velocity += (1 - beta2) * (gradient * gradient - velocity)
            embedding -= learning_rate * momentum / (sqrt(velocity) + 1e-7)
        
        cdef inline void update_embedding_adam(
                float[:, ::1] embedding,
                const float[:, ::1] gradients,
                float[:, ::1] momentum,
                float[:, ::1] velocity,
                const unsigned num_cells,
                const float beta1,
                const float beta2,
                float learning_rate,
                const unsigned iteration):
            cdef unsigned i
            learning_rate = learning_rate * sqrt(1 - beta2 ** (iteration + 1)) / \
                (1 - beta1 ** (iteration + 1))
            for i in range(num_cells):
                update_cell_embedding_adam(embedding[i, 0], gradients[i, 0], 
                                           momentum[i, 0], velocity[i, 0], num_cells, 
                                           beta1, beta2, learning_rate)
                update_cell_embedding_adam(embedding[i, 1], gradients[i, 1], 
                                           momentum[i, 1], velocity[i, 1], num_cells, 
                                           beta1, beta2, learning_rate)
        
        cdef inline void update_embedding_adam_parallel(
                float[:, ::1] embedding,
                const float[:, ::1] gradients,
                float[:, ::1] momentum,
                float[:, ::1] velocity,
                const unsigned num_cells,
                const float beta1,
                const float beta2,
                float learning_rate,
                const unsigned iteration,
                const unsigned num_threads) noexcept nogil:
            cdef unsigned i
            learning_rate = learning_rate * sqrt(1 - beta2 ** (iteration + 1)) / \
                (1 - beta1 ** (iteration + 1))
            for i in prange(num_cells, num_threads=num_threads):
                update_cell_embedding_adam(embedding[i, 0], gradients[i, 0], 
                                           momentum[i, 0], velocity[i, 0], num_cells, 
                                           beta1, beta2, learning_rate)
                update_cell_embedding_adam(embedding[i, 1], gradients[i, 1], 
                                           momentum[i, 1], velocity[i, 1], num_cells, 
                                           beta1, beta2, learning_rate)
        
        def PaCMAP_fast(const float[:, ::1] PCs,
                        float[:, ::1] embedding,
                        const unsigned[:, ::1] neighbor_pairs,
                        const unsigned[:, ::1] mid_near_pairs,
                        const unsigned[:, ::1] further_pairs,
                        const unsigned num_phase_1_iterations,
                        const unsigned num_phase_2_iterations,
                        const unsigned num_phase_3_iterations,
                        const float learning_rate):
            cdef unsigned i, iteration, num_cells = PCs.shape[0], \
                num_iterations = num_phase_1_iterations + \
                num_phase_2_iterations + num_phase_3_iterations, \
                w_mid_near_init = 1000
            cdef float iteration_fraction, w_mid_near, w_neighbors, beta1 = 0.9, \
                beta2 = 0.999
            cdef uninitialized_vector[float] gradients_buffer
            cdef vector[float] momentum_buffer, velocity_buffer
            gradients_buffer.resize(num_cells * 2)
            momentum_buffer.resize(num_cells * 2)
            velocity_buffer.resize(num_cells * 2)
            cdef float[:, ::1] \
                gradients = <float[:num_cells, :2]> gradients_buffer.data(), \
                momentum = <float[:num_cells, :2]> momentum_buffer.data(), \
                velocity = <float[:num_cells, :2]> velocity_buffer.data()
            
            # Initialize the embedding
            for i in range(num_cells):
                embedding[i, 0] = 0.01 * PCs[i, 0]
                embedding[i, 1] = 0.01 * PCs[i, 1]
            
            # Optimize the embedding
            for iteration in range(num_iterations):
                if iteration < num_phase_1_iterations:
                    iteration_fraction = <float> iteration / num_phase_1_iterations
                    w_mid_near = (1 - iteration_fraction) * w_mid_near_init + \
                        iteration_fraction * 3
                    w_neighbors = 2
                elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                    w_mid_near = 3
                    w_neighbors = 3
                else:
                    w_mid_near = 0
                    w_neighbors = 1
                    
                # Calculate gradients
                gradients[:] = 0
                get_gradients_fast(embedding, neighbor_pairs, mid_near_pairs,
                                   further_pairs, w_neighbors, w_mid_near,
                                   gradients)
                
                # Update the embedding based on the gradients, via the Adam
                # optimizer
                update_embedding_adam(embedding, gradients, momentum, velocity,
                                      num_cells, beta1, beta2, learning_rate,
                                      iteration)
                
                # Check for KeyboardInterrupts every 10 iterations
                if iteration % 10 == 9:
                    PyErr_CheckSignals()

        def PaCMAP(const float[:, ::1] PCs,
                   float[:, ::1] embedding,
                   const unsigned[::1] neighbor_pair_indices,
                   const unsigned[::1] neighbor_pair_indptr,
                   const unsigned[::1] mid_near_pair_indices,
                   const unsigned[::1] mid_near_pair_indptr,
                   const unsigned[::1] further_pair_indices,
                   const unsigned[::1] further_pair_indptr,
                   const unsigned num_phase_1_iterations,
                   const unsigned num_phase_2_iterations,
                   const unsigned num_phase_3_iterations,
                   const float learning_rate):
            cdef unsigned i, iteration, num_cells = PCs.shape[0], \
                num_iterations = num_phase_1_iterations + \
                num_phase_2_iterations + num_phase_3_iterations, \
                w_mid_near_init = 1000
            cdef float iteration_fraction, w_mid_near, w_neighbors, beta1 = 0.9, \
                beta2 = 0.999
            cdef uninitialized_vector[float] gradients_buffer
            cdef vector[float] momentum_buffer, velocity_buffer
            gradients_buffer.resize(num_cells * 2)
            momentum_buffer.resize(num_cells * 2)
            velocity_buffer.resize(num_cells * 2)
            cdef float[:, ::1] \
                gradients = <float[:num_cells, :2]> gradients_buffer.data(), \
                momentum = <float[:num_cells, :2]> momentum_buffer.data(), \
                velocity = <float[:num_cells, :2]> velocity_buffer.data()
            
            # Initialize the embedding
            for i in range(num_cells):
                embedding[i, 0] = 0.01 * PCs[i, 0]
                embedding[i, 1] = 0.01 * PCs[i, 1]
            
            # Optimize the embedding
            for iteration in range(num_iterations):
                if iteration < num_phase_1_iterations:
                    iteration_fraction = <float> iteration / num_phase_1_iterations
                    w_mid_near = (1 - iteration_fraction) * w_mid_near_init + \
                        iteration_fraction * 3
                    w_neighbors = 2
                elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                    w_mid_near = 3
                    w_neighbors = 3
                else:
                    w_mid_near = 0
                    w_neighbors = 1
                    
                # Calculate gradients
                get_gradients(embedding, neighbor_pair_indices, neighbor_pair_indptr,
                              mid_near_pair_indices, mid_near_pair_indptr,
                              further_pair_indices, further_pair_indptr, w_neighbors,
                              w_mid_near, gradients)
                
                # Update the embedding based on the gradients, via the Adam optimizer
                update_embedding_adam(embedding, gradients, momentum, velocity,
                                      num_cells, beta1, beta2, learning_rate,
                                      iteration)
                
                # Check for KeyboardInterrupts every 10 iterations
                if iteration % 10 == 9:
                    PyErr_CheckSignals()

        def PaCMAP_parallel(const float[:, ::1] PCs,
                            float[:, ::1] embedding,
                            const unsigned[::1] neighbor_pair_indices,
                            const unsigned[::1] neighbor_pair_indptr,
                            const unsigned[::1] mid_near_pair_indices,
                            const unsigned[::1] mid_near_pair_indptr,
                            const unsigned[::1] further_pair_indices,
                            const unsigned[::1] further_pair_indptr,
                            const unsigned num_phase_1_iterations,
                            const unsigned num_phase_2_iterations,
                            const unsigned num_phase_3_iterations,
                            const float learning_rate,
                            const unsigned num_threads):
            cdef unsigned i, iteration, num_cells = PCs.shape[0], \
                num_iterations = num_phase_1_iterations + \
                num_phase_2_iterations + num_phase_3_iterations, \
                w_mid_near_init = 1000
            cdef float iteration_fraction, w_mid_near, w_neighbors, beta1 = 0.9, \
                beta2 = 0.999
            cdef uninitialized_vector[float] gradients_buffer
            cdef vector[float] momentum_buffer, velocity_buffer
            gradients_buffer.resize(num_cells * 2)
            momentum_buffer.resize(num_cells * 2)
            velocity_buffer.resize(num_cells * 2)
            cdef float[:, ::1] \
                gradients = <float[:num_cells, :2]> gradients_buffer.data(), \
                momentum = <float[:num_cells, :2]> momentum_buffer.data(), \
                velocity = <float[:num_cells, :2]> velocity_buffer.data()
            
            with nogil:
                # Initialize the embedding
                for i in prange(num_cells, num_threads=num_threads):
                    embedding[i, 0] = 0.01 * PCs[i, 0]
                    embedding[i, 1] = 0.01 * PCs[i, 1]
                
                # Optimize the embedding
                for iteration in range(num_iterations):
                    if iteration < num_phase_1_iterations:
                        iteration_fraction = <float> iteration / num_phase_1_iterations
                        w_mid_near = (1 - iteration_fraction) * w_mid_near_init + \
                            iteration_fraction * 3
                        w_neighbors = 2
                    elif iteration < num_phase_1_iterations + num_phase_2_iterations:
                        w_mid_near = 3
                        w_neighbors = 3
                    else:
                        w_mid_near = 0
                        w_neighbors = 1
                        
                    # Calculate gradients
                    get_gradients_parallel(embedding, neighbor_pair_indices,
                                           neighbor_pair_indptr, mid_near_pair_indices,
                                           mid_near_pair_indptr, further_pair_indices,
                                           further_pair_indptr, w_neighbors,
                                           w_mid_near, gradients, num_threads)
                    
                    # Update the embedding based on the gradients, via the Adam
                    # optimizer
                    update_embedding_adam_parallel(embedding, gradients, momentum,
                                                   velocity, num_cells, beta1, beta2,
                                                   learning_rate, iteration,
                                                   num_threads)
                    
                    # Check for KeyboardInterrupts every 10 iterations
                    if iteration % 10 == 9:
                        with gil:
                            PyErr_CheckSignals()
            ''')
        get_neighbor_pairs = cython_functions['get_neighbor_pairs']
        sample_mid_near_pairs = cython_functions['sample_mid_near_pairs']
        sample_further_pairs = cython_functions['sample_further_pairs']
        
        # Select the `num_neighbors` of the
        # `num_neighbors + num_extra_neighbors` nearest-neighbor pairs with the
        # lowest scaled distances
        neighbor_pairs = np.empty((num_cells, num_neighbors), dtype=np.uint32)
        too_large = get_neighbor_pairs(
            X=PCs, neighbors=neighbors, distances=distances,
            neighbor_pairs=neighbor_pairs, num_threads=num_threads)
        
        # If any nearest-neighbor indices were out of range, raise an error
        if too_large:
            error_message = (
                f'some nearest-neighbor indices in obsm[{neighbors_key!r}] '
                f'are >= the total number of cells, '
                f'{neighbors.shape[0]:,}. This may happen if '
                f'you subset this SingleCell dataset between neighbors() and '
                f'embed(); if so, make sure to run neighbors() after, not '
                f'before, subsetting.')
            raise ValueError(error_message)
        
        # Sample mid-near pairs
        mid_near_pairs = np.empty((num_cells, num_mid_near_pairs),
                                  dtype=np.uint32)
        sample_mid_near_pairs(PCs, mid_near_pairs, seed, num_threads)
        
        # Sample further pairs
        further_pairs = np.empty((num_cells, num_further_pairs),
                                 dtype=np.uint32)
        sample_further_pairs(PCs, neighbor_pairs, further_pairs,
                             seed + mid_near_pairs.size, num_threads)
        
        # Run PaCMAP. If multithreaded, or single-threaded with
        # `match_parallel=True`, reformat the three lists of pairs to
        # ensure deterministic parallelism. Specifically, transform pairs of
        # cell indices from the original format of a 2D array `pairs` where
        # `pairs[i]` contains all js for which (i, j) is a pair, to a pair of
        # 1D arrays `pair_indices` and `pair_indptr` forming a sparse array,
        # where `pair_indices[pair_indptr[i]:pair_indptr[i + 1]]` contains all
        # js for which (i, j) is a pair or (j, i) is a pair. `pair_indices`
        # must have length `2 * pairs.size`, since each pair will appear twice,
        # once for (i, j) and once for (j, i). `pair_indptr` must have length
        # equal to the number of cells plus one, just like for scipy sparse
        # matrices.
        num_phase_1_iterations, num_phase_2_iterations, \
            num_phase_3_iterations = num_iterations
        embedding = np.empty((num_cells, 2), dtype=np.float32)
        if num_threads == 1 and not match_parallel:
            PaCMAP_fast = cython_functions['PaCMAP_fast']
            PaCMAP_fast(PCs, embedding, neighbor_pairs, mid_near_pairs,
                        further_pairs, num_phase_1_iterations,
                        num_phase_2_iterations, num_phase_3_iterations,
                        learning_rate)
        else:
            reformat_for_parallel = cython_functions['reformat_for_parallel']
            neighbor_pair_indices = np.empty(2 * neighbor_pairs.size,
                                             dtype=np.uint32)
            neighbor_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(neighbor_pairs, neighbor_pair_indices,
                                  neighbor_pair_indptr)
            del neighbor_pairs
            mid_near_pair_indices = \
                np.empty(2 * mid_near_pairs.size, dtype=np.uint32)
            mid_near_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(mid_near_pairs, mid_near_pair_indices,
                                  mid_near_pair_indptr)
            del mid_near_pairs
            further_pair_indices = \
                np.empty(2 * further_pairs.size, dtype=np.uint32)
            further_pair_indptr = np.empty(num_cells + 1, dtype=np.uint32)
            reformat_for_parallel(further_pairs, further_pair_indices,
                                  further_pair_indptr)
            del further_pairs
            if num_threads == 1:
                PaCMAP = cython_functions['PaCMAP']
                PaCMAP(PCs, embedding, neighbor_pair_indices,
                       neighbor_pair_indptr, mid_near_pair_indices,
                       mid_near_pair_indptr, further_pair_indices,
                       further_pair_indptr, num_phase_1_iterations,
                       num_phase_2_iterations, num_phase_3_iterations,
                       learning_rate)
            else:
                PaCMAP_parallel = cython_functions['PaCMAP_parallel']
                PaCMAP_parallel(PCs, embedding, neighbor_pair_indices,
                                neighbor_pair_indptr, mid_near_pair_indices,
                                mid_near_pair_indptr, further_pair_indices,
                                further_pair_indptr, num_phase_1_iterations,
                                num_phase_2_iterations, num_phase_3_iterations,
                                learning_rate, num_threads)
       
        # If `QC_column` was specified, back-project from QCed cells to all
        # cells, filling with `NaN`
        if QC_column is not None:
            embedding_QCed = embedding
            embedding = np.full((len(self), embedding_QCed.shape[1]), np.nan,
                                dtype=np.float32)
            # noinspection PyUnboundLocalVariable
            embedding[QCed_NumPy] = embedding_QCed
        # noinspection PyTypeChecker
        return SingleCell(X=self._X, obs=self._obs, var=self._var,
                          obsm=self._obsm | {embedding_key: embedding},
                          varm=self._varm, obsp=self._obsp, varp=self._varp,
                          uns=self._uns, num_threads=self._num_threads)
    
    # noinspection PyUnresolvedReferences
    def plot_embedding(
            self,
            color_column: SingleCellColumn | None,
            filename: str | Path | None = None,
            *,
            cells_to_plot_column: SingleCellColumn | None = 'passed_QC',
            embedding_key: str = 'PaCMAP',
            ax: 'Axes' | None = None,
            figure_kwargs: dict[str, Any] | None = None,
            point_size: int | float | np.integer | np.floating | str |
                        None = None,
            sort_by_frequency: bool = False,
            colormap: str | 'Colormap' | dict[Any, Color] = None,
            lightness_range: tuple[float | np.floating, float | np.floating] |
                             None = (100 / 3, 200 / 3),
            chroma_range: tuple[float | np.floating, float | np.floating] |
                          None = (50, 100),
            hue_range: tuple[float | np.floating, float | np.floating] |
                       None = None,
            first_color: Color = '#008cb9',
            stride: int | np.integer = 5,
            default_color: Color = 'lightgray',
            scatter_kwargs: dict[str, Any] | None = None,
            label: bool = False,
            label_kwargs: dict[str, Any] | None = None,
            legend: bool = True,
            legend_kwargs: dict[str, Any] | None = None,
            colorbar: bool = True,
            colorbar_kwargs: dict[str, Any] | None = None,
            title: str | None = None,
            title_kwargs: dict[str, Any] | None = None,
            xlabel: str | None = 'Component 1',
            xlabel_kwargs: dict[str, Any] | None = None,
            ylabel: str | None = 'Component 2',
            ylabel_kwargs: dict[str, Any] | None = None,
            xlim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            ylim: tuple[int | float | np.integer | np.floating,
                        int | float | np.integer | np.floating] | None = None,
            despine: bool = True,
            savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Plot an embedding created by `embed()`, using Matplotlib.
        
        Requires the colorspacious package. Install via:
        mamba install -y colorspacious
        
        Args:
            color_column: an optional column of `obs` indicating how to color
                          each cell in the plot. Can be a column name, a polars
                          expression, a polars Series, a 1D NumPy array, or a
                          function that takes in this SingleCell dataset and
                          returns a polars Series or 1D NumPy array. Can be
                          discrete (e.g. cell-type labels), specified as a
                          String/Categorical/Enum column, or quantitative (e.g.
                          the number of UMIs per cell), specified as an
                          integer/floating-point column. Missing (`null`) cells
                          will be plotted with the color `default_color`. Set
                          to `None` to use `default_color` for all cells.
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            cells_to_plot_column: an optional Boolean column of `obs`
                                  indicating which cells to plot. Can be a
                                  column name, a polars expression, a polars
                                  Series, a 1D NumPy array, or a function that
                                  takes in this SingleCell dataset and returns
                                  a polars Series or 1D NumPy array. Set to
                                  `None` to plot all cells passing QC.
            embedding_key: the key of `obsm` containing the embedding to plot,
                           calculated with `embed()`
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            point_size: the size of the points for each cell; defaults to
                        30,000 divided by the number of cells, one quarter of
                        Scanpy's default. Can be a single number, or the name
                        of a column of `obs` to make each point a different
                        size.
            sort_by_frequency: if `True`, assign colors and sort the legend in
                               order of decreasing frequency; if `False` (the
                               default), use natural sorted order
                               (en.wikipedia.org/wiki/Natural_sort_order).
                               Cannot be `True` unless `colormap` is `None` and
                               `color_column` is discrete; if `colormap` is
                               not `None`, the plot order is determined by the
                               order of the keys in `colormap`.
            colormap: a string or Colormap object indicating the Matplotlib
                      colormap to use; or, if `color_column` is discrete, a
                      dictionary mapping values in `color_column` to Matplotlib
                      colors (cells with values of `color_column` that are not
                      in the dictionary will be plotted in the color
                      `default_color`). Defaults to
                      `plt.rcParams['image.cmap']` (`'viridis'` by default) if
                      `color_column` is continous, or the colors from a
                      maximally perceptually distinct colormap if
                      `color_column` is discrete (with colors assigned in
                      decreasing order of frequency). Cannot be specified if
                      `color_column` is `None`.
            lightness_range: a two-element tuple with the lightness range of
                             colors to generate, or `None` to take the full
                             range: `[0, 100]`. Can only be specified when
                             `color_column` is discrete and `colormap` is
                             `None`.
            chroma_range: a two-element tuple with the chroma range of colors
                          to generate, or `None` to take the full range:
                          `[0, 100]`. Grays have low chroma, and vivid colors
                          have high chroma. Can only be specified when
                          `color_column` is discrete and `colormap` is `None`.
            hue_range: a two-element tuple with the hue range of colors to
                       generate, or `None` to take the full range: `[0, 360]`.
                       Red is at 0°, green at 120°, and blue at 240°. Because
                       it wraps around, the first element of the tuple can be
                       greater than the second, unlike for `lightness_range`
                       and `chroma_range`. Can only be specified when
                       `color_column` is discrete and `colormap` is `None`.
            first_color: the first color of the palette. Can be any valid
                         Matplotlib color, like a hex string (e.g.
                         `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            stride: as an optimization, consider only RGB colors where R, G,
                    and B are all multiples of this value. Must be a small
                    divisor of 255: 1, 3, 5, 15, or 17. Set to 1 for the best
                    possible solution, at orders of magnitude more
                    computational cost.
            default_color: the default color to plot cells in when
                           `color_column` is `None`, or when certain cells have
                           missing (`null`) values for `color_column`, or when
                           `colormap` is a dictionary and some cells have
                           values of `color_column` that are not in the
                           dictionary. Can be any valid Matplotlib color, like
                           a hex string (e.g. `'#FF0000'`), a named color (e.g.
                           'red'), a 3- or 4-element RGB/RGBA tuple of integers
                           0-255 or floats 0-1, or a single float 0-1 for
                           grayscale.
            scatter_kwargs: a dictionary of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              `colormap` maps the numbers in `color_column` to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s`, `c`/`color`, or `cmap` will raise
                            an error, since these arguments conflict with the
                            `point_size`, `color_column`, and `colormap`
                            arguments, respectively.
            label: whether to label cells with each distinct value of
                   `color_column`. Labels will be placed at the median x and y
                   position of the points with that color. Can only be `True`
                   when `color_column` is discrete. When set to `True`, you may
                   also want to set `legend=False` to avoid redundancy.
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.text()` when adding labels to control the text
                          properties, such as:
                           - `color` and `size` to modify the text color/size
                           - `verticalalignment` and `horizontalalignment` to
                             control vertical and horizontal alignment. By
                             default, unlike Matplotlib, these are both set to
                             `'center'`.
                           - `path_effects` to set properties for the border
                             around the text. By default, set to
                             `matplotlib.patheffects.withStroke(
                                  linewidth=3, foreground='white', alpha=0.75)`
                             instead of Matplotlib's default of `None`, to put
                             a semi-transparent white border around the labels
                             for better contrast.
                          Can only be specified when `label=True`.
            legend: whether to add a legend for each value in `color_column`.
                    Ignored unless `color_column` is discrete.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location. By default, `loc` is set to
                             `'center left'` and `bbox_to_anchor` to `(1, 0.5)`
                             to put the legend to the right of the plot,
                             anchored at the middle.
                           - `ncols` to set its number of columns. By
                             default, set to
                             `obs[color_column].n_unique() // 16 + 1` to have
                             at most 16 items per column.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color its
                             border. `frameon` defaults to `False`, instead of
                             Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when `color_column` is
                           discrete and `legend=True`.
            colorbar: whether to add a colorbar. Ignored unless `color_column`
                      is quantitative.
            colorbar_kwargs: a dictionary of keyword arguments to be passed to
                             `plt.colorbar()`, such as:
                             - `location`: `'left'`, `'right'`, `'top'`, or
                               `'bottom'`
                             - `orientation`: `'vertical'` or `'horizontal'`
                             - `fraction`: the fraction of the axes to
                               allocate to the colorbar. Defaults to 0.15.
                             - `shrink`: the fraction to multiply the size of
                               the colorbar by. Defaults to 0.5, instead of
                               Matplotlib's default of 1.
                             - `aspect`: the ratio of the colorbar's long to
                               short dimensions. Defaults to 20.
                             - `pad`: the fraction of the axes between the
                               colorbar and the rest of the figure. Defaults to
                               0.01, instead of Matplotlib's default of 0.05 if
                               vertical and 0.15 if horizontal.
                             Can only be specified when `color_column` is
                             quantitative and `colorbar=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            xlim: a length-2 tuple of the left and right x-axis limits, or
                 `None` to set the limits based on the data
            ylim: a length-2 tuple of the bottom and top y-axis limits, or
                 `None` to set the limits based on the data
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to `'tight'` (crop
                              out any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to `'layout'` (use the padding
                              from the constrained layout engine, when `ax` is
                              not `None`) instead of Matplotlib's default of
                              0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `filename` ends with `'.pdf'`) and
                              `False` otherwise, instead of Matplotlib's
                              default of always being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
       
        # Get `cells_to_plot_column`, if not `None`
        original_cells_to_plot_column = cells_to_plot_column
        if cells_to_plot_column is not None:
            cells_to_plot_column = self._get_column(
                'obs', cells_to_plot_column, 'cells_to_plot_column',
                pl.Boolean, allow_missing=cells_to_plot_column == 'passed_QC')
        
        # If `color_column` was specified, check that it either discrete
        # (Categorical, Enum, or String) or quantitative (integer or
        # floating-point). If discrete, check that `color_column` has at least
        # two distinct values.
        original_color_column = color_column
        if color_column is not None:
            color_column = self._get_column(
                'obs', color_column, 'color_column',
                (pl.Categorical, pl.Enum, pl.String, 'integer',
                 'floating-point'), allow_null=True,
                QC_column=cells_to_plot_column)
            unique_color_column = color_column.unique().drop_nulls()
            dtype = color_column.dtype
            discrete = dtype in (pl.Categorical, pl.Enum, pl.String)
            if discrete and len(unique_color_column) == 1:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} must have at least two '
                    f'distinct values when its data '
                    f'type is {dtype.base_type()!r}')
                raise ValueError(error_message)
       
        # If `filename` was specified, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
      
        # Check that `embedding_key` is the name of a key in `obsm`
        check_type(embedding_key, 'embedding_key', str, 'a string')
        if embedding_key not in self._obsm:
            error_message = (
                f'embedding_key {embedding_key!r} is not a key of obsm; '
                f'did you forget to run embed() before plot_embedding()?')
            raise ValueError(error_message)
     
        # Check that the embedding `embedding_key` references is 2D.
        embedding = self._obsm[embedding_key]
        if embedding.shape[1] != 2:
            error_message = (
                f'the embedding at obsm[{embedding_key!r}] is '
                f'{embedding.shape[1]:,}-dimensional, but must be '
                f'2-dimensional to be plotted')
            raise ValueError(error_message)
       
        # If `cells_to_plot_column` was specified, subset to these cells
        if cells_to_plot_column is not None:
            embedding = embedding[cells_to_plot_column.to_numpy()]
            if color_column is not None:
                color_column = color_column.filter(cells_to_plot_column)
                unique_color_column = color_column.unique().drop_nulls()
    
        # Check that the embedding does not contain NaNs
        if np.isnan(embedding).any():
            error_message = \
                f'the embedding at obsm[{embedding_key!r}] contains NaNs; '
            if cells_to_plot_column is None:
                error_message += (
                    'did you forget to set QC_column to None in embed(), to '
                    'match the fact that you set cells_to_plot_column to '
                    'None in plot_embedding()?')
            else:
                cells_to_plot_column_description = \
                    SingleCell._describe_column('cells_to_plot_column',
                                                original_cells_to_plot_column)
                error_message += (
                    f'does your {cells_to_plot_column_description} contain '
                    f'cells that were excluded by the QC_column used in '
                    f'embed()?')
            raise ValueError(error_message)
     
        # For each of the kwargs arguments, if the argument was specified,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (colorbar_kwargs, 'colorbar_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        
        # If `figure_kwargs` was specified, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
       
        # If `point_size` is `None`, default to 30,000 / num_cells; otherwise,
        # check that it is a positive number or the name of a numeric column of
        # `obs` with all-positive numbers
        num_cells = \
            len(self) if cells_to_plot_column is None else len(embedding)
        if point_size is None:
            # noinspection PyUnboundLocalVariable
            point_size = 30_000 / num_cells
        else:
            check_type(point_size, 'point_size', (int, float, str),
                       'a positive number or string')
            if isinstance(point_size, (int, float)):
                check_bounds(point_size, 'point_size', 0, left_open=True)
            else:
                if point_size not in self._obs:
                    error_message = \
                        f'point_size {point_size!r} is not a column of obs'
                    raise ValueError(error_message)
                point_size = self._obs[point_size]
                if not (point_size.dtype.is_integer() or
                        point_size.dtype.is_float()):
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], must '
                        f'have an integer or floating-point data type, but '
                        f'has data type {point_size.dtype.base_type()!r}')
                    raise TypeError(error_message)
                if point_size.min() <= 0:
                    error_message = (
                        f'the point_size column, obs[{point_size!r}], does '
                        f'not have all-positive elements')
                    raise ValueError(error_message)
        
        # If `sort_by_frequency=True`, ensure `colormap` is `None` and
        # `color_column` is discrete
        check_type(sort_by_frequency, 'sort_by_frequency', bool, 'Boolean')
        if sort_by_frequency:
            if colormap is not None:
                error_message = (
                    f'sort_by_frequency must be False when colormap is '
                    f'specified')
                raise ValueError(error_message)
            if color_column is None:
                error_message = \
                    'sort_by_frequency must be False when color_column is None'
                raise ValueError(error_message)
            # noinspection PyUnboundLocalVariable
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'sort_by_frequency must be False when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
       
        # Handle coloring based on the values of `colormap` and `color_column`
        if colormap is not None:
            # If `colormap` was specified, check that it is a string in
            # `plt.colormaps`, Colormap object, or dictionary where all keys
            # are in `color_column` and all values are valid Matplotlib colors.
            # Normalize the color(s) to hex codes. Make sure `color_column` is
            # not `None` and `lightness_range`, `chroma_range`, `hue_range`,
            # `first_color`, and `stride` have their default values.
            check_type(colormap, 'colormap',
                       (str, plt.matplotlib.colors.Colormap, dict),
                       'a string, matplotlib Colormap object, or dictionary')
            if color_column is None:
                error_message = \
                    'colormap must be None when color_column is None'
                raise ValueError(error_message)
            if not (isinstance(lightness_range, tuple) and
                    len(lightness_range) == 2 and
                    isinstance(lightness_range[0], float) and
                    lightness_range[0] == 100 / 3 and
                    isinstance(lightness_range[1], float) and
                    lightness_range[1] == 200 / 3):
                error_message = (
                    f'lightness_range cannot be specified when colormap is '
                    f'specified')
                raise ValueError(error_message)
            if not (isinstance(chroma_range, tuple) and
                    len(chroma_range) == 2 and
                    isinstance(chroma_range[0], int) and
                    chroma_range[0] == 50 and
                    isinstance(chroma_range[1], int) and
                    chroma_range[1] == 100):
                error_message = (
                    f'chroma_range cannot be specified when colormap is '
                    f'specified')
                raise ValueError(error_message)
            for arg, arg_name in ((hue_range, 'hue_range'),
                                  (first_color, 'first_color'),
                                  (stride, 'stride')):
                if arg is not None:
                    error_message = \
                        f'{arg_name} must be None when colormap is specified'
                    raise ValueError(error_message)
            if isinstance(colormap, str):
                colormap = plt.colormaps[colormap]
            elif isinstance(colormap, dict):
                # noinspection PyUnboundLocalVariable
                if not discrete:
                    color_column_description = \
                        SingleCell._describe_column('color_column',
                                                    original_color_column)
                    error_message = (
                        f'colormap cannot be a dictionary when '
                        f'{color_column_description} is continuous')
                    raise ValueError(error_message)
                for key, value in colormap.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of colormap must be strings, but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    # noinspection PyUnboundLocalVariable
                    if key not in unique_color_column:
                        error_message = (
                            f'colormap is a dictionary containing the key '
                            f'{key!r}, which is not one of the values in '
                            f'obs[{color_column!r}]')
                        raise ValueError(error_message)
                    if not plt.matplotlib.colors.is_color_like(value):
                        error_message = (
                            f'colormap[{key!r}] is not a valid Matplotlib '
                            f'color')
                        raise ValueError(error_message)
                    colormap[key] = plt.matplotlib.colors.to_hex(value)
        else:
            # noinspection PyUnboundLocalVariable
            if color_column is not None and discrete:
                # `color_column` is discrete and `colormap` was not specified;
                # generate a maximally perceptually distinct colormap. Assign
                # colors in natural sort order, or decreasing order of
                # frequency if `sort_by_frequency=True`.
                # noinspection PyUnboundLocalVariable
                color_order = color_column\
                    .value_counts(sort=True)\
                    .to_series()\
                    .drop_nulls() if sort_by_frequency else \
                        sorted(unique_color_column,
                               key=lambda color_label: [
                                   int(text) if text.isdigit() else
                                   text.lower() for text in
                                   re.split('([0-9]+)', color_label)])
                colormap = generate_palette(num_colors=len(color_order),
                                            lightness_range=lightness_range,
                                            chroma_range=chroma_range,
                                            hue_range=hue_range,
                                            first_color=first_color,
                                            stride=stride)
                colormap = dict(zip(color_order, colormap))
            else:
                # `color_column` is `None` or continuous, so make sure
                # `lightness_range`, `chroma_range`, `hue_range`,
                # `first_color`, and `stride` have their default values
                for arg, arg_name in ((lightness_range, 'lightness_range'),
                                      (chroma_range, 'chroma_range'),
                                      (hue_range, 'hue_range'),
                                      (first_color, 'first_color'),
                                      (stride, 'stride')):
                    if arg is lightness_range:
                        if isinstance(lightness_range, tuple) and \
                                len(lightness_range) == 2 and \
                                isinstance(lightness_range[0], float) and \
                                lightness_range[0] == 100 / 3 and \
                                isinstance(lightness_range[1], float) and \
                                lightness_range[1] == 200 / 3:
                            continue
                    elif arg is chroma_range:
                        if isinstance(chroma_range, tuple) and \
                                len(chroma_range) == 2 and \
                                isinstance(chroma_range[0], int) and \
                                chroma_range[0] == 50 and \
                                isinstance(chroma_range[1], int) and \
                                chroma_range[1] == 100:
                            continue
                    elif arg is None:
                        continue
                    if color_column is None:
                        error_message = (
                            f'{arg_name} must be None when color_column is '
                            f'None')
                        raise ValueError(error_message)
                    else:
                        color_column_description = \
                            SingleCell._describe_column(
                                'color_column', original_color_column)
                        error_message = (
                            f'{arg_name} must be None when '
                            f'{color_column_description} is continuous')
                        raise ValueError(error_message)
       
        # Check that `default_color` is a valid Matplotlib color, and convert
        # it to hex
        if not plt.matplotlib.colors.is_color_like(default_color):
            error_message = 'default_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        default_color = plt.matplotlib.colors.to_hex(default_color)
       
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
            if scatter_kwargs is not None else default_scatter_kwargs
       
        # Check that `scatter_kwargs` does not contain the `s`, `c`/`color`, or
        # `cmap` keys
        if 's' in scatter_kwargs:
            error_message = (
                "'s' cannot be specified as a key in scatter_kwargs; specify "
                "the point_size argument instead")
            raise ValueError(error_message)
        for key in 'c', 'color', 'cmap':
            if key in scatter_kwargs:
                error_message = (
                    f'{key!r} cannot be specified as a key in scatter_kwargs; '
                    f'specify the color_column, colormap, lightness_range, '
                    f'chroma_range, hue_range, first_color, stride, and/or '
                    f'default_color arguments instead')
                raise ValueError(error_message)
      
        # If `label=True`, check that `color_column` is discrete.
        # If `label=False`, check that `label_kwargs` is `None`.
        check_type(label, 'label', bool, 'Boolean')
        if label:
            if color_column is None:
                error_message = 'color_column cannot be None when label=True'
                raise ValueError(error_message)
            if not discrete:
                color_column_description = \
                    SingleCell._describe_column('color_column',
                                                original_color_column)
                error_message = (
                    f'{color_column_description} cannot be continuous when '
                    f'label=True')
                raise ValueError(error_message)
        elif label_kwargs is not None:
            error_message = 'label_kwargs must be None when label=False'
            raise ValueError(error_message)
        
        # Only add a legend if `legend=True` and `color_column` is discrete.
        # If not adding a legend, check that `legend_kwargs` is `None`.
        check_type(legend, 'legend', bool, 'Boolean')
        add_legend = legend and color_column is not None and discrete
        if not add_legend and legend_kwargs is not None:
            if color_column is None:
                error_message = \
                    'legend_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = SingleCell._describe_column(
                    'color_column', original_color_column)
                error_message = (
                    f'legend_kwargs must be None when '
                    f'{color_column_description} is continuous')
                raise ValueError(error_message)
        
        # Only add a colorbar if `colorbar=True` and `color_column` is
        # continuous. If not adding a colorbar, check that `colorbar_kwargs` is
        # `None`.
        check_type(colorbar, 'colorbar', bool, 'Boolean')
        add_colorbar = colorbar and color_column is not None and not discrete
        if not add_colorbar and colorbar_kwargs is not None:
            if color_column is None:
                error_message = \
                    'colorbar_kwargs must be None when color_column is None'
                raise ValueError(error_message)
            else:
                color_column_description = SingleCell._describe_column(
                    'color_column', original_color_column)
                error_message = (
                    f'colorbar_kwargs must be None when '
                    f'{color_column_description} is discrete')
                raise ValueError(error_message)
       
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
       
        # Check that `xlim` and `ylim` are be length-2 tuples or `None`, with
        # the first element less than the second
        for arg, arg_name in (xlim, 'xlim'), (ylim, 'ylim'):
            if arg is not None:
                check_type(arg, arg_name, tuple, 'a length-2 tuple')
                if len(arg) != 2:
                    error_message = (
                        f'{arg_name} must be a length-2 tuple, but has length '
                        f'{len(arg):,}')
                    raise ValueError(error_message)
                if arg[0] >= arg[1]:
                    error_message = \
                        f'{arg_name}[0] must be less than {arg_name}[1]'
                    raise ValueError(error_message)
       
        # If `color_column` is `None`, plot all cells in `default_color`. If
        # `colormap` is a dictionary, generate an explicit list of colors to
        # plot each cell in. If `colormap` is a Colormap, just pass it as the
        # cmap` argument. If `colormap` is missing and `color_column` is
        # continuous, set it to `plt.rcParams['image.cmap']` ('viridis' by
        # default)
        if color_column is None:
            c = default_color
            cmap = None
        elif isinstance(colormap, dict):
            # Note: `replace_strict(..., default=default_color)` fills both
            # missing values and values missing from `colormap` with
            # `default_color`
            c = color_column\
                .replace_strict(colormap, default=default_color,
                                return_dtype=pl.String)\
                .to_numpy()
            cmap = None
        else:
            # Need to `copy()` because `set_bad()` is in-place
            c = color_column.to_numpy()
            if colormap is not None:
                cmap = colormap.copy()
                # noinspection PyUnresolvedReferences
                cmap.set_bad(default_color)
            else:  # `color_column` is continuous
                cmap = plt.rcParams['image.cmap']
       
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
        
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            
            # Make a scatter plot of the embedding with equal x-y aspect ratios
            scatter = ax.scatter(embedding[:, 0], embedding[:, 1],
                                 s=point_size, c=c, cmap=cmap,
                                 **scatter_kwargs)
            ax.set_aspect('equal')
            
            # Add the title, axis labels and axis limits
            if title is not None:
                if title_kwargs is None:
                    ax.set_title(title)
                else:
                    ax.set_title(title, **title_kwargs)
            if xlabel is not None:
                if xlabel_kwargs is None:
                    ax.set_xlabel(xlabel)
                else:
                    ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ax.set_ylabel(ylabel)
                else:
                    ax.set_ylabel(ylabel, **ylabel_kwargs)
            if xlim is not None:
                ax.set_xlim(*xlim)
            if ylim is not None:
                ax.set_ylim(*ylim)
            
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if add_legend:
                default_legend_kwargs = dict(
                    loc='center left', bbox_to_anchor=(1, 0.5), frameon=False,
                    ncols=len(unique_color_column) // 16 + 1)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                if isinstance(colormap, dict):
                    for color_label, color in colormap.items():
                        ax.scatter([], [], c=color, label=color_label,
                                   **scatter_kwargs)
                    plt.legend(**legend_kwargs)
                else:
                    plt.legend(*scatter.legend_elements(), **legend_kwargs)
            
            # Add the colorbar; override the defaults for certain keys of
            # `colorbar_kwargs`
            if add_colorbar:
                default_colorbar_kwargs = dict(shrink=0.5, pad=0.01)
                colorbar_kwargs = default_colorbar_kwargs | colorbar_kwargs \
                    if colorbar_kwargs is not None else default_colorbar_kwargs
                cbar = plt.colorbar(scatter, ax=ax, **colorbar_kwargs)
                cbar.outline.set_visible(False)
            
            # Label cells; override the defaults for certain keys of
            # `label_kwargs`
            if label:
                from matplotlib.patheffects import withStroke
                if label_kwargs is None:
                    label_kwargs = {}
                # noinspection PyUnresolvedReferences
                label_kwargs |= dict(
                    horizontalalignment=label_kwargs.pop(
                        'horizontalalignment',
                        label_kwargs.pop('ha', 'center')),
                    verticalalignment=label_kwargs.pop(
                        'verticalalignment',
                        label_kwargs.pop('va', 'center')),
                    path_effects=[withStroke(linewidth=3, foreground='white',
                                             alpha=0.75)])
                for color_label in unique_color_column:
                    ax.text(*np.median(embedding[color_column == color_label],
                                       axis=0), color_label, **label_kwargs)
            
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
            
            # Save, if `filename` is not `None`; override the defaults for
            # certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise


class Pseudobulk:
    """
    A pseudobulked single-cell dataset resulting from calling `pseudobulk()`
    on a SingleCell dataset.
    
    Has slots for:
    - `X`: a dict of NumPy arrays of counts per cell and gene for each cell
      type
    - `obs`: a dict of polars DataFrames of sample metadata for each cell type
    - `var`: a dict of polars DataFrames of gene metadata for each cell type
    as well as `obs_names` and `var_names`, aliases for a dict of `obs[:, 0]`
    and `var[:, 0]` for each cell type.
    
    In many ways, Pseudobulk objects behave like dictionaries:
    - `pb1 | pb2` combines pseudobulks with non-overlapping cell types into one
      big pseudobulk
    - `cell_type in pb` tests whether `cell_type` is a cell type in the
      pseudobulk
    - `for cell_type in pb:` and `for cell_type in pb.keys():` yield the cell
      type names
    - `for X, obs, var in pb.values():` yields each cell type's `X`, `obs`, and
      `var`
    - `for cell_type, (X, obs, var) in pseudobulk.items():` yields both the
      name and the `X`, `obs` and `var` for each cell type
      
    There are also custom iterators if you just want one field per cell type:
    - `for X in pseudobulk.iter_X():` yields just the `X` for each cell type
    - `for X in pseudobulk.iter_obs():` yields just the `obs` for each cell
       type
    - `for X in pseudobulk.iter_var():` yields just the `var` for each cell
       type
    """
    def __init__(self,
                 X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                     np.floating]]] |
                    None = None,
                 *,
                 obs: dict[str, pl.DataFrame] = None,
                 var: dict[str, pl.DataFrame] = None,
                 num_threads: int | np.integer | None = None) -> None:
        """
        Load a saved Pseudobulk dataset, or create one from an in-memory count
        matrix + metadata for each cell type.
        
        Args:
            X: a {cell type: NumPy array} dictionary of counts or log CPMs, or
               a directory to load a saved Pseudobulk dataset from (see save())
            obs: a {cell type: polars DataFrame} dict of metadata per sample,
                 when `X` is a dictionary. The first column must be String,
                 Enum, or Categorical.
            var: a {cell type: polars DataFrame} dict of metadata per gene,
                 when `X` is a dictionary. The first column must be String,
                 Enum, or Categorical.
            num_threads: the default number of threads to use for all
                         subsequent operations on this Pseudobulk dataset. By
                         default (`num_threads=None`), use all available cores,
                         as determined by `os.cpu_count()`, if running
                         free-threaded Python 3.13+, and one core otherwise.
        """
        # Initialize this Pseudobulk dataset's `num_threads`
        if num_threads is None:
            try:
                # noinspection PyUnresolvedReferences
                self._num_threads = \
                    1 if sys._is_gil_enabled() else os.cpu_count()
            except AttributeError:
                self._num_threads = 1
        else:
            check_type(num_threads, 'num_threads', int,
                       'a positive integer, -1, or None')
            if num_threads == 1:
                self._num_threads = 1
            else:
                num_threads = int(num_threads)
                if num_threads <= 0 and num_threads != -1:
                    error_message = (
                        f'num_threads is {num_threads:,}, but must be a '
                        f'positive integer, -1, or None')
                    raise ValueError(error_message)
                try:
                    # noinspection PyUnresolvedReferences
                    has_GIL = sys._is_gil_enabled()
                except AttributeError:
                    has_GIL = True
                if has_GIL:
                    error_message = (
                        f'num_threads is {num_threads}, but multithreading '
                        f'for Pseudobulk datasets is only supported for '
                        f'"free-threaded" builds of Python 3.13 and later '
                        f'with the global interpreter lock (GIL) disabled')
                    raise ValueError(error_message)
                self._num_threads = \
                    os.cpu_count() if num_threads == -1 else num_threads
        
        if isinstance(X, dict):
            if obs is None:
                error_message = (
                    'obs is None, but since X is a dictionary, obs must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if var is None:
                error_message = (
                    'var is None, but since X is a dictionary, var must also '
                    'be a dictionary')
                raise TypeError(error_message)
            if not X:
                error_message = 'X is an empty dictionary'
                raise ValueError(error_message)
            if X.keys() != obs.keys():
                error_message = (
                    'X and obs must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            if X.keys() != var.keys():
                error_message = (
                    'X and var must have the same cell types (keys), in the '
                    'same order')
                raise ValueError(error_message)
            for cell_type in X:
                if not isinstance(cell_type, str):
                    error_message = (
                        f'all keys of X (cell types) must be strings, but X '
                        f'contains a key of type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                check_type(X[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                if X[cell_type].ndim != 2:
                    error_message = (
                        f'X[{cell_type!r}] is a {X[cell_type].ndim:,}-'
                        f'dimensional NumPy array, but must be 2-dimensional')
                    raise ValueError(error_message)
                check_type(obs[cell_type], f'obs[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
                check_type(var[cell_type], f'var[{cell_type!r}]', pl.DataFrame,
                           'a polars DataFrame')
            self._X = X
            self._obs = obs
            self._var = var
        elif isinstance(X, (str, Path)):
            X = str(X)
            if not os.path.exists(X):
                error_message = f'Pseudobulk directory {X!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{X}/cell_types.txt')]
            self._X = {cell_type: np.load(
                os.path.join(X, f'{cell_type.replace("/", "-")}.X.npy'))
                for cell_type in cell_types}
            self._obs = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.obs.parquet'))
                for cell_type in cell_types}
            self._var = {cell_type: pl.read_parquet(
                os.path.join(X, f'{cell_type.replace("/", "-")}.var.parquet'))
                for cell_type in cell_types}
        else:
            error_message = (
                f'X must be a dictionary of NumPy arrays or a directory '
                f'containing a saved Pseudobulk dataset, but has type '
                f'{type(X).__name__!r}')
            raise ValueError(error_message)
        for cell_type in self._X:
            dtype = self._X[cell_type].dtype
            if dtype != np.int32 and dtype != np.int64 and \
                    dtype != np.float32 and dtype != np.float64 and \
                    dtype != np.uint32 and dtype != np.uint64:
                error_message = (
                    f'X must be (u)int32/64 or float32/64, but has data type '
                    f'{str(dtype)}')
                raise TypeError(error_message)
            if len(self._obs[cell_type]) == 0:
                error_message = \
                    f'len(obs[{cell_type!r}]) is 0: no samples remain'
                raise ValueError(error_message)
            if len(self._var[cell_type]) == 0:
                error_message = \
                    f'len(var[{cell_type!r}]) is 0: no genes remain'
                raise ValueError(error_message)
            if len(self._obs[cell_type]) != len(self._X[cell_type]):
                error_message = (
                    f'len(obs[{cell_type!r}]) is '
                    f'{len(self._obs[cell_type]):,}, but '
                    f'len(X[{cell_type!r}]) is {len(X[cell_type]):,}')
                raise ValueError(error_message)
            if len(self._var[cell_type]) != self._X[cell_type].shape[1]:
                error_message = (
                    f'len(var[{cell_type!r}]) is '
                    f'{len(self._var[cell_type]):,}, but '
                    f'X[{cell_type!r}].shape[1] is '
                    f'{self._X[cell_type].shape[1]:,}')
                raise ValueError(error_message)
            if self._obs[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of obs[{cell_type!r}] '
                    f'({self._obs[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._obs[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
            if self._var[cell_type][:, 0].dtype not in \
                    (pl.String, pl.Categorical, pl.Enum):
                error_message = (
                    f'the first column of var[{cell_type!r}] '
                    f'({self._var[cell_type].columns[0]!r}) must be String, '
                    f'Enum, or Categorical, but has data type '
                    f'{self._var[cell_type][:, 0].dtype.base_type()!r}')
                raise ValueError(error_message)
    
    @staticmethod
    def _setter_check(new: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      old: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                            np.floating]] |
                                     pl.DataFrame],
                      name: str) -> None:
        """
        When setting `X`, `obs` or `var`, raise an error if the new value is
        not a dictionary, the new cell types (keys) differ from the old ones,
        or the new values differ in length (or shape, in the case of `X`) from
        the old ones. For `obs` and `var`, also check that the first column is
        String, Enum, or Categorical.
        
        Args:
            new: the new `X`, `obs` or `var`
            old: the old `X`, `obs` or `var`
            name: the name of the field: `'X'`, `'obs'` or `'var'`
        """
        if not isinstance(new, dict):
            error_message = (
                f'new {name} must be a dictionary, but has type '
                f'{type(new).__name__!r}')
            raise TypeError(error_message)
        if new.keys() != old.keys():
            error_message = (
                f'new {name} has different cell types (keys) from the old '
                f'{name}, or has the same cell types in a different order')
            raise ValueError(error_message)
        if name == 'X':
            for cell_type in new:
                check_type(new[cell_type], f'X[{cell_type!r}]', np.ndarray,
                           'a NumPy array')
                new_shape = new[cell_type].shape
                old_shape = old[cell_type].shape
                if new_shape != old_shape:
                    error_message = (
                        f'new X[{cell_type!r}] is {new_shape.shape[0]:,} × '
                        f'{new_shape.shape[1]:,}, but old X is '
                        f'{old_shape.shape[0]:,} × {old_shape.shape[1]:,}')
                    raise ValueError(error_message)
                dtype = new[cell_type].dtype
                if dtype != np.int32 and dtype != np.int64 and \
                        dtype != np.float32 and dtype != np.float64 and \
                        dtype != np.uint32 and dtype != np.uint64:
                    error_message = (
                        f'X must be (u)int32/64 or float32/64, but new '
                        f'X[{cell_type!r}] data type {str(dtype)}')
                    raise TypeError(error_message)
        else:
            for cell_type in new:
                check_type(new[cell_type], f'{name}[{cell_type!r}]',
                           pl.DataFrame, 'a polars DataFrame')
                if new[cell_type][:, 0].dtype not in (
                        pl.String, pl.Categorical, pl.Enum):
                    error_message = (
                        f'the first column of {name}[{cell_type!r}] '
                        f'({new[cell_type].columns[0]!r}) must be String, '
                        f'Enum, or Categorical, but the first column of the '
                        f'new {name} has data type '
                        f'{new[cell_type][:, 0].dtype.base_type()!r}')
                    raise ValueError(error_message)
                if len(new) != len(old):
                    error_message = (
                        f'new {name}[{cell_type!r}] has length {len(new):,}, '
                        f'but old {name} has length {len(old):,}')
                    raise ValueError(error_message)
    
    @property
    def X(self) -> dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]:
        return self._X
    
    @X.setter
    def X(self, X: dict[str, np.ndarray[2, np.dtype[np.integer |
                                                    np.floating]]]) -> None:
        self._setter_check(X, self._X, 'X')
        self._X = X
    
    @property
    def obs(self) -> dict[str, pl.DataFrame]:
        return self._obs
    
    @obs.setter
    def obs(self, obs: dict[str, pl.DataFrame]) -> None:
        self._setter_check(obs, self._obs, 'obs')
        self._obs = obs

    @property
    def var(self) -> dict[str, pl.DataFrame]:
        return self._var
    
    @var.setter
    def var(self, var: dict[str, pl.DataFrame]) -> None:
        self._setter_check(var, self._var, 'var')
        self._var = var

    @property
    def obs_names(self) -> dict[str, pl.Series]:
        return {cell_type: obs[:, 0] for cell_type, obs in self._obs.items()}
    
    @property
    def var_names(self) -> dict[str, pl.Series]:
        return {cell_type: var[:, 0] for cell_type, var in self._var.items()}

    @property
    def num_threads(self) -> int:
        """
        Get the default number of threads used for this Pseudobulk dataset's
        operations.
        
        Returns:
            The default number of threads.
        """
        return self._num_threads
     
    @num_threads.setter
    def num_threads(self, num_threads: int | np.integer) -> None:
        """
        Set the default number of threads used for this Pseudobulk dataset's
        operations.
        
        Multithreading is only supported for "free-threaded" builds of Python
        3.13 and later with the global interpreter lock (GIL) disabled.
        
        Args:
            num_threads: the new default number of threads. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`.
        """
        check_type(num_threads, 'num_threads', int, 'a positive integer or -1')
        if num_threads == 1:
            self._num_threads = 1
            return
        num_threads = int(num_threads)
        if num_threads <= 0 and num_threads != -1:
            error_message = (
                f'num_threads is {num_threads:,}, but must be a positive '
                f'integer or -1')
            raise ValueError(error_message)
        try:
            # noinspection PyUnresolvedReferences
            has_GIL = sys._is_gil_enabled()
        except AttributeError:
            has_GIL = True
        if has_GIL:
            error_message = (
                f'num_threads is {num_threads}, but multithreading for '
                f'Pseudobulk datasets is only supported for "free-threaded" '
                f'builds of Python 3.13 and later with the global interpreter '
                f'lock (GIL) disabled')
            raise ValueError(error_message)
        self._num_threads = \
            os.cpu_count() if num_threads == -1 else num_threads
    
    def _process_num_threads(self,
                             num_threads: int | np.integer | None) -> int:
        """
        Process a `num_threads` value specified by the user as an argument to a
        Pseudobulk function.

        Args:
            num_threads: the number of threads specified by the user

        Returns:
            The actual number of threads to use. If `num_threads` is `None`,
            return `self.num_threads`. If the user lacks free-threaded Python,
            raise an error if `num_threads` is -1 or greater than 1. Then,
            return `os.cpu_count()` if `num_threads` is -1, otherwise return it
            unchanged.
        """
        if num_threads is None:
            return self._num_threads
        check_type(num_threads, 'num_threads', int,
                   'a positive integer, -1, or None')
        if num_threads == 1:
            return 1
        num_threads = int(num_threads)
        if num_threads <= 0 and num_threads != -1:
            error_message = (
                f'num_threads is {num_threads:,}, but must be a positive '
                f'integer, -1, or None')
            raise ValueError(error_message)
        try:
            # noinspection PyUnresolvedReferences
            has_GIL = sys._is_gil_enabled()
        except AttributeError:
            has_GIL = True
        if has_GIL:
            error_message = (
                f'num_threads is {num_threads}, but multithreading for '
                f'Pseudobulk datasets is only supported for "free-threaded" '
                f'builds of Python 3.13 and later with the global interpreter '
                f'lock (GIL) disabled')
            raise ValueError(error_message)
        return os.cpu_count() if num_threads == -1 else num_threads

    def _process_cell_types(self,
                            cell_types: str | Iterable[str] | None,
                            excluded_cell_types: str | Iterable[str] | None,
                            *,
                            return_description: bool = False) -> \
            tuple[str, ...] | tuple[tuple[str, ...], str]:
        """
        Process the `cell_types` and `excluded_cell_types` arguments of various
        Pseudobulk functions.
        
        Args:
            cell_types: one or more cell types to include in the calling
                        function's operation
            excluded_cell_types: one or more cell types to exclude from the
                                 calling function's operation
            return_description: whether to return a description of the cell
                                types, to use in error messages

        Returns:
            A tuple of cell-type names, or a two-element tuple of
            (cell-type names, cell-type description) if
            `return_description=True`.
        """
        if cell_types is not None:
            if excluded_cell_types is not None:
                error_message = (
                    'cell_types and excluded_cell_types cannot both be '
                    'specified')
                raise ValueError(error_message)
            is_string = isinstance(cell_types, str)
            cell_types = \
                to_tuple_checked(cell_types, 'cell_types', str, 'strings')
            for cell_type in cell_types:
                if cell_type not in self._X:
                    if is_string:
                        error_message = (
                            f'cell_types is {cell_type!r}, which is not a '
                            f'cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            if return_description:
                cell_type_description = 'the cell_types argument'
        elif excluded_cell_types is not None:
            excluded_cell_types = to_tuple_checked(
                excluded_cell_types, 'cell_types', str, 'strings')
            for cell_type in excluded_cell_types:
                if cell_type not in self._X:
                    if excluded_cell_types:
                        error_message = (
                            f'excluded_cell_types is {cell_type!r}, which is '
                            f'not a cell type in this Pseudobulk dataset')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'one of the elements of excluded_cell_types, '
                            f'{cell_type!r}, is not a cell type in this '
                            f'Pseudobulk dataset')
                        raise ValueError(error_message)
            cell_types = tuple(cell_type for cell_type in self._X
                               if cell_type not in excluded_cell_types)
            if len(cell_types) == 0:
                error_message = \
                    'all cell types were excluded by excluded_cell_types'
                raise ValueError(error_message)
            if return_description:
                cell_type_description = (
                    'this Pseudobulk dataset (after excluding the cell types '
                    'in excluded_cell_types)')
        else:
            cell_types = tuple(self._X)
            if return_description:
                cell_type_description = 'this Pseudobulk dataset'
        # noinspection PyUnboundLocalVariable
        return (cell_types, cell_type_description) \
            if return_description else cell_types
    
    def set_obs_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of `obs`, i.e. the `obs_names`.
        
        Args:
            column: the column name in `obs`; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's `obs`. If `column` is already the first column for
            every cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._obs[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                if column not in cell_type_obs:
                    error_message = \
                        f'{column!r} is not a column of obs[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_obs, f'obs[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                obs[cell_type] = \
                    cell_type_obs.select(column, pl.exclude(column))
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var,
                          num_threads=self._num_threads)
    
    def set_var_names(self,
                      column: str,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] |
                                           None = None) -> Pseudobulk:
        """
        Sets a column as the new first column of `var`, i.e. the `var_names`.
        
        Args:
            column: the column name in `var`; must have String, Categorical, or
                    Enum data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with `column` as the first column of each
            cell type's `var`. If `column` is already the first column for
            every cell type, return this dataset unchanged.
        """
        check_type(column, 'column', str, 'a string')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if all(column == self._var[cell_type].columns[0]
               for cell_type in cell_types):
            return self
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                if column not in cell_type_var:
                    error_message = \
                        f'{column!r} is not a column of var[{cell_type!r}]'
                    raise ValueError(error_message)
                check_dtype(cell_type_var, f'var[{column!r}]',
                            (pl.String, pl.Categorical, pl.Enum))
                var[cell_type] = \
                    cell_type_var.select(column, pl.exclude(column))
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var,
                          num_threads=self._num_threads)

    def keys(self) -> KeysView[str]:
        """
        Get a KeysView (like you would get from `dict.keys()`) of this
        Pseudobulk dataset's cell types. `for cell_type in pb.keys():` is
        equivalent to `for cell_type in pb:`.
        
        Returns:
            A KeysView of the cell types.
        """
        return self._X.keys()
    
    def values(self) -> ValuesView[tuple[np.ndarray[2, np.dtype[np.integer |
                                                                np.floating]],
                                       pl.DataFrame, pl.DataFrame]]:
        """
        Get a `ValuesView` (like you would get from `dict.values()`) of
        `(X, obs, var)` tuples for each cell type in this Pseudobulk dataset.
        
        Returns:
            A `ValuesView` of `(X, obs, var)` tuples for each cell type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.values()
    
    def items(self) -> ItemsView[str, tuple[np.ndarray[2, np.dtype[
            np.integer | np.floating]], pl.DataFrame, pl.DataFrame]]:
        """
        Get an `ItemsView` (like you would get from `dict.items()`) of
        `(cell_type, (X, obs, var))` tuples for each cell type in this
        Pseudobulk dataset.
        
        Yields:
            An `ItemsView` of `(cell_type, (X, obs, var))` tuples for each cell
            type.
        """
        return {cell_type: (self._X[cell_type], self._obs[cell_type],
                            self._var[cell_type])
                for cell_type in self._X}.items()
    
    def iter_X(self) -> Iterable[np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]]:
        """
        Iterate over each cell type's `X`.
        
        Yields:
            `X` for each cell type.
        """
        for X in self._X.values():
            yield X
    
    def iter_obs(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's `obs`.
        
        Yields:
            `obs` for each cell type.
        """
        for obs in self._obs.values():
            yield obs
    
    def iter_var(self) -> Iterable[pl.DataFrame]:
        """
        Iterate over each cell type's `var`.
        
        Yields:
            `var` for each cell type.
        """
        for var in self._var.values():
            yield var
    
    def __eq__(self, other: Pseudobulk) -> bool:
        """
        Test for equality with another Pseudobulk dataset.
        
        Args:
            other: the other Pseudobulk dataset to test for equality with

        Returns:
            Whether the two Pseudobulk datasets are identical.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `==` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        # noinspection PyUnresolvedReferences
        return tuple(self.keys()) == tuple(other.keys()) and \
            all(obs.equals(other_obs) for obs, other_obs in
                zip(self._obs.values(), other._obs.values())) and \
            all(var.equals(other_var) for var, other_var in
                zip(self._var.values(), other._var.values())) and \
            all(array_equal(X, other_X) for X, other_X in
                zip(self._X.values(), other._X.values()))
    
    def __or__(self, other: Pseudobulk) -> Pseudobulk:
        """
        Combine the cell types of this Pseudobulk dataset with another. The
        two datasets must have non-overlapping cell types.
        
        Args:
            other: the other Pseudobulk dataset to combine with this one

        Returns:
            A Pseudobulk dataset with each of the cell types in the first
            Pseudobulk dataset, followed by each of the cell types in the
            second.
        """
        if not isinstance(other, Pseudobulk):
            error_message = (
                f'the left-hand operand of `|` is a Pseudobulk dataset, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        if self.keys() & other.keys():
            error_message = (
                'the left- and right-hand operands of `|` are Pseudobulk '
                'datasets that share some cell types')
            raise ValueError(error_message)
        return Pseudobulk(X=self._X | other._X, obs=self._obs | other._obs,
                          var=self._var | other._var,
                          num_threads=self._num_threads)
    
    def __contains__(self, cell_type: str) -> bool:
        """
        Check if this Pseudobulk dataset contains the specified cell type.
        
        Args:
            cell_type: the cell type

        Returns:
            Whether the cell type is present in the Pseudobulk dataset.
        """
        check_type(cell_type, 'cell_type', str, 'a string')
        return cell_type in self._X
    
    @staticmethod
    def _getitem_error(item: Indexer | tuple[str, Indexer, Indexer]) -> None:
        """
        Raise an error if the indexer is invalid.
        
        Args:
            item: the indexer
        """
        types = tuple(type(elem).__name__ for elem in to_tuple(item))
        if len(types) == 1:
            types = types[0]
        error_message = (
            f'Pseudobulk indices must be a cell-type string, a length-1 tuple '
            f'of (cell_type,), a length-2 tuple of (cell_type, samples), or a '
            f'length-3 tuple of (cell_type, samples, genes). Samples and '
            f'genes must each be a string or integer; a slice of strings or '
            f'integers; or a list, NumPy array, or polars Series of strings, '
            f'integers, or Booleans. You indexed with: {types}.')
        raise ValueError(error_message)
    
    @staticmethod
    def _getitem_by_string(df: pl.DataFrame, string: str) -> int:
        """
        Get the index where df[:, 0] == string, raising an error if no rows or
        multiple rows match.
        
        Args:
            df: a DataFrame (`obs` or `var`)
            string: the string to find the index of in the first column of df

        Returns:
            The integer index of the string within the first column of df.
        """
        first_column = df.columns[0]
        try:
            return df\
                .select(pl.int_range(pl.len(), dtype=pl.Int32)
                        .alias('__Pseudobulk_getitem'), first_column)\
                .row(by_predicate=pl.col(first_column) == string)\
                [0]
        except pl.exceptions.NoRowsReturnedError:
            raise KeyError(string)
    
    # noinspection PyTypeChecker
    @staticmethod
    def _getitem_process(item: Indexer | tuple[str, Indexer, Indexer],
                         index: int,
                         df: pl.DataFrame) -> list[int] | slice | pl.Series:
        """
        Process an element of an item passed to `__getitem__()`.
        
        Args:
            item: the item
            index: the index of the element to process
            df: the DataFrame (`obs` or `var`) to process the element with
                respect to

        Returns:
            A new indexer indicating the rows/columns to index.
        """
        subitem = item[index]
        if isinstance(subitem, (int, np.integer)):
            return [subitem]
        elif isinstance(subitem, str):
            return [Pseudobulk._getitem_by_string(df, subitem)]
        elif isinstance(subitem, slice):
            start = subitem.start
            stop = subitem.stop
            step = subitem.step
            if isinstance(start, str):
                start = Pseudobulk._getitem_by_string(df, start)
            elif start is not None and \
                    not isinstance(start, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if isinstance(stop, str):
                stop = Pseudobulk._getitem_by_string(df, stop)
            elif stop is not None and not isinstance(stop, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            if step is not None and not isinstance(step, (int, np.integer)):
                Pseudobulk._getitem_error(item)
            return slice(start, stop, step)
        elif isinstance(subitem, (list, np.ndarray, pl.Series)):
            subitem = pl.Series(subitem)
            if subitem.is_null().any():
                error_message = 'your indexer contains missing values'
                raise ValueError(error_message)
            dtype = subitem.dtype
            if dtype in (pl.String, pl.Categorical, pl.Enum):
                names_dtype = df[:, 0].dtype
                if dtype != names_dtype:
                    subitem = subitem.cast(names_dtype)
                indices = subitem\
                    .to_frame(df.columns[0])\
                    .join(df.with_columns(_Pseudobulk_index=pl.int_range(
                              pl.len(), dtype=pl.UInt32)),
                          on=df.columns[0], how='left')\
                    ['_Pseudobulk_index']
                if indices.null_count():
                    error_message = subitem.filter(indices.is_null())[0]
                    raise KeyError(error_message)
                return indices
            elif dtype.is_integer() or dtype == pl.Boolean:
                return subitem
            else:
                Pseudobulk._getitem_error(item)
        else:
            Pseudobulk._getitem_error(item)
    
    def __getitem__(self, item: Indexer | tuple[str, Indexer, Indexer]) -> \
            Pseudobulk:
        """
        Subset to specific cell type(s), sample(s), and/or gene(s).
        
        Index with a tuple of `(cell_types, samples, genes)`. If `samples` and
        `genes` are integers, arrays/lists/slices of integers, or arrays/lists
        of Booleans, the result will be a Pseudobulk dataset subset to
        `X[samples, genes]`, `obs[samples]`, and `var[genes]` for each of the
        cell types in `cell_types`. However, `samples` and/or `genes` can
        instead be strings (or arrays or slices of strings), in which case they
        refer to the first column of `obs` and/or `var`, respectively.
        
        Examples:
        - Subset to one cell type:
          pseudobulk['Astro']
        - Subset to multiple cell types:
          pseudobulk[['Astro', 'Micro']]
        - Subset to one cell type and sample, for all genes:
          pb['Astro', 'H19.30.002']
          pb['Astro', 2]
        - Subset to one gene, for all cell types and samples:
          pb[:, :, 'APOE']
          pb[:, :, 13196]
        - Subset to one cell type, sample and gene:
          pb['Astro', 'H18.30.002', 'APOE']
          pb['Astro', 2, 13196]
        - Subset to one cell type and a range of samples and genes:
          pb['Astro', 'H18.30.002':'H19.33.004', 'APOE':'TREM2']
          pb['Astro', 'H18.30.002':'H19.33.004', 13196:34268]
        - Subset to one a cell type and specific samples and genes:
          pb['Astro', ['H18.30.002', 'H19.33.004']]
          pb['Astro', :, pl.Series(['APOE', 'TREM2'])]
          pb['Astro', ('H18.30.002', 'H19.33.004'),
             np.array(['APOE', 'TREM2'])]
        
        Args:
            item: the item to index with

        Returns:
            A new Pseudobulk dataset subset to the specified cell types,
            samples, and/or genes.
        """
        is_slice = False
        if isinstance(item, tuple):
            if not 1 <= len(item) <= 3:
                self._getitem_error(item)
            if isinstance(item[0], slice):
                if item[0] == slice(None):
                    cell_types = tuple(self._X)
                    is_slice = True
                else:
                    error_message = (
                        'slicing cell types is not currently supported, '
                        'except for selecting all cell types (e.g. pb[:, '
                        'samples, genes])')
                    raise ValueError(error_message)
            else:
                cell_types = to_tuple(item[0])
        elif isinstance(item, (list, np.ndarray, pl.Series)):
            cell_types = to_tuple(item)
        elif isinstance(item, str):
            cell_types = item,
        elif isinstance(item, slice):
            if item == slice(None):
                cell_types = tuple(self._X)
                is_slice = True
            else:
                error_message = (
                    'slicing cell types is not currently supported, except '
                    'for selecting all cell types (e.g. pb[:, samples, '
                    'genes])')
                raise ValueError(error_message)
        else:
            self._getitem_error(item)
        if not is_slice:
            # noinspection PyUnboundLocalVariable
            for cell_type in cell_types:
                if cell_type not in self._X:
                    if isinstance(cell_type, str):
                        error_message = (
                            f'tried to select {cell_type!r}, which is not a '
                            f'cell type in this Pseudobulk')
                        raise ValueError(error_message)
                    else:
                        error_message = (
                            f'tried to select a non-existent cell type of '
                            f'type {type(cell_type).__name__!r}')
                        raise TypeError(error_message)
        if not isinstance(item, tuple) or len(item) == 1:
            return Pseudobulk(X={cell_type: self._X[cell_type]
                                 for cell_type in cell_types},
                              obs={cell_type: self._obs[cell_type]
                                   for cell_type in cell_types},
                              var={cell_type: self._var[cell_type]
                                   for cell_type in cell_types},
                              num_threads=self._num_threads)
        X, obs, var = {}, {}, {}
        for cell_type in cell_types:
            rows = self._getitem_process(item, 1, self._obs[cell_type])
            if isinstance(rows, pl.Series):
                obs[cell_type] = self._obs[cell_type].filter(rows) \
                    if rows.dtype == pl.Boolean else self._obs[cell_type][rows]
                rows = rows.to_numpy()
            else:
                obs[cell_type] = self._obs[cell_type][rows]
            if len(item) == 2:
                X[cell_type] = self._X[cell_type][rows]
                var[cell_type] = self._var[cell_type]
            else:
                columns = self._getitem_process(item, 2, self._var[cell_type])
                if isinstance(columns, pl.Series):
                    var[cell_type] = self._var[cell_type].filter(columns) \
                        if columns.dtype == pl.Boolean \
                        else self._var[cell_type][columns]
                    columns = columns.to_numpy()
                else:
                    var[cell_type] = self._var[cell_type][columns]
                X[cell_type] = self._X[cell_type][rows, columns] \
                    if isinstance(rows, slice) or \
                       isinstance(columns, slice) else \
                    self._X[cell_type][np.ix_(rows, columns)]
        return Pseudobulk(X=X, obs=obs, var=var, num_threads=self._num_threads)
    
    def sample(self, cell_type: str, sample: str) -> np.ndarray[1, Any]:
        """
        Get the row of `X[cell_type]` corresponding to a single sample, based
        on the sample's name in `obs_names`.
        
        Args:
            cell_type: the cell type to retrieve the row of `X` from
            sample: the name of the sample in `obs_names`
        
        Returns:
            The corresponding row of `X[cell_type]`, as a dense 1D NumPy array
            with zeros included.
        """
        row_index = Pseudobulk._getitem_by_string(self._obs[cell_type], sample)
        return self._X[cell_type][row_index]
    
    def gene(self, cell_type: str, gene: str) -> np.ndarray[1, Any]:
        """
        Get the column of `X[cell_type]` corresponding to a single gene, based
        on the gene's name in `var_names`.
        
        Args:
            cell_type: the cell type to retrieve the row of `X` from
            gene: the name of the gene in `var_names`
        
        Returns:
            The corresponding column of `X[cell_type]`, as a dense 1D NumPy
            array with zeros included.
        """
        column_index = \
            Pseudobulk._getitem_by_string(self._var[cell_type], gene)
        return self._X[cell_type][:, column_index]
    
    def __iter__(self) -> Iterable[str]:
        """
        Iterate over the cell types of this Pseudobulk dataset.
        `for cell_type in pb` is equivalent to `for cell_type in pb.keys()`.
        
        Returns:
            An iterator over the cell types.
        """
        return iter(self._X)
    
    def __len__(self) -> dict[str, int]:
        """
        Get the number of samples in each cell type of this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to its number of samples.
        """
        return {cell_type: len(X_cell_type)
                for cell_type, X_cell_type in self._X.items()}
    
    def __repr__(self) -> str:
        """
        Get a string representation of this Pseudobulk dataset.
        
        Returns:
            A string summarizing the dataset.
        """
        min_samples = min(len(obs) for obs in self._obs.values())
        max_samples = max(len(obs) for obs in self._obs.values())
        min_genes = min(len(var) for var in self._var.values())
        max_genes = max(len(var) for var in self._var.values())
        samples_string = f'{min_samples:,} {plural("sample", max_samples)}' \
            if min_samples == max_samples else \
            f'{min_samples:,}-{max_samples:,} samples'
        genes_string = f'{min_genes:,} {plural("gene", max_genes)}' \
            if min_genes == max_genes else \
            f'{min_genes:,}-{max_genes:,} genes'
        try:
            terminal_width = os.get_terminal_size().columns
        except AttributeError:
            terminal_width = 80  # for Jupyter notebooks
        return f'Pseudobulk dataset with {len(self._X):,} cell ' \
               f'{"types, each" if len(self._X) > 1 else "type,"} with ' \
               f'{samples_string} (obs) and {genes_string} (var)\n' + \
            fill(f'    Cell types: {", ".join(self._X)}',
                 width=terminal_width, subsequent_indent=' ' * 17)
    
    @property
    def shape(self) -> dict[str, tuple[int, int]]:
        """
        Get the shape of each cell type in this Pseudobulk dataset.
        
        Returns:
            A dictionary mapping each cell type to a length-2 tuple where the
            first element is the number of samples, and the second is the
            number of genes.
        """
        return {cell_type: X_cell_type.shape
                for cell_type, X_cell_type in self._X.items()}
    
    def save(self,
             directory: str | Path,
             overwrite: bool = False,
             *,
             cell_types: str | Iterable[str] | None = None,
             excluded_cell_types: str | Iterable[str] | None = None) -> None:
        """
        Saves a Pseudobulk dataset to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with three files per cell type:
        the `X` at `f'{cell_type}.X.npy'`, the `obs` at
        `f'{cell_type}.obs.parquet'`, and the `var` at
        `f'{cell_type}.var.parquet'`. Also saves a text file, `cell_types.txt`,
        containing the cell types.
        
        Args:
            directory: the directory to save the Pseudobulk dataset to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
            cell_types: one or more cell types to save; if `None`, save all
                        cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from saving;
                                 mutually exclusive with `cell_types`
        """
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        cell_types = self._process_cell_types(cell_types, excluded_cell_types)
        with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
            # noinspection PyTypeChecker
            print('\n'.join(cell_types), file=f)
        for cell_type in cell_types:
            escaped_cell_type = cell_type.replace('/', '-')
            np.save(os.path.join(directory, f'{escaped_cell_type}.X.npy'),
                    self._X[cell_type])
            self._obs[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.obs.parquet'))
            self._var[cell_type].write_parquet(
                os.path.join(directory, f'{escaped_cell_type}.var.parquet'))
    
    def copy(self, *, deep: bool = False) -> Pseudobulk:
        """
        Make a copy of this Pseudobulk dataset.
        
        Args:
            deep: whether to make a deep copy instead of a shallow one. Since
                  polars DataFrames are immutable, `obs[cell_type]` and
                  `var[cell_type]` will always point to the same underlying
                  data as the original for all cell types. The only difference
                  when `deep=True` is that `X[cell_type]` will point to a fresh
                  copy of the data, rather than the same data. When
                  `deep=False`, any modifications to the underlying count
                  matrix will modify both the original and the copy.

        Returns:
            A copy of the Pseudobulk dataset.
        """
        check_type(deep, 'deep', bool, 'Boolean')
        return Pseudobulk(X={cell_type: cell_type_X.copy()
                             for cell_type, cell_type_X in self._X.items()}
                            if deep else self._X,
                          obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    def to_df(self,
              *,
              obs_columns: str | Iterable[str] | None = None,
              genes: str | Iterable[str] | None = None,
              cell_type_column: str = 'cell_type') -> pl.DataFrame:
        """
        Convert this Pseudobulk object to a polars DataFrame, with one row per
        (sample, cell type) pair and one column per gene.
        
        The first columns of the DataFrame will contain metadata: a `cell_type`
        column, a sample ID column (the `obs_names`), a `num_cells` column, and
        whichever additional columns are specified in `obs_columns`.
        
        Genes or columns of obs not present in every cell type will contain
        `null` values for cell types where they are missing.
        
        Args:
            obs_columns: one or more names of columns of `obs` to include in
                         the DataFrame, in addition to the cell type, the
                         sample ID, and the number of cells
            genes: one or more genes to include as columns
            cell_type_column: the name of the cell-type column to be added as
                              the first column of the DataFrame

        Returns:
            A polars DataFrame containing the gene counts and metadata for each
            (sample, cell type) pair.
        """
        # Check that `cell_type_column` is a string, and not `num_cells`
        check_type(cell_type_column, 'cell_type_column', str, 'a string')
        if cell_type_column == 'num_cells':
            error_message = "the cell_type_column cannot be named 'num_cells'"
            raise ValueError(error_message)
        
        # Get `obs_columns` and `genes` as tuples, if they are single values,
        # and check that all their elements are strings and that
        # `cell_type_column` name is not in them
        if obs_columns is not None:
            obs_columns = to_tuple_checked(obs_columns, 'obs_columns', str,
                                           'strings')
            if cell_type_column in obs_columns:
                error_message = (
                    f'cell_type_column {cell_type_column!r} is in '
                    f'obs_columns; specify a different name for the '
                    f'cell_type_column, or remove {cell_type_column!r} from '
                    f'obs_columns')
                raise ValueError(error_message)
        if genes is not None:
            genes = to_tuple_checked(genes, 'genes', str, 'strings')
            if cell_type_column in genes:
                error_message = (
                    f'cell_type_column {cell_type_column!r} is in genes; '
                    f'specify a different name for the cell_type_column, or '
                    f'remove {cell_type_column!r} from genes')
                raise ValueError(error_message)
        
        # Get the DataFrame for each cell type
        dfs = []
        for cell_type, (X, obs, var) in self.items():
            columns = [pl.lit(cell_type).alias(cell_type_column),
                       obs[:, 0].name]
            if 'num_cells' in obs:
                columns.append('num_cells')
            if obs_columns is not None:
                columns += [column for column in obs_columns if column in obs]
            df = obs.select(columns)
            if genes is None:
                gene_df = pl.from_numpy(X, schema=var[:, 0].to_list())
            else:
                gene_df = pl.from_numpy(X[:, genes], schema=genes)
            df = pl.concat((df, gene_df), how='horizontal')
            dfs.append(df)
        
        # Concatenate across cell types
        df = pl.concat(dfs, how='diagonal_relaxed')
        
        # Check that all `obs_columns` and/or `genes` appear in the DataFrame
        # (i.e. were present in at least one cell type), if either argument was
        # specified
        for column_set, column_set_name, obs_or_var in \
                (obs_columns, 'obs_columns', 'obs'), (genes, 'genes', 'var'):
            if column_set is not None:
                for column in column_set:
                    if column not in df:
                        error_message = (
                            f"column {column!r} was specified in "
                            f"{column_set_name}, but did not appear in any "
                            f"cell type's {obs_or_var}")
                        raise ValueError(error_message)
        return df
    
    def concat_obs(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate one or more other Pseudobulk datasets with this one,
        sample-wise. All datasets must have the same cell types, and all
        datasets must have distinct `obs_names`.
        
        By default, all datasets must have the same `var`. They must also have
        the same columns in `obs`, with the same data types.
        
        Conversely, if `flexible=True`, subset to genes present in all datasets
        (according to the first column of `var`, i.e. the `var_names`) before
        concatenating. Subset to columns of `var` that are identical in all
        datasets after this subsetting. Also, subset to columns of `obs` that
        are present in all datasets, and have the same data types. All
        datasets' `obs_names` must have the same name and dtype, and similarly
        for their `var_names`.
        
        The one exception to the `obs` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `obs`.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to genes and columns of `obs` and `var`
                      common to all datasets before concatenating, rather than
                      raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
       
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._obs:
            if flexible:
                # Check that `obs_names` and `var_names` have the same name and
                # data type for each cell type across all datasets
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0] == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0].name == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                
                # Subset to genes in common across all datasets
                genes_in_common = self._var[cell_type][:, 0]\
                    .filter(self._var[cell_type][:, 0]
                            .is_in(pl.concat([dataset._var[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(genes_in_common) == 0:
                    error_message = (
                        f'no genes are shared across all Pseudobulk datasets '
                        f'for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_var = []
                for dataset in datasets:
                    gene_indices = dataset._getitem_process(
                        genes_in_common, 1, dataset._var[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, gene_indices.to_numpy()])
                    cell_type_var.append(dataset._var[cell_type][gene_indices])
                
                # Subset to columns of `var` that are identical in all datasets
                # after this subsetting
                var_columns_in_common = [
                    column.name for column in cell_type_var[0][:, 1:]
                    if all(column.name in dataset_cell_type_var and
                           dataset_cell_type_var[column.name].equals(column)
                           for dataset_cell_type_var in cell_type_var[1:])]
                cell_type_var = cell_type_var[0]
                cell_type_var = cell_type_var.select(cell_type_var.columns[0],
                                                     var_columns_in_common)
                
                # Subset to columns of `obs` that are present in all datasets,
                # and have the same data types. Also include columns of `obs`
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                obs_mismatched_categoricals = {
                    column for column, dtype in self._obs[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._obs[cell_type] and
                           dataset._obs[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._obs[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                obs_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._obs[cell_type].schema.items(), 1, None)
                    if column in obs_mismatched_categoricals or
                       all(column in dataset[cell_type]._obs and
                           dataset._obs[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._obs[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in obs_mismatched_categoricals}
                cell_type_obs = [
                    dataset._obs[cell_type]
                    .cast(cast_dict)
                    .select(obs_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all `var` are identical
                cell_type_var = self._var[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._var[cell_type].equals(cell_type_var):
                        error_message = (
                            f'all Pseudobulk datasets must have the same var '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                
                # Check that all `obs` have the same columns and data types
                schema = self._obs[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._obs[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in obs for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_obs = [dataset._obs[cell_type]
                                 for dataset in datasets]
            
            # Concatenate
            obs[cell_type] = pl.concat(cell_type_obs)
            num_unique = obs[cell_type][:, 0].n_unique()
            if num_unique < len(obs[cell_type]):
                error_message = (
                    f'obs_names contains {len(obs[cell_type]) - num_unique:,} '
                    f'duplicates after concatenation for cell type '
                    f'{cell_type!r}')
                raise ValueError(error_message)
            X[cell_type] = np.vstack(cell_type_X)
            var[cell_type] = cell_type_var
        return Pseudobulk(X=X, obs=obs, var=var, num_threads=self._num_threads)

    def concat_var(self,
                   datasets: Pseudobulk | Iterable[Pseudobulk],
                   *more_datasets: Pseudobulk,
                   flexible: bool = False) -> Pseudobulk:
        """
        Concatenate one or more other Pseudobulk datasets with this one,
        gene-wise. This is much less common than the sample-wise concatenation
        provided by `concat_obs()`. All datasets must have the same cell types,
        and all datasets must have distinct `var_names`.
        
        By default, all datasets must have the same `obs`. They must also have
        the same columns in `var`, with the same data types.
        
        Conversely, if `flexible=True`, subset to samples present in all
        datasets (according to the first column of `obs`, i.e. the `obs_names`)
        before concatenating. Subset to columns of `obs` that are identical in
        all datasets after this subsetting. Also, subset to columns of `var`
        that are present in all datasets, and have the same data types. All
        datasets' `obs_names` must have the same name and dtype, and similarly
        for their `var_names`.
        
        The one exception to the `var` "same data type" rule: if a column is
        Enum in some datasets and Categorical in others, or Enum in all
        datasets but with different categories in each dataset, that column
        will be retained as an Enum column (with the union of the categories)
        in the concatenated `var`.
        
        Args:
            datasets: one or more Pseudobulk datasets to concatenate with this
                      one
            *more_datasets: additional Pseudobulk datasets to concatenate with
                            this one, specified as positional arguments
            flexible: whether to subset to samples and columns of `obs` and
                      `var` common to all datasets before concatenating, rather
                      than raising an error on any mismatches
        
        Returns:
            The concatenated Pseudobulk dataset.
        """
        # Check inputs
        datasets = (self,) + to_tuple(datasets) + more_datasets
        if len(datasets) == 1:
            error_message = \
                'need at least one other Pseudobulk dataset to concatenate'
            raise ValueError(error_message)
        check_types(datasets[1:], 'datasets', Pseudobulk,
                    'Pseudobulk datasets')
        check_type(flexible, 'flexible', bool, 'Boolean')
        
        # Check that cell types match across all datasets
        if not all(set(self.keys()) == set(dataset.keys())
                   for dataset in datasets[1:]) if flexible else \
                all(self.keys() == dataset.keys() for dataset in datasets[1:]):
            error_message = \
                'not all Pseudobulk datasets have the same cell types'
            raise ValueError(error_message)
        
        # Perform either flexible or non-flexible concatenation
        X = {}
        obs = {}
        var = {}
        for cell_type in self._var:
            if flexible:
                # Check that `var_names` and `obs_names` have the same name and
                # data type for each cell type across all datasets
                var_names_name = self._var[cell_type][:, 0].name
                if not all(dataset._var[cell_type][:, 0] == var_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of var (the var_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                obs_names_name = self._obs[cell_type][:, 0].name
                if not all(dataset._obs[cell_type][:, 0].name == obs_names_name
                           for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same name for '
                        f'the first column of obs (the obs_names column) for '
                        f'cell type {cell_type!r}')
                    raise ValueError(error_message)
                var_names_dtype = self._var[cell_type][:, 0].dtype
                if not all(dataset._var[cell_type][:, 0].dtype ==
                           var_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of var (the var_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                obs_names_dtype = self._obs[cell_type][:, 0].dtype
                if not all(dataset._obs[cell_type][:, 0].dtype ==
                           obs_names_dtype for dataset in datasets[1:]):
                    error_message = (
                        f'not all Pseudobulk datasets have the same data type '
                        f'for the first column of obs (the obs_names column) '
                        f'for cell type {cell_type!r}')
                    raise TypeError(error_message)
                
                # Subset to samples in common across all datasets
                samples_in_common = self._obs[cell_type][:, 0]\
                    .filter(self._obs[cell_type][:, 0]
                            .is_in(pl.concat([dataset._obs[cell_type][:, 0]
                                              for dataset in datasets[1:]])))
                if len(samples_in_common) == 0:
                    error_message = (
                        f'no samples are shared across all Pseudobulk '
                        f'datasets for cell type {cell_type!r}')
                    raise ValueError(error_message)
                cell_type_X = []
                cell_type_obs = []
                for dataset in datasets:
                    sample_indices = dataset._getitem_process(
                        samples_in_common, 1, dataset._obs[cell_type])
                    cell_type_X.append(
                        dataset._X[cell_type][:, sample_indices.to_numpy()])
                    cell_type_obs.append(
                        dataset._obs[cell_type][sample_indices])
                
                # Subset to columns of `obs` that are identical in all datasets
                # after this subsetting
                obs_columns_in_common = [
                    column.name for column in cell_type_obs[0][:, 1:]
                    if all(column.name in dataset_cell_type_obs and
                           dataset_cell_type_obs[column.name].equals(column)
                           for dataset_cell_type_obs in cell_type_obs[1:])]
                cell_type_obs = cell_type_obs[0]
                cell_type_obs = cell_type_obs.select(cell_type_obs.columns[0],
                                                     obs_columns_in_common)
                
                # Subset to columns of `var` that are present in all datasets,
                # and have the same data types. Also include columns of `var`
                # that are Enum in some datasets and Categorical in others, or
                # Enum in all datasets but with different categories in each
                # dataset; cast these to Categorical.
                var_mismatched_categoricals = {
                    column for column, dtype in self._var[cell_type][:, 1:]
                    .select(pl.col(pl.Categorical, pl.Enum)).schema.items()
                    if all(column in dataset._var[cell_type] and
                           dataset._var[cell_type][column].dtype in
                           (pl.Categorical, pl.Enum)
                           for dataset in datasets[1:]) and
                       not all(dataset._var[cell_type][column].dtype == dtype
                               for dataset in datasets[1:])}
                var_columns_in_common = [
                    column
                    for column, dtype in islice(
                        self._var[cell_type].schema.items(), 1, None)
                    if column in var_mismatched_categoricals or
                       all(column in dataset[cell_type]._var and
                           dataset._var[cell_type][column].dtype == dtype
                           for dataset in datasets[1:])]
                cast_dict = {column: pl.Enum(
                    pl.concat([dataset._var[cell_type][column]
                              .cat.get_categories() for dataset in datasets])
                    .unique(maintain_order=True))
                    for column in var_mismatched_categoricals}
                cell_type_var = [
                    dataset._var[cell_type]
                    .cast(cast_dict)
                    .select(var_columns_in_common) for dataset in datasets]
            else:  # non-flexible
                # Check that all `obs` are identical
                cell_type_obs = self._obs[cell_type]
                for dataset in datasets[1:]:
                    if not dataset._obs[cell_type].equals(cell_type_obs):
                        error_message = (
                            f'all Pseudobulk datasets must have the same obs '
                            f'for cell type {cell_type!r}, unless '
                            f'flexible=True')
                        raise ValueError(error_message)
                
                # Check that all `var` have the same columns and data types
                schema = self._var[cell_type].schema
                for dataset in datasets[1:]:
                    if dataset._var[cell_type].schema != schema:
                        error_message = (
                            f'all Pseudobulk datasets must have the same '
                            f'columns in var for cell type {cell_type!r}, '
                            f'with the same data types, unless flexible=True')
                        raise ValueError(error_message)
                cell_type_X = [dataset._X[cell_type] for dataset in datasets]
                cell_type_var = [dataset._var[cell_type]
                                 for dataset in datasets]
            
            # Concatenate
            var[cell_type] = pl.concat(cell_type_var)
            num_unique = var[cell_type][:, 0].n_unique()
            if num_unique < len(var[cell_type]):
                error_message = (
                    f'var_names contains {len(var[cell_type]) - num_unique:,} '
                    f'duplicates after concatenation for cell type '
                    f'{cell_type!r}')
                raise ValueError(error_message)
            X[cell_type] = np.hstack(cell_type_X)
            obs[cell_type] = cell_type_obs
        return Pseudobulk(X=X, obs=obs, var=var, num_threads=self._num_threads)
    
    def _get_column(self,
                    obs_or_var_name: Literal['obs', 'var'],
                    column: PseudobulkColumn | None |
                            dict[str, PseudobulkColumn | None],
                    variable_name: str,
                    dtypes: pl.datatypes.classes.DataTypeClass | str |
                            tuple[pl.datatypes.classes.DataTypeClass | str,
                                  ...],
                    custom_error: str | None = None,
                    allow_None: bool = True,
                    allow_null: bool = False,
                    cell_types: Sequence[str] | None = None) -> \
            dict[str, pl.Series | None]:
        """
        Get a column of the same length as `obs` or `var` for each cell type.
        
        Args:
            obs_or_var_name: the name of the DataFrame the column is with
                             respect to, i.e. `'obs'` or `'var'`
            column: a string naming a column of each cell type's `obs`/`var`, a
                    polars expression that evaluates to a single column when
                    applied to each cell type's `obs`/`var`, a polars Series or
                    NumPy array of the same length as each cell type's
                    `obs`/`var`, or a function that takes in two arguments,
                    `self` and a cell type, and returns a polars Series or
                    NumPy array of the same length as `obs`/`var`. Or, a
                    dictionary mapping cell-type names to any of the above;
                    each cell type in this Pseudobulk dataset must be present.
                    May also be `None` (or a dictionary containing `None`
                    values) if `allow_None=True`.
            variable_name: the name of the variable corresponding to `columns`
            dtypes: the required dtype(s) of the column
            custom_error: a custom error message for when (an element of)
                          `columns` is a string and is not found in
                          `obs`/`var`; use `{}` as a placeholder for the names
                          of the column and cell type (which must appear in
                          that order)
            allow_None: whether to allow `columns` or its elements to be `None`
            allow_null: whether to allow `columns` to contain `null` values
            cell_types: a list of cell types; if `None`, use all cell types. If
                        specified and `column` is a Sequence, `column` and
                        `cell_types` should have the same length.
        
        Returns:
            A dictionary mapping each cell type to a polars Series of the same
            length as the cell type's `obs`/`var`. Or, if `columns` is `None`
            (or if some elements are `None`), a dict where all (or some) values
            are `None`.
        """
        obs_or_var = self._obs if obs_or_var_name == 'obs' else self._var
        if cell_types is None:
            cell_types = self._X
        if column is None:
            if not allow_None:
                error_message = f'{variable_name} is None'
                raise TypeError(error_message)
            return {cell_type: None for cell_type in cell_types}
        columns = {}
        if isinstance(column, str):
            for cell_type in cell_types:
                if column not in obs_or_var[cell_type]:
                    error_message = (
                        f'{variable_name} {column!r} is not a column of '
                        f'{obs_or_var_name}[{cell_type!r}]'
                        if custom_error is None else
                        custom_error.format(f'{column!r}', f'{cell_type!r}'))
                    raise ValueError(error_message)
                columns[cell_type] = obs_or_var[cell_type][column]
        elif isinstance(column, pl.Expr):
            for cell_type in cell_types:
                columns[cell_type] = obs_or_var[cell_type].select(column)
                if columns[cell_type].width > 1:
                    error_message = (
                        f'{variable_name} is a polars expression that expands '
                        f'to {columns[cell_type].width:,} columns rather '
                        f'than 1 for cell type {cell_type!r}')
                    raise ValueError(error_message)
                columns[cell_type] = columns[cell_type].to_series()
        elif isinstance(column, pl.Series):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a polars Series of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = column
        elif isinstance(column, np.ndarray):
            for cell_type in cell_types:
                if len(column) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a NumPy array of length '
                        f'{len(column):,}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = pl.Series(variable_name, column)
        elif callable(column):
            function = column
            for cell_type in cell_types:
                columns = function(self, cell_type)
                if isinstance(columns, np.ndarray):
                    if columns.ndim != 1:
                        error_message = (
                            f'{variable_name} is a function that returns a '
                            f'{columns.ndim:,}D NumPy array, but must return '
                            f'a polars Series or 1D NumPy array')
                        raise ValueError(error_message)
                    columns = pl.Series(variable_name, columns)
                elif not isinstance(columns, pl.Series):
                    error_message = (
                        f'{variable_name} is a function that returns a '
                        f'variable of type {type(columns).__name__}, but must '
                        f'return a polars Series or 1D NumPy array')
                    raise TypeError(error_message)
                if len(columns) != len(obs_or_var[cell_type]):
                    error_message = (
                        f'{variable_name} is a function that returns a column '
                        f'of length {len(columns):,} for cell type '
                        f'{cell_type!r}, which differs from the length of '
                        f'{obs_or_var_name}[{cell_type!r}] '
                        f'({len(obs_or_var[cell_type]):,})')
                    raise ValueError(error_message)
                columns[cell_type] = columns
        elif isinstance(column, dict):
            if len(column) != len(cell_types):
                error_message = (
                    f'{variable_name} is a dictionary of length '
                    f'{len(column):,}, which differs from the number of cell '
                    f'types ({len(cell_types):,})')
                raise ValueError(error_message)
            column_set = set(column)
            cell_type_set = set(cell_types)
            if column_set != cell_type_set:
                overlap = len(column_set & cell_type_set)
                if overlap:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but only {overlap:,} '
                        f'of its {len(cell_types):,} keys '
                        f'{plural("correspond", overlap)} to cell types')
                    raise ValueError(error_message)
                else:
                    error_message = (
                        f'{variable_name} is a dictionary of the same length '
                        f'as the number of cell types, but None of its '
                        f'{len(cell_types):,} keys correspond to cell types')
                    raise ValueError(error_message)
            for cell_type, col in column.items():
                if col is None:
                    if allow_None:
                        columns[cell_type] = None
                    else:
                        error_message = \
                            f'{variable_name}[{cell_type!r}] is None'
                        raise TypeError(error_message)
                elif isinstance(col, str):
                    columns[cell_type] = col
                elif isinstance(col, pl.Expr):
                    col = obs_or_var[cell_type].select(col)
                    if col.width > 1:
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'expression that expands to {col.width:,} '
                            f'columns rather than 1')
                        raise ValueError(error_message)
                    columns[cell_type] = col.to_series()
                elif isinstance(col, pl.Series):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a polars '
                            f'Series of length {len(col):,}, which differs '
                            f'from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                elif isinstance(col, np.ndarray):
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a NumPy array '
                            f'of length {len(col):,}, which differs from the '
                            f'length of {obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = pl.Series(variable_name, col)
                elif callable(col):
                    # noinspection PyCallingNonCallable
                    col = col(self, cell_type)
                    if isinstance(col, np.ndarray):
                        if col.ndim != 1:
                            error_message = (
                                f'{variable_name}[{cell_type!r}] is a '
                                f'function that returns a {col.ndim:,}D NumPy '
                                f'array, but must return a polars Series or '
                                f'1D NumPy array')
                            raise ValueError(error_message)
                        col = pl.Series(variable_name, col)
                    elif not isinstance(col, pl.Series):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a variable of type '
                            f'{type(col).__name__}, but must return a '
                            f'polars Series or 1D NumPy array')
                        raise TypeError(error_message)
                    if len(col) != len(obs_or_var[cell_type]):
                        error_message = (
                            f'{variable_name}[{cell_type!r}] is a function '
                            f'that returns a column of length {len(col):,}, '
                            f'which differs from the length of '
                            f'{obs_or_var_name}[{cell_type!r}] '
                            f'({len(obs_or_var[cell_type]):,})')
                        raise ValueError(error_message)
                    columns[cell_type] = col
                else:
                    error_message = (
                        f'{variable_name}[{cell_type!r}] must be a string '
                        f'column name, a polars expression or Series, a 1D '
                        f'NumPy array, or a function that returns any of '
                        f'these when applied to this Pseudobulk dataset and a '
                        f'given cell type, but has type '
                        f'{type(col).__name__!r}')
                    raise TypeError(error_message)
        else:
            error_message = (
                f'{variable_name} must be a string column name, a polars '
                f'expression or Series, a 1D NumPy array, or a function that '
                f'returns any of these when applied to this Pseudobulk '
                f'dataset and a given cell type, but has type '
                f'{type(column).__name__!r}')
            raise TypeError(error_message)
        
        # Check dtypes
        if not isinstance(dtypes, tuple):
            dtypes = dtypes,
        for cell_type, col in columns.items():
            base_type = col.dtype.base_type()
            for expected_type in dtypes:
                if base_type == expected_type or expected_type == 'integer' \
                        and base_type in pl.INTEGER_DTYPES or \
                        expected_type == 'floating-point' and \
                        base_type in pl.FLOAT_DTYPES:
                    break
            else:
                if len(dtypes) == 1:
                    dtypes = str(dtypes[0])
                elif len(dtypes) == 2:
                    dtypes = ' or '.join(map(str, dtypes))
                else:
                    dtypes = \
                        ', '.join(map(str, dtypes[:-1])) + f', or {dtypes[-1]}'
                if isinstance(columns, str):
                    error_message = (
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]'
                        f'[{columns!r}] must be {dtypes}, but has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
                else:
                    error_message = (
                        f'{variable_name} must be {dtypes}, but has data type '
                        f'{base_type!r} for cell type {cell_type!r}')
                    raise TypeError(error_message)
        
        # Check `null` values, if `allow_null=False`
        if not allow_null:
            for cell_type, col in columns.items():
                null_count = col.null_count()
                if null_count > 0:
                    full_variable_name = \
                        f'{variable_name} {obs_or_var_name}[{cell_type!r}]' \
                        f'[{columns!r}]' if isinstance(columns, str) else \
                            variable_name
                    error_message = (
                        f'{full_variable_name} contains {null_count:,} '
                        f'{plural("null value", null_count)} for cell type '
                        f'{cell_type!r}, but must not contain any')
                    raise ValueError(error_message)
        return columns
    
    def _describe_column(self,
                         column_name: str,
                         column: SingleCellColumn,
                         cell_type: str):
        """
        Describe a column-name argument in an error message.
        
        Args:
            column_name: the name of the column-name argument
            column: the value of the column-name argument
            cell_type: the cell type where the error was triggered
    
        Returns:
            The column's description: just the argument's name unless the value
            (for this cell type) is a string (i.e. the column's name in `obs`
            or `var`), in which case also include the value.
        """
        if isinstance(column, Sequence):
            cell_type_index = next(i for i, cell_type_i in enumerate(self._obs)
                                   if cell_type_i == cell_type)
            column = column[cell_type_index]
        return f'{column_name} {column!r}' \
            if isinstance(column, str) else column_name
        
    def filter_obs(self,
                   *predicates: str | pl.Expr | pl.Series |
                                Iterable[str | pl.Expr | pl.Series] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both `obs` and
        `X` for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to samples
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to samples passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        obs = {}
        for cell_type in self._obs:
            if cell_type in cell_types:
                obs[cell_type] = self._obs[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.UInt32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    obs[cell_type]['__Pseudobulk_index'].to_numpy()]
                obs[cell_type] = obs[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                obs[cell_type] = self._obs[cell_type]
        return Pseudobulk(X=X, obs=obs, var=self._var,
                          num_threads=self._num_threads)
    
    def filter_var(self,
                   *predicates: pl.Expr | pl.Series | str |
                                Iterable[pl.Expr | pl.Series | str] | bool |
                                list[bool] | np.ndarray[1, np.dtype[np.bool_]],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **constraints: Any) -> Pseudobulk:
        """
        Equivalent to `df.filter()` from polars, but applied to both `var` and
        `X` for each cell type.
        
        Args:
            *predicates: one or more column names, expressions that evaluate to
                         Boolean Series, Boolean Series, lists of Booleans,
                         and/or 1D Boolean NumPy arrays
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **constraints: column filters: `name=value` filters to genes
                           where the column named `name` has the value `value`
        
        Returns:
            A new Pseudobulk dataset filtered to genes passing all the
            Boolean filters in `predicates` and `constraints`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        X = {}
        var = {}
        for cell_type in self._var:
            if cell_type in cell_types:
                var[cell_type] = self._var[cell_type]\
                    .with_columns(__Pseudobulk_index=pl.int_range(
                        pl.len(), dtype=pl.UInt32))\
                    .filter(*predicates, **constraints)
                X[cell_type] = self._X[cell_type][
                    :, var[cell_type]['__Pseudobulk_index'].to_numpy()]
                var[cell_type] = var[cell_type].drop('__Pseudobulk_index')
            else:
                X[cell_type] = self._X[cell_type]
                var[cell_type] = self._var[cell_type]
        return Pseudobulk(X=X, obs=self._obs, var=var,
                          num_threads=self._num_threads)
    
    def select_obs(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's `obs`. `obs_names` will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `obs[cell_type]=obs[cell_type].select(*exprs, **named_exprs)` for
            all cell types in `obs`, and `obs_names` as the first column unless
            already included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type in cell_types:
                new_cell_type_obs = cell_type_obs.select(*exprs, **named_exprs)
                if cell_type_obs.columns[0] not in new_cell_type_obs:
                    new_cell_type_obs = \
                        new_cell_type_obs.select(cell_type_obs[:, 0], pl.all())
                obs[cell_type] = new_cell_type_obs
            else:
                obs[cell_type] = cell_type_obs
        return Pseudobulk(X=self._X, obs=obs, var=self._var,
                          num_threads=self._num_threads)
    
    def select_var(self,
                   *exprs: Scalar | pl.Expr | pl.Series |
                           Iterable[Scalar | pl.Expr | pl.Series],
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None,
                   **named_exprs: Scalar | pl.Expr | pl.Series) -> Pseudobulk:
        """
        Equivalent to `df.select()` from polars, but applied to each cell
        type's `var`. `var_names` will be automatically included as the first
        column, if not included explicitly.
        
        Args:
            *exprs: column(s) to select, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to select, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `var[cell_type]=var[cell_type].select(*exprs, **named_exprs)` for
            all cell types in `var`, and `var_names` as the first column unless
            already included explicitly.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type in cell_types:
                new_cell_type_var = cell_type_var.select(*exprs, **named_exprs)
                if cell_type_var.columns[0] not in new_cell_type_var:
                    new_cell_type_var = \
                        new_cell_type_var.select(cell_type_var[:, 0], pl.all())
                var[cell_type] = new_cell_type_var
            else:
                var[cell_type] = cell_type_var
        return Pseudobulk(X=self._X, obs=self._obs, var=var,
                          num_threads=self._num_threads)
    
    def select_cell_types(self,
                          cell_types: str | Iterable[str],
                          *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset subset to the cell type(s) in
        `cell_types` and `more_cell_types`.
        
        Args:
            cell_types: cell type(s) to select
            *more_cell_types: additional cell types to select, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset subset to the specified cell type(s).
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types += more_cell_types
        for cell_type in cell_types:
            if cell_type not in self._X:
                error_message = (
                    f'tried to select {cell_type!r}, which is not a cell type '
                    f'in this Pseudobulk')
                raise ValueError(error_message)
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in cell_types},
                          num_threads=self._num_threads)
    
    def with_columns_obs(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's `obs`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `obs[cell_type]=obs[cell_type].with_columns(*exprs, **named_exprs)`
            for all cell types.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(
            X=self._X,
            obs={cell_type: obs.with_columns(*exprs, **named_exprs)
                 if cell_type in cell_types else obs
                 for cell_type, obs in self._obs.items()},
            var=self._var, num_threads=self._num_threads)
    
    def with_columns_var(self,
                         *exprs: Scalar | pl.Expr | pl.Series |
                                 Iterable[Scalar | pl.Expr | pl.Series],
                         cell_types: str | Iterable[str] | None = None,
                         excluded_cell_types: str | Iterable[str] |
                                              None = None,
                         **named_exprs: Scalar | pl.Expr | pl.Series) -> \
            Pseudobulk:
        """
        Equivalent to `df.with_columns()` from polars, but applied to each cell
        type's `var`.
        
        Args:
            *exprs: column(s) to add, specified as positional arguments.
                    Accepts expression input. Strings are parsed as column
                    names, other non-expression inputs are parsed as literals.
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **named_exprs: additional columns to add, specified as keyword
                           arguments. The columns will be renamed to the
                           keyword used.
        
        Returns:
            A new Pseudobulk dataset with
            `var[cell_type]=var[cell_type].with_columns(*exprs, **named_exprs)`
            for all cell types.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(
            X=self._X, obs=self._obs,
            var={cell_type: var.with_columns(*exprs, **named_exprs)
                 if cell_type in cell_types else var
                 for cell_type, var in self._var.items()},
            num_threads=self._num_threads)

    def drop_obs(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from `obs`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X,
                          obs={cell_type: obs.drop(columns)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var, num_threads=self._num_threads)

    def drop_var(self,
                 columns: pl.type_aliases.ColumnNameOrSelector |
                          Iterable[pl.type_aliases.ColumnNameOrSelector],
                 *more_columns: pl.type_aliases.ColumnNameOrSelector,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with `columns` and `more_columns`
        removed from `var`.
        
        Args:
            columns: columns(s) to drop
            *more_columns: additional columns to drop, specified as
                           positional arguments
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) removed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        columns = to_tuple(columns) + more_columns
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: var.drop(columns)
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()},
                          num_threads=self._num_threads)
    
    def drop_cell_types(self,
                        cell_types: str | Iterable[str],
                        *more_cell_types: str) -> Pseudobulk:
        """
        Create a new Pseudobulk dataset with `cell_types` and `more_cell_types`
        removed. Raises an error if all cell types would be dropped.
        
        Args:
            cell_types: cell type(s) to drop
            *more_cell_types: additional cell types to drop, specified as
                              positional arguments
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) removed.
        """
        cell_types = to_tuple_checked(cell_types, 'cell_types', str, 'strings')
        check_types(more_cell_types, 'more_cell_types', str, 'strings')
        cell_types = set(cell_types) | set(more_cell_types)
        # noinspection PyTypeChecker
        original_cell_types = set(self)
        if not cell_types < original_cell_types:
            if cell_types == original_cell_types:
                error_message = 'all cell types would be dropped'
                raise ValueError(error_message)
            for cell_type in cell_types:
                if cell_type not in original_cell_types:
                    error_message = (
                        f'tried to drop {cell_type!r}, which is not a cell '
                        f'type in this Pseudobulk')
                    raise ValueError(error_message)
        new_cell_types = \
            [cell_type for cell_type in self if cell_type not in cell_types]
        return Pseudobulk(X={cell_type: self._X[cell_type]
                             for cell_type in new_cell_types},
                          obs={cell_type: self._obs[cell_type]
                               for cell_type in new_cell_types},
                          var={cell_type: self._var[cell_type]
                               for cell_type in new_cell_types},
                          num_threads=self._num_threads)
    
    def rename_obs(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of `obs` renamed for
        each cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of `obs` renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(
            X=self._X,
            obs={cell_type: obs.rename(mapping)
                            if cell_type in cell_types else obs
                 for cell_type, obs in self._obs.items()},
            var=self._var, num_threads=self._num_threads)
    
    def rename_var(self,
                   mapping: dict[str, str] | Callable[[str], str],
                   *,
                   cell_types: str | Iterable[str] | None = None,
                   excluded_cell_types: str | Iterable[str] | None = None) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with column(s) of `var` renamed for
        each cell type.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with the old
                     names as keys and the new names as values, or a function
                     that takes an old name and returns a new name
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
        
        Returns:
            A new Pseudobulk dataset with the column(s) of `var` renamed.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(
            X=self._X, obs=self._obs,
            var={cell_type: var.rename(mapping)
                            if cell_type in cell_types else var
                 for cell_type, var in self._var.items()},
            num_threads=self._num_threads)
    
    def rename_cell_types(self,
                          mapping: dict[str, str] | Callable[[str], str]) -> \
            Pseudobulk:
        """
        Create a new Pseudobulk dataset with cell type(s) renamed.
        
        Args:
            mapping: the renaming to apply, either as a dictionary with old
                     cell type names as keys and new names as values, or a
                     function that takes an old name and returns a new name.
                     If `mapping` is a dictionary, cell types missing from its
                     keys will retain their original names.
        
        Returns:
            A new Pseudobulk dataset with the cell type(s) renamed.
        """
        if isinstance(mapping, dict):
            new_cell_types = [mapping.get(cell_type, cell_type)
                              for cell_type in self._X]
        elif callable(mapping):
            new_cell_types = [mapping(cell_type) for cell_type in self._X]
        else:
            raise TypeError(f'mapping must be a dictionary or function, but '
                            f'has type {type(mapping).__name__!r}')
        return Pseudobulk(X={new_cell_type: X
                             for new_cell_type, X in
                             zip(new_cell_types, self._X.values())},
                          obs={new_cell_type: obs
                               for new_cell_type, obs in
                               zip(new_cell_types, self._obs.values())},
                          var={new_cell_type: var
                               for new_cell_type, var in
                               zip(new_cell_types, self._var.values())},
                          num_threads=self._num_threads)
    
    def cast_X(self,
               dtype: np._typing.DTypeLike,
               *,
               cell_types: str | Iterable[str] | None = None,
               excluded_cell_types: str | Iterable[str] |
                                    None = None) -> Pseudobulk:
        """
        Cast each cell type's `X` to the specified data type.
        
        Args:
            dtype: a NumPy data type
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`

        Returns:
            A new Pseudobulk dataset with each cell type's `X` cast to the
            specified data type.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X={cell_type: X.astype(dtype)
                                        if X in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    def cast_obs(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's `obs` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's `obs`
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs={cell_type: obs.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var, num_threads=self._num_threads)
    
    def cast_var(self,
                 dtypes: Mapping[pl.type_aliases.ColumnNameOrSelector |
                                 pl.type_aliases.PolarsDataType,
                                 pl.type_aliases.PolarsDataType] |
                         pl.type_aliases.PolarsDataType,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 strict: bool = True) -> Pseudobulk:
        """
        Cast column(s) of each cell type's `var` to the specified data type(s).
        
        Args:
            dtypes: a mapping of column names (or selectors) to data types, or
                    a single data type to which all columns will be cast
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            strict: whether to raise an error if a cast could not be performed
                    (for instance, due to numerical overflow)

        Returns:
            A new Pseudobulk dataset with column(s) of each cell type's `var`
            cast to the specified data type(s).
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        return Pseudobulk(X=self._X,
                          obs=self._obs,
                          var={cell_type: var.cast(dtypes, strict=strict)
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()},
                          num_threads=self._num_threads)
    
    def join_obs(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Left join each cell type's `obs` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's `obs` with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in `obs`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `obs` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `obs`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's `obs`.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        # noinspection PyTypeChecker
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    f"either 'on' or both of 'left_on' and 'right_on' must be "
                    f"specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        obs = {}
        for cell_type, cell_type_obs in self._obs.items():
            if cell_type not in cell_types:
                obs[cell_type] = cell_type_obs
                continue
            left = cell_type_obs
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'obs[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            obs[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(obs[cell_type]) > len(self._obs[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=obs, var=self._var,
                          num_threads=self._num_threads)
    
    def join_var(self,
                 other: pl.DataFrame,
                 on: str | pl.Expr | Sequence[str | pl.Expr] | None = None,
                 *,
                 cell_types: str | Iterable[str] | None = None,
                 excluded_cell_types: str | Iterable[str] | None = None,
                 left_on: str | pl.Expr | Sequence[str | pl.Expr] |
                          None = None,
                 right_on: str | pl.Expr | Sequence[str | pl.Expr] |
                           None = None,
                 suffix: str = '_right',
                 validate: Literal['m:m', 'm:1', '1:m', '1:1'] = 'm:m',
                 join_nulls: bool = False,
                 coalesce: bool = True) -> Pseudobulk:
        """
        Join each cell type's `var` with another DataFrame.
        
        Args:
            other: a polars DataFrame to join each cell type's `var` with
            on: the name(s) of the join column(s) in both DataFrames
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            left_on: the name(s) of the join column(s) in `var`
            right_on: the name(s) of the join column(s) in `other`
            suffix: a suffix to append to columns with a duplicate name
            validate: checks whether the join is of the specified type. Can be:
                      - 'm:m' (many-to-many): the default, no checks performed.
                      - '1:1' (one-to-one): check that none of the values in
                        the join column(s) appear more than once in `var` or
                        more than once in `other`.
                      - '1:m' (one-to-many): check that none of the values in
                        the join column(s) appear more than once in `var`.
                      - 'm:1' (many-to-one): check that none of the values in
                        the join column(s) appear more than once in `other`.
            join_nulls: whether to include `null` as a valid value to join on.
                        By default, `null` values will never produce matches.
            coalesce: if `True`, coalesce each of the pairs of join columns
                      (the columns in `on` or `left_on`/`right_on`) from `obs`
                      and `other` into a single column, filling missing values
                      from one with the corresponding values from the other.
                      If `False`, include both as separate columns, adding
                      `suffix` to the join columns from `other`.
        
        Returns:
            A new Pseudobulk dataset with the columns from `other` joined to
            each cell type's `var`.
        
        Note:
            If a column of `on`, `left_on` or `right_on` is Enum in `obs` and
            Categorical in `other` (or vice versa), or Enum in both but with
            different categories in each, that pair of columns will be
            automatically cast to a common Enum data type (with the union of
            the categories) before joining.
        """
        check_type(other, 'other', pl.DataFrame, 'a polars DataFrame')
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        if on is None:
            if left_on is None and right_on is None:
                error_message = (
                    "either 'on' or both of 'left_on' and 'right_on' must be "
                    "specified")
                raise ValueError(error_message)
            elif left_on is None:
                error_message = \
                    'right_on is specified, so left_on must be specified'
                raise ValueError(error_message)
            elif right_on is None:
                error_message = \
                    'left_on is specified, so right_on must be specified'
                raise ValueError(error_message)
        else:
            if left_on is not None:
                error_message = "'on' is specified, so 'left_on' must be None"
                raise ValueError(error_message)
            if right_on is not None:
                error_message = "'on' is specified, so 'right_on' must be None"
                raise ValueError(error_message)
        var = {}
        for cell_type, cell_type_var in self._var.items():
            if cell_type not in cell_types:
                var[cell_type] = cell_type_var
                continue
            left = cell_type_var
            right = other
            if on is None:
                left_columns = left.select(left_on)
                right_columns = right.select(right_on)
            else:
                left_columns = left.select(on)
                right_columns = right.select(on)
            left_cast_dict = {}
            right_cast_dict = {}
            for left_column, right_column in zip(left_columns, right_columns):
                left_dtype = left_column.dtype
                right_dtype = right_column.dtype
                if left_dtype == right_dtype:
                    continue
                if (left_dtype == pl.Enum or left_dtype == pl.Categorical) \
                        and (right_dtype == pl.Enum or
                             right_dtype == pl.Categorical):
                    common_dtype = \
                        pl.Enum(pl.concat([left_column.cat.get_categories(),
                                           right_column.cat.get_categories()])
                                .unique(maintain_order=True))
                    left_cast_dict[left_column.name] = common_dtype
                    right_cast_dict[right_column.name] = common_dtype
                else:
                    error_message = (
                        f'var[{cell_type!r}][{left_column.name!r}] has data '
                        f'type {left_dtype.base_type()!r}, but '
                        f'other[{cell_type!r}][{right_column.name!r}] has '
                        f'data type {right_dtype.base_type()!r}')
                    raise TypeError(error_message)
            if left_cast_dict is not None:
                left = left.cast(left_cast_dict)
                right = right.cast(right_cast_dict)
            var[cell_type] = \
                left.join(right, on=on, how='left', left_on=left_on,
                          right_on=right_on, suffix=suffix, validate=validate,
                          join_nulls=join_nulls, coalesce=coalesce)
            if len(var[cell_type]) > len(self._var[cell_type]):
                other_on = to_tuple(right_on if right_on is not None else on)
                assert other.select(other_on).is_duplicated().any()
                duplicate_column = other_on[0] if len(other_on) == 1 else \
                    next(column for column in other_on
                         if other[column].is_duplicated().any())
                error_message = (
                    f'other[{duplicate_column!r}] contains duplicate values, '
                    f'so it must be deduplicated before being joined on')
                raise ValueError(error_message)
        return Pseudobulk(X=self._X, obs=self._obs, var=var,
                          num_threads=self._num_threads)
    
    def peek_obs(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of `obs` (the first row, by default) for a cell type (the
        first cell type, by default) with each column on its own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._obs))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._obs[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def peek_var(self, cell_type: str | None = None, row: int = 0) -> None:
        """
        Print a row of `var` (the first row, by default) for a cell type (the
        first cell type, by default) with each column on its own line.
        
        Args:
            cell_type: the cell type to print the row for, or `None` to use the
                       first cell type
            row: the index of the row to print
        """
        if cell_type is None:
            cell_type = next(iter(self._var))
        else:
            check_type(cell_type, 'cell_type', str, 'a string')
        check_type(row, 'row', int, 'an integer')
        with pl.Config(tbl_rows=-1):
            print(self._var[cell_type][row]
                  .with_columns(pl.col(pl.Enum, pl.Categorical)
                                .cast(pl.String))
                  .unpivot(variable_name='column'))
    
    def subsample_obs(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of samples.
        
        Args:
            n: the number of samples to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of samples to return; mutually exclusive
                      with `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `obs` to subsample by. Can be `None`, a column name,
                       a polars expression, a polars Series, a 1D NumPy array,
                       or a function that takes in this Pseudobulk dataset and
                       a cell type and returns a polars Series or 1D NumPy
                       array. Or, a dictionary mapping cell-type names to any
                       of the above; each cell type in this Pseudobulk dataset
                       must be present. Specifying `by_column` ensures that the
                       same fraction of cells with each value of `by_column`
                       are subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups can be undersampled by one
                       element. Can contain `null` entries: the corresponding
                       samples will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              obs indicating the subsampled samples; if `None`,
                              subset to these samples instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `obs`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.
        
        Returns:
            A new Pseudobulk dataset subset to the subsampled cells, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `obs`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._obs[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of obs[{cell_type!r}]; did you '
                            f'already run subsample_obs()? Set overwrite=True '
                            f'to overwrite')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'obs', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign `n` to be a vector of sample sizes per group, broadcast
            # to the length of `obs`. The total sample size should exactly
            # match the original `n`; if necessary, oversample the smallest
            # groups or undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n / len(by_column) * pl.len())
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            obs = {}
            for cell_type, cell_type_obs in self._obs.items():
                if cell_type in cell_types:
                    cell_type_obs = cell_type_obs\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.UInt32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_obs['__Pseudobulk_index'].to_numpy()]
                    obs[cell_type] = cell_type_obs.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    obs[cell_type] = cell_type_obs
            return Pseudobulk(X=X, obs=obs, var=self._var,
                              num_threads=self._num_threads)
        else:
            return Pseudobulk(
                X=self._X,
                obs={cell_type: obs.with_columns(expressions[cell_type]
                                                 .alias(subsample_column))
                     if cell_type in cell_types else obs
                     for cell_type, obs in self._obs.items()},
                var=self._var, num_threads=self._num_threads)
    
    def subsample_var(self,
                      n: int | np.integer | None = None,
                      *,
                      cell_types: str | Iterable[str] | None = None,
                      excluded_cell_types: str | Iterable[str] | None = None,
                      fraction: int | float | np.integer | np.floating |
                                None = None,
                      by_column: PseudobulkColumn | None |
                                 dict[str, PseudobulkColumn | None] = None,
                      subsample_column: str | None = None,
                      seed: int | np.integer = 0,
                      overwrite: bool = False) -> Pseudobulk:
        """
        Subsample a specific number or fraction of genes.
        
        Args:
            n: the number of genes to return; mutually exclusive with
               `fraction`
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            fraction: the fraction of genes to return; mutually exclusive with
                      `n`
            by_column: an optional String, Enum, Categorical, or integer column
                       of `var` to subsample by. Can be `None`, a column name,
                       a polars expression, a polars Series, a 1D NumPy array,
                       or a function that takes in this Pseudobulk dataset and
                       a cell type and returns a polars Series or 1D NumPy
                       array. Or, a dictionary mapping cell-type names to any
                       of the above; each cell type in this Pseudobulk dataset
                       must be present. Specifying `by_column` ensures that the
                       same fraction of cells with each value of `by_column`
                       are subsampled. When combined with `n`, to make sure the
                       total number of samples is exactly `n`, some of the
                       smallest groups may be oversampled by one element, or
                       some of the largest groups may be undersampled by one
                       element. Can contain `null` entries: the corresponding
                       genes will not be included in the result.
            subsample_column: an optional name of a Boolean column to add to
                              var indicating the subsampled genes; if `None`,
                              subset to these genes instead
            seed: the random seed to use when subsampling
            overwrite: if `True`, overwrite `subsample_column` if already
                       present in `var`, instead of raising an error. Must be
                       `False` when `subsample_column` is `None`.

        Returns:
            A new Pseudobulk dataset subset to the subsampled genes, or if
            `subsample_column` is specified, the full dataset with
            `subsample_column` added to `var`.
        """
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if subsample_column is not None:
            check_type(subsample_column, 'subsample_column', str, 'a string')
            if not overwrite:
                for cell_type in cell_types:
                    if subsample_column in self._var[cell_type]:
                        error_message = (
                            f'subsample_column {subsample_column!r} is '
                            f'already a column of var[{cell_type!r}]; did you '
                            f'already run subsample_var()? Set overwrite=True '
                            f'to overwrite')
                        raise ValueError(error_message)
        elif overwrite:
            error_message = \
                'overwrite must be False when subsample_column is None'
            raise ValueError(error_message)
        if n is not None:
            check_type(n, 'n', int, 'a positive integer')
            check_bounds(n, 'n', 1)
        elif fraction is not None:
            check_type(fraction, 'fraction', float,
                       'a floating-point number between 0 and 1')
            check_bounds(fraction, 'fraction', 0, 1, left_open=True,
                         right_open=True)
        else:
            error_message = 'one of n and fraction must be specified'
            raise ValueError(error_message)
        if n is not None and fraction is not None:
            error_message = 'only one of n and fraction must be specified'
            raise ValueError(error_message)
        by_column = self._get_column(
            'var', by_column, 'by_column',
            (pl.String, pl.Categorical, pl.Enum, 'integer'), allow_null=True)
        check_type(seed, 'seed', int, 'an integer')
        by = lambda expr, cell_type: \
            expr if by_column[cell_type] is None else \
            expr.over(by_column[cell_type])
        if by_column is not None and n is not None:
            # Reassign `n` to be a vector of sample sizes per group, broadcast
            # to the length of `var`. The total sample size should exactly
            # match the original `n`; if necessary, oversample the smallest
            # groups or undersample the largest groups to make this happen.
            cell_type_n = {}
            for cell_type in cell_types:
                cell_type_by_column = by_column[cell_type]
                if cell_type_by_column is None:
                    cell_type_n[cell_type] = n
                else:
                    by_frame = cell_type_by_column.to_frame()
                    by_name = cell_type_by_column.name
                    group_counts = by_frame\
                        .group_by(by_name)\
                        .agg(pl.len(), n=(n / len(by_column) * pl.len())
                                         .round().cast(pl.Int32))\
                        .drop_nulls(by_name)
                    diff = n - group_counts['n'].sum()
                    if diff != 0:
                        group_counts = group_counts\
                            .sort('len', descending=diff < 0)\
                            .with_columns(n=pl.col.n +
                                            pl.int_range(pl.len(),
                                                         dtype=pl.Int32)
                                            .lt(abs(diff)).cast(pl.Int32) *
                                            pl.lit(diff).sign())
                    cell_type_n[cell_type] = \
                        group_counts.join(by_frame, on=by_name)['n']
        # noinspection PyUnboundLocalVariable,PyUnresolvedReferences
        expressions = {
            cell_type: pl.int_range(pl.len(), dtype=pl.Int32)
                       .shuffle(seed=seed)
                       .pipe(by, cell_type=cell_type)
                       .lt((cell_type_n[cell_type] if by_column is not None
                            else n) if fraction is None else
                           fraction * pl.len().pipe(by, cell_type=cell_type))
                       for cell_type in cell_types}
        if subsample_column is None:
            X = {}
            var = {}
            for cell_type, cell_type_var in self._var.items():
                if cell_type in cell_types:
                    cell_type_var = cell_type_var\
                        .with_columns(__Pseudobulk_index=pl.int_range(
                            pl.len(), dtype=pl.UInt32))\
                        .filter(expressions[cell_type])
                    X[cell_type] = self._X[cell_type][
                        cell_type_var['__Pseudobulk_index'].to_numpy()]
                    var[cell_type] = cell_type_var.drop('__Pseudobulk_index')
                else:
                    X[cell_type] = self._X[cell_type]
                    var[cell_type] = cell_type_var
            return Pseudobulk(X=X, obs=self._obs, var=var,
                              num_threads=self._num_threads)
        else:
            return Pseudobulk(
                X=self._X,
                obs=self._obs,
                var={cell_type: var.with_columns(expressions[cell_type]
                                                 .alias(subsample_column))
                                if cell_type in cell_types else var
                     for cell_type, var in self._var.items()},
                num_threads=self._num_threads)
    
    def pipe(self,
             function: Callable[[Pseudobulk, ...], Any],
             *args: Any,
             **kwargs: Any) -> Any:
        """
        Apply a function to a Pseudobulk dataset.
        
        Args:
            function: the function to apply
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            The result of applying the function to this Pseudobulk dataset.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return function(self, *args, **kwargs)
    
    def pipe_X(self,
               function: Callable[[dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]], ...],
                                  dict[str, np.ndarray[2, np.dtype[
                                      np.integer | np.floating]]]],
               *args: Any,
               **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `X`. To apply a function to
        each cell type's `X`, rather than to `X` as a whole, use `map_X()`.
        
        Args:
            function: the function to apply to `X`. It must take the old `X` as
                      its first argument and return the new `X`. The function
                      may also take other arguments after `X`, which can be
                      specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            `X`.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return Pseudobulk(X=function(self._X, *args, **kwargs),
                          obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    def pipe_obs(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `obs`. To apply a function
        to each cell type's `obs`, rather than to `obs` as a whole, use
        `map_obs()`.
        
        Args:
            function: the function to apply to `obs`. It must take the old
                      `obs` as its first argument and return the new `obs`. The
                      function may also take other arguments after `obs`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            obs.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return Pseudobulk(X=self._X, obs=function(self._obs, *args, **kwargs),
                          var=self._var, num_threads=self._num_threads)
    
    def pipe_var(self,
                 function: Callable[[dict[str, pl.DataFrame], ...],
                                    dict[str, pl.DataFrame]],
                 *args: Any,
                 **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to a Pseudobulk dataset's `var`. To apply a function
        to each cell type's `var`, rather than to `var` as a whole, use
        `map_var()`.
        
        Args:
            function: the function to apply to `var`. It must take the old
                      `var` as its first argument and return the new `var`. The
                      function may also take other arguments after `var`, which
                      can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            var.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        return Pseudobulk(X=self._X, obs=self._obs,
                          var=function(self._var, *args, **kwargs),
                          num_threads=self._num_threads)

    def map_X(self,
              function: Callable[[np.ndarray[2, np.dtype[np.integer |
                                                         np.floating]], ...],
                                 np.ndarray[2, np.dtype[np.integer |
                                                        np.floating]]],
              *args: Any,
              cell_types: str | Iterable[str] | None = None,
              excluded_cell_types: str | Iterable[str] | None = None,
              **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `X`. To apply a function to `X` as
        a whole, rather than each cell type's `X`, use `pipe_X()`.
        
        Args:
            function: the function to apply to each cell type's `X`. It must
                      take the old `X` for a cell type and return the new `X`.
                      The function may also take other arguments after `X`,
                      which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `X`.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        # Get the cell type(s) to operate on
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        
        return Pseudobulk(X={cell_type: function(X, *args, **kwargs)
                                        if cell_type in cell_types else X
                             for cell_type, X in self._X.items()},
                          obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    def map_obs(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `obs`. To apply a function to
        `obs` as a whole, rather than each cell type's `obs`, use `pipe_obs()`.
        
        Args:
            function: the function to apply to each cell type's `obs`. It must
                      take the old `obs` for a cell type and return the new
                      `obs`. The function may also take other arguments after
                      `obs`, which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `obs`.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        # Get the cell type(s) to operate on
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        
        return Pseudobulk(X=self._X,
                          obs={cell_type: function(obs, *args, **kwargs)
                                          if cell_type in cell_types else obs
                               for cell_type, obs in self._obs.items()},
                          var=self._var, num_threads=self._num_threads)
    
    def map_var(self,
                function: Callable[[pl.DataFrame, ...], pl.DataFrame],
                *args: Any,
                cell_types: str | Iterable[str] | None = None,
                excluded_cell_types: str | Iterable[str] | None = None,
                **kwargs: Any) -> Pseudobulk:
        """
        Apply a function to each cell type's `var`. To apply a function to
        `var` as a whole, rather than each cell type's `var`, use `pipe_var()`.
        
        Args:
            function: the function to apply to each cell type's `var`. It must
                      take the old `var` for a cell type and return the new
                      `var`. The function may also take other arguments after
                      `var`, which can be specified via `args` and `kwargs`.
            *args: the positional arguments to the function
            cell_types: one or more cell types to operate on; if `None`,
                        operate on all cell types. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from the
                                 operation; mutually exclusive with
                                 `cell_types`
            **kwargs: the keyword arguments to the function

        Returns:
            A new Pseudobulk dataset where the function has been applied to
            each cell type's `var`.
        """
        # Check that `function` is callable
        if not callable(function):
            error_message = (
                f'function is not callable; it has type '
                f'{type(function).__name__}')
            raise TypeError(error_message)
        
        # Get the cell type(s) to operate on
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        
        return Pseudobulk(X=self._X, obs=self._obs,
                          var={cell_type: function(var, *args, **kwargs)
                                          if cell_type in cell_types else var
                               for cell_type, var in self._var.items()},
                          num_threads=self._num_threads)
    
    @staticmethod
    def _too_few_samples(obs: pl.DataFrame,
                         group_column: pl.Series | None,
                         min_samples: int | np.integer,
                         cell_type: str,
                         verbose: bool,
                         after_filtering: bool = False) -> bool:
        """
        Skip cell types with fewer than `min_samples` samples, or with fewer
        than `min_samples` samples in any group if `group_column` is not None.
        
        Args:
            obs: the cell type's `obs`, after applying one or more QC filters
            group_column: the column with sample group information (e.g. which
                          samples are disease cases and which are controls),
                          after applying one or more QC filters. The samples
                          in this column must be the same as those in `obs`.
            min_samples: filter to cell types with at least this many samples
                         in each group, or with at least this many total
                         samples if `group_column` is `None`
            cell_type: the name of the cell type
            verbose: whether to explain why the cell type is being skipped, if
                     it is
            after_filtering: whether this function is being run after sample
                             filtering

        Returns:
            Whether this cell type has too few samples and should be skipped.
        """
        num_samples = len(obs)
        if num_samples == 0:
            if verbose:
                print(f'[{cell_type}] Skipping this cell type because '
                      f'it has 0 samples'
                      f'{" after filtering" if after_filtering else ""}.')
            return True
        elif group_column is None:
            if num_samples < min_samples:
                if verbose:
                    print(f'[{cell_type}] Skipping this cell type because '
                          f'it has only {num_samples:,} '
                          f'{plural("sample", num_samples)}'
                          f'{" after filtering" if after_filtering else ""}, '
                          f'which is fewer than min_samples '
                          f'({min_samples:,})')
                return True
        else:
            too_small_groups = group_column\
                .value_counts()\
                .filter(pl.col.count < min_samples)\
                .drop_nulls()
            if len(too_small_groups) > 0:
                if verbose:
                    count = too_small_groups['count'][-1]
                    group_description = (
                        f'{count:,} {plural("sample", count)} where '
                        f'group_column = {too_small_groups.to_series()[-1]}')
                    if len(too_small_groups) > 1:
                        group_description = (
                            ', '.join(f'{count:,} {plural("sample", count)} '
                                      f'where group_column = {group}'
                                      for group, count in
                                      too_small_groups[:-1].iter_rows()) +
                            f' and {group_description}')
                    print(f'[{cell_type}] Skipping this cell type because it '
                          f'has only {group_description} after filtering, '
                          f'which '
                          f'{"is" if len(too_small_groups) == 1 else "are"} '
                          f'fewer than min_samples ({min_samples:,})')
                return True
        return False
    
    def qc(self,
           group_column: PseudobulkColumn | None |
                         dict[str, PseudobulkColumn | None],
           *,
           custom_filter: PseudobulkColumn | None |
                          dict[str, PseudobulkColumn | None] = None,
           min_samples: int | np.integer = 2,
           min_cells: int | np.integer | None = 10,
           max_standard_deviations: int | float | np.integer | np.floating |
                                    None = 3,
           min_nonzero_fraction: int | float | np.integer | np.floating |
                                 None = 0.8,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           error_if_negative_counts: bool = True,
           allow_float: bool = False,
           verbose: bool = True) -> Pseudobulk:
        """
        Subsets each cell type to samples passing quality control (QC). If
        samples fall into discrete groups (e.g. disease cases versus controls),
        these should be specified via the `group_column` argument.
        
        Filters, in order, to:
        - samples that pass the `custom_filter` (if specified), have
          non-missing values for `group_column` (if specified), and have at
          least `min_cells` cells of that type (default: 10)
        - samples where the number of genes with 0 counts is at most
          `max_standard_deviations` standard deviations above the mean
          (default: 3)
        - genes with at least 1 count in `100 * min_nonzero_fraction`%
          (default: 80%) of samples (in every group, if `group_column` is
          specified)
        
        If at any point during this filtering process, there are fewer than
        `min_samples` samples (in any group, if `group_column` is specified),
        the cell type is filtered out entirely.
        
        Args:
            group_column: an optional String, Categorical, Enum, Boolean, or
                          integer column of `obs` with sample group
                          information, e.g. which samples are disease cases and
                          which are controls. If specified, the
                          `min_nonzero_fraction` and `min_samples` filters must
                          pass for every group, rather than merely passing for
                          the dataset as a whole. Set to `None` if samples do
                          not fall into discrete groups. Can be `None`, a
                          column name, a polars expression, a polars Series, a
                          1D NumPy array, or a function that takes in this
                          Pseudobulk dataset and a cell type and returns a
                          polars Series or 1D NumPy array. Or, a dictionary
                          mapping cell-type names to any of the above; each
                          cell type in this Pseudobulk dataset must be present.
                          Can contain `null` entries: the corresponding samples
                          will be deemed to fail QC.
            custom_filter: an optional Boolean column of `obs` containing a
                           filter to apply on top of the other QC filters;
                           `True` elements will be kept. Can be `None`, a
                           column name, a polars expression, a polars Series, a
                           1D NumPy array, or a function that takes in this
                           Pseudobulk dataset and a cell type and returns a
                           polars Series or 1D NumPy array. Or, a dictionary
                           mapping cell-type names to any of the above; each
                           cell type in this Pseudobulk dataset must be
                           present.
            min_samples: filter to cell types with at least this many samples
                         in every group, or with at least this many total
                         samples if `group_column` is `None`
            min_cells: if not `None`, filter to samples with ≥ this many cells
                       of each cell type
            max_standard_deviations: if not `None`, filter to samples where the
                                     number of genes with 0 counts is at most
                                     this many standard deviations above the
                                     mean
            min_nonzero_fraction: if not `None`, filter to genes with at least
                                  one count in this fraction of samples in each
                                  group, or if `group_column` is `None`, at
                                  least one count in this fraction of samples
                                  overall. Note: `min_nonzero_fraction=0`
                                  filters out only genes with all-zero counts,
                                  while `min_nonzero_fraction=None` does not
                                  filter out any genes.
            cell_types: one or more cell types to QC; if `None`, QC all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude from QC;
                                 mutually exclusive with `cell_types`
            error_if_negative_counts: if `True`, raise an error if any counts
                                      are negative
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            verbose: whether to print how many samples and genes were filtered
                     out at each step of the QC process
        
        Returns:
            A new Pseudobulk dataset with each cell type's `X`, `obs` and `var`
            subset to samples and genes passing QC.
        
        Note:
            This function may give an incorrect output if the count matrix
            contains negative values: this is not checked for, due to speed
            considerations.
        """
        # Check inputs
        cell_types = \
            set(self._process_cell_types(cell_types, excluded_cell_types))
        group_column = self._get_column(
            'obs', group_column, 'group_column',
            (pl.String, pl.Categorical, pl.Enum, pl.Boolean, 'integer'),
            allow_null=True)
        custom_filter = self._get_column(
            'obs', custom_filter, 'custom_filter', pl.Boolean)
        check_type(min_samples, 'min_samples', int,
                   'an integer greater than or equal to 2')
        check_bounds(min_samples, 'min_samples', 2)
        if min_cells is not None:
            check_type(min_cells, 'min_cells', int, 'a positive integer')
            check_bounds(min_cells, 'min_cells', 1)
        if max_standard_deviations is not None:
            check_type(max_standard_deviations, 'max_standard_deviations',
                       (int, float), 'a positive number')
            check_bounds(max_standard_deviations, 'max_standard_deviations', 0,
                         left_open=True)
        if min_nonzero_fraction is not None:
            check_type(min_nonzero_fraction, 'min_nonzero_fraction',
                       (int, float), 'a number between 0 and 1, inclusive')
            check_bounds(min_nonzero_fraction, 'min_nonzero_fraction', 0, 1)
        check_type(error_if_negative_counts, 'error_if_negative_counts', bool,
                   'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `group_column` is None when neither the
        # `min_nonzero_fraction` nor the `min_samples` filters will be applied
        if group_column is not None and min_nonzero_fraction is None and \
                min_samples is None:
            error_message = (
                'group_column must be None when min_nonzero_fraction and '
                'min_samples are both None')
            raise ValueError(error_message)
        
        # If `error_if_negative_counts=True`, raise an error if `X` has any
        # negative values for any cell type
        if error_if_negative_counts:
            for cell_type in cell_types:
                if self._X[cell_type].ravel().min() < 0:
                    error_message = f'X[{cell_type!r}] has negative counts'
                    raise ValueError(error_message)
        
        # If `allow_float=False`, raise an error if `X` is floating-point for
        # any cell type
        if not allow_float:
            for cell_type in cell_types:
                dtype = self._X[cell_type].dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"qc() requires raw counts but X[{cell_type!r}] "
                        f"has data type {str(dtype)!r}, a floating-point data "
                        f"type; if you are sure that all values are raw "
                        f"integer counts, i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        
        # QC each cell type
        X_qced, obs_qced, var_qced = {}, {}, {}
        for cell_type in self._X:
            X = self._X[cell_type]
            obs = self._obs[cell_type]
            var = self._var[cell_type]
            if cell_type not in cell_types:
                X_qced[cell_type] = X
                obs_qced[cell_type] = obs
                var_qced[cell_type] = var
                if verbose:
                    if cell_types is not None:
                        print(f'\n[{cell_type}] Skipping this cell type due '
                              f'to being absent from cell_types')
                    else:
                        print(f'\n[{cell_type}] Skipping this cell type due '
                              f'to being present in excluded_cell_types')
                continue
            if verbose:
                print(f'\n[{cell_type}] Starting with {len(obs):,} '
                      f'{plural("sample", len(obs))} and {len(var):,} '
                      f'{plural("gene", len(var))}.')
            
            # Get the group column for this cell type.
            groups = \
                group_column[cell_type] if group_column is not None else None
            
            # Check if we have enough samples for this cell type
            if Pseudobulk._too_few_samples(obs, groups, min_samples, cell_type,
                                           verbose):
                continue
            
            # Get a mask of samples passing the custom filter, if specified
            if custom_filter is not None:
                if verbose:
                    print(f'[{cell_type}] Applying the custom filter...')
                sample_mask = custom_filter[cell_type]
                if sample_mask is not None:
                    if verbose:
                        num_samples = sample_mask.sum()
                        remain_string = 'sample remains' \
                            if num_samples == 1 else 'samples remain'
                        print(
                            f'[{cell_type}] {num_samples:,} {remain_string} '
                            f'after applying the custom filter.')
            else:
                sample_mask = None
            
            # If `groups` is not `None` and some samples have missing groups,
            # get a mask of samples with non-missing groups
            if groups is not None:
                null_count = groups.null_count()
                if null_count:
                    if verbose:
                        print(f'[{cell_type}] Filtering to samples with '
                              f'non-missing values for group_column...')
                    if sample_mask is None:
                        sample_mask = groups.is_not_null()
                    else:
                        sample_mask &= groups.is_not_null()
                    if verbose:
                        num_samples = sample_mask.sum()
                        remain_string = 'sample remains' \
                            if num_samples == 1 else 'samples remain'
                        print(f'[{cell_type}] {num_samples:,} {remain_string} '
                              f'after filtering to samples with non-missing '
                              f'values for group_column.')
            
            # Get a mask of samples with at least `min_cells` cells of this
            # cell type, if `min_cells` was specified. Combine this with the
            # sample mask from `custom_filter` above, if both were specified.
            if min_cells is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples with at least '
                          f'{min_cells:,} {cell_type} '
                          f'{plural("cell", min_cells)}...')
                if sample_mask is None:
                    sample_mask = obs['num_cells'] >= min_cells
                else:
                    sample_mask &= obs['num_cells'] >= min_cells
                if verbose:
                    num_samples = sample_mask.sum()
                    remain_string = 'sample remains' \
                        if num_samples == 1 else 'samples remain'
                    print(f'[{cell_type}] {num_samples:,} {remain_string} '
                          f'after filtering to samples with at least '
                          f'{min_cells:,} {cell_type} '
                          f'{plural("cell", min_cells)}.')
            
            # Now apply the sample mask, which contains the samples passing the
            # custom filter and/or `min_cells` filter
            if sample_mask is not None:
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                # Check if we still have enough samples for this cell type,
                # after applying these three filters
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
                # noinspection PyUnresolvedReferences
                X = X[sample_mask.to_numpy()]
            
            # Filter to samples where the number of genes with 0 counts is less
            # than `max_standard_deviations` standard deviations above the mean
            if max_standard_deviations is not None:
                if verbose:
                    print(f'[{cell_type}] Filtering to samples where the '
                          f'number of genes with 0 counts is '
                          f'<{max_standard_deviations} standard deviations '
                          f'above the mean...')
                # noinspection PyUnresolvedReferences
                num_zero_counts = (X == 0).sum(axis=1, dtype=np.uint64)
                sample_mask_NumPy = \
                    num_zero_counts < num_zero_counts.mean() + \
                    max_standard_deviations * num_zero_counts.std()
                sample_mask = pl.Series(sample_mask_NumPy)
                obs = obs.filter(sample_mask)
                if groups is not None:
                    groups = groups.filter(sample_mask)
                if verbose:
                    remain_string = 'sample remains' \
                        if len(obs) == 1 else 'samples remain'
                    print(f'[{cell_type}] {len(obs):,} {remain_string} after '
                          f'filtering to samples where the number of genes '
                          f'with 0 counts is <{max_standard_deviations} '
                          f'standard deviations above the mean.')
                
                # Check if we have still enough samples for this cell type,
                # after applying this filter
                if Pseudobulk._too_few_samples(obs, groups, min_samples,
                                               cell_type, verbose,
                                               after_filtering=True):
                    continue
                X = X[sample_mask_NumPy]
            
            # Filter to genes with at least 1 count in
            # `100 * min_nonzero_fraction`% of samples (or samples in each
            # group, if `group_column` is not None for this cell type)
            if min_nonzero_fraction is not None:
                if groups is not None:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples in '
                              f'each group...')
                    gene_mask = np.logical_and.reduce([
                        np.quantile(X[mask.to_numpy()],
                                    1 - min_nonzero_fraction, axis=0) > 0
                        for mask in groups.to_dummies().cast(pl.Boolean)])
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        remain_string = 'gene remains' \
                            if len(var) == 1 else 'genes remain'
                        print(f'[{cell_type}] {len(var):,} {remain_string} '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples in each group.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Filtering to genes with at '
                              f'least one count in '
                              f'{100 * min_nonzero_fraction}% of samples...')
                    gene_mask = np.quantile(X, 1 - min_nonzero_fraction,
                                            axis=0) > 0
                    X = X[:, gene_mask]
                    var = var.filter(gene_mask)
                    if verbose:
                        remain_string = 'gene remains' \
                            if len(var) == 1 else 'genes remain'
                        print(f'[{cell_type}] {len(var):,} {remain_string} '
                              f'after filtering to genes with at least one '
                              f'count in {100 * min_nonzero_fraction}% of '
                              f'samples.')
            X_qced[cell_type] = np.ascontiguousarray(X)
            obs_qced[cell_type] = obs
            var_qced[cell_type] = var
        if not X_qced:
            error_message = (
                'all cell types were skipped due to having too few samples '
                'passing one or more QC filters')
            if not verbose:
                error_message += \
                    '; re-run with verbose=True to see which filter(s)'
            raise ValueError(error_message)
        return Pseudobulk(X=X_qced, obs=obs_qced, var=var_qced,
                          num_threads=self._num_threads)
    
    @staticmethod
    def _library_size(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                      cell_type: str,
                      *,
                      logratio_trim: int | float | np.integer |
                                     np.floating = 0.3,
                      sum_trim: int | float | np.integer | np.floating = 0.05,
                      A_cutoff: int | float | np.integer |
                                np.floating = -1e10) -> \
            np.ndarray[1, np.dtype[np.float32]]:
        """
        Calculate normalization factor-adjusted library sizes according to the
        method of edgeR's `calcNormFactors()` with the default `method='TMM'`.
        Used by `library_size()`.

        Results differ from edgeR due to the presence of a floating-point bug
        in the original `calcNormFactors()` implementation. When calculating
        `logR`, the log2 ratio of `count / library_size` for a gene between a
        particular sample and a "reference" sample, the numerator and
        denominator of the ratio both involve a division by their sample's
        library size. In principle, these divisions by library size are
        equivalent to multiplying by the same constant across genes, namely the
        ratio of the two samples' library sizes. But in practice, even if two
        genes have the same count ratio between the two samples, they may still
        have slightly different `count / library_size` ratios due to
        floating-point roundoff, leading to these genes erroneously being
        assigned different `logR` ranks instead of being treated as tied. Our
        implementation fixes this bug by changing the order of operations so
        that the library size ratio is calculated first, then multiplied by the
        count ratio.

        Because this bug affects which genes are included in the trimmed mean,
        its impact can be relatively large, sometimes leading to a >1% error in
        edgeR's estimated library size relative to our correct implementation.

        Does not support the `lib.size` and `refColumn` arguments to
        `calcNormFactors()`; these are both assumed to be `NULL` (the default)
        and will always be calculated internally. The `doWeighting` argument is
        also not supported and is assumed to be `TRUE` (the default), so
        asymptotic binomial precision weights will always be used.

        Args:
            X: a matrix of raw (read) counts. `X` is assumed to have the
               opposite orientation from the original `calcNormFactors()`:
               samples are rows and genes are columns.
            cell_type: the cell type `X` is from, used in error messages
            logratio_trim: the amount of trim to use on log-ratios ("M"
                           values); must be greater than 0 and less than 1
            sum_trim: the amount of trim to use on the combined absolute levels
                      ("A" values); must be greater than 0 and less than 1
            A_cutoff: the cutoff on "A" values to use before trimming

        Returns:
            The norm factor-corrected library sizes: raw library sizes (column
            sums) times norm factors.
        """
        # Degenerate cases
        num_samples, num_genes = X.shape
        if num_samples == 1 or num_genes == 0:
            return np.ones(num_samples, dtype=np.float32)
        
        # Raise an error if `X` is not C-contiguous
        if not X.flags['C_CONTIGUOUS']:
            error_message = (
                f'X[{cell_type!r}] is not C-contiguous; did you forget to run '
                f'Pseudobulk.qc()?')
            raise ValueError(error_message)
        
        # Raise an error if there are any all-zero columns (genes)
        has_all_zero_columns = cython_inline('''
            ctypedef fused numeric:
                int
                unsigned
                long
                unsigned long
                float
                double
            
            def has_all_zero_columns(const numeric[:, ::1] X):
                cdef unsigned i, j
                for j in range(X.shape[1]):
                    for i in range(X.shape[0]):
                        if X[i, j] != 0:
                            break
                    else:
                        return True
                return False
            ''', warn_undeclared=False)['has_all_zero_columns'](X=X)
        if has_all_zero_columns:
            error_message = (
                f'[{cell_type}] some genes have all-zero counts; did you '
                f'forget to run Pseudobulk.qc()?')
            raise ValueError(error_message)

        # Calculate raw library sizes
        library_size = X.sum(axis=1)

        # Raise an error if any raw library sizes are 0
        if library_size.min() == 0:
            error_message = (
                'some samples have all-zero counts; did you forget to run '
                'Pseudobulk.qc()?')
            raise ValueError(error_message)

        # Determine which sample is the reference sample
        f75 = np.quantile(X, 0.75, axis=1) / library_size
        if np.median(f75) < 1e-20:
            ref_sample = np.argmax(np.sqrt(X).sum(axis=1))
        else:
            ref_sample = np.argmin(np.abs(f75 - f75.mean()))
        
        # Calculate norm factors
        norm_factors = np.empty(num_samples, dtype=np.float32)
        cython_inline(_uninitialized_vector_import + r'''
        from libcpp.algorithm cimport sort
        from libcpp.cmath cimport abs, exp, log, log2
        
        ctypedef fused numeric:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        ctypedef fused numeric2:
            int
            unsigned
            long
            unsigned long
            float
            double
        
        cdef extern from * nogil:
            """
            struct CompareLess {
                const float* data;
                CompareLess() noexcept {}
                CompareLess(const float* d) noexcept : data(d) {}
                bool operator()(unsigned a, unsigned b) const noexcept {
                    return data[a] < data[b];
                }
            };
            """
            cdef cppclass CompareLess:
                CompareLess(const float*) noexcept
                bint operator()(unsigned, unsigned) noexcept
        
        cdef inline void argsort(const float* arr, unsigned* indices,
                                 const unsigned n) noexcept nogil:
            cdef unsigned i
            for i in range(n):
                indices[i] = i
            sort(indices, indices + n, CompareLess(arr))

        cdef inline void rankdata(const float* data,
                                  unsigned* indices,
                                  float* ranks,
                                  const unsigned n):
            cdef unsigned i = 0, start_pos = 0
            cdef float current_val, rank
            cdef bint end = False

            argsort(&data[0], &indices[0], n)

            while True:
                current_val = data[indices[i]]

                # Count elements equal to current value
                i += 1
                if i == n:
                    end = True
                else:
                    while data[indices[i]] == current_val:
                        i += 1
                        if i == n:
                            end = True
                            break

                # Assign average rank to all tied elements
                rank = 0.5 * (start_pos + i) + 0.5
                while start_pos < i:
                    ranks[indices[start_pos]] = rank
                    start_pos += 1

                if end:
                    break

        def calc_norm_factors(
                const numeric[:, ::1] X,
                const float logratio_trim,
                const float sum_trim,
                const float A_cutoff,
                const unsigned ref_sample,
                float[::1] norm_factors,
                numeric2[::1] library_size):
            
            cdef numeric ref_count, count
            cdef numeric2 ref_library_size
            cdef unsigned i, j, n, loL, hiL, loS, hiS, \
                num_samples = X.shape[0], num_genes = X.shape[1]
            cdef float inverse_ref_library_size, inverse_relative_library_size_i, \
                logR_, absE_, inverse_library_size, total_inverse_library_size, \
                norm_factor, total_weight, variance, weight, scale
            cdef bint large_enough_logR
            cdef uninitialized_vector[float] inverse_relative_library_size_buffer, \
                log_normalized_X_ref_buffer, logR_buffer, absE_buffer, \
                logR_rank_buffer, absE_rank_buffer
            cdef uninitialized_vector[numeric] counts_buffer, ref_counts_buffer
            cdef uninitialized_vector[unsigned] indices_buffer
            inverse_relative_library_size_buffer.resize(num_samples)
            log_normalized_X_ref_buffer.resize(num_genes)
            logR_buffer.resize(num_genes)
            absE_buffer.resize(num_genes)
            logR_rank_buffer.resize(num_genes)
            absE_rank_buffer.resize(num_genes)
            counts_buffer.resize(num_genes)
            ref_counts_buffer.resize(num_genes)
            indices_buffer.resize(num_genes)
            cdef float[::1] \
                inverse_relative_library_size = \
                    <float[:num_samples]> \
                        inverse_relative_library_size_buffer.data(), \
                log_normalized_X_ref = <float[:num_genes]> \
                    log_normalized_X_ref_buffer.data(), \
                logR = <float[:num_genes]> logR_buffer.data(), \
                absE = <float[:num_genes]> absE_buffer.data(), \
                logR_rank = <float[:num_genes]> logR_rank_buffer.data(), \
                absE_rank = <float[:num_genes]> absE_rank_buffer.data()
            cdef numeric[::1] \
                counts = <numeric[:num_genes]> counts_buffer.data(), \
                ref_counts = <numeric[:num_genes]> ref_counts_buffer.data()
            cdef unsigned[::1] indices = <unsigned[:num_genes]> indices_buffer.data()
            
            # Calculate each sample's library size relative to the
            # reference sample's (to use in the `logR` calculation)
            ref_library_size = library_size[ref_sample]
            for i in range(num_samples):
                inverse_relative_library_size[i] = \
                    <float> ref_library_size / library_size[i]

            # Calculate each gene's log normalized expression (to use in
            # the `absE` calculation)
            inverse_ref_library_size = 1. / ref_library_size
            for j in range(num_genes):
                count = X[ref_sample, j]
                log_normalized_X_ref[j] = \
                    log2(count * inverse_ref_library_size)

            # Calculate the normalization factor for each sample
            for i in range(num_samples):
                inverse_library_size = 1. / library_size[i]
                inverse_relative_library_size_i = inverse_relative_library_size[i]
                large_enough_logR = False
                n = 0
                for j in range(num_genes):
                    # Get the count and reference count for this gene; skip
                    # the gene if either are 0
                    ref_count = X[ref_sample, j]
                    if ref_count == 0:
                        continue

                    count = X[i, j]
                    if count == 0:
                        continue

                    # Calculate the log ratio of expression accounting for
                    # library size
                    logR_ = log2(inverse_relative_library_size_i * (
                        <float> count / ref_count))

                    # Calculate "absolute expression": the average log2
                    # expression of this gene between this sample and the
                    # reference sample
                    absE_ = 0.5 * (log2(count * inverse_library_size) +
                                   log_normalized_X_ref[j])

                    # Cutoff based on `A_cutoff`
                    if absE_ <= A_cutoff:
                        continue

                    # Store `logR`, `absE`, and the count for genes passing
                    # the infinite value and `A_cutoff` filters above
                    logR[n] = logR_
                    absE[n] = absE_
                    counts[n] = count
                    ref_counts[n] = ref_count
                    n += 1

                    # Keep track of whether any gene's `logR` is above 1e-6
                    # in magnitude for this sample
                    large_enough_logR |= abs(logR_) >= 1e-6

                # If every gene's `logR` is below 1e-6 in magnitude for
                # this sample (i.e. expression is extremely low across the
                # board), set the sample's norm factor to 1
                if not large_enough_logR:
                    norm_factors[i] = 1
                    continue

                # Rank genes by `logR` and `absE`
                loL = <unsigned> (n * logratio_trim)
                hiL = n - loL
                loS = <unsigned> (n * sum_trim)
                hiS = n - loS
                rankdata(&logR[0], &indices[0], &logR_rank[0], n)
                rankdata(&absE[0], &indices[0], &absE_rank[0], n)

                # Calculate the norm factors themselves. Find genes with
                # intermediate ranks of both `logR` and `absE` (this is the
                # "trimmed" part, the "T" in "TMM"). The norm factors are 2
                # to the power of the weighted average of the logRs for
                # these intermediate-ranked genes, where the weights are
                # the inverse asymptotic variances.
                total_inverse_library_size = \
                    inverse_library_size + inverse_ref_library_size
                norm_factor = 0
                total_weight = 0
                for j in range(n):
                    if loL + 1 <= logR_rank[j] <= hiL and \
                            loS + 1 <= absE_rank[j] <= hiS:
                        variance = 1. / counts[j] + 1. / ref_counts[j] + \
                            total_inverse_library_size
                        weight = 1 / variance
                        norm_factor += weight * logR[j]
                        total_weight += weight
                norm_factor = 2 ** (norm_factor / total_weight)

                # Results will be missing if the two libraries share no
                # features with positive counts; in this case, set to 1
                if norm_factor != norm_factor:  # i.e. NaN
                    norm_factor = 1

                norm_factors[i] = norm_factor

            # Normalize factors across samples so that they multiply to 1
            scale = 0
            for i in range(num_samples):
                scale += log(norm_factors[i])
            scale = exp(-scale / num_samples)
            for i in range(num_samples):
                norm_factors[i] *= scale

            # Multiply norm factors by library sizes
            for i in range(num_samples):
                norm_factors[i] *= library_size[i]

            ''', warn_undeclared=False)['calc_norm_factors'](
                X=X, logratio_trim=logratio_trim, sum_trim=sum_trim,
                A_cutoff=A_cutoff, ref_sample=ref_sample,
                norm_factors=norm_factors, library_size=library_size)

        return norm_factors  # this is actually `library_size * norm_factors`
    
    def library_size(self,
                     *,
                     library_size_column: str = 'library_size',
                     cell_types: str | Iterable[str] | None = None,
                     excluded_cell_types: str | Iterable[str] | None = None,
                     logratio_trim: int | float | np.integer |
                                    np.floating = 0.3,
                     sum_trim: int | float | np.integer | np.floating = 0.05,
                     A_cutoff: int | float | np.integer |
                               np.floating = -1e10,
                     allow_float: bool = False,
                     overwrite: bool = False,
                     num_threads: int | np.integer | None = None) -> \
            Pseudobulk:
        """
        Calculate normalization factor-adjusted library sizes for each sample
        in each cell type, via the approach of edgeR's `calcNormFactors()`.
        
        Uses the same method as `calcNormFactors()` with the default
        `method='TMM'`. However, results differ from edgeR due to the presence
        of a floating-point bug in edgeR's `calcNormFactors()` implementation.
        When calculating `logR`,the log2 ratio of `count / library_size` for a
        gene between a particular sample and a "reference" sample, the
        numerator and denominator of the ratio both involve a division by their
        sample's library size. In principle, these divisions by library size
        are equivalent to multiplying by the same constant across genes, namely
        the ratio of the two samples' library sizes. But in practice, even if
        two genes have the same count ratio between the two samples, they may
        still have slightly different `count / library_size` ratios due to
        floating-point roundoff, leading to these genes erroneously being
        assigned different `logR` ranks instead of being treated as tied. Our
        implementation fixes this bug by changing the order of operations so
        that the library size ratio is calculated first, then multiplied by the
        count ratio. Because this bug affects which genes are included in the
        trimmed mean, its impact can be relatively large, sometimes leading to
        a >1% error in edgeR's estimated library size relative to our correct
        implementation.

        Does not support the `lib.size` and `refColumn` arguments to
        `calcNormFactors()`; these are both assumed to be `NULL` (the default)
        and will always be calculated internally. The `doWeighting` argument is
        also not supported and is assumed to be `TRUE` (the default), so
        asymptotic binomial precision weights will always be used.

        Args:
            library_size_column: the name of a floating-point column to add to
                                 `obs` containing each sample's library size
            cell_types: one or more cell types to calculate library sizes for;
                        if `None`, calculate library sizes for all cell types.
                        Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when
                                 calculating library sizes; mutually exclusive
                                 with `cell_types`
            logratio_trim: the amount of trim to use on log-ratios ("M"
                           values); must be greater than 0 and less than 1
            sum_trim: the amount of trim to use on the combined absolute levels
                      ("A" values); must be greater than 0 and less than 1
            A_cutoff: the cutoff on "A" values to use before trimming
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            overwrite: if `True`, overwrite `library_size_column` if already
                       present in `obs`, instead of raising an error.
            num_threads: the number of threads to use when calculating library
                         sizes. Multithreading is only supported for
                         "free-threaded" builds of Python 3.13 and later with
                         the global interpreter lock (GIL) disabled. Set
                         `num_threads=-1` to use all available cores, as
                         determined by `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Parallelization takes place
                         across cell types, so specifying more cores than the
                         number of cell types may not improve performance. Does
                         not affect the returned Pseudobulk dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.
        
        Returns:
            A new Pseudobulk dataset where `obs[library_size_column]` contains
            the norm factor-corrected library sizes for each cell type: raw
            library sizes (column sums) times norm factors.
        """
        # Get the list of cell types to calculate library sizes for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        
        # Check that `library_size_column` is a string
        check_type(library_size_column, 'library_size_column', str, 'a string')
        
        # Check that `overwrite` is Boolean
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        
        # Check that `library_size_column` is not already a column of `obs` for
        # any cell type, unless `overwrite=True`
        if not overwrite:
            for cell_type, obs in self._obs.items():
                if library_size_column in obs:
                    error_message = (
                        f'library_size_column {library_size_column!r} is '
                        f'already a column of obs for cell type '
                        f'{cell_type!r}; did you already run library_size()? '
                        f'Set overwrite=True to overwrite.')
                    raise ValueError(error_message)
        
        # Check that `logratio_trim`, `sum_trim`, and `A_cutoff` are
        # floating-point numbers with the correct ranges
        check_type(logratio_trim, 'logratio_trim', float,
                   'a floating-point number')
        check_bounds(logratio_trim, 'logratio_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(sum_trim, 'sum_trim', float, 'a floating-point number')
        check_bounds(sum_trim, 'sum_trim', 0, 1, left_open=True,
                     right_open=True)
        check_type(A_cutoff, 'A_cutoff', float, 'a floating-point number')
        
        # Check that `allow_float` is Boolean
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        
        # If `allow_float=False`, raise an error if `X` is floating-point for
        # any cell type
        if not allow_float:
            for cell_type in cell_types:
                dtype = self._X[cell_type].dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"library_size() requires raw counts but "
                        f"X[{cell_type!r}] has data type {str(dtype)!r}, a "
                        f"floating-point data type; if you are sure that all "
                        f"values are raw integer counts, i.e. that "
                        f"(X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads` if free-threaded and 1 otherwise,
        # and if -1, set to `os.cpu_count()`. Raise an error if the user
        # specified multiple threads but lacks free-threaded Python.
        num_threads = self._process_num_threads(num_threads)
        
        # Compute library sizes for each cell type
        if num_threads == 1:
            library_sizes = [Pseudobulk._library_size(
                X=self._X[cell_type], cell_type=cell_type,
                logratio_trim=logratio_trim, sum_trim=sum_trim,
                A_cutoff=A_cutoff) for cell_type in cell_types]
        else:
            from concurrent.futures import ThreadPoolExecutor
            kwargs_list = [{'X': self._X[cell_type], 'cell_type': cell_type,
                            'logratio_trim': logratio_trim,
                            'sum_trim': sum_trim, 'A_cutoff': A_cutoff}
                           for cell_type in cell_types]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                library_sizes = executor.map(
                    lambda kwargs: self._library_size(**kwargs), kwargs_list)
        
        # Add library sizes to each cell type's `obs`
        obs = self._obs.copy()
        for cell_type, library_size in zip(cell_types, library_sizes):
            obs[cell_type] = obs[cell_type]\
                .with_columns(pl.lit(library_size).alias(library_size_column))
        
        # Return a new Pseudobulk dataset with the residuals
        return Pseudobulk(X=self._X, obs=obs, var=self._var,
                          num_threads=self._num_threads)
    
    def CPM(self,
            *,
            library_size_column: PseudobulkColumn = 'library_size',
            allow_float: bool = False) -> Pseudobulk:
        """
        Calculate counts per million for each cell type.
        
        Must be run after `library_size()`. Must not be run before DE(), since
        `DE()` already normalizes the data internally.

        Args:
            library_size_column: a floating-point column of `obs` containing
                                 each sample's library size. Can be a column
                                 name, a polars expression, a polars Series, a
                                 1D NumPy array, or a function that takes in
                                 this Pseudobulk dataset and a cell type and
                                 returns a polars Series or 1D NumPy array. Or,
                                 a dictionary mapping cell-type names to any of
                                 the above; each cell type in this Pseudobulk
                                 dataset must be present.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
        Returns:
            A new Pseudobulk dataset containing the CPMs.
        """
        # Get the library size column
        library_sizes = self._get_column(
            'obs', library_size_column, 'library_size_column',
            'floating-point',
            custom_error='library_size_column {} is not a column of obs[{}]; '
                         'did you forget to run library_size()?',
            allow_None=False)
        
        # Check that `allow_float` is Boolean
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        
        # If `allow_float=False`, raise an error if `X` is floating-point for
        # any cell type
        if not allow_float:
            for cell_type, X in self._X.items():
                dtype = X.dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"CPM() requires raw counts but X[{cell_type!r}] "
                        f"has data type {str(dtype)!r}, a floating-point data "
                        f"type; if you are sure that all values are raw "
                        f"integer counts, i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        
        # Calculate CPMs
        CPMs = {}
        for cell_type, X in self._X.items():
            library_size = library_sizes[cell_type].to_numpy()
            CPMs[cell_type] = X / library_size[:, None] * 1e6
        return Pseudobulk(X=CPMs, obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    def log_CPM(self,
                *,
                library_size_column: PseudobulkColumn = 'library_size',
                prior_count: int | float | np.integer | np.floating = 2,
                allow_float: bool = False) -> Pseudobulk:
        """
        Calculate log counts per million for each cell type.
        
        Must be run after `library_size()`. Must not be run before `DE()`,
        since `DE()` already normalizes the data internally.
        
        Results were verified to match edgeR to within floating-point error.
        
        Args:
            library_size_column: a floating-point column of `obs` containing
                                 each sample's library size. Can be a column
                                 name, a polars expression, a polars Series, a
                                 1D NumPy array, or a function that takes in
                                 this Pseudobulk dataset and a cell type and
                                 returns a polars Series or 1D NumPy array. Or,
                                 a dictionary mapping cell-type names to any of
                                 the above; each cell type in this Pseudobulk
                                 dataset must be present.
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts); if `True`, disable this sanity check
            prior_count: the pseudocount to add before log-transforming. In the
                         current version of edgeR, prior.count is now 2 instead
                         of the old value of 0.5: code.bioconductor.org/browse/
                         edgeR/blob/RELEASE_3_18/R/cpm.R
        
        Returns:
            A new Pseudobulk dataset containing the log(CPMs).
        """
        # Get the library size column
        library_sizes = self._get_column(
            'obs', library_size_column, 'library_size_column',
            'floating-point',
            custom_error='library_size_column {} is not a column of obs[{}]; '
                         'did you forget to run library_size()?',
            allow_None=False)
        
        # Check that `prior_count` is a positive number
        check_type(prior_count, 'prior_count', (int, float),
                   'a positive number')
        check_bounds(prior_count, 'prior_count', 0, left_open=True)
        
        # Check that `allow_float` is Boolean
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        
        # If `allow_float=False`, raise an error if `X` is floating-point for
        # any cell type
        if not allow_float:
            for cell_type, X in self._X.items():
                dtype = X.dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"log_CPM() requires raw counts but X[{cell_type!r}] "
                        f"has data type {str(dtype)!r}, a floating-point data "
                        f"type; if you are sure that all values are raw "
                        f"integer counts, i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        
        # Get the log CPMs; this code is based on the R translation of edgeR's
        # C++ `cpm()` code at bioinformatics.stackexchange.com/a/4990.
        log_CPMs = {}
        for cell_type, X in self._X.items():
            library_size = library_sizes[cell_type].to_numpy(writable=True)
            pseudocount = prior_count * library_size / library_size.mean()
            library_size += 2 * pseudocount
            log_CPMs[cell_type] = np.log2(X + pseudocount[:, None]) - \
                np.log2(library_size[:, None]) + np.log2(1e6)
        return Pseudobulk(X=log_CPMs, obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    @staticmethod
    def _get_unique_variables(formulas: str | Iterable[str],
                              composite: bool = False) -> list[str]:
        """
        Get a list of the unique variables referenced in one or more R
        formulas. Include backtick-quoted variable names that contain spaces
        or other characters that would otherwise be invalid in R variables.
        Do not include R functions (e.g. `exp(x1)` adds `'x1'` to the list, but
        not `exp`) or numbers.

        Args:
            formulas: one or more R formulas, represented as Python strings
            composite: if `True`, avoid splitting "composite" variables like
                       `x1:x2`, so that the unique variables are columns of the
                       design matrix. If `False`, split these into their
                       components, so that the unique variables are columns of
                       obs.

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        if isinstance(formulas, str):
            formulas = formulas,
        pattern = f'[+\\-*/^{":" if composite else ""}()]|`[^`]+`|[\\w.]+'
        seen = set()
        # noinspection PyUnresolvedReferences
        unique_variables = [
            token[1:-1] if token[0] == '`' else token
            for formula in formulas
            for token, next_token in pairwise(re.findall(pattern, formula) +
                                              [''])
            if (token not in seen and not seen.add(token) and
                not token.isdigit() and
                (token[0] == '`' or re.fullmatch(r'[\w.]*', token)) and
                next_token != '(')]
        return unique_variables
    
    @staticmethod
    def _process_formula_variables(formula: str,
                                   cell_type: str,
                                   obs: pl.DataFrame) -> list[str]:
        """
        Check that all variables referenced in `formula` are Categorical, Enum,
        Boolean, integer, or floating-point columns of `obs`. Make a set of
        these variables. Used by `DE()` and `regress_out()`.
        
        Args:
            formula: the formula to process
            cell_type: the cell type the formula is for, used in error messages
            obs: the `obs` for this cell type

        Returns:
            A list of the unique variables in `formula`, in order of first
            appearance.
        """
        unique_formula_variables = \
            Pseudobulk._get_unique_variables(formula)
        valid_dtypes = pl.FLOAT_DTYPES | pl.INTEGER_DTYPES | \
                       {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
        # noinspection PyUnboundLocalVariable
        for variable in unique_formula_variables:
            if variable not in obs:
                error_message = (
                    f'formula contains the variable {variable!r}, which is '
                    f'not the name of a column of obs[{cell_type!r}]')
                raise ValueError(error_message)
            base_type = obs[variable].dtype.base_type()
            if base_type not in valid_dtypes:
                error_message = (
                    f'all columns of obs referenced in formula must be '
                    f'Categorical, Enum, Boolean, integer, or floating-point, '
                    f'but it contains the variable {variable!r} and '
                    f'obs[{cell_type!r}][{variable!r}] has data type '
                    f'{base_type!r}')
                raise TypeError(error_message)
        return unique_formula_variables
    
    @staticmethod
    def _create_design_matrix(formula: str,
                              cell_type: str,
                              obs: pl.DataFrame,
                              obs_names: pl.Series,
                              obs_columns: list[str],
                              categorical_columns: str | Iterable[str] | None,
                              prefix: str) -> None:
        """
        Create a design matrix from a formula. Used by `DE()` and
        `regress_out()`.
        
        Adds variables called `{prefix}.formula`, `{prefix}.obs`, and
        `{prefix}.design.matrix` to the ryp R workspace, which need to be
        deleted by the calling function.
        
        Args:
            formula: the formula to construct the design matrix from
            cell_type: the cell type the formula is for
            obs: the `obs` for this cell type
            obs_names: the `obs_names` for this cell type
            obs_columns: the columns of `obs` that need to be converted to R
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal
            prefix: a prefix to use for the three variables to be added to the
                    ryp R workspace
        """
        from ryp import r, to_py, to_r, \
            _bytestring_to_character_vector, _rlib, _RMemory
        
        # Convert the formula to R
        to_r(formula, f'{prefix}.formula')
        r(f'{prefix}.formula = as.formula({prefix}.formula)')
        
        # Subset `obs` to just the columns we need
        obs = obs.select(*obs_columns)
        
        # Check that these columns do not contain any `null` values
        for column in obs:
            null_count = column.null_count()
            if null_count > 0:
                error_message = (
                    f'{column.name} contains {null_count:,} '
                    f'{plural("null value", null_count)} for cell type '
                    f'{cell_type!r}, but must not contain any')
                raise ValueError(error_message)
        
        # Convert the integer columns in `categorical_columns` to Enums, after
        # checking that all elements of `categorical_columns` are integer or
        # Enum columns of `obs`
        if categorical_columns is not None:
            for column in categorical_columns:
                if column not in obs:
                    error_message = (
                        f'one of the columns in categorical_columns, '
                        f'{column!r}, is not a column of obs[{cell_type!r}]')
                    raise ValueError(error_message)
                base_type = obs[column].dtype.base_type()
                if base_type != pl.Enum and base_type not in pl.INTEGER_DTYPES:
                    error_message = (
                        f'all columns in categorical_columns must be integer '
                        f'or Enum, but one of the columns is {column!r} and '
                        f'obs[{cell_type!r}][{column!r}] has data type '
                        f'{base_type!r}')
                    raise TypeError(error_message)
            obs = obs\
                .cast({row[0]: pl.Enum(row[1]) for row in
                       obs.select(
                           (pl.selectors.integer() &
                            pl.selectors.by_name(categorical_columns))
                           .unique(maintain_order=True)
                           .implode()
                           .list.drop_nulls())
                      .unpivot()
                      .cast({'value': pl.List(pl.String)})
                      .rows()})
        
        # Convert the selected columns of `obs` to R
        obs_name = f'{prefix}.obs'
        to_r(obs, obs_name, rownames=obs_names)
        
        # If specified, convert the columns listed in `categorical_columns`
        # from ordered to unordered factors by removing the `'ordered'`
        # class and leaving only the `'factor'` class. This has to be done
        # through the R C API to be in-place.
        if categorical_columns is not None:
            R_obs = _rlib.Rf_findVar(_rlib.Rf_install(obs_name.encode()),
                                     _rlib.R_GlobalEnv)
            with _RMemory(_rlib) as rmemory:
                new_class = \
                    _bytestring_to_character_vector(b'factor', rmemory)
                for column in categorical_columns:
                    column_index = obs.columns.index(column)
                    R_column = _rlib.VECTOR_ELT(R_obs, column_index)
                    _rlib.Rf_setAttrib(R_column, _rlib.R_ClassSymbol,
                                       new_class)
        
        # Create the design matrix
        r(f'{prefix}.design.matrix = '
          f'model.matrix({prefix}.formula, {prefix}.obs)')
        
        # Check that the design matrix has more rows than columns
        height = to_py(f'nrow({prefix}.design.matrix)')
        width = to_py(f'ncol({prefix}.design.matrix)')
        if width >= height:
            error_message = (
                f'the design matrix must have more rows (samples) than '
                f'columns (one plus the number of covariates), but has '
                f'{height:,} {plural("row", height)} and {width:,} '
                f'{plural("column", width)} for cell type {cell_type!r}. '
                f'Either reduce the number of covariates, or exclude this '
                f'cell type with e.g. excluded_cell_types={cell_type!r}.')
            raise ValueError(error_message)
        
        # Check that the design matrix is non-empty (which can happen e.g. if
        # the user specifies `formula='~0'`)
        if width == 0:
            error_message = 'the design matrix is empty'
            raise ValueError(error_message)
    
    # noinspection PyTypeChecker
    @staticmethod
    def _regress_out(X: np.ndarray[2, np.dtype[np.integer | np.floating]],
                     obs: pl.DataFrame,
                     cell_type: str,
                     cell_type_index: int,
                     formula: str,
                     categorical_columns: str | Iterable[str] | None,
                     error_if_int: bool,
                     verbose: bool) -> np.ndarray[2, np.dtype[np.integer |
                                                              np.floating]]:
        """
        Regress out covariates from `obs` for a single cell type. Used by
        `regress_out()`.
        
        Args:
            X: the `X` for this cell type
            obs: the `obs` for this cell type
            cell_type: the cell type covariates will be regressed out for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of `obs`,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process

        Returns:
            `X` with covariates regressed out.
        """
        from ryp import r, to_py
        
        # If `error_if_int=True`, raise an error if `X` has an integer dtype
        if error_if_int and np.issubdtype(X.dtype, np.integer):
            error_message = (
                f'X[{cell_type!r}] has data type {str(X.dtype)!r}, an integer '
                f'data type; did you forget to run log_CPM() before '
                f'regress_out()?')
            raise ValueError(error_message)
        
        # Check that all variables referenced by `formula` are Categorical,
        # Enum, Boolean, integer, or floating-point columns of `obs`. Make a
        # set of these variables.
        if verbose:
            print(f'[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
        
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        
        # Create the design matrix
        try:
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            obs_names = obs[:, 0]
            if obs_names.dtype in pl.INTEGER_DTYPES:
                obs_names = obs_names.cast(pl.String)
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_names, obs_columns,
                                             categorical_columns, prefix)
            design_matrix = to_py(f'{prefix}.design.matrix', format='numpy')
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix")))')
        
        # Regress out the design matrix; silence warnings with `rcond=None`
        if verbose:
            print(f'[{cell_type}] Regressing out...')
        beta, _, rank, _ = np.linalg.lstsq(design_matrix, X, rcond=None)
       
        # Check that the design matrix is full-rank
        if rank < design_matrix.shape[1]:
            error_message = (
                f'the design matrix is not full-rank for cell type '
                f'{cell_type!r} (rank {rank} with {design_matrix.shape[1]} '
                f'columns); some of your covariates are linear '
                f'combinations of other covariates')
            raise ValueError(error_message)
       
        # Calculate the residuals
        residuals = X - design_matrix @ beta
        return residuals
    
    def regress_out(self,
                    formula: str | dict[str, str],
                    *,
                    categorical_columns: str | Iterable[str] | None |
                                         dict[str, str | Iterable[str] |
                                                   None] = None,
                    cell_types: str | Iterable[str] | None = None,
                    excluded_cell_types: str | Iterable[str] | None = None,
                    error_if_int: bool = True,
                    verbose: bool = True,
                    num_threads: int | np.integer | None = None) -> Pseudobulk:
        """
        Regress out covariates from `obs`. Must be run after `log_CPM()`.
        
        *To avoid confounding due to library size or the number of cells
        included in the pseudobulk, we strongly recommend including
        `'+ log2(num_cells) + log2(library_size)'` to the `formula`, where
        `'library_size'` is a column of `obs` that can be added by running
        `library_size()` before this function.*
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of `obs` referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of `obs` represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```r
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the
                     design matrix to regress out in terms of columns of `obs`,
                     e.g. `'~ disease_status + age + sex'`. Will be converted
                     into an R formula object with R's `as.formula()` function
                     and then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            categorical_columns: one or more names of integer or Enum columns
                                 to treat as categorical (i.e. convert to
                                 unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to regress the covariates out
                        of; if `None`, regress covariates out of all cell
                        types. Mutually exclusive with `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when
                                 regressing out covariates; mutually exclusive
                                 with `cell_types`
            error_if_int: if `True`, raise an error if `X.dtype` is integer
                          (indicating the user may not have run log_CPM() yet)
            verbose: whether to print out details of the regressing-out process
            num_threads: the number of threads to use when regressing out.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores, as determined by
                         `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Parallelization takes place
                         across cell types, so specifying more cores than the
                         number of cell types may not improve performance. Does
                         not affect the returned Pseudobulk dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.

        Returns:
            A new Pseudobulk dataset with covariates regressed out.
        """
        from ryp import r
        
        # Get the list of cell types to regress out covariates for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings, and that each string is a valid R formula
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
                if not value.lstrip().startswith('~'):
                    error_message = \
                        f'formula[{key!r}] must start with a tilde (~)'
                    raise ValueError(error_message)
                try:
                    r(f'as.formula({value!r})')
                except RuntimeError as e:
                    error_message = \
                        f'formula[{key!r}] is not a valid R formula'
                    raise ValueError(error_message) from e
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        else:
            if not formula.lstrip().startswith('~'):
                error_message = 'formula must start with a tilde (~)'
                raise ValueError(error_message)
            try:
                r(f'as.formula({formula!r})')
            except RuntimeError as e:
                error_message = 'formula is not a valid R formula'
                raise ValueError(error_message) from e
        
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        
        # Check that `error_if_int` and `verbose` are Boolean
        check_type(error_if_int, 'error_if_int', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads` if free-threaded and 1 otherwise,
        # and if -1, set to `os.cpu_count()`. Raise an error if the user
        # specified multiple threads but lacks free-threaded Python.
        num_threads = self._process_num_threads(num_threads)
        
        # Compute residuals for each cell type
        if num_threads == 1:
            # noinspection PyUnboundLocalVariable
            residuals = {
                cell_type: Pseudobulk._regress_out(
                    X=X, obs=obs, cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type] if formula_is_dict else
                            formula,
                    categorical_columns=categorical_columns,
                    error_if_int=error_if_int, verbose=verbose)
                    for cell_type_index, (cell_type, (X, obs, var))
                    in enumerate(self.items())}
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            kwargs_list = [{
                'X': X,
                'obs': obs,
                'cell_type': 'cell_type',
                'cell_type_index': 'cell_type_index',
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'error_if_int': error_if_int,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                residuals = dict(zip(cell_types, executor.map(
                    lambda kwargs: self._DE(**kwargs), kwargs_list)))
        
        # Return a new Pseudobulk dataset with the residuals
        return Pseudobulk(X=residuals, obs=self._obs, var=self._var,
                          num_threads=self._num_threads)
    
    # A slightly reformatted version of the voomByGroup source code from
    # github.com/YOU-k/voomByGroup/blob/main/voomByGroup.R, which is available
    # under the MIT license. Copyright (c) 2023 Yue You. Also added
    # `drop=FALSE` to the `countsi` subsetting to avoid an error with length-1
    # groups.
    _voomByGroup_source_code = r'''
    voomByGroup <- function (counts, group = NULL, design = NULL,
                             lib.size = NULL, dynamic = NULL,
                             normalize.method = "none", span = 0.5,
                             save.plot = FALSE, print = TRUE, plot = c("none",
                             "all", "separate", "combine"),
                             col.lines = NULL, pos.legend = c("inside",
                             "outside", "none"), fix.y.axis = FALSE, ...) {
      out <- list()
      if (is(counts, "DGEList")) {
        out$genes <- counts$genes
        out$targets <- counts$samples
        if(is.null(group))
          group <- counts$samples$group
        if (is.null(lib.size))
          lib.size <- with(counts$samples, lib.size * norm.factors)
        counts <- counts$counts
      }
      else {
        isExpressionSet <-
          suppressPackageStartupMessages(is(counts, "ExpressionSet"))
        if (isExpressionSet) {
          if (length(Biobase::fData(counts)))
            out$genes <- Biobase::fData(counts)
          if (length(Biobase::pData(counts)))
            out$targets <- Biobase::pData(counts)
          counts <- Biobase::exprs(counts)
        }
        else {
          counts <- as.matrix(counts)
        }
      }
      if (nrow(counts) < 2L)
        stop("Need at least two genes to fit a mean-variance trend")
      # Library size
      if(is.null(lib.size))
        lib.size <- colSums(counts)
      # Group
      if(is.null(group))
        group <- rep("Group1", ncol(counts))
      group <- as.factor(group)
      intgroup <- as.integer(group)
      levgroup <- levels(group)
      ngroups <- length(levgroup)
      # Design matrix
      if (is.null(design)) {
        design <- matrix(1L, ncol(counts), 1)
        rownames(design) <- colnames(counts)
        colnames(design) <- "GrandMean"
      }
      # Dynamic
      if (is.null(dynamic)) {
        dynamic <- rep(FALSE, ngroups)
      }
      # voom by group
      if(print)
        cat("Group:\n")
      E <- w <- counts
      xy <- line <- as.list(rep(NA, ngroups))
      names(xy) <- names(line) <- levgroup
      for (lev in 1L:ngroups) {
        if(print)
          cat(lev, levgroup[lev], "\n")
        i <- intgroup == lev
        countsi <- counts[, i, drop = FALSE]
        libsizei <- lib.size[i]
        designi <- design[i, , drop = FALSE]
        QR <- qr(designi)
        if(QR$rank<ncol(designi))
          designi <- designi[,QR$pivot[1L:QR$rank], drop = FALSE]
        if(ncol(designi)==ncol(countsi))
          designi <- matrix(1L, ncol(countsi), 1)
        voomi <- voom(counts = countsi, design = designi, lib.size = libsizei,
                      normalize.method = normalize.method, span = span,
                      plot = FALSE, save.plot = TRUE, ...)
        E[, i] <- voomi$E
        w[, i] <- voomi$weights
        xy[[lev]] <- voomi$voom.xy
        line[[lev]] <- voomi$voom.line
      }
      #voom overall
      if (TRUE %in% dynamic){
        voom_all <- voom(counts = counts, design = design, lib.size = lib.size,
                         normalize.method = normalize.method, span = span,
                         plot = FALSE, save.plot = TRUE, ...)
        E_all <- voom_all$E
        w_all <- voom_all$weights
        xy_all <- voom_all$voom.xy
        line_all <- voom_all$voom.line
        dge <- DGEList(counts)
        disp <- estimateCommonDisp(dge)
        disp_all <- disp$common
      }
      # Plot, can be "both", "none", "separate", or "combine"
      plot <- plot[1]
      if(plot!="none"){
        disp.group <- c()
        for (lev in levgroup) {
          dge.sub <- DGEList(counts[,group == lev])
          disp <- estimateCommonDisp(dge.sub)
          disp.group[lev] <- disp$common
        }
        if(plot %in% c("all", "separate")){
          if (fix.y.axis == TRUE) {
            yrange <- sapply(levgroup, function(lev){
              c(min(xy[[lev]]$y), max(xy[[lev]]$y))
            }, simplify = TRUE)
            yrange <- c(min(yrange[1,]) - 0.1, max(yrange[2,]) + 0.1)
          }
          for (lev in 1L:ngroups) {
            if (fix.y.axis == TRUE){
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25,
                   ylim = yrange)
            } else {
              plot(xy[[lev]], xlab = "log2( count size + 0.5 )",
                   ylab = "Sqrt( standard deviation )", pch = 16, cex = 0.25)
            }
            title(paste("voom: Mean-variance trend,", levgroup[lev]))
            lines(line[[lev]], col = "red")
            legend("topleft", bty="n", paste("BCV:",
              round(sqrt(disp.group[lev]), 3)), text.col="red")
          }
        }
        
        if(plot %in% c("all", "combine")){
          if(is.null(col.lines))
            col.lines <- 1L:ngroups
          if(length(col.lines)<ngroups)
            col.lines <- rep(col.lines, ngroups)
          xrange <- unlist(lapply(line, `[[`, "x"))
          xrange <- c(min(xrange)-0.3, max(xrange)+0.3)
          yrange <- unlist(lapply(line, `[[`, "y"))
          yrange <- c(min(yrange)-0.1, max(yrange)+0.3)
          plot(1L,1L, type="n", ylim=yrange, xlim=xrange,
               xlab = "log2( count size + 0.5 )",
               ylab = "Sqrt( standard deviation )")
          title("voom: Mean-variance trend")
          if (TRUE %in% dynamic){
            for (dy in which(dynamic)){
              line[[dy]] <- line_all
              disp.group[dy] <- disp_all
              levgroup[dy] <- paste0(levgroup[dy]," (all)")
            }
          }
          for (lev in 1L:ngroups)
            lines(line[[lev]], col=col.lines[lev], lwd=2)
          pos.legend <- pos.legend[1]
          disp.order <- order(disp.group, decreasing = TRUE)
          text.legend <-
            paste(levgroup, ", BCV: ", round(sqrt(disp.group), 3), sep="")
          if(pos.legend %in% c("inside", "outside")){
            if(pos.legend=="outside"){
              plot(1,1, type="n", yaxt="n", xaxt="n", ylab="", xlab="",
                   frame.plot=FALSE)
              legend("topleft", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            } else {
              legend("topright", text.col=col.lines[disp.order],
                     text.legend[disp.order], bty="n")
            }
          }
        }
      }
      # Output
      if (TRUE %in% dynamic){
        E[,intgroup %in% which(dynamic)] <-
          E_all[,intgroup %in% which(dynamic)]
        w[,intgroup %in% which(dynamic)] <-
          w_all[,intgroup %in% which(dynamic)]
      }
      out$E <- E
      out$weights <- w
      out$design <- design
      if(save.plot){
        out$voom.line <- line
        out$voom.xy <- xy
      }
      new("EList", out)
    }
    '''
    
    # noinspection PyTypeChecker
    def _DE(self,
            cell_type: str,
            cell_type_index: int,
            formula: str,
            coefficient: str | int | np.integer |
                         Iterable[str | int | np.integer],
            contrasts: dict[str, str] | None,
            group: Literal[False] | pl.Series | None,
            categorical_columns: str | Iterable[str] | None,
            library_size: pl.Series,
            robust: bool,
            return_voom_info: bool,
            verbose: bool) -> pl.DataFrame | \
                              tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """
        Compute differential expression for a single cell type. Used by `DE()`.
        
        Args:
            cell_type: the cell type DE will be calculated for
            cell_type_index: the integer index of the cell type in `cell_types`
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of `obs`, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Negative indices work in the
                         usual Python way. Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. If
                       specified, DE will be performed with respect to each
                       contrast by running limma's `makeContrasts()` and
                       `contrasts.fit()` functions after `lmFit()`. Mutually
                       exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of `obs`
                   referenced in `coefficient`, or the columns of `obs`
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   polars Series, force the use of voomByGroup and group on the
                   unique values of that column.
            categorical_columns: one or more names of integer or Enum columns
                                 of `obs` to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal
            library_size: a floating-point polars Series containing each
                          sample's library size
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            verbose: whether to print out details of the DE estimation

        Returns:
            A DataFrame of the DE results for this cell type. Or, if
            `return_voom_info=True`, a tuple of three DataFrames: the DE
            results, the voom weights, and the voom plot info for this cell
            type.
        """
        from ryp import r, to_py, to_r
        
        # Get the data for this cell type
        X = self._X[cell_type]
        obs = self._obs[cell_type]
        var = self._var[cell_type]
        
        # Check that all variables referenced by `formula` are Categorical,
        # Enum, Boolean, integer, or floating-point columns of `obs`. Make a
        # set of these variables so we can subset to them before converting
        # `obs` to R.
        if verbose:
            print(f'\n[{cell_type}] Validating formula...')
        obs_columns = \
            Pseudobulk._process_formula_variables(formula, cell_type, obs)
            
        # Make a unique prefix for all R variables for this cell type, to
        # avoid name conflicts with other cell types when multithreading
        # and with other R objects the user might have defined in the ryp R
        # workspace
        prefix = f'.Pseudobulk.{cell_type_index}'
        try:
            # Get obs and var names
            obs_names = obs[:, 0]
            if obs_names.dtype in pl.INTEGER_DTYPES:
                obs_names = obs_names.cast(pl.String)
            var_names = var[:, 0]
            if var_names.dtype in pl.INTEGER_DTYPES:
                var_names = var_names.cast(pl.String)
            # Create the design matrix
            if verbose:
                print(f'[{cell_type}] Creating design matrix...')
            Pseudobulk._create_design_matrix(formula, cell_type, obs,
                                             obs_names, obs_columns,
                                             categorical_columns, prefix)
            design_matrix_columns = to_py(f'colnames({prefix}.design.matrix)',
                                          squeeze=False)
            
            # Check that the design matrix is full-rank
            rank = to_py(f'qr({prefix}.design.matrix)$rank')
            width = len(design_matrix_columns)
            if rank < width:
                error_message = (
                    f'the design matrix is not full-rank for cell type '
                    f'{cell_type!r} (rank {rank:,} with {width:,} columns); '
                    f'some of your covariates are linear combinations of '
                    f'other covariates')
                raise ValueError(error_message)
            
            # If `contrasts` was specified, validate `contrasts`; otherwise,
            # validate `coefficient`
            if contrasts is None:
                if verbose:
                    print(f'[{cell_type}] Validating coefficient...')
                
                # Check that all string entries of `coefficient` are names of
                # columns of the design matrix
                for coef in coefficient:
                    if isinstance(coef, str) and \
                            coef not in design_matrix_columns:
                        error_message = (
                            f'coefficient {coef!r} is not a column of the '
                            f'design matrix for cell type {cell_type!r}. The '
                            f'design matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns: '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
                
                # Extract the name of the design matrix column corresponding to
                # each integer in `coefficient`; make sure none of the integers
                # are >= the design matrix width
                for coef in coefficient:
                    if isinstance(coef, (int, np.integer)) and coef >= width:
                        contains_string = 'is' \
                            if len(coefficient) == 1 else 'contains the number'
                        error_message = (
                            f'coefficient {contains_string} {coef}, which is '
                            f'more than the number of columns of the design '
                            f'matrix ({width:,}) minus 1 for cell type '
                            f'{cell_type!r}')
                        raise ValueError(error_message)
                coefficient = [design_matrix_columns[coef]
                               if isinstance(coef, (int, np.integer)) else coef
                               for coef in coefficient]
                
                # Convert `coefficient` to R
                to_r(pl.Series(coefficient), f'{prefix}.coef')
            else:
                if verbose:
                    print(f'[{cell_type}] Validating contrasts...')
                
                # Check that all variables referenced in `contrasts` are in the
                # design matrix. Use `composite=True` to avoid splitting e.g.
                # `x1:x2` into `x1` and `x2`.
                contrasts = {contrast_name: contrast.replace(' ', '')
                             for contrast_name, contrast in contrasts.items()}
                valid_dtypes = pl.INTEGER_DTYPES | \
                               {pl.String, pl.Categorical, pl.Enum, pl.Boolean}
                unique_contrast_variables = \
                    Pseudobulk._get_unique_variables(contrasts.values(),
                                                     composite=True)
                for variable in unique_contrast_variables:
                    if variable not in design_matrix_columns:
                        error_message = (
                            f'a contrast contains the variable {variable!r}, '
                            f'which is not the name of a column of the design '
                            f'matrix for cell type {cell_type!r}. The design '
                            f'matrix has ')
                        if len(design_matrix_columns) == 1:
                            error_message += \
                                f'one column: {design_matrix_columns[0]!r}'
                        else:
                            all_but_last = ', '.join(
                                f'{column!r}'
                                for column in design_matrix_columns[:-1])
                            error_message += (
                                f'{len(design_matrix_columns):,} columns '
                                f'{all_but_last} and '
                                f'{design_matrix_columns[-1]!r}')
                        error_message += '.'
                        raise ValueError(error_message)
            if group is None or contrasts is not None:
                # Get the list of columns of `obs` referenced in `coefficient`
                # or `contrasts`. This can be done by:
                # 1. Getting the columns of the design matrix referenced in
                #    `coefficient` or `contrasts`. For `coefficient`, this is
                #    just the entries of `coefficient` themselves. For
                #    `contrasts`, this is the `unique_contrast_variables`
                #    variable we defined above.
                # noinspection PyUnboundLocalVariable
                referenced_design_matrix_columns = coefficient \
                    if contrasts is None else unique_contrast_variables
                
                # 2. Mapping each column of the design matrix listed in
                #    `coefficient` or `contrasts` back to the integer index of
                #    the term in `formula` it was derived from, using the
                #    design matrix's `assign` attribute. This is 0-based, where
                #    0 indicates the intercept.
                assign = to_py(f'attr({prefix}.design.matrix, "assign")',
                               squeeze=False)
                referenced_term_indices = assign\
                    .filter(design_matrix_columns
                            .is_in(referenced_design_matrix_columns))
                
                # 3. Using R's `terms()` function to expand the formula into
                #    a list of its component terms. This does not include the
                #    intercept.
                term_labels = \
                    to_py(f'attr(terms({prefix}.formula), "term.labels")',
                          squeeze=False)
                
                # 4. Pulling out the terms corresponding to the indices from
                #    step 2, to get the terms of `formula` referenced in
                #    `coefficient` or `contrasts`. Since
                #    `referenced_term_indices` includes the intercept but
                #    `term_labels` does not, there's an off-by-one issue. To
                #    fix it, remove any 0s present in `referenced_term_indices`
                #    (we don't care about references to the intercept, since we
                #    are looking for columns of obs referenced in the formula,
                #    and the intercept is not a column of obs), then subtract 1
                #    from `referenced_term_indices`.
                referenced_terms = term_labels[referenced_term_indices.filter(
                    referenced_term_indices != 0) - 1]
                
                # 5. Getting the unique variables in these terms. These are the
                #    columns of `obs` referenced in `coefficient` or
                #    `contrasts`.
                referenced_columns = \
                    Pseudobulk._get_unique_variables(referenced_terms)
            
            # If `contrasts` was specified, check that all columns of `obs`
            # referenced in `contrasts` are categorical, now that we know which
            # columns are referenced
            if contrasts is not None:
                # noinspection PyUnboundLocalVariable
                for column in referenced_columns:
                    base_type = obs[column].dtype.base_type()
                    # noinspection PyUnboundLocalVariable
                    if base_type not in valid_dtypes:
                        error_message = (
                            f'all columns of obs referenced in contrasts must '
                            f'be String, Categorical, Enum, Boolean, or '
                            f'integer, but a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}], which has data '
                            f'type {base_type!r}')
                        raise TypeError(error_message)
                    if (base_type == pl.Enum or
                        base_type in pl.INTEGER_DTYPES) and (
                            categorical_columns is None or
                            column not in categorical_columns):
                        error_message = (
                            f'a contrast references the column '
                            f'obs[{cell_type!r}][{column!r}] with data type '
                            f'{base_type!r}, but all columns referenced in '
                            f'contrasts must be categorical and integer/Enum '
                            f'columns are not treated as categorical unless '
                            f'specified in categorical_columns; did you '
                            f'forget to add {column!r} to '
                            f'categorical_columns?')
                        raise TypeError(error_message)
            
            # If `group=None`, group on the unique combinations of values of
            # the categorical columns of `obs` referenced in `coefficient` or
            # `contrasts`
            if group is None:
                if verbose:
                    print(f'[{cell_type}] Defining groups...')
                if contrasts is not None:
                    # If using `contrasts`, always use voomByGroup, and group
                    # on the unique combinations of the columns referenced in
                    # the contrasts (recall that we already checked that they
                    # are categorical)
                    # noinspection PyUnboundLocalVariable
                    group_columns = referenced_columns
                else:
                    # If using `coefficient`, group on the unique combinations
                    # of the categorical columns referenced in the
                    # coefficients. If no columns are categorical, use voom
                    # instead of voomByGroup.
                    categorical_selector = \
                        pl.selectors.by_dtype(pl.String, pl.Categorical,
                                              pl.Boolean)
                    if categorical_columns is not None:
                        categorical_selector |= \
                            pl.selectors.by_name(categorical_columns)
                    group_columns = pl.selectors.expand_selector(
                        obs,
                        pl.selectors.by_name(referenced_columns) &
                        categorical_selector)
                if len(group_columns) > 0:
                    # Create a descriptive name for each combination of values
                    # in the `group_columns`
                    group = obs\
                        .select(pl.format(', '.join(f'{column} = {{}}'
                                                    for column in
                                                    group_columns),
                                          *group_columns))\
                        .to_series()
                    group = group\
                        .cast(pl.Enum(group.unique().sort().to_list()))
                    
                    # If the group columns have the same values for every
                    # sample, disable grouping
                    single_group = len(group.cat.get_categories())
                    if single_group:
                        group = None
                    
                    # If `verbose=True`, print whether and how we're grouping
                    if verbose:
                        if len(group_columns) == 1:
                            group_column_description = f'{group_columns[0]!r}'
                        elif len(group_columns) == 2:
                            group_column_description = \
                                f'{group_columns[0]!r} and ' \
                                f'{group_columns[1]!r}'
                        else:
                            group_column_description = \
                                ', '.join(map(repr, group_columns[:-1])) + \
                                f', and {group_columns[-1]!r}'
                        if single_group:
                            if len(group_columns) == 1:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group column '
                                      f'{group_column_description} has the '
                                      f'same value for every sample')
                            else:
                                print(f'[{cell_type}] Not grouping since the '
                                      f'group columns '
                                      f'{group_column_description} have the '
                                      f'same values for every sample')
                        else:
                            print(f'[{cell_type}] Grouping on the '
                                  f'{group_column_description} '
                                  f'{plural("column", len(group_columns))} of '
                                  f'obs.')
                else:
                    if verbose:
                        print(f'[{cell_type}] Not grouping since coefficient '
                              f'does not reference any categorical variables.')
                    group = None
            
            # If grouping, check that all groups have at least two samples
            if group is not None:
                group_counts = group.value_counts().sort('count')
                if group_counts['count'][0] == 1:
                    error_message = (
                        f'all groups must have at least two samples, but '
                        f'group {group_counts[0, 0]!r} has only one '
                        f'sample for cell type {cell_type}')
                    raise ValueError(error_message)
            
            # Convert the expression matrix and library sizes to R
            if verbose:
                if group is not None:
                    print(f'[{cell_type}] Converting the expression matrix, '
                          f'library sizes and groups to R...')
                else:
                    print(f'[{cell_type}] Converting the expression matrix '
                          f'and library sizes to R...')
            to_r(X.T, f'{prefix}.X.T', rownames=var_names, colnames=obs_names)
            to_r(library_size, f'{prefix}.library.size', rownames=obs_names)
            to_r(group, f'{prefix}.group')
            
            # Run voom
            to_r(return_voom_info, 'save.plot')
            if group is not None:
                if verbose:
                    print(f'[{cell_type}] Running voomByGroup...')
                r(f'{prefix}.voom.result = voomByGroup('
                  f'{prefix}.X.T, {prefix}.group, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot, print=FALSE)')
            else:
                if verbose:
                    print(f'[{cell_type}] Running voom...')
                r(f'{prefix}.voom.result = voom('
                  f'{prefix}.X.T, {prefix}.design.matrix, '
                  f'{prefix}.library.size, save.plot=save.plot)')
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                voom_weights = \
                    to_py(f'{prefix}.voom.result$weights', index='gene')
                if group is not None:
                    voom_plot_data = var_names.to_frame('gene')
                    for group_name in group.unique(maintain_order=True):
                        group_voom_plot_data = pl.DataFrame({
                            'gene': to_py(
                                f'names({prefix}.voom.result$voom.xy$'
                                f'`{group_name}`$x)')} | {
                            f'{prop}_{dim}_{group_name}': to_py(
                                f'{prefix}.voom.result$voom.{prop}$'
                                f'`{group_name}`${dim}', index=False)
                            for prop in ('xy', 'line')
                            for dim in ('x', 'y')})
                        voom_plot_data = voom_plot_data\
                            .join(group_voom_plot_data, on='gene', how='left')
                else:
                    voom_plot_data = pl.DataFrame({
                        'gene': to_py(
                            f'names({prefix}.voom.result$voom.xy$x)')} | {
                        f'{prop}_{dim}': to_py(
                            f'{prefix}.voom.result$voom.{prop}${dim}',
                            index=False)
                        for prop in ('xy', 'line') for dim in ('x', 'y')})
            
            # Run `lmFit()`
            if verbose:
                print(f'[{cell_type}] Running lmFit...')
            r(f'{prefix}.lmFit.result = lmFit('
              f'{prefix}.voom.result, {prefix}.design.matrix)')
            if contrasts is not None:
                # Convert contrasts to R
                to_r(pl.Series(contrasts.values()), f'{prefix}.contrasts')
                
                # Make the contrast terms and design matrix colnames into
                # valid R variable names by temporarily:
                # - escaping interaction terms (e.g. renaming `A:B` to `A.B`)
                #   in both the contrasts and the design matrix colnames
                # - renaming the design matrix column `(Intercept)` to
                #   `.Intercept` (not `Intercept` in case the user already has
                #   a column called `Intercept` for some reason)
                r(f'{prefix}.contrasts = gsub(":", ".", {prefix}.contrasts)')
                r(f'{prefix}.columns = colnames({prefix}.design.matrix)')
                r(rf'{prefix}.levels = gsub("\\(Intercept\\)", ".Intercept", '
                  rf'gsub(":", ".", {prefix}.columns))')
                r(f'colnames({prefix}.design.matrix) = {prefix}.levels')
                
                # Make contrasts
                r(f'{prefix}.contrasts = makeContrasts('
                  f'contrasts={prefix}.contrasts, levels={prefix}.levels)')
               
                # Rename the design matrix colnames (and the contrast rownames)
                # to the original design matrix colnames
                r(f'rownames({prefix}.contrasts) = {prefix}.columns')
                r(f'colnames({prefix}.design.matrix) = {prefix}.columns')
               
                # Set the colnames of the contrasts to `contrasts.keys()`, so
                # they display as those names in the output DE table
                to_r(pl.Series(contrasts.keys()), f'{prefix}.coef')
                r(f'colnames({prefix}.contrasts) = {prefix}.coef')
              
                # Fit contrasts
                r(f'{prefix}.lmFit.result = contrasts.fit('
                  f'{prefix}.lmFit.result, {prefix}.contrasts)')
           
            # Run `eBayes()`
            if verbose:
                print(f'[{cell_type}] Running eBayes...')
            to_r(robust, f'{prefix}.robust')
            r(f'{prefix}.eBayes.result = eBayes('
              f'{prefix}.lmFit.result, trend=FALSE, robust={prefix}.robust)')
           
            # Make a table of the DE results
            if verbose:
                print(f'[{cell_type}] Collating results...')
            gene = to_py(f'rownames({prefix}.eBayes.result)')
            logFC = to_py(f'{prefix}.eBayes.result$coefficients['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            SE = to_py(f'{prefix}.eBayes.result$s2.post').sqrt() * \
                 to_py(f'{prefix}.eBayes.result$stdev.unscaled['
                          f',{prefix}.coef, drop=FALSE]', index=False)
            margin_error = \
                SE * stdtrit(to_py(f'{prefix}.eBayes.result$df.total'), 0.975)
            LCI = logFC - margin_error
            UCI = logFC + margin_error
            AveExpr = to_py(f'{prefix}.eBayes.result$Amean', index=False)
            p = to_py(f'{prefix}.eBayes.result$p.value['
                      f',{prefix}.coef, drop=FALSE]', index=False)
            DE_results = pl.concat([
                pl.DataFrame({
                    'coefficient': coef, 'gene': gene, 'logFC': logFC[coef],
                    'SE': SE[coef], 'LCI': LCI[coef], 'UCI': UCI[coef],
                    'AveExpr': AveExpr, 'p': p[coef]})
                .with_columns(Bonferroni=bonferroni(pl.col.p),
                              FDR=fdr(pl.col.p))
                for coef in (
                    contrasts if contrasts is not None else coefficient)])
        finally:
            r(f'rm(list = Filter(exists, c("{prefix}.obs", '
              f'"{prefix}.formula", "{prefix}.design.matrix", '
              f'"{prefix}.library.size", "{prefix}.X.T", "{prefix}.group", '
              f'"{prefix}.voom.result", "{prefix}.lmFit.result", '
              f'"{prefix}.contrasts", "{prefix}.columns", "{prefix}.levels", '
              f'"{prefix}.robust", "{prefix}.eBayes.result", '
              f'"{prefix}.coef")))')
        # noinspection PyUnboundLocalVariable
        return DE_results, voom_weights, voom_plot_data \
            if return_voom_info else DE_results
    
    def DE(self,
           formula: str | dict[str, str],
           coefficient: str | int | np.integer |
                        Iterable[str | int | np.integer] |
                        dict[str, str | int | np.integer |
                                  Iterable[str | int | np.integer]] = 1,
           *,
           contrasts: dict[str, str] | None |
                      dict[str, dict[str, str] | None] = None,
           group: Literal[False] | PseudobulkColumn | None |
                  dict[str, Literal[False] | str | pl.Expr | pl.Series |
                            np.ndarray | None] = None,
           categorical_columns: str | Iterable[str] | None |
                                dict[str, str | Iterable[str] | None] = None,
           cell_types: str | Iterable[str] | None = None,
           excluded_cell_types: str | Iterable[str] | None = None,
           library_size_column: PseudobulkColumn = 'library_size',
           robust: bool = False,
           return_voom_info: bool = True,
           allow_float: bool = False,
           verbose: bool = True,
           num_threads: int | np.integer | None = None) -> DE:
        """
        Perform differential expression (DE) on a Pseudobulk dataset with
        limma-voom. The DE design is specified via an R formula string that
        references columns from `obs`.
        
        Must be run after `library_size()`. Requires raw counts, so must not be
        run after `CPM()` or `log_CPM()`.
        
        *To avoid confounding due to library size or the number of cells
        included in the pseudobulk, we strongly recommend including
        `'+ log2(num_cells) + log2(library_size)'` to the `formula`, where
        `'library_size'` is a column of `obs` that will be added when running
        `library_size()` before this function.*
        
        By default, DE is reported with respect to the first column of the
        design matrix after the intercept, which is usually just the first term
        in the formula. This can be changed via the `coefficient` and
        `contrasts` arguments.
        
        By default, voomByGroup is used instead of voom when reporting DE with
        respect to a categorical variable, e.g. when comparing disease cases to
        healthy controls. This can be changed via the `group` argument.
        
        The design matrix is constructed via the `model.matrix()` R function.
        String and Categorical columns of `obs` referenced in `formula` are
        converted to unordered factors, which by default are one-hot encoded
        into `N - 1` columns of the design matrix, where `N` is the number of
        unique values (`contr.treatment` in R). Conversely, Enum columns are
        converted to ordered factors, which by default are treated ordinally,
        as equally-spaced points, and converted into `N - 1` columns in the
        design matrix, where each column represents increasingly complex
        polynomial terms (linear, quadratic, cubic, etc.) calculated from these
        points (`contr.poly` in R).
        
        Often, however, integer and Enum columns of `obs` represent categorical
        variables and should be one-hot encoded. To do so, specify their names
        in `categorical_columns`. The encodings of unordered and ordered
        factors can also be changed globally; for example, to use Helmert
        contrasts for ordered factors:
        
        ```
        from ryp import r
        r('options(contrasts=c(unordered="contr.treatment", '
          '                    ordered="contr.helmert"))')
        ```
        
        To view the current value of the `contrasts` option, use:
        
        ```r
        from ryp import r
        r('getOption("contrasts")')
        ```
        
        Args:
            formula: a string representation of an R formula specifying the DE
                     design in terms of columns of `obs`, e.g.
                     `'~ disease_status + age + sex'`. Will be converted into
                     an R formula object with R's `as.formula()` function and
                     then expanded into a design matrix with R's
                     `model.matrix()` function. Must begin with a tilde (`~`).
                     May also be a dictionary mapping cell-type names to
                     formulas; each cell type in this Pseudobulk dataset must
                     be present.
            coefficient: the name or 0-based index of a coefficient in the
                         design matrix to report DE with respect to, or a
                         sequence of names or indices to report DE with respect
                         to multiple coefficients. Or, a dictionary mapping
                         cell-type names to any of the above; each cell type in
                         this Pseudobulk dataset must be present. Negative
                         indices work in the usual Python way. Defaults to
                         `coefficient=1`, the first variable in `formula`
                         (`coefficient=0` would refer to the intercept).
                         Mutually exclusive with `contrasts`.
            contrasts: an optional dictionary mapping contrast names to string
                       representations of R formulas specifying contrasts
                       between names of columns in the design matrix (e.g.
                       `{'DrugA_vs_Control': 'DrugA - Control'}`); the contrast
                       names (keys of the dictionary) will appear in the
                       `'Coefficient'` column of the output DE object. Or, a
                       dictionary mapping cell-type names to these
                       dictionaries; each cell type in this Pseudobulk dataset
                       must be present. If specified, DE will be performed with
                       respect to each contrast by running limma's
                       `makeContrasts()` and `contrasts.fit()` functions after
                       `lmFit()`. Mutually exclusive with `coefficient`.
            group: if `group=False`, force the use of voom instead of
                   voomByGroup. If `group=None`, group on the unique
                   combinations of values of the categorical columns of `obs`
                   referenced in `coefficient`, or the columns of `obs`
                   referenced in `contrasts`. Here, categorical columns are
                   those that are String, Categorical, Boolean, or integer or
                   Enum and specified in `categorical_columns`. If `group` is a
                   column (the name of a String, Categorical, Enum, Boolean, or
                   integer column of `obs`, a polars expression, a polars
                   Series, a 1D NumPy array, or a function that takes in this
                   Pseudobulk dataset and a cell type and returns a polars
                   Series or 1D NumPy array), force the use of voomByGroup and
                   group on the unique values of that column. `group` can also
                   be a dictionary mapping cell-type names to `False`, `None`,
                   or a column for each cell type. When using voomByGroup, the
                   same groups are also used as the `group` argument to
                   `calcNormFactors()` when normalizing by library size. All
                   groups must have at least two samples.
            categorical_columns: one or more names of integer or Enum columns
                                 of `obs` to treat as categorical (i.e. convert
                                 to unordered factors) rather than continuous
                                 or ordinal, or a dictionary mapping cell-type
                                 names to names of integer or Enum columns
            cell_types: one or more cell types to test for DE; if `None`, test
                        all cell types. If specified and using dictionaries for
                        `formula`, `coefficient`, `contrasts`, or `group`, the
                        keys of these dictionaries must match the cell types
                        specified here. Mutually exclusive with
                        `excluded_cell_types`.
            excluded_cell_types: one or more cell types to exclude when testing
                                 for DE. If specified and using dictionaries
                                 for `formula`, `coefficient`, `contrasts`, or
                                 `group`, the keys of these dictionaries must
                                 also exclude these cell types. Mutually
                                 exclusive with `cell_types`.
            library_size_column: a floating-point column of `obs` containing
                                 each sample's library size. Can be a column
                                 name, a polars expression, a polars Series, a
                                 1D NumPy array, or a function that takes in
                                 this Pseudobulk dataset and a cell type and
                                 returns a polars Series or 1D NumPy array. Or,
                                 a dictionary mapping cell-type names to any of
                                 the above; each cell type in this Pseudobulk
                                 dataset must be present.
            robust: whether to specify `robust=True` in limma's `eBayes()`
                    function. You may wish to specify this if your dataset
                    contains outliers.
            return_voom_info: whether to include the voom weights and voom plot
                              data in the returned DE object; set to `False`
                              for reduced runtime if you do not need to use the
                              voom weights or generate voom plots
            allow_float: if `False`, raise an error if `X.dtype` is
                         floating-point (suggesting the user may not be using
                         the raw counts, e.g. due to accidentally having run
                         `log_CPM()` already); if `True`, disable this sanity
                         check
            verbose: whether to print out details of the DE estimation
            num_threads: the number of threads to use for DE estimation.
                         Multithreading is only supported for "free-threaded"
                         builds of Python 3.13 and later with the global
                         interpreter lock (GIL) disabled. Set `num_threads=-1`
                         to use all available cores, as determined by
                         `os.cpu_count()`, or leave unset to use
                         `self.num_threads` cores. Parallelization takes place
                         across cell types, so specifying more cores than the
                         number of cell types may not improve performance. Does
                         not affect the returned Pseudobulk dataset's
                         `num_threads`; this will always be the same as the
                         original dataset's `num_threads`.

        Returns:
            A DE object with a `table` attribute containing a polars DataFrame
            of the DE results, with columns:
            - cell_type: the cell type in which DE was tested
            - coefficient: the coefficient (or contrast) for which DE was
                           tested
            - gene: the gene for which DE was tested
            - logFC: the log2 fold change of the gene, i.e. its effect size
            - SE: the standard error of the effect size
            - LCI: the lower 95% confidence interval of the effect size
            - UCI: the upper 95% confidence interval of the effect size
            - AveExpr: the gene's average expression in this cell type, in log
                       CPM
            - p: the DE p-value
            - Bonferroni: the Bonferroni-corrected DE p-value
            - FDR: the FDR q-value for the DE
            If `return_voom_info=True`, the DE object also includes a
            `voom_weights` attribute containing a {cell_type: DataFrame}
            dictionary of voom weights, and a `voom_plot_data` attribute
            containing a {cell_type: DataFrame} dictionary of info necessary to
            construct a voom plot with `DE.plot_voom()`.
        """
        # Import required Python and R packages, and source voomByGroup code
        from ryp import r
        r('suppressPackageStartupMessages(library(limma))')
        r(self._voomByGroup_source_code)
        
        # Get the list of cell types to compute DE for
        cell_types, cell_type_description = \
            self._process_cell_types(cell_types, excluded_cell_types,
                                     return_description=True)
        
        # Check that `formula` is a string or a dictionary mapping cell types
        # to strings, and that each formula is a valid R formula
        check_type(formula, 'formula', (str, dict),
                   'a string or dictionary of strings')
        formula_is_dict = isinstance(formula, dict)
        if formula_is_dict:
            for key, value in formula.items():
                if not isinstance(key, str):
                    error_message = (
                        f'when formula is a dictionary, all its keys must be '
                        f'strings (cell types), but it contains a key of type '
                        f'{type(key).__name__!r}')
                    raise TypeError(error_message)
                check_type(value, f'formula[{key!r}]', str, 'a string')
                if not value.lstrip().startswith('~'):
                    error_message = \
                        f'formula[{key!r}] must start with a tilde (~)'
                    raise ValueError(error_message)
                try:
                    r(f'as.formula({value!r})')
                except RuntimeError as e:
                    error_message = \
                        f'formula[{key!r}] is not a valid R formula'
                    raise ValueError(error_message) from e
            if tuple(formula) != cell_types:
                error_message = (
                    f'formula is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
            formulas = formula
        else:
            if not formula.lstrip().startswith('~'):
                error_message = 'formula must start with a tilde (~)'
                raise ValueError(error_message)
            try:
                r(f'invisible(as.formula({formula!r}))')
            except RuntimeError as e:
                error_message = 'formula is not a valid R formula'
                raise ValueError(error_message) from e
        
        # Check that `coefficient` is one or more strings or integers, or a
        # dictionary mapping cell types to one or more strings or integers.
        # Convert it (or its values, if a dictionary) to tuples, storing the
        # result as a new variable, `coefficients`.
        coefficient_is_dict = isinstance(coefficient, dict)
        if coefficient_is_dict:
            coefficients = {}
            for cell_type, value in coefficient.items():
                if not isinstance(cell_type, str):
                    error_message = (
                        f'when coefficient is a dictionary, all its keys must '
                        f'be strings (cell types), but it contains a key of '
                        f'type {type(cell_type).__name__!r}')
                    raise TypeError(error_message)
                coefficients[cell_type] = to_tuple_checked(
                    value, f'coefficient[{cell_type!r}]', (str, int),
                    'strings or integers')
            if tuple(coefficients) != cell_types:
                error_message = (
                    f'coefficient is a dictionary, but does not have the same '
                    f'cell types (keys) as {cell_type_description}, or has '
                    f'the same cell types in a different order')
                raise ValueError(error_message)
        else:
            coefficients = to_tuple_checked(coefficient, 'coefficient',
                                            (str, int), 'strings or integers')
        
        # Check that `contrasts` is `None`, a dictionary mapping strings to
        # strings, or a dictionary mapping cell types to `None` or dictionaries
        # mapping strings to strings. Check that each contrast is a valid
        # R formula.
        contrasts_is_nested_dict = False
        if contrasts is not None:
            check_type(contrasts, 'contrasts', dict, 'a dictionary')
            if not isinstance(coefficient, (int, np.integer)) or \
                    coefficient != 1:
                error_message = \
                    'coefficient and contrasts cannot both be specified'
                raise ValueError(error_message)
            for key in contrasts:
                if not isinstance(key, str):
                    error_message = (
                        f'all keys of contrasts must be strings, but it '
                        f'contains a key of type {type(key).__name__!r}')
                    raise TypeError(error_message)
            if all(isinstance(value, str) for value in contrasts.values()):
                # `contrasts` is a dictionary mapping strings to strings
                for key, value in contrasts.items():
                    try:
                        r(f'as.formula({f"~{value}"!r})')
                    except RuntimeError as e:
                        error_message = \
                            f'contrasts[{key!r}] is not a valid R formula'
                        raise ValueError(error_message) from e
            elif all(isinstance(value, dict) or value is None
                     for value in contrasts.values()):
                # `contrasts` is a dictionary mapping cell types to `None` or
                # dictionaries mapping strings to strings
                contrasts_is_nested_dict = True
                if tuple(contrasts) != cell_types:
                    error_message = (
                        f'contrasts is a dictionary of dictionaries, but does '
                        f'not have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
                for key, value in contrasts.values():
                    if value is not None:
                        for inner_key, inner_value in value.items():
                            if not isinstance(inner_key, str):
                                error_message = (
                                    f'all keys of contrasts[{key!r}] must be '
                                    f'strings, but it contains a key of type '
                                    f'{type(inner_key).__name__!r}')
                                raise TypeError(error_message)
                            if not isinstance(inner_value, str):
                                error_message = (
                                    f'all values of contrasts[{key!r}] must '
                                    f'be strings, but it contains a value of '
                                    f'type {type(inner_value).__name__!r}')
                                raise TypeError(error_message)
                            try:
                                r(f'as.formula({f"~{inner_value}"!r})')
                            except RuntimeError as e:
                                error_message = (
                                    f'contrasts[{key!r}][{inner_key!r}] is '
                                    f'not a valid R formula')
                                raise ValueError(error_message) from e
                all_contrasts = contrasts
            else:
                error_message = (
                    'contrasts.values() must either be all strings or all '
                    'dictionaries/None')
                raise TypeError(error_message)
        
        # Check that `group` is `False`, `None`, a categorical column, or a
        # dictionary mapping cell types to `False`, `None`, or categorical
        # columns.
        if group is not None and group is not False:
            if group is True:
                error_message = \
                    'group must be None, False, or a column of obs, not True'
                raise TypeError(error_message)
            if isinstance(group, dict):
                for key, value in group.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when group is a dictionary, all its keys must '
                            f'be strings (cell types), but it contains a key '
                            f'of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None and value is not False:
                        if value is True:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, not True')
                            raise TypeError(error_message)
                        else:
                            error_message = (
                                f'group[{key!r}] must be None, False, or a '
                                f'column of obs, but has type '
                                f'{type(value).__name__!r}')
                            raise TypeError(error_message)
                # noinspection PyTypeChecker
                if tuple(group) != cell_types:
                    error_message = (
                        f'group is a dictionary, but does not have the same '
                        f'cell types (keys) as {cell_type_description}, or '
                        f'has the same cell types in a different order')
                    raise ValueError(error_message)
                groups = {}
                for cell_type, column in group.items():
                    obs = self._obs[cell_type]
                    if column is None:
                        groups[cell_type] = None
                    if isinstance(column, str):
                        if column not in obs:
                            error_message = (
                                f'group[{cell_type!r}] is {column!r}, which '
                                f'is not a column of obs[{cell_type!r}]')
                            raise ValueError(error_message)
                        groups[cell_type] = obs[column]
                    elif isinstance(column, pl.Expr):
                        groups[cell_type] = obs.select(column)
                        if groups[cell_type].width > 1:
                            # noinspection PyUnresolvedReferences
                            error_message = (
                                f'group[{cell_type!r}] is a polars expression '
                                f'that expands to {group[cell_type].width:,} '
                                f'columns rather than 1 for cell type ')
                            raise ValueError(error_message)
                        # noinspection PyUnresolvedReferences
                        groups[cell_type] = groups[cell_type].to_series()
                    elif isinstance(column, pl.Series):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a polars Series of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = column
                    elif isinstance(column, np.ndarray):
                        if len(column) != len(obs):
                            error_message = (
                                f'group[{cell_type!r}] is a NumPy array of '
                                f'length {len(column):,}, which differs from '
                                f'the length of obs[{cell_type!r}] '
                                f'({len(obs):,})')
                            raise ValueError(error_message)
                        groups[cell_type] = pl.Series('group', column)
                    else:
                        error_message = (
                            f'group[{cell_type!r}] must be None, False, a '
                            f'string column name, a polars expression or '
                            f'Series, or a 1D NumPy array, but has type '
                            f'{type(column).__name__!r}')
                        raise TypeError(error_message)
                    
                    # Check dtype
                    base_type = column.dtype.base_type()
                    if base_type not in (pl.String, pl.Categorical, pl.Enum,
                                         pl.Boolean) and \
                            base_type not in pl.INTEGER_DTYPES:
                        error_message = (
                            f'group[{cell_type!r}] must be String, '
                            f'Categorical, Enum, Boolean, or integer, but has '
                            f'data type {base_type!r}')
                        raise TypeError(error_message)
                    
                    # Check `null` values
                    null_count = column.null_count()
                    if null_count > 0:
                        error_message = (
                            f'group[{cell_type!r}] contains {null_count:,} '
                            f'{plural("null value", null_count)}, but must '
                            f'not contain any')
                        raise ValueError(error_message)
            else:
                # noinspection PyTypeChecker
                groups = self._get_column(
                    'obs', group, 'group',
                    (pl.String, pl.Categorical, pl.Enum, pl.Boolean,
                     'integer'))
                for cell_type, group in groups.items():
                    if group is not None and group is not False:
                        null_count = group.null_count()
                        if null_count > 0:
                            error_message = (
                                f'group contains {null_count:,} '
                                f'{plural("null value", null_count)} for cell '
                                f'type {cell_type!r}, but must not contain '
                                f'any. Specify the same group column via the '
                                f'group_column argument to Pseudobulk.qc(), '
                                f'explicitly remove nulls from your group '
                                f'column with fill_null(), or specify a '
                                f'different group column (or none at all, to '
                                f'group automatically).')
                            raise ValueError(error_message)
        else:
            groups = None
        
        # Check that `categorical_columns` is one or more strings or `None`, or
        # a dictionary mapping cell types to one or more strings or `None`.
        # Convert it (or its values, if a dictionary) to tuples.
        categorical_columns_is_dict = isinstance(categorical_columns, dict)
        if categorical_columns is not None:
            if categorical_columns_is_dict:
                all_categorical_columns = {}
                for key, value in categorical_columns.items():
                    if not isinstance(key, str):
                        error_message = (
                            f'when categorical_columns is a dictionary, all '
                            f'its keys must be strings (cell types), but it '
                            f'contains a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
                    if value is not None:
                        value_name = f'categorical_columns[{key!r}]'
                        value = \
                            to_tuple_checked(value, value_name, str, 'strings')
                        check_type(value, value_name, str, 'a string')
                    all_categorical_columns[key] = value
                if tuple(categorical_columns) != cell_types:
                    error_message = (
                        f'categorical_columns is a dictionary, but does not '
                        f'have the same cell types (keys) as '
                        f'{cell_type_description}, or has the same cell types '
                        f'in a different order')
                    raise ValueError(error_message)
            else:
                categorical_columns = to_tuple_checked(
                    categorical_columns, 'categorical_columns', str, 'strings')
        
        # Get the library size column
        library_sizes = self._get_column(
            'obs', library_size_column, 'library_size_column',
            'floating-point',
            custom_error='library_size_column {} is not a column of obs[{}]; '
                         'did you forget to run library_size()?',
            allow_None=False)
        
        # Check that Boolean arguments are Boolean
        check_type(robust, 'robust', bool, 'Boolean')
        check_type(return_voom_info, 'return_voom_info', bool, 'Boolean')
        check_type(allow_float, 'allow_float', bool, 'Boolean')
        check_type(verbose, 'verbose', bool, 'Boolean')
        
        # Check that `num_threads` is a positive integer, -1 or `None`; if
        # `None`, set to `self.num_threads` if free-threaded and 1 otherwise,
        # and if -1, set to `os.cpu_count()`. Raise an error if the user
        # specified multiple threads but lacks free-threaded Python.
        num_threads = self._process_num_threads(num_threads)
        
        # If `allow_float=False`, raise an error if `X` is floating-point for
        # any cell type
        if not allow_float:
            for cell_type in cell_types:
                dtype = self._X[cell_type].dtype
                if np.issubdtype(dtype, np.floating):
                    error_message = (
                        f"DE() requires raw counts but X[{cell_type!r}] "
                        f"has data type {str(dtype)!r}, a floating-point data "
                        f"type; if you are sure that all values are raw "
                        f"integer counts, i.e. that (X[{cell_type!r}].data == "
                        f"X[{cell_type!r}].data.astype(int)).all(), then set "
                        f"allow_float=True (or just cast X to an integer data "
                        f"type).")
                    raise TypeError(error_message)
        
        # Compute DE for each cell type
        if num_threads == 1:
            DE_results = {}
            if return_voom_info:
                voom_weights = {}
                voom_plot_data = {}
            for cell_type_index, cell_type in enumerate(cell_types):
                # noinspection PyUnboundLocalVariable
                results = self._DE(
                    cell_type=cell_type,
                    cell_type_index=cell_type_index,
                    formula=formulas[cell_type]
                            if formula_is_dict else formula,
                    coefficient=coefficients[cell_type]
                                if coefficient_is_dict else coefficients,
                    contrasts=all_contrasts[cell_type]
                              if contrasts_is_nested_dict else contrasts,
                    group=groups[cell_type] if groups is not None else None,
                    categorical_columns=all_categorical_columns[cell_type]
                                        if categorical_columns_is_dict else
                                        categorical_columns,
                    library_size=library_sizes[cell_type],
                    robust=robust,
                    return_voom_info=return_voom_info,
                    verbose=verbose)
                if return_voom_info:
                    # noinspection PyUnboundLocalVariable
                    DE_results[cell_type], voom_weights[cell_type], \
                        voom_plot_data[cell_type] = results
                else:
                    DE_results[cell_type] = results
        else:
            from concurrent.futures import ThreadPoolExecutor
            # noinspection PyUnboundLocalVariable
            kwargs_list = [{
                'cell_type': cell_type,
                'cell_type_index': cell_type_index,
                'formula': formulas[cell_type] if formula_is_dict else formula,
                'coefficient': coefficients[cell_type]
                               if coefficient_is_dict else coefficients,
                'contrasts': all_contrasts[cell_type]
                             if contrasts_is_nested_dict else contrasts,
                'group': groups[cell_type] if groups is not None else None,
                'categorical_columns': all_categorical_columns[cell_type]
                                       if categorical_columns_is_dict else
                                       categorical_columns,
                'library_size': library_sizes[cell_type],
                'robust': robust,
                'return_voom_info': return_voom_info,
                'verbose': verbose}
                for cell_type_index, cell_type in enumerate(cell_types)]
            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                results = tuple(executor.map(
                    lambda kwargs: self._DE(**kwargs), kwargs_list))
            if return_voom_info:
                # noinspection PyUnboundLocalVariable
                DE_results = {cell_type: result[0] for cell_type, result in
                              zip(cell_types, results)}
                voom_weights = {cell_type: result[1] for cell_type, result in
                                zip(cell_types, results)}
                voom_plot_data = {cell_type: result[2] for cell_type, result in
                                  zip(cell_types, results)}
            else:
                DE_results = dict(zip(cell_types, results))
        
        # Concatenate across cell types
        table = pl.concat([
            cell_type_DE_results
            .select(pl.lit(cell_type).alias('cell_type'), pl.all())
            for cell_type, cell_type_DE_results in DE_results.items()])
        if return_voom_info:
            return DE(table, voom_weights, voom_plot_data)
        else:
            return DE(table)


class DE:
    """
    Differential expression results returned by Pseudobulk.DE().
    """
    
    def __init__(self,
                 table: pl.DataFrame,
                 voom_weights: dict[str, pl.DataFrame] | None = None,
                 voom_plot_data: dict[str, pl.DataFrame] | None = None) -> \
            None:
        """
        Initialize the DE object.
        
        Args:
            table: a polars DataFrame containing the DE results, with columns:
                   - cell_type: the cell type in which DE was tested
                   - coefficient: the coefficient (or contrast) for which DE
                                  was tested
                   - gene: the gene for which DE was tested
                   - logFC: the log2 fold change of the gene, i.e. its effect
                            size
                   - SE: the standard error of the effect size
                   - LCI: the lower 95% confidence interval of the effect size
                   - UCI: the upper 95% confidence interval of the effect size
                   - AveExpr: the gene's average expression in this cell type,
                              in log CPM
                   - p: the DE p-value
                   - Bonferroni: the Bonferroni-corrected DE p-value
                   - FDR: the FDR q-value for the DE
                   Or, a directory containing a DE object saved with `save()`.
            voom_weights: an optional {cell_type: DataFrame} dictionary of voom
                         weights, where rows are genes and columns are samples.
                         The first column of each cell type's DataFrame,
                         'gene', contains the gene names.
            voom_plot_data: an optional {cell_type: DataFrame} dictionary of
                            info necessary to construct a voom plot with
                            `DE.plot_voom()`
        """
        if isinstance(table, pl.DataFrame):
            if voom_weights is not None:
                if voom_plot_data is None:
                    error_message = (
                        'voom_plot_data must be specified when voom_weights '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_weights, 'voom_weights', dict, 'a dictionary')
                if voom_weights.keys() != voom_plot_data.keys():
                    error_message = (
                        'voom_weights and voom_plot_data must have matching '
                        'cell types (keys)')
                    raise ValueError(error_message)
                for key in voom_weights:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of voom_weights and voom_plot_data '
                            f'must be strings (cell types), but they contain '
                            f'a key of type {type(key).__name__!r}')
                        raise TypeError(error_message)
            if voom_plot_data is not None:
                if voom_weights is None:
                    error_message = (
                        'voom_weights must be specified when voom_plot_data '
                        'is specified')
                    raise ValueError(error_message)
                check_type(voom_plot_data, 'voom_plot_data', dict,
                           'a dictionary')
        elif isinstance(table, (str, Path)):
            table = str(table)
            if not os.path.exists(table):
                error_message = f'DE object directory {table!r} does not exist'
                raise FileNotFoundError(error_message)
            cell_types = [line.rstrip('\n') for line in
                          open(f'{table}/cell_types.txt')]
            voom_weights = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_weights.parquet'))
                for cell_type in cell_types}
            voom_plot_data = {cell_type: pl.read_parquet(
                os.path.join(table, f'{cell_type.replace("/", "-")}.'
                                    f'voom_plot_data.parquet'))
                for cell_type in cell_types}
            table = pl.read_parquet(os.path.join(table, 'table.parquet'))
        else:
            error_message = (
                f'table must be a polars DataFrame or a directory (string or '
                f'pathlib.Path) containing a saved DE object, but has type '
                f'{type(table).__name__!r}')
            raise TypeError(error_message)
        self.table = table
        self.voom_weights = voom_weights
        self.voom_plot_data = voom_plot_data
    
    def __repr__(self) -> str:
        """
        Get a string representation of this DE object.
        
        Returns:
            A string summarizing the object.
        """
        num_cell_types = self.table['cell_type'].n_unique()
        descr = (
            f'DE object with {len(self.table):,} '
            f'{"entries" if len(self.table) != 1 else "entry"} across '
            f'{num_cell_types:,} {plural("cell type", num_cell_types)}:\n'
            f'{self.table}')
        return descr
    
    def __eq__(self, other: DE) -> bool:
        """
        Test for equality with another DE object.
        
        Args:
            other: the other DE object to test for equality with

        Returns:
            Whether the two DE objects are identical.
        """
        if not isinstance(other, DE):
            error_message = (
                f'the left-hand operand of `==` is a DE object, but '
                f'the right-hand operand has type {type(other).__name__!r}')
            raise TypeError(error_message)
        return self.table.equals(other.table) and \
            (other.voom_weights is None if self.voom_weights is None else
             self.voom_weights.keys() == other.voom_weights.keys() and
             all(self.voom_weights[cell_type].equals(
                     other.voom_weights[cell_type]) and
                 self.voom_plot_data[cell_type].equals(
                     other.voom_plot_data[cell_type])
                 for cell_type in self.voom_weights))
    
    @property
    def groups(self) -> dict[str, tuple[str, ...] | None] | None:
        """
        Get the groups used by voomByGroup for each cell type.
        
        Returns:
            A dictionary mapping cell type names to group names used by
            voomByGroup for that cell type, or None if voomByGroup was not used
            for that cell type. If `Pseudobulk.DE()` was called with
            `return_voom_info=False`, `groups` will be None instead of a
            dictionary.
        """
        return {cell_type: None if 'xy_x' in data.columns else
                           tuple(column[5:] for column in data.columns
                                 if column[:4] == 'xy_x')
                    for cell_type, data in self.voom_plot_data.items()} \
            if self.voom_plot_data is not None else None
    
    def save(self, directory: str | Path, overwrite: bool = False) -> None:
        # noinspection GrazieInspection
        """
        Save a DE object to `directory` (which must not exist unless
        `overwrite=True`, and will be created) with the table at
        `table.parquet`.
        
        If the DE object contains voom info (i.e. was created with
        `return_voom_info=True` in `Pseudobulk.DE()`, the default), also saves
        each cell type's voom weights and voom plot data to
        f'{cell_type}_voom_weights.parquet' and
        f'{cell_type}_voom_plot_data.parquet', as well as a text file,
        cell_types.txt, containing the cell types.
        
        Args:
            directory: the directory to save the DE object to
            overwrite: if `False`, raises an error if the directory exists; if
                       `True`, overwrites files inside it as necessary
        """
        check_type(directory, 'directory', (str, Path),
                   'a string or pathlib.Path')
        directory = str(directory)
        check_type(overwrite, 'overwrite', bool, 'Boolean')
        if not overwrite and os.path.exists(directory):
            error_message = (
                f'directory {directory!r} already exists; set overwrite=True '
                f'to overwrite')
            raise FileExistsError(error_message)
        os.makedirs(directory, exist_ok=overwrite)
        self.table.write_parquet(os.path.join(directory, 'table.parquet'))
        if self.voom_weights is not None:
            with open(os.path.join(directory, 'cell_types.txt'), 'w') as f:
                # noinspection PyTypeChecker
                print('\n'.join(self.voom_weights), file=f)
            for cell_type in self.voom_weights:
                escaped_cell_type = cell_type.replace('/', '-')
                self.voom_weights[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_weights.parquet'))
                self.voom_plot_data[cell_type].write_parquet(
                    os.path.join(directory, f'{escaped_cell_type}.'
                                            f'voom_plot_data.parquet'))
    
    def get_hits(self,
                 significance_column: str = 'FDR',
                 threshold: int | float | np.integer | np.floating = 0.05,
                 num_top_hits: int | np.integer | None = None) -> pl.DataFrame:
        """
        Get all (or the top) differentially expressed genes.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            num_top_hits: the number of top hits to report for each cell type;
                          if `None`, report all hits

        Returns:
            The `table` attribute of this DE object, subset to (top) DE hits.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        if num_top_hits is not None:
            check_type(num_top_hits, 'num_top_hits', int, 'a positive integer')
            check_bounds(num_top_hits, 'num_top_hits', 1)
        return self.table\
            .filter(pl.col(significance_column) < threshold)\
            .pipe(lambda df: df.group_by('cell_type', maintain_order=True)
                  .head(num_top_hits) if num_top_hits is not None else df)
    
    def get_num_hits(self,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer |
                                np.floating = 0.05) -> pl.DataFrame:
        """
        Get the number of differentially expressed genes in each cell type.
        
        Args:
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`

        Returns:
            A DataFrame with one row per cell type and two columns:
            'cell_type' and 'num_hits'.
        """
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        return self.table\
            .lazy()\
            .filter(pl.col(significance_column) < threshold)\
            .group_by('cell_type')\
            .agg(num_hits=pl.len())\
            .sort('cell_type')\
            .collect()
    
    # noinspection PyUnresolvedReferences
    def plot_voom(self,
                  cell_type: str,
                  filename: str | Path | None = None,
                  *,
                  ax: 'Axes' | None = None,
                  figure_kwargs: dict[str, Any] | None = None,
                  point_color: Color | dict[str, Color] | None = None,
                  point_size: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1,
                  line_color: Color | dict[str, Color] | None = None,
                  line_width: int | float | np.integer | np.floating |
                              dict[str, int | float | np.integer |
                                        np.floating] = 1.5,
                  scatter_kwargs: dict[str, Any] | None |
                                  dict[str, dict[str, Any] | None] = None,
                  plot_kwargs: dict[str, Any] | None |
                               dict[str, dict[str, Any] | None] = None,
                  legend: bool = True,
                  legend_kwargs: dict[str, Any] | None = None,
                  title: str | None = None,
                  title_kwargs: dict[str, Any] | None = None,
                  xlabel: str | None = 'Average log2(count + 0.5)',
                  xlabel_kwargs: dict[str, Any] | None = None,
                  ylabel: str | None = 'sqrt(standard deviation)',
                  ylabel_kwargs: dict[str, Any] | None = None,
                  despine: bool = True,
                  savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a voom plot for a cell type that differential expression was
        calculated for.
        
        Voom plots consist of a scatter plot with one point per gene. They
        visualize how the mean expression of each gene across samples (x)
        relates to the gene's variation in expression across samples (y). The
        plot also includes a LOESS (also called LOWESS) curve, a type of
        non-linear curve fit, of the mean-variance (x-y) trend.
        
        Specifically, the x position of a gene's point is the average, across
        samples, of the base-2 logarithm of the gene's count in each sample,
        plus a pseudocount of 0.5: in other words, mean(log2(count + 0.5)).
        The y position is the square root of the standard deviation, across
        samples, of the gene's log counts per million after regressing out,
        across samples, the differential expression design matrix.
        
        When running differential expression with voomByGroup, voom is run
        separately within each group, so the voom plot will show a separate
        LOESS trendline for each group, with the points and trendlines for each
        group shown in distinct colors.
        
        Many arguments to this function can be either a single value or a
        dictionary mapping group names to values. The group names can be viewed
        with `self.groups[cell_type]`.
        
        Args:
            cell_type: the cell type to generate the voom plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            point_color: the color of the points in the voom plot. Can be a
                         single color or a dictionary mapping each of the group
                         names in `self.groups[cell_type]` to colors. When not
                         using voomByGroup, defaults to `'#666666'` (gray).
                         When using voomByGroup with two groups, defaults to
                         `'#666666'` for the first group in
                         `self.groups[cell_type]` and `'#FF6666'` (red) for the
                         second. When using voomByGroup with more than two
                         groups, must be specified manually. Can be any valid
                         Matplotlib color, like a hex string (e.g.
                         `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            point_size: the size of the points in the voom plot. Can be a
                        single number or a dictionary mapping each of the group
                        names in `self.groups[cell_type]` to numbers.
            line_color: the color of the LOESS trendline. Can be a single color
                        or a dictionary mapping each of the group names in
                        `self.groups[cell_type]` to colors. When not using
                        voomByGroup, defaults to `'#000000'` (black). When
                        using voomByGroup with two groups, defaults to
                        `'#000000'` for the first group and `'#FF0000'` (red).
                        for the second. When using voomByGroup with more than
                        two groups, must be specified manually. Can be any
                        valid Matplotlib color, like a hex string (e.g.
                        `'#FF0000'`), a named color (e.g. 'red'), a 3- or
                         4-element RGB/RGBA tuple of integers 0-255 or floats
                         0-1, or a single float 0-1 for grayscale.
            line_width: the width of the LOESS trendline. Can be a single
                        number or a dictionary mapping each of the group names
                        in `self.groups[cell_type]` to numbers.
            scatter_kwargs: a dictionary (or dictionary mapping each of the
                            group names in `self.groups[cell_type]` to
                            dictionaries) of keyword arguments to be passed to
                            `ax.scatter()`, such as:
                            - `rasterized`: whether to convert the scatter plot
                              points to a raster (bitmap) image when saving to
                              a vector format like PDF. Defaults to `True`,
                              instead of Matplotlib's default of `False`.
                            - `marker`: the shape to use for plotting each cell
                            - `norm`, `vmin`, and `vmax`: control how the
                              numbers in `color_column` are converted to
                              colors, if `color_column` is numeric
                            - `alpha`: the transparency of each point
                            - `linewidths` and `edgecolors`: the width and
                              color of the borders around each marker. These
                              are absent by default (`linewidths=0`), unlike
                              Matplotlib's default. Both arguments can be
                              either single values or sequences.
                            - `zorder`: the order in which the cells are
                              plotted, with higher values appearing on top of
                              lower ones.
                            Specifying `s` or `c`/`color`/`norm`/`vmin`/`vmax`
                            will raise an error, since these arguments conflict
                            with the `point_size` and `point_color` arguments,
                            respectively.
            plot_kwargs: a dictionary (or dictionary mapping each of the group
                         names in `self.groups[cell_type]` to dictionaries) of
                         keyword arguments to be passed to `ax.plot()` when
                         plotting the trendlines, such as `linestyle='--'` for
                         dashed trendlines. Specifying `color`/`c` or
                         `linewidth` will raise an error, since these arguments
                         conflict with the `line_color` and `line_width`
                         arguments, respectively.
            legend: whether to add a legend with the colors for each group when
                    using voomByGroup. Only `legend=False` has an effect, and
                    it can only be specified when using voomByGroup. Without
                    groups, there will never be a legend, so specifying
                    `legend=False` would be redundant.
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border. `frameon` defaults to `False`, instead
                             of Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when using voomByGroup with
                           `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the voom plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        
        # Check that this DE object contains `voom_plot_data`, the data
        # necessary to generate the voom plot from (scatter-plot points and
        # LOESS trendlines)
        if self.voom_plot_data is None:
            error_message = (
                'this DE object does not contain the voom_plot_data '
                'attribute, which is necessary to generate voom plots; re-run '
                'Pseudobulk.DE() with return_voom_info=True to include this '
                'attribute')
            raise AttributeError(error_message)
        
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.voom_plot_data:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        
        # Get the voom plot data for this cell type
        voom_plot_data = self.voom_plot_data[cell_type]
        
        # Get the voomByGroup groups for this cell type (`None` if voomByGroup
        # was not used)
        groups = None if 'xy_x' in voom_plot_data.columns else \
            tuple(column[5:] for column in voom_plot_data.columns
                  if column[:4] == 'xy_x')
        
        # If `filename` was specified, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        
        # If `figure_kwargs` was specified, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        
        # Check that `point_color` and `line_color` are valid Matplotlib colors
        # or dictionaries thereof, and convert them to hex. Or, if `None`, set
        # to their default value if there are no groups or exactly two groups.
        point_color_is_dict = isinstance(point_color, dict)
        if point_color is None:
            if groups is None:
                point_color = '#666666'
            elif len(groups) == 2:
                point_color = {groups[0]: '#666666', groups[1]: '#FF6666'}
                point_color_is_dict = True
            else:
                error_message = (
                    f'point_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif point_color_is_dict:
            for group, group_point_color in point_color.items():
                if not plt.matplotlib.colors.is_color_like(group_point_color):
                    error_message = (
                        f'point_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            point_color = {
                group: plt.matplotlib.colors.to_hex(group_point_color)
                for group, group_point_color in point_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(point_color):
                error_message = (
                    f'point_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            point_color = plt.matplotlib.colors.to_hex(point_color)
        line_color_is_dict = isinstance(line_color, dict)
        if line_color is None:
            if groups is None:
                line_color = '#000000'
            elif len(groups) == 2:
                line_color = {groups[0]: '#000000', groups[1]: '#FF0000'}
                line_color_is_dict = True
            else:
                error_message = (
                    f'line_color must be specified manually when there are '
                    f'three or more groups; here, there are {len(groups)!r}')
                raise ValueError(error_message)
        elif line_color_is_dict:
            for group, group_line_color in line_color.items():
                if not plt.matplotlib.colors.is_color_like(group_line_color):
                    error_message = (
                        f'line_color[{group!r}] is not a valid Matplotlib '
                        f'color or sequence of valid colors')
                    raise ValueError(error_message)
            line_color = {
                group: plt.matplotlib.colors.to_hex(group_line_color)
                for group, group_line_color in line_color.items()}
        else:
            if not plt.matplotlib.colors.is_color_like(line_color):
                error_message = (
                    f'line_color is not a valid Matplotlib color or '
                    f'sequence of valid colors')
                raise ValueError(error_message)
            line_color = plt.matplotlib.colors.to_hex(line_color)
        
        # Check that `point_size` and `line_width` are positive numbers or
        # dicts thereof
        point_size_is_dict = isinstance(point_size, dict)
        line_width_is_dict = isinstance(line_width, dict)
        for number, number_name, is_dict in (
                (point_size, 'point_size', point_size_is_dict),
                (line_width, 'line_width', line_width_is_dict)):
            if is_dict:
                for group, group_number in number.items():
                    check_type(group_number, f'{number_name}[{group!r}]',
                               (int, float), 'a positive number')
                    check_bounds(group_number, f'{number_name}[{group!r}]', 0,
                                 left_open=True)
            else:
                check_type(number, number_name, (int, float),
                           'a positive number')
                check_bounds(number, number_name, 0, left_open=True)
        
        # For each of the kwargs arguments, if the argument was specified,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (scatter_kwargs, 'scatter_kwargs'),
                                    (plot_kwargs, 'plot_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (title_kwargs, 'title_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
        
        # If using voomByGroup, for each of `scatter_kwargs` and `plot_kwargs`,
        # if the kwarg was specified, check that either all keys are group
        # names and in the correct order, or that no keys are group names. If
        # all keys are group names, check that all values are either `None` or
        # dictionaries with all-string keys, and make note that the kwargs is a
        # nested dict.
        scatter_kwargs_is_nested_dict = False
        plot_kwargs_is_nested_dict = False
        if groups is not None:
            for kwargs, kwargs_name in ((scatter_kwargs, 'scatter_kwargs'),
                                        (plot_kwargs, 'plot_kwargs')):
                if kwargs is not None:
                    if tuple(kwargs) == groups:
                        # The kwargs's keys exactly match the group names
                        for key, value in kwargs.items():
                            if value is not None:
                                check_type(value, f'{kwargs_name}[{key!r}]',
                                           dict, 'a dictionary')
                                for inner_key in value:
                                    if not isinstance(inner_key, str):
                                        error_message = (
                                            f'all keys of '
                                            f'{kwargs_name}[{key!r}] must be '
                                            f'strings, but it contains a key '
                                            f'of type '
                                            f'{type(inner_key).__name__!r}')
                                        raise TypeError(error_message)
                        if kwargs is scatter_kwargs:
                            scatter_kwargs_is_nested_dict = True
                        else:
                            plot_kwargs_is_nested_dict = True
                    else:
                        # Check that none of the kwargs's keys are group names
                        for group in groups:
                            if group in kwargs:
                                if set(groups) == set(kwargs):
                                    error_message = (
                                        f'{kwargs_name}.keys() does have the '
                                        f'same groups as '
                                        f'self.groups[{cell_type!r}], but '
                                        f'they are in a different order')
                                    raise ValueError(error_message)
                                else:
                                    error_message = (
                                        f'some keys of {kwargs_name}.keys() '
                                        f'are groups in '
                                        f'self.groups[{cell_type!r}], but '
                                        f'others are not')
                                    raise ValueError(error_message)
        
        # Override the defaults for certain keys of `scatter_kwargs`
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        if scatter_kwargs_is_nested_dict:
            for key, value in scatter_kwargs.items():
                scatter_kwargs[key] = default_scatter_kwargs | value \
                    if value is not None else default_scatter_kwargs
        else:
            scatter_kwargs = default_scatter_kwargs | scatter_kwargs \
                if scatter_kwargs is not None else default_scatter_kwargs
        
        # Set `plot_kwargs` to `{}` if it is `None`, or set the `None` values
        # of `plot_kwargs` to `{}` if `plot_kwargs` is a nested dict
        if plot_kwargs is None:
            plot_kwargs = {}
        elif plot_kwargs_is_nested_dict:
            for key, value in plot_kwargs.items():
                if value is None:
                    plot_kwargs[key] = {}
        
        # Check that `scatter_kwargs` does not contain the `s` or
        # `c`/`color`/`norm`/`vmin`/`vmax` keys and that `plot_kwargs` does
        # not contain the `c`/`color`/`norm`/`vmin`/`vmax` or `linewidth` keys,
        # or that their non-`None` values do not contain these keys if a nested
        # dict
        for kwargs, kwargs_name, alternate_color, is_nested_dict in (
                (scatter_kwargs, 'scatter_kwargs', 'line_color',
                 scatter_kwargs_is_nested_dict),
                (plot_kwargs, 'plot_kwargs', 'point_color',
                 plot_kwargs_is_nested_dict)):
            bad_keys = (('linewidth', 'line_width')
                        if kwargs is plot_kwargs else ('s', 'point_size'),
                        ('c', alternate_color),
                        ('color', alternate_color),
                        ('norm', alternate_color),
                        ('vmin', alternate_color),
                        ('vmax', alternate_color))
            if is_nested_dict:
                for key, value in kwargs.items():
                    if value is not None:
                        for bad_key, alternate_argument in bad_keys:
                            if bad_key in value:
                                error_message = (
                                    f'{bad_key!r} cannot be specified as a '
                                    f'key in {kwargs_name}[{key}!r]; specify '
                                    f'the {alternate_argument} argument '
                                    f'instead')
                                raise ValueError(error_message)
            elif kwargs is not None:
                for bad_key, alternate_argument in bad_keys:
                    if bad_key in kwargs:
                        error_message = (
                            f'{bad_key!r} cannot be specified as a key in '
                            f'{kwargs_name}; specify the {alternate_argument} '
                            f'argument instead')
                        raise ValueError(error_message)
       
        # Check that `legend` is Boolean. If not using voomByGroup, check that
        # the user did not specify `legend=False`.
        check_type(legend, 'legend', bool, 'Boolean')
        if groups is None:
            if not legend:
                error_message = (
                    'legend=False cannot be specified when there are no '
                    'groups, since it would be redundant: without groups, '
                    'there will never be a legend')
                raise ValueError(error_message)
      
        # Override the defaults for certain values of `legend_kwargs`; check
        # that it is `None` when not using a legend
        default_legend_kwargs = dict(frameon=False)
        if legend_kwargs is not None:
            if groups is None:
                error_message = (
                    'legend_kwargs cannot be specified when there are no '
                    'groups, since there will not be a legend')
                raise ValueError(error_message)
            if not legend:
                error_message = \
                    'legend_kwargs cannot be specified when legend=False'
                raise ValueError(error_message)
            legend_kwargs = default_legend_kwargs | legend_kwargs
        else:
            legend_kwargs = default_legend_kwargs
      
        # If `title` was specified, check that it is a string
        if title is not None:
            check_type(title, 'title', str, 'a string')
       
        # Check that `title_kwargs` is `None` when `title` is `None`
        if title is None and title_kwargs is not None:
            error_message = 'title_kwargs cannot be specified when title=None'
            raise ValueError(error_message)
      
        # Check that `xlabel` is a string or `None`; if `None`, check that
        # `xlabel_kwargs` is `None` as well. Ditto for `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
     
        # Check that `despine` is Boolean
        check_type(despine, 'despine', bool, 'Boolean')
       
        # Override the defaults for certain values of `savefig_kwargs`
        default_savefig_kwargs = \
            dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                 transparent=filename is not None and
                             filename.endswith('.pdf'))
        savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
            if savefig_kwargs is not None else default_savefig_kwargs
       
        # If `ax` is `None`, create a new figure with
        # `constrained_layout=True`; otherwise, check that it is a Matplotlib
        # axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            if groups is not None:
                if legend:
                    legend_patches = []
                for group in groups:
                    # Get this group's point size, point color, line color,
                    # line width, plot kwargs, and scatter kwargs
                    group_point_size = point_size[group] \
                        if point_size_is_dict else point_size
                    group_point_color = point_color[group] \
                        if point_color_is_dict else point_color
                    group_line_color = line_color[group] \
                        if line_color_is_dict else line_color
                    group_line_width = line_width[group] \
                        if line_width_is_dict else line_width
                    group_scatter_kwargs = scatter_kwargs[group] \
                        if scatter_kwargs_is_nested_dict else scatter_kwargs
                    group_plot_kwargs = plot_kwargs[group] \
                        if plot_kwargs_is_nested_dict else plot_kwargs
                    
                    # Plot the scatter plot for this group
                    ax.scatter(voom_plot_data[f'xy_x_{group}'].drop_nulls(),
                               voom_plot_data[f'xy_y_{group}'].drop_nulls(),
                               s=group_point_size, c=group_point_color,
                               **group_scatter_kwargs)
                    
                    # Plot the LOESS trendline for this group
                    ax.plot(voom_plot_data[f'line_x_{group}'].drop_nulls(),
                            voom_plot_data[f'line_y_{group}'].drop_nulls(),
                            c=group_line_color, linewidth=group_line_width,
                            **group_plot_kwargs)
                    
                    # Create a rectangle for the legend for this group, where
                    # the border matches the color of the trendline and the
                    # fill matches the color of the scatter plot points
                    if legend:
                        # noinspection PyUnboundLocalVariable
                        # noinspection PyUnresolvedReferences
                        legend_patches.append(plt.matplotlib.patches.Patch(
                            facecolor=group_point_color,
                            edgecolor=group_line_color,
                            linewidth=group_line_width, label=group))
                
                # Add the legend
                if legend:
                    ax.legend(handles=legend_patches, **legend_kwargs)
            else:
                # Plot the scatter plot
                ax.scatter(voom_plot_data['xy_x'], voom_plot_data['xy_y'],
                           s=point_size, c=point_color, **scatter_kwargs)
                
                # Plot the LOESS trendline
                ax.plot(voom_plot_data['line_x'], voom_plot_data['line_y'],
                         c=line_color, linewidth=line_width, **plot_kwargs)
            
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not False:
                if title_kwargs is None:
                    title_kwargs = {}
                # noinspection PyCallingNonCallable
                ax.set_title(title[cell_type] if isinstance(title, dict)
                             else title if isinstance(title, str) else
                             title(cell_type) if isinstance(title, Callable)
                             else cell_type, **title_kwargs)
           
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
         
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise
    
    # noinspection PyUnresolvedReferences
    def plot_volcano(self,
                     cell_type: str,
                     filename: str | Path | None = None,
                     *,
                     ax: 'Axes' | None = None,
                     figure_kwargs: dict[str, Any] | None = None,
                     significance_column: str = 'FDR',
                     threshold: int | float | np.integer | np.floating = 0.05,
                     genes_to_label: int | np.integer | str |
                                     Iterable[str] = 10,
                     label_kwargs: dict[str, Any] | None = None,
                     upregulated_size: int | float | np.integer |
                                       np.floating = 6,
                     downregulated_size: int | float | np.integer |
                                         np.floating = 6,
                     non_significant_size: int | float | np.integer |
                                           np.floating = 4,
                     upregulated_color: Color = '#FC4E07',
                     downregulated_color: Color = '#00AFBB',
                     non_significant_color: Color = 'lightgray',
                     upregulated_scatter_kwargs: dict[str, Any] | None = None,
                     downregulated_scatter_kwargs: dict[str, Any] |
                                                   None = None,
                     non_significant_scatter_kwargs: dict[str, Any] |
                                                     None = None,
                     legend: bool = True,
                     legend_kwargs: dict[str, Any] | None = None,
                     title: str | None = None,
                     title_kwargs: dict[str, Any] | None = None,
                     xlabel: str | None = '$log_2(FC)$',
                     xlabel_kwargs: dict[str, Any] | None = None,
                     ylabel: str | None = '$-log_{10}(FDR)$',
                     ylabel_kwargs: dict[str, Any] | None = None,
                     despine: bool = True,
                     savefig_kwargs: dict[str, Any] | None = None) -> None:
        """
        Generate a volcano plot of the DE hits, with negative log FDRs (or
        another `significance_column`) on the y-axis plotted against log fold
        changes on the x-axis. Upregulated, downregulated and non-significant
        genes are plotted in three different colors.
        
        When labeling genes (`genes_to_label != 0`), requires the textalloc
        package (github.com/ckjellson/textalloc) to make sure the labels don't
        overlap. Install it with:
        
        pip install --no-deps --no-build-isolation textalloc
        
        Args:
            cell_type: the cell type to generate the volcano plot for
            filename: the file to save to. If `None`, generate the plot but do
                      not save it, which allows it to be shown interactively or
                      modified further before saving.
            ax: the Matplotlib axes to save the plot onto; if `None`, create a
                new figure with Matpotlib's constrained layout and plot onto it
            figure_kwargs: a dictionary of keyword arguments to be passed to
                           `plt.figure` when `ax` is `None`, such as:
                           - `figsize`: a two-element sequence of the width and
                             height of the figure in inches. Defaults to
                             Matplotlib's default of `[6.4, 4.8]`.
                           - `layout`: the layout mechanism used by Matplotlib
                             to avoid overlapping plot elements. Defaults to
                             `'constrained'`, instead of Matplotlib's default
                             of `None`.
            significance_column: the name of a numeric column of `self.table`
                                 to determine significance from
            threshold: the significance threshold corresponding to
                       `significance_column`
            genes_to_label: an integer number of top DE genes to label, a name
                            or sequence of names of genes to label, or `None`
                            to not add labels
            label_kwargs: a dictionary of keyword arguments to be passed to
                          `textalloc.allocate()` when adding gene labels to
                          control the text properties, such as:
                           - `textcolor`/`textsize`: the text color and size
                           - `x_scatter`/`y_scatter`: the x/y coordinates of
                             points in the scatter plot, to repel labels away
                             from. Defaults to all points in the plot.
                           - `min_distance`/`max_distance`: the minimum and
                             maximum distances from each point to its label, as
                             a proportion of the width of the x-axis. Defaults
                             to 0 and 0.02, instead of textalloc's defaults of
                             0.015 and 0.2
                           - `draw_lines`: whether to draw lines between each
                              label and its corresponding point. Defaults to
                              `False`, instead of textalloc's default of
                              `True`.
                          See github.com/ckjellson/textalloc#parameters for the
                          full list of possible arguments. Can only be
                          specified when `genes_to_label` is non-zero.
            upregulated_size: the size of each upregulated gene's point
            downregulated_size: the size of each downregulated gene's point
            non_significant_size: the size of each non-significant gene's point
            upregulated_color: the color of each upregulated gene's point. Can
                               be any valid Matplotlib color, like a hex string
                               (e.g. `'#FF0000'`), a named color (e.g. 'red'),
                               a 3- or 4-element RGB/RGBA tuple of integers
                               0-255 or floats 0-1, or a single float 0-1 for
                               grayscale.
            downregulated_color: the color of each downregulated gene's point.
                                 Can be any valid Matplotlib color, like a hex
                                 string (e.g. `'#FF0000'`), a named color (e.g.
                                 'red'), a 3- or 4-element RGB/RGBA tuple of
                                 integers 0-255 or floats 0-1, or a single
                                 float 0-1 for grayscale.
            non_significant_color: the color of each non-significant gene's
                                   point. Can be any valid Matplotlib color,
                                   like a hex string (e.g. `'#FF0000'`), a
                                   named color (e.g. 'red'), a 3- or 4-element
                                   RGB/RGBA tuple of integers 0-255 or floats
                                   0-1, or a single float 0-1 for grayscale.
            upregulated_scatter_kwargs: a dictionary of keyword arguments to be
                                        passed to `ax.scatter()` for
                                        upregulated genes, such as:
                                        - `rasterized`: whether to convert the
                                          scatter plot points to a raster
                                          (bitmap) image when saving to a
                                          vector format like PDF. Defaults to
                                          `True`, instead of Matplotlib's
                                          default of `False`.
                                        - `marker`: the shape to use for
                                          plotting each gene
                                        - `alpha`: the transparency of each
                                          point
                                        - `linewidths` and `edgecolors`: the
                                          width and color of the borders around
                                          each marker. These are absent by
                                          default (`linewidths=0`), unlike
                                          Matplotlib's default. Both arguments
                                          can be either single values or
                                          sequences.
                                        - `zorder`: the order in which the
                                          genes are plotted, with higher values
                                          appearing on top of lower ones.
                                        Specifying `s` or `c`/`color`/`norm`/
                                        `vmin`/`vmax` will raise an error,
                                        since these arguments conflict with the
                                        `upregulated_size` and
                                        `upregulated_color` arguments,
                                        respectively.
            downregulated_scatter_kwargs: a dictionary of keyword arguments to
                                          be passed to `ax.scatter()` for
                                          downregulated genes; see the
                                          documentation of the
                                          `upregulated_scatter_kwargs` argument
                                          for details
            non_significant_scatter_kwargs: a dictionary of keyword arguments
                                            to be passed to `ax.scatter()` for
                                            non-significant genes; see the
                                            documentation of the
                                            `upregulated_scatter_kwargs`
                                            argument for details
            legend: whether to add a legend showing the marker style for
                    upregulated, downregulated, and non-significant points
            legend_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.legend()` to modify the legend, such as:
                           - `loc`, `bbox_to_anchor`, and `bbox_transform` to
                             set its location.
                           - `prop`, `fontsize`, and `labelcolor` to set its
                             font properties
                           - `facecolor` and `framealpha` to set its background
                             color and transparency
                           - `frameon=True` or `edgecolor` to add or color
                             its border. `frameon` defaults to `False`, instead
                             of Matplotlib's default of `True`.
                           - `title` to add a legend title
                           Can only be specified when `legend=True`.
            title: the title of the plot, or `None` to not add a title
            title_kwargs: a dictionary of keyword arguments to be passed to
                          `ax.set_title()` to control text properties, such as
                          `color` and `size`. Can only be specified when
                          `title` is not `None`.
            xlabel: the x-axis label, or `None` to not label the x-axis
            xlabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_xlabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `xlabel` is not `None`.
            ylabel: the y-axis label, or `None` to not label the y-axis
            ylabel_kwargs: a dictionary of keyword arguments to be passed to
                           `ax.set_ylabel()` to control text properties, such
                           as `color` and `size`. Can only be specified when
                           `ylabel` is not `None`.
            despine: whether to remove the top and right spines (borders of the
                     plot area) from the volcano plot
            savefig_kwargs: a dictionary of keyword arguments to be passed to
                            `plt.savefig()`, such as:
                            - `dpi`: defaults to 300 instead of Matplotlib's
                              default of 150
                            - `bbox_inches`: the bounding box of the portion of
                              the figure to save; defaults to 'tight' (crop out
                              any blank borders) instead of Matplotlib's
                              default of `None` (save the entire figure)
                            - `pad_inches`: the number of inches of padding to
                              add on each of the four sides of the figure when
                              saving. Defaults to 'layout' (use the padding
                              from the constrained layout engine), instead of
                              Matplotlib's default of 0.1.
                            - `transparent`: whether to save with a transparent
                              background; defaults to `True` if saving to a PDF
                              (i.e. when `PNG=False`) and `False` if saving to
                              a PNG, instead of Matplotlib's default of always
                              being `False`.
                            Can only be specified when `filename` is specified.
        """
        import matplotlib.pyplot as plt
        # Check that `cell_type` is a cell type in this DE object
        check_type(cell_type, 'cell_type', str, 'a string')
        if cell_type not in self.table['cell_type']:
            error_message = \
                f'cell_type {cell_type!r} is not a cell type in this DE object'
            raise ValueError(error_message)
        
        # If `filename` was specified, check that it is a string or
        # `pathlib.Path` and that its base directory exists; if `filename` is
        # `None`, make sure `savefig_kwargs` is also `None`
        if filename is not None:
            check_type(filename, 'filename', (str, Path),
                       'a string or pathlib.Path')
            directory = os.path.dirname(filename)
            if directory and not os.path.isdir(directory):
                error_message = (
                    f'{filename} refers to a file in the directory '
                    f'{directory!r}, but this directory does not exist')
                raise NotADirectoryError(error_message)
            filename = str(filename)
        elif savefig_kwargs is not None:
            error_message = 'savefig_kwargs must be None when filename is None'
            raise ValueError(error_message)
        
        # If `figure_kwargs` was specified, check that `ax` is `None`
        if figure_kwargs is not None and ax is not None:
            error_message = (
                'figure_kwargs must be None when ax is not None, since a new '
                'figure does not need to be generated when plotting onto an '
                'existing axis')
            raise ValueError(error_message)
        
        # Check that `significance_column` is the name of a floating-point
        # column in `self.table`
        check_type(significance_column, 'significance_column', str, 'a string')
        if significance_column not in self.table:
            error_message = (
                f'significance_column ({significance_column!r}) is not a '
                f'column of self.table')
            raise ValueError(error_message)
        check_dtype(self.table[significance_column],
                    f'self.table[{significance_column!r}]', 'floating-point')
        
        # Check that `threshold` is greater than 0 and less than or equal to 1
        check_type(threshold, 'threshold', (int, float),
                   'a number > 0 and ≤ 1')
        check_bounds(threshold, 'threshold', 0, 1, left_open=True)
        
        # Subset `self.table` to the selected cell type, and log-transform the
        # significance column and the threshold
        table = self.table.filter(cell_type=cell_type)\
            .with_columns(-pl.col(significance_column).log10())
        threshold = -np.log10(threshold)
        
        # Check that `genes_to_label` is an integer, a sequence of strings, or
        # `None`. If an integer, take that many gene names.
        if isinstance(genes_to_label, (int, np.integer)):
            label = genes_to_label != 0
            if label:
                x_to_label = table['logFC'][:genes_to_label]
                y_to_label = table[significance_column][:genes_to_label]
                genes_to_label = table['gene'][:genes_to_label]
            elif label_kwargs is not None:
                error_message = (
                    'label_kwargs cannot be specified when genes_to_label=0, '
                    'since no genes are being labeled')
                raise ValueError(error_message)
        else:
            label = True
            if genes_to_label is not None:
                genes_to_label = \
                    to_tuple_checked(genes_to_label, 'genes_to_label', str,
                                     'strings')
                genes_to_label = pl.DataFrame({'gene': genes_to_label})\
                    .join(table.select('gene', 'logFC', significance_column),
                          how='left', on='gene')
                num_missing = genes_to_label['logFC'].null_count()
                if num_missing == len(genes_to_label):
                    error_message = (
                        "none of the specified genes were found in "
                        "table['gene']")
                    raise ValueError(error_message)
                elif num_missing > 0:
                    gene = genes_to_label\
                        .filter(pl.col.logFC.is_null())['gene'][0]
                    error_message = (
                        f"one of the specified genes, {gene!r}, was not found "
                        f"in table['gene']")
                    raise ValueError(error_message)
                x_to_label = genes_to_label['logFC']
                y_to_label = genes_to_label[significance_column]
                genes_to_label = genes_to_label['gene']
        if label:
            # noinspection PyUnresolvedReferences
            import textalloc
        
        # Check that `upregulated_size`, `downregulated_size`, and
        # `non_significant_size` are positive numbers
        for size, size_name in (upregulated_size, 'upregulated_size'), \
                (downregulated_size, 'downregulated_size'), \
                (non_significant_size, 'non_significant_size'):
            check_type(size, size_name, (int, float), 'a positive number')
            check_bounds(size, size_name, 0, left_open=True)
       
        # Check that `upregulated_color`, `downregulated_color`, and
        # `non_significant_color` are valid Matplotlib colors, and convert them
        # to hex
        if not plt.matplotlib.colors.is_color_like(upregulated_color):
            error_message = 'upregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        upregulated_color = plt.matplotlib.colors.to_hex(upregulated_color)
        if not plt.matplotlib.colors.is_color_like(downregulated_color):
            error_message = \
                'downregulated_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        downregulated_color = plt.matplotlib.colors.to_hex(downregulated_color)
        if not plt.matplotlib.colors.is_color_like(non_significant_color):
            error_message = \
                'non_significant_color is not a valid Matplotlib color'
            raise ValueError(error_message)
        non_significant_color = \
            plt.matplotlib.colors.to_hex(non_significant_color)
       
        # Check that the three `scatter_kwargs` arguments do not contain
        # the `s` or `c`/`color`/`cmap`/`norm`/`vmin`/`vmax` keys
        for kwargs, kwargs_prefix in (
                (upregulated_scatter_kwargs, 'upregulated'),
                (downregulated_scatter_kwargs, 'downregulated'),
                (non_significant_scatter_kwargs, 'non_significant')):
            if kwargs is None:
                continue
            if 's' in kwargs:
                error_message = (
                    f"'s' cannot be specified as a key in "
                    f"{kwargs_prefix}_scatter_kwargs; specify the "
                    f"{kwargs_prefix}_size argument instead")
                raise ValueError(error_message)
            for key in 'c', 'color', 'cmap', 'norm', 'vmin', 'vmax':
                if key in kwargs:
                    error_message = (
                        f'{key!r} cannot be specified as a key in '
                        f'scatter_kwargs; specify the {kwargs_prefix}_color '
                        f'argument instead')
                    raise ValueError(error_message)
       
        # Override the defaults for certain values of the three
        # `scatter_kwargs` arguments
        default_scatter_kwargs = dict(rasterized=True, linewidths=0)
        upregulated_scatter_kwargs = \
            default_scatter_kwargs | upregulated_scatter_kwargs \
            if upregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        downregulated_scatter_kwargs = \
            default_scatter_kwargs | downregulated_scatter_kwargs \
            if downregulated_scatter_kwargs is not None else \
                default_scatter_kwargs
        non_significant_scatter_kwargs = \
            default_scatter_kwargs | non_significant_scatter_kwargs \
            if non_significant_scatter_kwargs is not None else \
                default_scatter_kwargs
       
        # Check that `title` is a string or `None`; if `None`, check that
        # `title_kwargs` is `None` as well. Ditto for `xlabel` and `ylabel`.
        for arg, arg_name, arg_kwargs in (
                (title, 'title', title_kwargs),
                (xlabel, 'xlabel', xlabel_kwargs),
                (ylabel, 'ylabel', ylabel_kwargs)):
            if arg is not None:
                check_type(arg, arg_name, str, 'a string')
            elif arg_kwargs is not None:
                error_message = \
                    f'{arg_name}_kwargs must be None when {arg_name} is None'
                raise ValueError(error_message)
      
        # For each of the kwargs arguments, if the argument was specified,
        # check that it is a dictionary and that all its keys are strings.
        for kwargs, kwargs_name in ((figure_kwargs, 'figure_kwargs'),
                                    (label_kwargs, 'label_kwargs'),
                                    (upregulated_scatter_kwargs,
                                     'upregulated_scatter_kwargs'),
                                    (downregulated_scatter_kwargs,
                                     'downregulated_scatter_kwargs'),
                                    (non_significant_scatter_kwargs,
                                     'non_significant_scatter_kwargs'),
                                    (legend_kwargs, 'legend_kwargs'),
                                    (title_kwargs, 'title_kwargs'),
                                    (xlabel_kwargs, 'xlabel_kwargs'),
                                    (ylabel_kwargs, 'ylabel_kwargs'),
                                    (savefig_kwargs, 'savefig_kwargs')):
            if kwargs is not None:
                check_type(kwargs, kwargs_name, dict, 'a dictionary')
                for key in kwargs:
                    if not isinstance(key, str):
                        error_message = (
                            f'all keys of {kwargs_name} must be strings, but '
                            f'it contains a key of type '
                            f'{type(key).__name__!r}')
                        raise TypeError(error_message)
      
        # Check that `legend` and `despine` are Boolean
        check_type(legend, 'legend', bool, 'Boolean')
        check_type(despine, 'despine', bool, 'Boolean')
      
        # If `ax` is `None`, create a new figure; otherwise, check that it is a
        # Matplotlib axis
        make_new_figure = ax is None
        try:
            if make_new_figure:
                default_figure_kwargs = dict(layout='constrained')
                figure_kwargs = default_figure_kwargs | figure_kwargs \
                    if figure_kwargs is not None else default_figure_kwargs
                plt.figure(**figure_kwargs)
                ax = plt.gca()
            else:
                check_type(ax, 'ax', plt.Axes, 'a Matplotlib axis')
            
            # Make the volcano plot
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC > 0)
                       .to_numpy()
                       .T, s=upregulated_size, c=upregulated_color,
                       label='Upregulated', **upregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) >= threshold,
                               pl.col.logFC < 0)
                       .to_numpy()
                       .T, s=downregulated_size, c=downregulated_color,
                       label='Downregulated', **downregulated_scatter_kwargs)
            ax.scatter(*table
                       .select('logFC', significance_column)
                       .filter(pl.col(significance_column) < threshold)
                       .to_numpy()
                       .T, s=non_significant_size, c=non_significant_color,
                       label='Non-significant',
                       **non_significant_scatter_kwargs)
            ax.set_ylim(bottom=0)
            
            # Add labels, using textalloc to avoid overlap
            if label:
                # noinspection PyUnboundLocalVariable
                default_label_kwargs = dict(
                    ax=ax, x=x_to_label, y=y_to_label,
                    text_list=genes_to_label,
                    x_scatter=table['logFC'].to_numpy(),
                    y_scatter=table[significance_column].to_numpy(),
                    min_distance=0, max_distance=0.02, draw_lines=False)
                label_kwargs = default_label_kwargs | label_kwargs \
                    if label_kwargs is not None else default_label_kwargs
                textalloc.allocate(**label_kwargs)
           
            # Add the legend; override the defaults for certain values of
            # `legend_kwargs`
            if legend:
                default_legend_kwargs = dict(frameon=False)
                legend_kwargs = default_legend_kwargs | legend_kwargs \
                    if legend_kwargs is not None else default_legend_kwargs
                ax.legend(**legend_kwargs)
          
            # Add the title and axis labels
            if xlabel is not None:
                if xlabel_kwargs is None:
                    xlabel_kwargs = {}
                ax.set_xlabel(xlabel, **xlabel_kwargs)
            if ylabel is not None:
                if ylabel_kwargs is None:
                    ylabel_kwargs = {}
                ax.set_ylabel(ylabel, **ylabel_kwargs)
            if title is not None:
                if title_kwargs is None:
                    title_kwargs = {}
                ax.set_title(title, **title_kwargs)
           
            # Despine, if specified
            if despine:
                spines = ax.spines
                spines['top'].set_visible(False)
                spines['right'].set_visible(False)
          
            # Save; override the defaults for certain keys of `savefig_kwargs`
            if filename is not None:
                default_savefig_kwargs = \
                    dict(dpi=300, bbox_inches='tight', pad_inches='layout',
                         transparent=filename is not None and
                                     filename.endswith('.pdf'))
                savefig_kwargs = default_savefig_kwargs | savefig_kwargs \
                    if savefig_kwargs is not None else default_savefig_kwargs
                plt.savefig(filename, **savefig_kwargs)
                if make_new_figure:
                    plt.close()
        except:
            # If we made a new figure, make sure to close it if there's an
            # exception (but not if there was no error and `filename` is
            # `None`, in case the user wants to modify it further before
            # saving)
            if make_new_figure:
                plt.close()
            raise


def concat_obs(datasets: SingleCell | Iterable[SingleCell] |
                         Pseudobulk | Iterable[Pseudobulk],
               *more_datasets: SingleCell | Pseudobulk,
               flexible: bool = False) -> SingleCell | Pseudobulk:
    """
    Concatenate multiple SingleCell datasets cell-wise, or multiple Pseudobulk
    datasets sample-wise.
    
    Delegates to `SingleCell.concat_obs()` or `Pseudobulk.concat_obs()`,
    depending on whether the datasets are SingleCell or Pseudobulk. For
    Pseudobulk datasets, all datasets must have the same cell types.
    
    Args:
        datasets: one or more SingleCell or Pseudobulk datasets to concatenate
        *more_datasets: additional Pseudobulk datasets to concatenate with this
                        one, specified as positional arguments
        flexible: whether to subset to genes, columns of `obs` and `var`, and
                  (for SingleCell datasets) keys of `obsm`, `varm` and `uns`
                  common to all datasets before concatenating, rather than
                  raising an error on any mismatches

    Returns:
        The concatenated SingleCell or Pseudobulk dataset.
    """
    datasets = to_tuple(datasets) + more_datasets
    check_type(datasets[0], 'the first dataset', (SingleCell, Pseudobulk),
               'a SingleCell or Pseudobulk dataset')
    return datasets[0].concat_obs(datasets[1:], flexible=flexible)


def concat_var(datasets: SingleCell | Iterable[SingleCell] |
                         Pseudobulk | Iterable[Pseudobulk],
               *more_datasets: SingleCell | Pseudobulk,
               flexible: bool = False) -> SingleCell | Pseudobulk:
    """
    Concatenate multiple SingleCell datasets or multiple Pseudobulk datasets,
    gene-wise. This is much less common than the cell- or sample-wise
    concatenation provided by `concat_obs()`.
    
    Delegates to `SingleCell.concat_var()` or `Pseudobulk.concat_var()`,
    depending on whether the datasets are SingleCell or Pseudobulk. For
    Pseudobulk datasets, all datasets must have the same cell types.
    
    Args:
        datasets: one or more SingleCell or Pseudobulk datasets to concatenate
        *more_datasets: additional Pseudobulk datasets to concatenate with this
                        one, specified as positional arguments
        flexible: whether to subset to cells/samples, columns of `obs` and
                  `var`, and (for SingleCell datasets) keys of `obsm`, `varm`
                  and `uns` common to all datasets before concatenating, rather
                  than raising an error on any mismatches

    Returns:
        The concatenated SingleCell or Pseudobulk dataset.
    """
    datasets = to_tuple(datasets) + more_datasets
    check_type(datasets[0], 'the first dataset', (SingleCell, Pseudobulk),
               'a SingleCell or Pseudobulk dataset')
    return datasets[0].concat_var(datasets[1:], flexible=flexible)
